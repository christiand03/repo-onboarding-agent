{
  "basic_info": {
    "projekt_uebersicht": {
      "titel": "repo-onboarding-agent documentation",
      "beschreibung": "```",
      "aktueller_status": "Information not found",
      "key_features": "Information not found",
      "tech_stack": "Information not found"
    },
    "installation": {
      "dependencies": "- a\u0000l\u0000t\u0000a\u0000i\u0000r\u0000=\u0000=\u00004\u0000.\u00002\u0000.\u00002\u0000\n- \u0000\n- \u0000a\u0000n\u0000n\u0000o\u0000t\u0000a\u0000t\u0000e\u0000d\u0000-\u0000t\u0000y\u0000p\u0000e\u0000s\u0000=\u0000=\u00000\u0000.\u00007\u0000.\u00000\u0000\n- \u0000\n- \u0000a\u0000n\u0000y\u0000i\u0000o\u0000=\u0000=\u00004\u0000.\u00001\u00001\u0000.\u00000\u0000\n- \u0000\n- \u0000a\u0000t\u0000t\u0000r\u0000s\u0000=\u0000=\u00002\u00005\u0000.\u00004\u0000.\u00000\u0000\n- \u0000\n- \u0000b\u0000c\u0000r\u0000y\u0000p\u0000t\u0000=\u0000=\u00005\u0000.\u00000\u0000.\u00000\u0000\n- \u0000\n- \u0000b\u0000l\u0000i\u0000n\u0000k\u0000e\u0000r\u0000=\u0000=\u00001\u0000.\u00009\u0000.\u00000\u0000\n- \u0000\n- \u0000c\u0000a\u0000c\u0000h\u0000e\u0000t\u0000o\u0000o\u0000l\u0000s\u0000=\u0000=\u00006\u0000.\u00002\u0000.\u00002\u0000\n- \u0000\n- \u0000c\u0000a\u0000p\u0000t\u0000c\u0000h\u0000a\u0000=\u0000=\u00000\u0000.\u00007\u0000.\u00001\u0000\n- \u0000\n- \u0000c\u0000e\u0000r\u0000t\u0000i\u0000f\u0000i\u0000=\u0000=\u00002\u00000\u00002\u00005\u0000.\u00001\u00001\u0000.\u00001\u00002\u0000\n- \u0000\n- \u0000c\u0000f\u0000f\u0000i\u0000=\u0000=\u00002\u0000.\u00000\u0000.\u00000\u0000\n- \u0000\n- \u0000c\u0000h\u0000a\u0000r\u0000s\u0000e\u0000t\u0000-\u0000n\u0000o\u0000r\u0000m\u0000a\u0000l\u0000i\u0000z\u0000e\u0000r\u0000=\u0000=\u00003\u0000.\u00004\u0000.\u00004\u0000\n- \u0000\n- \u0000c\u0000l\u0000i\u0000c\u0000k\u0000=\u0000=\u00008\u0000.\u00003\u0000.\u00001\u0000\n- \u0000\n- \u0000c\u0000o\u0000l\u0000o\u0000r\u0000a\u0000m\u0000a\u0000=\u0000=\u00000\u0000.\u00004\u0000.\u00006\u0000\n- \u0000\n- \u0000c\u0000r\u0000y\u0000p\u0000t\u0000o\u0000g\u0000r\u0000a\u0000p\u0000h\u0000y\u0000=\u0000=\u00004\u00006\u0000.\u00000\u0000.\u00003\u0000\n- \u0000\n- \u0000d\u0000n\u0000s\u0000p\u0000y\u0000t\u0000h\u0000o\u0000n\u0000=\u0000=\u00002\u0000.\u00008\u0000.\u00000\u0000\n- \u0000\n- \u0000d\u0000o\u0000t\u0000e\u0000n\u0000v\u0000=\u0000=\u00000\u0000.\u00009\u0000.\u00009\u0000\n- \u0000\n- \u0000e\u0000n\u0000t\u0000r\u0000y\u0000p\u0000o\u0000i\u0000n\u0000t\u0000s\u0000=\u0000=\u00000\u0000.\u00004\u0000\n- \u0000\n- \u0000e\u0000x\u0000t\u0000r\u0000a\u0000-\u0000s\u0000t\u0000r\u0000e\u0000a\u0000m\u0000l\u0000i\u0000t\u0000-\u0000c\u0000o\u0000m\u0000p\u0000o\u0000n\u0000e\u0000n\u0000t\u0000s\u0000=\u0000=\u00000\u0000.\u00001\u0000.\u00008\u00001\u0000\n- \u0000\n- \u0000f\u0000i\u0000l\u0000e\u0000t\u0000y\u0000p\u0000e\u0000=\u0000=\u00001\u0000.\u00002\u0000.\u00000\u0000\n- \u0000\n- \u0000g\u0000i\u0000t\u0000d\u0000b\u0000=\u0000=\u00004\u0000.\u00000\u0000.\u00001\u00002\u0000\n- \u0000\n- \u0000G\u0000i\u0000t\u0000P\u0000y\u0000t\u0000h\u0000o\u0000n\u0000=\u0000=\u00003\u0000.\u00001\u0000.\u00004\u00005\u0000\n- \u0000\n- \u0000g\u0000o\u0000o\u0000g\u0000l\u0000e\u0000-\u0000a\u0000i\u0000-\u0000g\u0000e\u0000n\u0000e\u0000r\u0000a\u0000t\u0000i\u0000v\u0000e\u0000l\u0000a\u0000n\u0000g\u0000u\u0000a\u0000g\u0000e\u0000=\u0000=\u00000\u0000.\u00009\u0000.\u00000\u0000\n- \u0000\n- \u0000g\u0000o\u0000o\u0000g\u0000l\u0000e\u0000-\u0000a\u0000p\u0000i\u0000-\u0000c\u0000o\u0000r\u0000e\u0000=\u0000=\u00002\u0000.\u00002\u00008\u0000.\u00001\u0000\n- \u0000\n- \u0000g\u0000o\u0000o\u0000g\u0000l\u0000e\u0000-\u0000a\u0000u\u0000t\u0000h\u0000=\u0000=\u00002\u0000.\u00004\u00003\u0000.\u00000\u0000\n- \u0000\n- \u0000g\u0000o\u0000o\u0000g\u0000l\u0000e\u0000a\u0000p\u0000i\u0000s\u0000-\u0000c\u0000o\u0000m\u0000m\u0000o\u0000n\u0000-\u0000p\u0000r\u0000o\u0000t\u0000o\u0000s\u0000=\u0000=\u00001\u0000.\u00007\u00002\u0000.\u00000\u0000\n- \u0000\n- \u0000g\u0000r\u0000p\u0000c\u0000i\u0000o\u0000=\u0000=\u00001\u0000.\u00007\u00006\u0000.\u00000\u0000\n- \u0000\n- \u0000g\u0000r\u0000p\u0000c\u0000i\u0000o\u0000-\u0000s\u0000t\u0000a\u0000t\u0000u\u0000s\u0000=\u0000=\u00001\u0000.\u00007\u00006\u0000.\u00000\u0000\n- \u0000\n- \u0000h\u00001\u00001\u0000=\u0000=\u00000\u0000.\u00001\u00006\u0000.\u00000\u0000\n- \u0000\n- \u0000h\u0000t\u0000t\u0000p\u0000c\u0000o\u0000r\u0000e\u0000=\u0000=\u00001\u0000.\u00000\u0000.\u00009\u0000\n- \u0000\n- \u0000h\u0000t\u0000t\u0000p\u0000x\u0000=\u0000=\u00000\u0000.\u00002\u00008\u0000.\u00001\u0000\n- \u0000\n- \u0000i\u0000d\u0000n\u0000a\u0000=\u0000=\u00003\u0000.\u00001\u00001\u0000\n- \u0000\n- \u0000J\u0000i\u0000n\u0000j\u0000a\u00002\u0000=\u0000=\u00003\u0000.\u00001\u0000.\u00006\u0000\n- \u0000\n- \u0000j\u0000s\u0000o\u0000n\u0000p\u0000a\u0000t\u0000c\u0000h\u0000=\u0000=\u00001\u0000.\u00003\u00003\u0000\n- \u0000\n- \u0000j\u0000s\u0000o\u0000n\u0000p\u0000o\u0000i\u0000n\u0000t\u0000e\u0000r\u0000=\u0000=\u00003\u0000.\u00000\u0000.\u00000\u0000\n- \u0000\n- \u0000j\u0000s\u0000o\u0000n\u0000s\u0000c\u0000h\u0000e\u0000m\u0000a\u0000=\u0000=\u00004\u0000.\u00002\u00005\u0000.\u00001\u0000\n- \u0000\n- \u0000j\u0000s\u0000o\u0000n\u0000s\u0000c\u0000h\u0000e\u0000m\u0000a\u0000-\u0000s\u0000p\u0000e\u0000c\u0000i\u0000f\u0000i\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000s\u0000=\u0000=\u00002\u00000\u00002\u00005\u0000.\u00009\u0000.\u00001\u0000\n- \u0000\n- \u0000l\u0000a\u0000n\u0000g\u0000c\u0000h\u0000a\u0000i\u0000n\u0000=\u0000=\u00001\u0000.\u00000\u0000.\u00008\u0000\n- \u0000\n- \u0000l\u0000a\u0000n\u0000g\u0000c\u0000h\u0000a\u0000i\u0000n\u0000-\u0000c\u0000o\u0000r\u0000e\u0000=\u0000=\u00001\u0000.\u00001\u0000.\u00000\u0000\n- \u0000\n- \u0000l\u0000a\u0000n\u0000g\u0000c\u0000h\u0000a\u0000i\u0000n\u0000-\u0000g\u0000o\u0000o\u0000g\u0000l\u0000e\u0000-\u0000g\u0000e\u0000n\u0000a\u0000i\u0000=\u0000=\u00003\u0000.\u00001\u0000.\u00000\u0000\n- \u0000\n- \u0000l\u0000a\u0000n\u0000g\u0000c\u0000h\u0000a\u0000i\u0000n\u0000-\u0000o\u0000l\u0000l\u0000a\u0000m\u0000a\u0000=\u0000=\u00001\u0000.\u00000\u0000.\u00000\u0000\n- \u0000\n- \u0000l\u0000a\u0000n\u0000g\u0000g\u0000r\u0000a\u0000p\u0000h\u0000=\u0000=\u00001\u0000.\u00000\u0000.\u00003\u0000\n- \u0000\n- \u0000l\u0000a\u0000n\u0000g\u0000g\u0000r\u0000a\u0000p\u0000h\u0000-\u0000c\u0000h\u0000e\u0000c\u0000k\u0000p\u0000o\u0000i\u0000n\u0000t\u0000=\u0000=\u00003\u0000.\u00000\u0000.\u00001\u0000\n- \u0000\n- \u0000l\u0000a\u0000n\u0000g\u0000g\u0000r\u0000a\u0000p\u0000h\u0000-\u0000p\u0000r\u0000e\u0000b\u0000u\u0000i\u0000l\u0000t\u0000=\u0000=\u00001\u0000.\u00000\u0000.\u00005\u0000\n- \u0000\n- \u0000l\u0000a\u0000n\u0000g\u0000g\u0000r\u0000a\u0000p\u0000h\u0000-\u0000s\u0000d\u0000k\u0000=\u0000=\u00000\u0000.\u00002\u0000.\u00009\u0000\n- \u0000\n- \u0000l\u0000a\u0000n\u0000g\u0000s\u0000m\u0000i\u0000t\u0000h\u0000=\u0000=\u00000\u0000.\u00004\u0000.\u00004\u00006\u0000\n- \u0000\n- \u0000M\u0000a\u0000r\u0000k\u0000u\u0000p\u0000S\u0000a\u0000f\u0000e\u0000=\u0000=\u00003\u0000.\u00000\u0000.\u00003\u0000\n- \u0000\n- \u0000n\u0000a\u0000r\u0000w\u0000h\u0000a\u0000l\u0000s\u0000=\u0000=\u00002\u0000.\u00001\u00002\u0000.\u00000\u0000\n- \u0000\n- \u0000n\u0000e\u0000t\u0000w\u0000o\u0000r\u0000k\u0000x\u0000=\u0000=\u00003\u0000.\u00006\u0000\n- \u0000\n- \u0000n\u0000u\u0000m\u0000p\u0000y\u0000=\u0000=\u00002\u0000.\u00003\u0000.\u00005\u0000\n- \u0000\n- \u0000o\u0000l\u0000l\u0000a\u0000m\u0000a\u0000=\u0000=\u00000\u0000.\u00006\u0000.\u00001\u0000\n- \u0000\n- \u0000o\u0000r\u0000j\u0000s\u0000o\u0000n\u0000=\u0000=\u00003\u0000.\u00001\u00001\u0000.\u00004\u0000\n- \u0000\n- \u0000o\u0000r\u0000m\u0000s\u0000g\u0000p\u0000a\u0000c\u0000k\u0000=\u0000=\u00001\u0000.\u00001\u00002\u0000.\u00000\u0000\n- \u0000\n- \u0000p\u0000a\u0000c\u0000k\u0000a\u0000g\u0000i\u0000n\u0000g\u0000=\u0000=\u00002\u00005\u0000.\u00000\u0000\n- \u0000\n- \u0000p\u0000a\u0000n\u0000d\u0000a\u0000s\u0000=\u0000=\u00002\u0000.\u00003\u0000.\u00003\u0000\n- \u0000\n- \u0000p\u0000i\u0000l\u0000l\u0000o\u0000w\u0000=\u0000=\u00001\u00002\u0000.\u00000\u0000.\u00000\u0000\n- \u0000\n- \u0000p\u0000r\u0000o\u0000t\u0000o\u0000-\u0000p\u0000l\u0000u\u0000s\u0000=\u0000=\u00001\u0000.\u00002\u00006\u0000.\u00001\u0000\n- \u0000\n- \u0000p\u0000r\u0000o\u0000t\u0000o\u0000b\u0000u\u0000f\u0000=\u0000=\u00006\u0000.\u00003\u00003\u0000.\u00001\u0000\n- \u0000\n- \u0000p\u0000y\u0000a\u0000r\u0000r\u0000o\u0000w\u0000=\u0000=\u00002\u00001\u0000.\u00000\u0000.\u00000\u0000\n- \u0000\n- \u0000p\u0000y\u0000a\u0000s\u0000n\u00001\u0000=\u0000=\u00000\u0000.\u00006\u0000.\u00001\u0000\n- \u0000\n- \u0000p\u0000y\u0000a\u0000s\u0000n\u00001\u0000_\u0000m\u0000o\u0000d\u0000u\u0000l\u0000e\u0000s\u0000=\u0000=\u00000\u0000.\u00004\u0000.\u00002\u0000\n- \u0000\n- \u0000p\u0000y\u0000c\u0000p\u0000a\u0000r\u0000s\u0000e\u0000r\u0000=\u0000=\u00002\u0000.\u00002\u00003\u0000\n- \u0000\n- \u0000p\u0000y\u0000d\u0000a\u0000n\u0000t\u0000i\u0000c\u0000=\u0000=\u00002\u0000.\u00001\u00002\u0000.\u00004\u0000\n- \u0000\n- \u0000p\u0000y\u0000d\u0000a\u0000n\u0000t\u0000i\u0000c\u0000_\u0000c\u0000o\u0000r\u0000e\u0000=\u0000=\u00002\u0000.\u00004\u00001\u0000.\u00005\u0000\n- \u0000\n- \u0000p\u0000y\u0000d\u0000e\u0000c\u0000k\u0000=\u0000=\u00000\u0000.\u00009\u0000.\u00001\u0000\n- \u0000\n- \u0000P\u0000y\u0000J\u0000W\u0000T\u0000=\u0000=\u00002\u0000.\u00001\u00000\u0000.\u00001\u0000\n- \u0000\n- \u0000p\u0000y\u0000m\u0000o\u0000n\u0000g\u0000o\u0000=\u0000=\u00004\u0000.\u00001\u00005\u0000.\u00004\u0000\n- \u0000\n- \u0000p\u0000y\u0000t\u0000h\u0000o\u0000n\u0000-\u0000d\u0000a\u0000t\u0000e\u0000u\u0000t\u0000i\u0000l\u0000=\u0000=\u00002\u0000.\u00009\u0000.\u00000\u0000.\u0000p\u0000o\u0000s\u0000t\u00000\u0000\n- \u0000\n- \u0000p\u0000y\u0000t\u0000h\u0000o\u0000n\u0000-\u0000d\u0000o\u0000t\u0000e\u0000n\u0000v\u0000=\u0000=\u00001\u0000.\u00002\u0000.\u00001\u0000\n- \u0000\n- \u0000p\u0000y\u0000t\u0000z\u0000=\u0000=\u00002\u00000\u00002\u00005\u0000.\u00002\u0000\n- \u0000\n- \u0000P\u0000y\u0000Y\u0000A\u0000M\u0000L\u0000=\u0000=\u00006\u0000.\u00000\u0000.\u00003\u0000\n- \u0000\n- \u0000r\u0000e\u0000f\u0000e\u0000r\u0000e\u0000n\u0000c\u0000i\u0000n\u0000g\u0000=\u0000=\u00000\u0000.\u00003\u00007\u0000.\u00000\u0000\n- \u0000\n- \u0000r\u0000e\u0000q\u0000u\u0000e\u0000s\u0000t\u0000s\u0000=\u0000=\u00002\u0000.\u00003\u00002\u0000.\u00005\u0000\n- \u0000\n- \u0000r\u0000e\u0000q\u0000u\u0000e\u0000s\u0000t\u0000s\u0000-\u0000t\u0000o\u0000o\u0000l\u0000b\u0000e\u0000l\u0000t\u0000=\u0000=\u00001\u0000.\u00000\u0000.\u00000\u0000\n- \u0000\n- \u0000r\u0000p\u0000d\u0000s\u0000-\u0000p\u0000y\u0000=\u0000=\u00000\u0000.\u00002\u00009\u0000.\u00000\u0000\n- \u0000\n- \u0000r\u0000s\u0000a\u0000=\u0000=\u00004\u0000.\u00009\u0000.\u00001\u0000\n- \u0000\n- \u0000s\u0000i\u0000x\u0000=\u0000=\u00001\u0000.\u00001\u00007\u0000.\u00000\u0000\n- \u0000\n- \u0000s\u0000m\u0000m\u0000a\u0000p\u0000=\u0000=\u00005\u0000.\u00000\u0000.\u00002\u0000\n- \u0000\n- \u0000s\u0000n\u0000i\u0000f\u0000f\u0000i\u0000o\u0000=\u0000=\u00001\u0000.\u00003\u0000.\u00001\u0000\n- \u0000\n- \u0000s\u0000t\u0000r\u0000e\u0000a\u0000m\u0000l\u0000i\u0000t\u0000=\u0000=\u00001\u0000.\u00005\u00001\u0000.\u00000\u0000\n- \u0000\n- \u0000s\u0000t\u0000r\u0000e\u0000a\u0000m\u0000l\u0000i\u0000t\u0000-\u0000a\u0000u\u0000t\u0000h\u0000e\u0000n\u0000t\u0000i\u0000c\u0000a\u0000t\u0000o\u0000r\u0000=\u0000=\u00000\u0000.\u00004\u0000.\u00002\u0000\n- \u0000\n- \u0000s\u0000t\u0000r\u0000e\u0000a\u0000m\u0000l\u0000i\u0000t\u0000-\u0000m\u0000e\u0000r\u0000m\u0000a\u0000i\u0000d\u0000=\u0000=\u00000\u0000.\u00003\u0000.\u00000\u0000\n- \u0000\n- \u0000t\u0000e\u0000n\u0000a\u0000c\u0000i\u0000t\u0000y\u0000=\u0000=\u00009\u0000.\u00001\u0000.\u00002\u0000\n- \u0000\n- \u0000t\u0000o\u0000m\u0000l\u0000=\u0000=\u00000\u0000.\u00001\u00000\u0000.\u00002\u0000\n- \u0000\n- \u0000t\u0000o\u0000o\u0000l\u0000z\u0000=\u0000=\u00001\u0000.\u00001\u0000.\u00000\u0000\n- \u0000\n- \u0000t\u0000o\u0000o\u0000n\u0000_\u0000f\u0000o\u0000r\u0000m\u0000a\u0000t\u0000 \u0000@\u0000 \u0000g\u0000i\u0000t\u0000+\u0000h\u0000t\u0000t\u0000p\u0000s\u0000:\u0000/\u0000/\u0000g\u0000i\u0000t\u0000h\u0000u\u0000b\u0000.\u0000c\u0000o\u0000m\u0000/\u0000t\u0000o\u0000o\u0000n\u0000-\u0000f\u0000o\u0000r\u0000m\u0000a\u0000t\u0000/\u0000t\u0000o\u0000o\u0000n\u0000-\u0000p\u0000y\u0000t\u0000h\u0000o\u0000n\u0000.\u0000g\u0000i\u0000t\u0000@\u00009\u0000c\u00004\u0000f\u00000\u0000c\u00000\u0000c\u00002\u00004\u0000f\u00002\u0000a\u00000\u0000b\u00000\u0000b\u00003\u00007\u00006\u00003\u00001\u00005\u0000f\u00004\u0000b\u00008\u00007\u00000\u00007\u0000f\u00008\u0000c\u00009\u00000\u00000\u00006\u0000d\u0000e\u00006\u0000\n- \u0000\n- \u0000t\u0000o\u0000r\u0000n\u0000a\u0000d\u0000o\u0000=\u0000=\u00006\u0000.\u00005\u0000.\u00002\u0000\n- \u0000\n- \u0000t\u0000y\u0000p\u0000i\u0000n\u0000g\u0000-\u0000i\u0000n\u0000s\u0000p\u0000e\u0000c\u0000t\u0000i\u0000o\u0000n\u0000=\u0000=\u00000\u0000.\u00004\u0000.\u00002\u0000\n- \u0000\n- \u0000t\u0000y\u0000p\u0000i\u0000n\u0000g\u0000_\u0000e\u0000x\u0000t\u0000e\u0000n\u0000s\u0000i\u0000o\u0000n\u0000s\u0000=\u0000=\u00004\u0000.\u00001\u00005\u0000.\u00000\u0000\n- \u0000\n- \u0000t\u0000z\u0000d\u0000a\u0000t\u0000a\u0000=\u0000=\u00002\u00000\u00002\u00005\u0000.\u00002\u0000\n- \u0000\n- \u0000u\u0000r\u0000l\u0000l\u0000i\u0000b\u00003\u0000=\u0000=\u00002\u0000.\u00005\u0000.\u00000\u0000\n- \u0000\n- \u0000w\u0000a\u0000t\u0000c\u0000h\u0000d\u0000o\u0000g\u0000=\u0000=\u00006\u0000.\u00000\u0000.\u00000\u0000\n- \u0000\n- \u0000x\u0000x\u0000h\u0000a\u0000s\u0000h\u0000=\u0000=\u00003\u0000.\u00006\u0000.\u00000\u0000\n- \u0000\n- \u0000z\u0000s\u0000t\u0000a\u0000n\u0000d\u0000a\u0000r\u0000d\u0000=\u0000=\u00000\u0000.\u00002\u00005\u0000.\u00000\u0000\n- \u0000\n- \u0000",
      "setup_anleitung": "Information not found",
      "quick_start_guide": "Information not found"
    }
  },
  "file_tree": {
    "name": "root",
    "type": "directory",
    "children": [
      {
        "path": ".env.example",
        "name": ".env.example",
        "size": 48,
        "type": "file"
      },
      {
        "path": ".gitignore",
        "name": ".gitignore",
        "size": 143,
        "type": "file"
      },
      {
        "name": "SystemPrompts",
        "type": "directory",
        "children": [
          {
            "path": "SystemPrompts/SystemPromptClassHelperLLM.txt",
            "name": "SystemPromptClassHelperLLM.txt",
            "size": 6645,
            "type": "file"
          },
          {
            "path": "SystemPrompts/SystemPromptFunctionHelperLLM.txt",
            "name": "SystemPromptFunctionHelperLLM.txt",
            "size": 5223,
            "type": "file"
          },
          {
            "path": "SystemPrompts/SystemPromptHelperLLM.txt",
            "name": "SystemPromptHelperLLM.txt",
            "size": 9350,
            "type": "file"
          },
          {
            "path": "SystemPrompts/SystemPromptMainLLM.txt",
            "name": "SystemPromptMainLLM.txt",
            "size": 8380,
            "type": "file"
          }
        ]
      },
      {
        "path": "analysis_output.json",
        "name": "analysis_output.json",
        "size": 569396,
        "type": "file"
      },
      {
        "name": "backend",
        "type": "directory",
        "children": [
          {
            "path": "backend/AST_Schema.py",
            "name": "AST_Schema.py",
            "size": 7382,
            "type": "file"
          },
          {
            "path": "backend/File_Dependency.py",
            "name": "File_Dependency.py",
            "size": 8740,
            "type": "file"
          },
          {
            "path": "backend/HelperLLM.py",
            "name": "HelperLLM.py",
            "size": 16175,
            "type": "file"
          },
          {
            "path": "backend/MainLLM.py",
            "name": "MainLLM.py",
            "size": 3610,
            "type": "file"
          },
          {
            "path": "backend/__init__.py",
            "name": "__init__.py",
            "size": 0,
            "type": "file"
          },
          {
            "path": "backend/basic_info.py",
            "name": "basic_info.py",
            "size": 7511,
            "type": "file"
          },
          {
            "path": "backend/callgraph.py",
            "name": "callgraph.py",
            "size": 9912,
            "type": "file"
          },
          {
            "path": "backend/getRepo.py",
            "name": "getRepo.py",
            "size": 5310,
            "type": "file"
          },
          {
            "path": "backend/main.py",
            "name": "main.py",
            "size": 15141,
            "type": "file"
          },
          {
            "path": "backend/relationship_analyzer.py",
            "name": "relationship_analyzer.py",
            "size": 8674,
            "type": "file"
          }
        ]
      },
      {
        "name": "database",
        "type": "directory",
        "children": [
          {
            "path": "database/db.py",
            "name": "db.py",
            "size": 5321,
            "type": "file"
          }
        ]
      },
      {
        "name": "frontend",
        "type": "directory",
        "children": [
          {
            "path": "frontend/Frontend.py",
            "name": "Frontend.py",
            "size": 12962,
            "type": "file"
          },
          {
            "path": "frontend/__init__.py",
            "name": "__init__.py",
            "size": 0,
            "type": "file"
          },
          {
            "name": "gifs",
            "type": "directory",
            "children": [
              {
                "path": "frontend/gifs/4j.gif",
                "name": "4j.gif",
                "size": 3306723,
                "type": "file"
              }
            ]
          }
        ]
      },
      {
        "name": "notizen",
        "type": "directory",
        "children": [
          {
            "path": "notizen/Report Agenda.txt",
            "name": "Report Agenda.txt",
            "size": 3492,
            "type": "file"
          },
          {
            "path": "notizen/Zwischenpraesentation Agenda.txt",
            "name": "Zwischenpraesentation Agenda.txt",
            "size": 809,
            "type": "file"
          },
          {
            "path": "notizen/doc_bestandteile.md",
            "name": "doc_bestandteile.md",
            "size": 847,
            "type": "file"
          },
          {
            "name": "grafiken",
            "type": "directory",
            "children": [
              {
                "path": "notizen/grafiken/File_Dependency_Graph_Repo.dot",
                "name": "File_Dependency_Graph_Repo.dot",
                "size": 1227,
                "type": "file"
              },
              {
                "name": "Flask-Repo",
                "type": "directory",
                "children": [
                  {
                    "path": "notizen/grafiken/Flask-Repo/__init__.dot",
                    "name": "__init__.dot",
                    "size": 89,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/__main__.dot",
                    "name": "__main__.dot",
                    "size": 87,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/app.dot",
                    "name": "app.dot",
                    "size": 78,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/auth.dot",
                    "name": "auth.dot",
                    "size": 1126,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/blog.dot",
                    "name": "blog.dot",
                    "size": 939,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/blueprints.dot",
                    "name": "blueprints.dot",
                    "size": 4601,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/cli.dot",
                    "name": "cli.dot",
                    "size": 8269,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/conf.dot",
                    "name": "conf.dot",
                    "size": 489,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/config.dot",
                    "name": "config.dot",
                    "size": 2130,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/conftest.dot",
                    "name": "conftest.dot",
                    "size": 1756,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/ctx.dot",
                    "name": "ctx.dot",
                    "size": 3009,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/db.dot",
                    "name": "db.dot",
                    "size": 782,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/debughelpers.dot",
                    "name": "debughelpers.dot",
                    "size": 1903,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/factory.dot",
                    "name": "factory.dot",
                    "size": 220,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/flask.dot",
                    "name": "flask.dot",
                    "size": 82,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/globals.dot",
                    "name": "globals.dot",
                    "size": 370,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/hello.dot",
                    "name": "hello.dot",
                    "size": 152,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/helpers.dot",
                    "name": "helpers.dot",
                    "size": 2510,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/importerrorapp.dot",
                    "name": "importerrorapp.dot",
                    "size": 155,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/logging.dot",
                    "name": "logging.dot",
                    "size": 564,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/make_celery.dot",
                    "name": "make_celery.dot",
                    "size": 99,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/multiapp.dot",
                    "name": "multiapp.dot",
                    "size": 88,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/provider.dot",
                    "name": "provider.dot",
                    "size": 1769,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/scaffold.dot",
                    "name": "scaffold.dot",
                    "size": 3700,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/sessions.dot",
                    "name": "sessions.dot",
                    "size": 4178,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/signals.dot",
                    "name": "signals.dot",
                    "size": 133,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/tag.dot",
                    "name": "tag.dot",
                    "size": 3750,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/tasks.dot",
                    "name": "tasks.dot",
                    "size": 312,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/templating.dot",
                    "name": "templating.dot",
                    "size": 2277,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_appctx.dot",
                    "name": "test_appctx.dot",
                    "size": 2470,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_async.dot",
                    "name": "test_async.dot",
                    "size": 1919,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_auth.dot",
                    "name": "test_auth.dot",
                    "size": 725,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_basic.dot",
                    "name": "test_basic.dot",
                    "size": 17383,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_blog.dot",
                    "name": "test_blog.dot",
                    "size": 1105,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_blueprints.dot",
                    "name": "test_blueprints.dot",
                    "size": 11515,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_cli.dot",
                    "name": "test_cli.dot",
                    "size": 6435,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_config.dot",
                    "name": "test_config.dot",
                    "size": 2854,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_config.png",
                    "name": "test_config.png",
                    "size": 288693,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_converters.dot",
                    "name": "test_converters.dot",
                    "size": 937,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_db.dot",
                    "name": "test_db.dot",
                    "size": 481,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_factory.dot",
                    "name": "test_factory.dot",
                    "size": 201,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_helpers.dot",
                    "name": "test_helpers.dot",
                    "size": 5857,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_instance_config.dot",
                    "name": "test_instance_config.dot",
                    "size": 1249,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_js_example.dot",
                    "name": "test_js_example.dot",
                    "size": 453,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_json.dot",
                    "name": "test_json.dot",
                    "size": 4021,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_json_tag.dot",
                    "name": "test_json_tag.dot",
                    "size": 1452,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_logging.dot",
                    "name": "test_logging.dot",
                    "size": 1348,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_regression.dot",
                    "name": "test_regression.dot",
                    "size": 763,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_reqctx.dot",
                    "name": "test_reqctx.dot",
                    "size": 3809,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_request.dot",
                    "name": "test_request.dot",
                    "size": 668,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_session_interface.dot",
                    "name": "test_session_interface.dot",
                    "size": 667,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_signals.dot",
                    "name": "test_signals.dot",
                    "size": 1671,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_subclassing.dot",
                    "name": "test_subclassing.dot",
                    "size": 612,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_templating.dot",
                    "name": "test_templating.dot",
                    "size": 5129,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_testing.dot",
                    "name": "test_testing.dot",
                    "size": 4081,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_user_error_handler.dot",
                    "name": "test_user_error_handler.dot",
                    "size": 5984,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/test_views.dot",
                    "name": "test_views.dot",
                    "size": 2641,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/testing.dot",
                    "name": "testing.dot",
                    "size": 2785,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/typing.dot",
                    "name": "typing.dot",
                    "size": 86,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/typing_app_decorators.dot",
                    "name": "typing_app_decorators.dot",
                    "size": 500,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/typing_error_handler.dot",
                    "name": "typing_error_handler.dot",
                    "size": 420,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/typing_route.dot",
                    "name": "typing_route.dot",
                    "size": 1717,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/views.dot",
                    "name": "views.dot",
                    "size": 1125,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/wrappers.dot",
                    "name": "wrappers.dot",
                    "size": 911,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Flask-Repo/wsgi.dot",
                    "name": "wsgi.dot",
                    "size": 19,
                    "type": "file"
                  }
                ]
              },
              {
                "name": "Repo-onboarding",
                "type": "directory",
                "children": [
                  {
                    "path": "notizen/grafiken/Repo-onboarding/AST.dot",
                    "name": "AST.dot",
                    "size": 1361,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Repo-onboarding/Frontend.dot",
                    "name": "Frontend.dot",
                    "size": 538,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Repo-onboarding/HelperLLM.dot",
                    "name": "HelperLLM.dot",
                    "size": 1999,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Repo-onboarding/HelperLLM.png",
                    "name": "HelperLLM.png",
                    "size": 234861,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Repo-onboarding/MainLLM.dot",
                    "name": "MainLLM.dot",
                    "size": 2547,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Repo-onboarding/agent.dot",
                    "name": "agent.dot",
                    "size": 2107,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Repo-onboarding/basic_info.dot",
                    "name": "basic_info.dot",
                    "size": 1793,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Repo-onboarding/callgraph.dot",
                    "name": "callgraph.dot",
                    "size": 1478,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Repo-onboarding/getRepo.dot",
                    "name": "getRepo.dot",
                    "size": 1431,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Repo-onboarding/graph_AST.png",
                    "name": "graph_AST.png",
                    "size": 132743,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Repo-onboarding/graph_AST2.png",
                    "name": "graph_AST2.png",
                    "size": 12826,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Repo-onboarding/graph_AST3.png",
                    "name": "graph_AST3.png",
                    "size": 136016,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Repo-onboarding/main.dot",
                    "name": "main.dot",
                    "size": 1091,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Repo-onboarding/tools.dot",
                    "name": "tools.dot",
                    "size": 1328,
                    "type": "file"
                  },
                  {
                    "path": "notizen/grafiken/Repo-onboarding/types.dot",
                    "name": "types.dot",
                    "size": 133,
                    "type": "file"
                  }
                ]
              },
              {
                "path": "notizen/grafiken/global_callgraph.png",
                "name": "global_callgraph.png",
                "size": 940354,
                "type": "file"
              },
              {
                "path": "notizen/grafiken/global_graph.png",
                "name": "global_graph.png",
                "size": 4756519,
                "type": "file"
              },
              {
                "path": "notizen/grafiken/global_graph_2.png",
                "name": "global_graph_2.png",
                "size": 242251,
                "type": "file"
              },
              {
                "path": "notizen/grafiken/repo.dot",
                "name": "repo.dot",
                "size": 13870,
                "type": "file"
              }
            ]
          },
          {
            "path": "notizen/notizen.md",
            "name": "notizen.md",
            "size": 4937,
            "type": "file"
          },
          {
            "path": "notizen/paul_notizen.md",
            "name": "paul_notizen.md",
            "size": 2033,
            "type": "file"
          },
          {
            "path": "notizen/praesentation_notizen.md",
            "name": "praesentation_notizen.md",
            "size": 2738,
            "type": "file"
          },
          {
            "path": "notizen/technische_notizen.md",
            "name": "technische_notizen.md",
            "size": 3616,
            "type": "file"
          }
        ]
      },
      {
        "path": "readme.md",
        "name": "readme.md",
        "size": 1905,
        "type": "file"
      },
      {
        "path": "requirements.txt",
        "name": "requirements.txt",
        "size": 3766,
        "type": "file"
      },
      {
        "name": "result",
        "type": "directory",
        "children": [
          {
            "path": "result/ast_schema_01_12_2025_11-49-24.json",
            "name": "ast_schema_01_12_2025_11-49-24.json",
            "size": 201215,
            "type": "file"
          },
          {
            "path": "result/report_01_12_2025_12-26-46_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md",
            "name": "report_01_12_2025_12-26-46_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md",
            "size": 79523,
            "type": "file"
          },
          {
            "path": "result/report_01_12_2025_12-55-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md",
            "name": "report_01_12_2025_12-55-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md",
            "size": 143883,
            "type": "file"
          },
          {
            "path": "result/report_14_11_2025_14-52-36.md",
            "name": "report_14_11_2025_14-52-36.md",
            "size": 16393,
            "type": "file"
          },
          {
            "path": "result/report_14_11_2025_15-21-53.md",
            "name": "report_14_11_2025_15-21-53.md",
            "size": 108585,
            "type": "file"
          },
          {
            "path": "result/report_14_11_2025_15-26-24.md",
            "name": "report_14_11_2025_15-26-24.md",
            "size": 11659,
            "type": "file"
          },
          {
            "path": "result/report_21_11_2025_15-43-30.md",
            "name": "report_21_11_2025_15-43-30.md",
            "size": 57940,
            "type": "file"
          },
          {
            "path": "result/report_21_11_2025_16-06-12.md",
            "name": "report_21_11_2025_16-06-12.md",
            "size": 76423,
            "type": "file"
          },
          {
            "path": "result/report_22_11_2025_14-01-50_Helper_llama3_Main_geminipro.md",
            "name": "report_22_11_2025_14-01-50_Helper_llama3_Main_geminipro.md",
            "size": 39595,
            "type": "file"
          },
          {
            "path": "result/report_22_11_2025_14-39-55_Helper_llama3_MainLLM_llama3.md",
            "name": "report_22_11_2025_14-39-55_Helper_llama3_MainLLM_llama3.md",
            "size": 2013,
            "type": "file"
          },
          {
            "path": "result/result_2025-11-11_12-30-53.md",
            "name": "result_2025-11-11_12-30-53.md",
            "size": 1232,
            "type": "file"
          },
          {
            "path": "result/result_2025-11-11_12-43-51.md",
            "name": "result_2025-11-11_12-43-51.md",
            "size": 33683,
            "type": "file"
          },
          {
            "path": "result/result_2025-11-11_12-45-37.md",
            "name": "result_2025-11-11_12-45-37.md",
            "size": 1193,
            "type": "file"
          }
        ]
      },
      {
        "name": "schemas",
        "type": "directory",
        "children": [
          {
            "path": "schemas/types.py",
            "name": "types.py",
            "size": 7559,
            "type": "file"
          }
        ]
      }
    ]
  },
  "ast_schema": {
    "files": {
      "backend/AST_Schema.py": {
        "ast_nodes": {
          "imports": [
            "ast",
            "networkx",
            "os",
            "callgraph.build_callGraph"
          ],
          "functions": [
            {
              "mode": "function_analysis",
              "identifier": "backend.AST_Schema.path_to_module",
              "name": "path_to_module",
              "args": [
                "filepath",
                "project_root"
              ],
              "docstring": "Wandelt einen Dateipfad in einen Python-Modulpfad um.",
              "source_code": "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    \n    module_path = rel_path.replace(os.path.sep, '.')\n    \n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n        \n    return module_path",
              "start_line": 7,
              "end_line": 22,
              "context": {
                "calls": [
                  "backend/AST_Schema.py::basename",
                  "backend/AST_Schema.py::endswith",
                  "backend/AST_Schema.py::relpath",
                  "backend/AST_Schema.py::replace"
                ],
                "called_by": [
                  {
                    "file": "AST_Schema.py",
                    "function": "__init__",
                    "mode": "method",
                    "line": 29
                  }
                ]
              }
            }
          ],
          "classes": [
            {
              "mode": "class_analysis",
              "identifier": "backend.AST_Schema.ASTVisitor",
              "name": "ASTVisitor",
              "docstring": null,
              "source_code": "class ASTVisitor(ast.NodeVisitor):\n    def __init__(self, source_code: str, file_path: str, project_root: str):\n        self.source_code = source_code\n        self.file_path = file_path\n        self.project_root = project_root\n        self.module_path = path_to_module(self.file_path, self.project_root)\n        self.schema = {\"imports\": [], \"functions\": [], \"classes\": []}\n        self._current_class = None\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(f\"{node.module}.{alias.name}\")\n        self.generic_visit(node)\n\n    def visit_ClassDef(self, node):\n        class_identifier = f\"{self.module_path}.{node.name}\"\n\n        class_info = {\n            \"mode\": \"class_analysis\",\n            \"identifier\": class_identifier,\n            \"name\": node.name,\n            \"docstring\": ast.get_docstring(node),\n            \"source_code\": ast.get_source_segment(self.source_code, node),\n            \"start_line\": node.lineno,\n            \"end_line\": node.end_lineno,\n            \"context\": {\n                \"dependencies\": [],\n                \"instantiated_by\": [],\n                \"method_context\": []\n            },   \n        }\n        self.schema[\"classes\"].append(class_info)\n        \n        self._current_class = class_info \n        self.generic_visit(node)\n        self._current_class = None\n\n    def visit_FunctionDef(self, node):\n        if self._current_class:\n            method_identifier = f\"{self._current_class['identifier']}.{node.name}\"\n            method_context_info = {\n                \"identifier\": method_identifier,\n                \"name\": node.name,\n                \"calls\": [],\n                \"called_by\": [],\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n            }\n            self._current_class[\"context\"][\"method_context\"].append(method_context_info)\n        else:\n            func_identifier = f\"{self.module_path}.{node.name}\"\n            func_info = {\n                \"mode\": \"function_analysis\",\n                \"identifier\": func_identifier,\n                \"name\": node.name,\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"source_code\": ast.get_source_segment(self.source_code, node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n                \"context\": {\n                    \"calls\": [],\n                    \"called_by\": []\n                }\n            }\n            self.schema[\"functions\"].append(func_info)\n            \n        self.generic_visit(node)\n    \n    def visit_AsyncFunctionDef(self, node):\n        self.visit_FunctionDef(node)",
              "start_line": 24,
              "end_line": 101,
              "context": {
                "dependencies": [],
                "instantiated_by": [
                  {
                    "file": "AST_Schema.py",
                    "function": "analyze_repository",
                    "mode": "method",
                    "line": 175
                  }
                ],
                "method_context": [
                  {
                    "identifier": "backend.AST_Schema.ASTVisitor.__init__",
                    "name": "__init__",
                    "calls": [
                      "backend/AST_Schema.py::ASTVisitor::path_to_module"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "source_code",
                      "file_path",
                      "project_root"
                    ],
                    "docstring": null,
                    "start_line": 25,
                    "end_line": 31
                  },
                  {
                    "identifier": "backend.AST_Schema.ASTVisitor.visit_Import",
                    "name": "visit_Import",
                    "calls": [
                      "backend/AST_Schema.py::ASTVisitor::append",
                      "backend/AST_Schema.py::ASTVisitor::generic_visit"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "node"
                    ],
                    "docstring": null,
                    "start_line": 33,
                    "end_line": 36
                  },
                  {
                    "identifier": "backend.AST_Schema.ASTVisitor.visit_ImportFrom",
                    "name": "visit_ImportFrom",
                    "calls": [
                      "backend/AST_Schema.py::ASTVisitor::append",
                      "backend/AST_Schema.py::ASTVisitor::generic_visit"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "node"
                    ],
                    "docstring": null,
                    "start_line": 38,
                    "end_line": 41
                  },
                  {
                    "identifier": "backend.AST_Schema.ASTVisitor.visit_ClassDef",
                    "name": "visit_ClassDef",
                    "calls": [
                      "backend/AST_Schema.py::ASTVisitor::append",
                      "backend/AST_Schema.py::ASTVisitor::generic_visit",
                      "backend/AST_Schema.py::ASTVisitor::get_docstring",
                      "backend/AST_Schema.py::ASTVisitor::get_source_segment"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "node"
                    ],
                    "docstring": null,
                    "start_line": 43,
                    "end_line": 64
                  },
                  {
                    "identifier": "backend.AST_Schema.ASTVisitor.visit_FunctionDef",
                    "name": "visit_FunctionDef",
                    "calls": [
                      "backend/AST_Schema.py::ASTVisitor::append",
                      "backend/AST_Schema.py::ASTVisitor::generic_visit",
                      "backend/AST_Schema.py::ASTVisitor::get_docstring",
                      "backend/AST_Schema.py::ASTVisitor::get_source_segment"
                    ],
                    "called_by": [
                      "backend/AST_Schema.py::ASTVisitor::visit_AsyncFunctionDef"
                    ],
                    "args": [
                      "self",
                      "node"
                    ],
                    "docstring": null,
                    "start_line": 66,
                    "end_line": 98
                  },
                  {
                    "identifier": "backend.AST_Schema.ASTVisitor.visit_AsyncFunctionDef",
                    "name": "visit_AsyncFunctionDef",
                    "calls": [
                      "backend/AST_Schema.py::ASTVisitor::visit_FunctionDef"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "node"
                    ],
                    "docstring": null,
                    "start_line": 100,
                    "end_line": 101
                  }
                ]
              }
            },
            {
              "mode": "class_analysis",
              "identifier": "backend.AST_Schema.ASTAnalyzer",
              "name": "ASTAnalyzer",
              "docstring": null,
              "source_code": "class ASTAnalyzer:\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def _enrich_schema_with_callgraph(schema: dict, call_graph: nx.DiGraph, filename: str):\n        for func in schema[\"functions\"]:\n            func_name_key = f\"{filename}::{func['name']}\"\n            if func_name_key in call_graph:\n                func['context']['calls'] = sorted(list(call_graph.successors(func_name_key)))\n                func['context']['called_by'] = sorted(list(call_graph.predecessors(func_name_key)))\n\n        for cls in schema[\"classes\"]:\n            for method_context in cls[\"context\"][\"method_context\"]:\n                func_name_key = f\"{filename}::{cls['name']}::{method_context['name']}\"\n                if func_name_key in call_graph:\n                    calls = sorted(list(call_graph.successors(func_name_key)))\n                    called_by = sorted(list(call_graph.predecessors(func_name_key)))\n                    \n                    method_context['calls'] = calls\n                    method_context['called_by'] = called_by\n\n    def merge_relationship_data(self, full_schema: dict, relationship_data: list) -> dict:\n        \n        rel_lookup = {item['identifier']: item.get('called_by', []) for item in relationship_data}\n\n        for file_path, file_data in full_schema.get(\"files\", {}).items():\n            ast_nodes = file_data.get(\"ast_nodes\", {})\n            \n            for func in ast_nodes.get(\"functions\", []):\n                func_id = func.get(\"identifier\")\n                if func_id and func_id in rel_lookup:\n                    func[\"context\"][\"called_by\"] = rel_lookup[func_id]\n\n            for cls in ast_nodes.get(\"classes\", []):\n                cls_id = cls.get(\"identifier\")\n                \n                if cls_id and cls_id in rel_lookup:\n                    cls[\"context\"][\"instantiated_by\"] = rel_lookup[cls_id]\n\n                for method in cls[\"context\"].get(\"method_context\", []):\n                    method_id = method.get(\"identifier\")\n                    if method_id and method_id in rel_lookup:\n                        method[\"called_by\"] = rel_lookup[method_id]\n        \n        return full_schema\n\n    def analyze_repository(self, files: list) -> dict:\n        full_schema = {\n            \"files\": {}\n        }\n\n        all_paths = [file_obj.path for file_obj in files]\n        if not all_paths:\n            return full_schema\n        \n        project_root = os.path.commonpath(all_paths)\n        if os.path.isfile(project_root):\n            project_root = os.path.dirname(project_root)\n\n        for file_obj in files:\n            if not file_obj.path.endswith('.py'):\n                continue\n            \n            file_content = file_obj.content\n            if not file_content.strip():\n                continue\n\n            try:\n                tree = ast.parse(file_content)\n                \n                visitor = ASTVisitor(\n                    source_code=file_content, \n                    file_path=file_obj.path, \n                    project_root=project_root\n                )\n                visitor.visit(tree)\n                file_schema_nodes = visitor.schema\n\n                call_graph = build_callGraph(tree, filename=file_obj.path)\n\n                self._enrich_schema_with_callgraph(\n                    file_schema_nodes, \n                    call_graph, \n                    file_obj.path\n                )\n\n                if file_schema_nodes[\"imports\"] or file_schema_nodes[\"functions\"] or file_schema_nodes[\"classes\"]:\n                    full_schema[\"files\"][file_obj.path] = {\n                        \"ast_nodes\": file_schema_nodes\n                    }\n\n            except (SyntaxError, ValueError) as e:\n                print(f\"Warnung: Konnte Datei '{file_obj.path}' nicht parsen. Fehler: {e}\")\n\n        return full_schema",
              "start_line": 103,
              "end_line": 199,
              "context": {
                "dependencies": [],
                "instantiated_by": [
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 166
                  }
                ],
                "method_context": [
                  {
                    "identifier": "backend.AST_Schema.ASTAnalyzer.__init__",
                    "name": "__init__",
                    "calls": [],
                    "called_by": [],
                    "args": [
                      "self"
                    ],
                    "docstring": null,
                    "start_line": 105,
                    "end_line": 106
                  },
                  {
                    "identifier": "backend.AST_Schema.ASTAnalyzer._enrich_schema_with_callgraph",
                    "name": "_enrich_schema_with_callgraph",
                    "calls": [
                      "backend/AST_Schema.py::ASTAnalyzer::list",
                      "backend/AST_Schema.py::ASTAnalyzer::predecessors",
                      "backend/AST_Schema.py::ASTAnalyzer::sorted",
                      "backend/AST_Schema.py::ASTAnalyzer::successors"
                    ],
                    "called_by": [
                      "backend/AST_Schema.py::ASTAnalyzer::analyze_repository"
                    ],
                    "args": [
                      "schema",
                      "call_graph",
                      "filename"
                    ],
                    "docstring": null,
                    "start_line": 109,
                    "end_line": 124
                  },
                  {
                    "identifier": "backend.AST_Schema.ASTAnalyzer.merge_relationship_data",
                    "name": "merge_relationship_data",
                    "calls": [
                      "backend/AST_Schema.py::ASTAnalyzer::get",
                      "backend/AST_Schema.py::ASTAnalyzer::items"
                    ],
                    "called_by": [
                      {
                        "file": "main.py",
                        "function": "main_workflow",
                        "mode": "function",
                        "line": 176
                      }
                    ],
                    "args": [
                      "self",
                      "full_schema",
                      "relationship_data"
                    ],
                    "docstring": null,
                    "start_line": 126,
                    "end_line": 149
                  },
                  {
                    "identifier": "backend.AST_Schema.ASTAnalyzer.analyze_repository",
                    "name": "analyze_repository",
                    "calls": [
                      "backend/AST_Schema.py::ASTAnalyzer::ASTVisitor",
                      "backend/AST_Schema.py::ASTAnalyzer::_enrich_schema_with_callgraph",
                      "backend/AST_Schema.py::ASTAnalyzer::build_callGraph",
                      "backend/AST_Schema.py::ASTAnalyzer::commonpath",
                      "backend/AST_Schema.py::ASTAnalyzer::dirname",
                      "backend/AST_Schema.py::ASTAnalyzer::endswith",
                      "backend/AST_Schema.py::ASTAnalyzer::isfile",
                      "backend/AST_Schema.py::ASTAnalyzer::parse",
                      "backend/AST_Schema.py::ASTAnalyzer::print",
                      "backend/AST_Schema.py::ASTAnalyzer::strip",
                      "backend/AST_Schema.py::ASTAnalyzer::visit"
                    ],
                    "called_by": [
                      {
                        "file": "main.py",
                        "function": "main_workflow",
                        "mode": "function",
                        "line": 167
                      }
                    ],
                    "args": [
                      "self",
                      "files"
                    ],
                    "docstring": null,
                    "start_line": 151,
                    "end_line": 199
                  }
                ]
              }
            }
          ]
        }
      },
      "backend/File_Dependency.py": {
        "ast_nodes": {
          "imports": [
            "networkx",
            "os",
            "ast.Assign",
            "ast.AST",
            "ast.ClassDef",
            "ast.FunctionDef",
            "ast.Import",
            "ast.ImportFrom",
            "ast.Name",
            "ast.NodeVisitor",
            "ast.literal_eval",
            "ast.parse",
            "ast.walk",
            "keyword.iskeyword",
            "pathlib.Path",
            "getRepo.GitRepository",
            "backend.callgraph.make_safe_dot",
            "collections.defaultdict"
          ],
          "functions": [
            {
              "mode": "function_analysis",
              "identifier": "backend.File_Dependency.build_file_dependency_graph",
              "name": "build_file_dependency_graph",
              "args": [
                "filename",
                "tree",
                "repo_root"
              ],
              "docstring": null,
              "source_code": "def build_file_dependency_graph(filename: str, tree: AST, repo_root: str) -> nx.DiGraph:\n    graph = nx.DiGraph()\n\n    tree_visitor = FileDependencyGraph(filename, repo_root)\n    tree_visitor.visit(tree)\n\n    for caller, callees in tree_visitor.import_dependencies.items():\n        graph.add_node(caller)\n        graph.add_nodes_from(callees)\n        for callee in callees:\n            graph.add_edge(caller, callee)\n    \n    return graph",
              "start_line": 156,
              "end_line": 168,
              "context": {
                "calls": [
                  "backend/File_Dependency.py::DiGraph",
                  "backend/File_Dependency.py::FileDependencyGraph",
                  "backend/File_Dependency.py::add_edge",
                  "backend/File_Dependency.py::add_node",
                  "backend/File_Dependency.py::add_nodes_from",
                  "backend/File_Dependency.py::items",
                  "backend/File_Dependency.py::visit"
                ],
                "called_by": [
                  {
                    "file": "File_Dependency.py",
                    "function": "build_repository_graph",
                    "mode": "function",
                    "line": 180
                  }
                ]
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "backend.File_Dependency.build_repository_graph",
              "name": "build_repository_graph",
              "args": [
                "repository"
              ],
              "docstring": null,
              "source_code": "def build_repository_graph(repository: GitRepository) -> nx.DiGraph:\n    all_files = repository.get_all_files()\n    repo_root = repository.temp_dir\n    global_graph = nx.DiGraph()\n\n    for file in all_files: \n        if not file.path.endswith(\".py\"):\n            continue\n        filename = str(os.path.basename(file.path)).removesuffix(\".py\")\n        tree = parse(file.content)\n        graph = build_file_dependency_graph(filename, tree, repo_root)\n        \n        for node in graph.nodes:\n            global_graph.add_node(node)\n\n        for caller, callee in graph.edges:\n            if callee:\n                global_graph.add_node(callee)\n                global_graph.add_edge(caller, callee)\n    \n    return global_graph",
              "start_line": 170,
              "end_line": 190,
              "context": {
                "calls": [
                  "backend/File_Dependency.py::DiGraph",
                  "backend/File_Dependency.py::add_edge",
                  "backend/File_Dependency.py::add_node",
                  "backend/File_Dependency.py::basename",
                  "backend/File_Dependency.py::build_file_dependency_graph",
                  "backend/File_Dependency.py::endswith",
                  "backend/File_Dependency.py::get_all_files",
                  "backend/File_Dependency.py::parse",
                  "backend/File_Dependency.py::removesuffix",
                  "backend/File_Dependency.py::str"
                ],
                "called_by": [
                  {
                    "file": "File_Dependency.py",
                    "function": "backend.File_Dependency",
                    "mode": "module",
                    "line": 236
                  }
                ]
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "backend.File_Dependency.get_all_temp_files",
              "name": "get_all_temp_files",
              "args": [
                "directory"
              ],
              "docstring": null,
              "source_code": "def get_all_temp_files(directory: str) -> list[Path]:\n    root_path = Path(directory).resolve()\n    all_files = [file.relative_to(root_path) for file in root_path.rglob(\"*.py\")]\n\n    return all_files",
              "start_line": 192,
              "end_line": 196,
              "context": {
                "calls": [
                  "backend/File_Dependency.py::Path",
                  "backend/File_Dependency.py::relative_to",
                  "backend/File_Dependency.py::resolve",
                  "backend/File_Dependency.py::rglob"
                ],
                "called_by": [
                  {
                    "file": "File_Dependency.py",
                    "function": "_resolve_module_name",
                    "mode": "method",
                    "line": 44
                  }
                ]
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "backend.File_Dependency.nx_to_mermaid_with_folders",
              "name": "nx_to_mermaid_with_folders",
              "args": [
                "G"
              ],
              "docstring": null,
              "source_code": "def nx_to_mermaid_with_folders(G: nx.DiGraph):\n    # Ordner \u2192 Liste der Dateien\n    folder_map = defaultdict(list)\n    for node in G.nodes:\n        parts = node.split(\"/\")\n        folder = \"/\".join(parts[:-1])  # alles au\u00dfer Datei = Ordner\n        folder_map[folder].append(parts[-1])  # nur der Dateiname als Knoten\n\n    lines = [\"graph TD\"]\n\n    # Subgraphs erstellen\n    for folder, files in folder_map.items():\n        if folder:  # nur wenn es einen Ordner gibt\n            lines.append(f\"    subgraph {folder.replace('/', '_')}\")\n            for f in files:\n                node_id = f\"{folder}/{f}\".replace('/', '_')\n                lines.append(f'        {node_id}[\"{f}\"]')\n            lines.append(\"    end\")\n        else:\n            # Dateien im Root\n            for f in files:\n                node_id = f.replace('/', '_')\n                lines.append(f'    {node_id}[\"{f}\"]')\n\n    # Kanten\n    for caller, callee in G.edges:\n        caller_id = caller.replace('/', '_')\n        callee_id = callee.replace('/', '_')\n        lines.append(f'    {caller_id} --> {callee_id}')\n\n    return \"\\n\".join(lines)",
              "start_line": 200,
              "end_line": 230,
              "context": {
                "calls": [
                  "backend/File_Dependency.py::append",
                  "backend/File_Dependency.py::defaultdict",
                  "backend/File_Dependency.py::items",
                  "backend/File_Dependency.py::join",
                  "backend/File_Dependency.py::replace",
                  "backend/File_Dependency.py::split"
                ],
                "called_by": [
                  {
                    "file": "File_Dependency.py",
                    "function": "backend.File_Dependency",
                    "mode": "module",
                    "line": 238
                  }
                ]
              }
            }
          ],
          "classes": [
            {
              "mode": "class_analysis",
              "identifier": "backend.File_Dependency.FileDependencyGraph",
              "name": "FileDependencyGraph",
              "docstring": null,
              "source_code": "class FileDependencyGraph(NodeVisitor):\n\n    import_dependencies: dict[str, set[str]] = {}\n    def __init__(self, filename: str, repo_root):\n        \"\"\"\n        Initialisiert den File Dependency Graphen\n\n        Args:\n\n        \"\"\"\n        self.filename = filename\n        self.repo_root = repo_root\n\n    def _resolve_module_name(self, node: ImportFrom) -> list[str]:\n        \"\"\"\n        L\u00f6st relative Imports der Form `from .. import name1, name2` auf.\n        Liefert die Liste der tats\u00e4chlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\n        Wirft ImportError, wenn nichts aufgel\u00f6st werden konnte.\n        \"\"\"\n        level_depth = node.level\n        names = [alias.name for alias in node.names]  \n        all_files = get_all_temp_files(self.repo_root)  \n\n        # Suche die aktuelle Datei (self.filename -> modulname ohne .py oder auch Pfad)\n        candidates = [p for p in all_files if p.stem == Path(self.filename).stem or p.name == f\"{self.filename}.py\"]\n\n        if not candidates:\n            raise ImportError(f\"Kann aktuelle Datei '{self.filename}' im Repo nicht finden.\")\n\n        # W\u00e4hle die plausibelste Datei (flachster Pfad)\n        candidates.sort(key=lambda p: len(p.parts))\n        current_rel_path = candidates[0]  # z.B. package/subpkg/module.py\n\n        base_dir = current_rel_path.parent\n        if level_depth <= 0:\n            raise ImportError(\"Erwarteter relativer Import (level >= 1).\")\n        for _ in range(level_depth - 1):\n            if base_dir == base_dir.parent and len(base_dir.parts) == 0:\n                raise ImportError(f\"Relative Import-Ebene ({level_depth}) zu gro\u00df f\u00fcr Datei '{current_rel_path}'.\")\n            base_dir = base_dir.parent\n\n        repo_root_path = Path(self.repo_root).resolve()\n        resolved: list[str] = []\n\n        def module_file_exists(rel_base: Path, name: str) -> bool:\n            file_path = repo_root_path / rel_base / f\"{name}.py\"\n            pkg_init = repo_root_path / rel_base / name / \"__init__.py\"\n            return file_path.exists() or pkg_init.exists()\n\n        def init_exports_symbol(rel_base: Path, symbol: str) -> bool:\n            \"\"\"\n            Pr\u00fcft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\n            oder als Name (funktion/klasse/assign) definiert ist.\n            \"\"\"\n            init_path = repo_root_path / rel_base / \"__init__.py\"\n            if not init_path.exists():\n                return False\n            try:\n                src = init_path.read_text(encoding=\"utf-8\")\n                mod = parse(src, filename=str(init_path))\n            except Exception:\n                return False\n\n            for node_ in walk(mod):\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == \"__all__\":\n                            try:\n                                value = literal_eval(node_.value)\n                                if isinstance(value, (list, tuple)) and symbol in value:\n                                    return True\n                            except Exception:\n                                pass\n                if isinstance(node_, (FunctionDef, ClassDef)) and node_.name == symbol:\n                    return True\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == symbol:\n                            return True\n            return False\n\n        for name in names:\n            if not name.isidentifier() or iskeyword(name):\n                continue\n\n            if module_file_exists(base_dir, name):\n                resolved.append(name)\n                continue\n            if init_exports_symbol(base_dir, name):\n                resolved.append(name)\n                continue\n\n        resolved = sorted(set(resolved))\n\n        if not resolved:\n            raise ImportError(\n                f\"Kein passendes Modul/Symbol f\u00fcr relative Import-Aufl\u00f6sung gefunden \"\n                f\"(level={level_depth}, names={names}, base_dir={base_dir})\"\n            )\n\n        return resolved\n    \n    def visit_Import(self, node: Import | ImportFrom, base_name: str | None = None):\n        for alias in node.names:\n        \n            if self.filename not in self.import_dependencies:\n                self.import_dependencies[self.filename] = set()\n\n            if base_name:\n                self.import_dependencies[self.filename].add(base_name)\n            else:\n                self.import_dependencies[self.filename].add(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node: ImportFrom):\n        \"\"\"\n        Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\n        also c, und dieser wird als callee f\u00fcr den caller, das File, gesetzt.\n        \"\"\"\n        module_name = node.module\n        if module_name:\n            module_base = module_name.split(\".\")[-1]\n            self.visit_Import(node, module_base)\n        else:\n            try:\n                resolved = self._resolve_module_name(node)\n                for base in resolved:\n                    self.visit_Import(node, base)\n            except ImportError as e:\n                print(f\"Aufl\u00f6sung eines relativen Imports fehlgeschlagen: {e}\")\n\n        self.generic_visit(node)",
              "start_line": 23,
              "end_line": 154,
              "context": {
                "dependencies": [],
                "instantiated_by": [
                  {
                    "file": "File_Dependency.py",
                    "function": "build_file_dependency_graph",
                    "mode": "function",
                    "line": 159
                  }
                ],
                "method_context": [
                  {
                    "identifier": "backend.File_Dependency.FileDependencyGraph.__init__",
                    "name": "__init__",
                    "calls": [],
                    "called_by": [],
                    "args": [
                      "self",
                      "filename",
                      "repo_root"
                    ],
                    "docstring": "Initialisiert den File Dependency Graphen\n\nArgs:",
                    "start_line": 26,
                    "end_line": 34
                  },
                  {
                    "identifier": "backend.File_Dependency.FileDependencyGraph._resolve_module_name",
                    "name": "_resolve_module_name",
                    "calls": [
                      "backend/File_Dependency.py::FileDependencyGraph::ImportError",
                      "backend/File_Dependency.py::FileDependencyGraph::Path",
                      "backend/File_Dependency.py::FileDependencyGraph::get_all_temp_files",
                      "backend/File_Dependency.py::FileDependencyGraph::len",
                      "backend/File_Dependency.py::FileDependencyGraph::range",
                      "backend/File_Dependency.py::FileDependencyGraph::resolve",
                      "backend/File_Dependency.py::FileDependencyGraph::sort"
                    ],
                    "called_by": [
                      "backend/File_Dependency.py::FileDependencyGraph::visit_ImportFrom"
                    ],
                    "args": [
                      "self",
                      "node"
                    ],
                    "docstring": "L\u00f6st relative Imports der Form `from .. import name1, name2` auf.\nLiefert die Liste der tats\u00e4chlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\nWirft ImportError, wenn nichts aufgel\u00f6st werden konnte.",
                    "start_line": 36,
                    "end_line": 123
                  },
                  {
                    "identifier": "backend.File_Dependency.FileDependencyGraph.module_file_exists",
                    "name": "module_file_exists",
                    "calls": [
                      "backend/File_Dependency.py::FileDependencyGraph::exists"
                    ],
                    "called_by": [
                      "<backend/File_Dependency.py>"
                    ],
                    "args": [
                      "rel_base",
                      "name"
                    ],
                    "docstring": null,
                    "start_line": 67,
                    "end_line": 70
                  },
                  {
                    "identifier": "backend.File_Dependency.FileDependencyGraph.init_exports_symbol",
                    "name": "init_exports_symbol",
                    "calls": [
                      "backend/File_Dependency.py::FileDependencyGraph::exists",
                      "backend/File_Dependency.py::FileDependencyGraph::isinstance",
                      "backend/File_Dependency.py::FileDependencyGraph::literal_eval",
                      "backend/File_Dependency.py::FileDependencyGraph::parse",
                      "backend/File_Dependency.py::FileDependencyGraph::read_text",
                      "backend/File_Dependency.py::FileDependencyGraph::str",
                      "backend/File_Dependency.py::FileDependencyGraph::walk"
                    ],
                    "called_by": [
                      "<backend/File_Dependency.py>"
                    ],
                    "args": [
                      "rel_base",
                      "symbol"
                    ],
                    "docstring": "Pr\u00fcft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\noder als Name (funktion/klasse/assign) definiert ist.",
                    "start_line": 72,
                    "end_line": 102
                  },
                  {
                    "identifier": "backend.File_Dependency.FileDependencyGraph.visit_Import",
                    "name": "visit_Import",
                    "calls": [
                      "backend/File_Dependency.py::FileDependencyGraph::add",
                      "backend/File_Dependency.py::FileDependencyGraph::generic_visit",
                      "backend/File_Dependency.py::FileDependencyGraph::set"
                    ],
                    "called_by": [
                      "backend/File_Dependency.py::FileDependencyGraph::visit_ImportFrom"
                    ],
                    "args": [
                      "self",
                      "node",
                      "base_name"
                    ],
                    "docstring": null,
                    "start_line": 125,
                    "end_line": 135
                  },
                  {
                    "identifier": "backend.File_Dependency.FileDependencyGraph.visit_ImportFrom",
                    "name": "visit_ImportFrom",
                    "calls": [
                      "backend/File_Dependency.py::FileDependencyGraph::_resolve_module_name",
                      "backend/File_Dependency.py::FileDependencyGraph::generic_visit",
                      "backend/File_Dependency.py::FileDependencyGraph::print",
                      "backend/File_Dependency.py::FileDependencyGraph::split",
                      "backend/File_Dependency.py::FileDependencyGraph::visit_Import"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "node"
                    ],
                    "docstring": "Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\nalso c, und dieser wird als callee f\u00fcr den caller, das File, gesetzt.",
                    "start_line": 137,
                    "end_line": 154
                  }
                ]
              }
            }
          ]
        }
      },
      "backend/HelperLLM.py": {
        "ast_nodes": {
          "imports": [
            "os",
            "json",
            "logging",
            "time",
            "typing.List",
            "typing.Dict",
            "typing.Any",
            "typing.Optional",
            "typing.Union",
            "dotenv.load_dotenv",
            "langchain_google_genai.ChatGoogleGenerativeAI",
            "langchain_ollama.ChatOllama",
            "langchain_openai.ChatOpenAI",
            "langchain.messages.HumanMessage",
            "langchain.messages.SystemMessage",
            "langchain.messages.AIMessage",
            "pydantic.ValidationError",
            "schemas.types.FunctionAnalysis",
            "schemas.types.ClassAnalysis",
            "schemas.types.FunctionAnalysisInput",
            "schemas.types.FunctionContextInput",
            "schemas.types.ClassAnalysisInput",
            "schemas.types.ClassContextInput"
          ],
          "functions": [
            {
              "mode": "function_analysis",
              "identifier": "backend.HelperLLM.main_orchestrator",
              "name": "main_orchestrator",
              "args": [],
              "docstring": "Dummy Data and processing loop for testing the LLMHelper class.\nThis version is syntactically correct and logically matches the Pydantic models.",
              "source_code": "def main_orchestrator():\n    \"\"\"\n    Dummy Data and processing loop for testing the LLMHelper class.\n    This version is syntactically correct and logically matches the Pydantic models.\n    \"\"\"\n    \n    # --- Step 1: Define the pre-computed analysis for each method ---\n    \n    # Example Input 1: For the 'add_item' function\n    add_item_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"source_code\": \"\"\"def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\\\"\\\"\\\"\n        if not isinstance(quantity, int) or quantity <= 0:\n            raise ValueError(\"Quantity must be a positive integer.\")\n    \n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n        print(f\"Added {quantity} of {item_name}. New total: {self.inventory[item_name]}\")\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"process_shipment\", \"restock_api_endpoint\"]\n        }\n    })\n\n    # Example Input 2: For the 'check_stock' function\n    check_stock_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"source_code\": \"\"\"def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Retrieves the current stock quantity for a given item.\\\"\\\"\\\"\n        stock_level = self.inventory.get(item_name, 0)\n        return stock_level\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"fulfill_order\", \"ui_display_handler\"]\n        }\n    })\n\n    # Example Input 3: For the 'generate_report' function\n    generate_report_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"source_code\": \"\"\"def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a timestamped string summary of the current inventory state.\\\"\\\"\\\"\n        timestamp = datetime.now()\n        report_header = \"Inventory Report as of: {ts}\\\\n\".format(ts=timestamp)\n        report_lines = [report_header]\n        \n        for item, quantity in self.inventory.items():\n            line = \"- {name}: {qty}\".format(name=item, qty=quantity)\n            report_lines.append(line)\n            \n        return \"\\\\n\".join(report_lines)\"\"\",\n        \"imports\": [\"from datetime import datetime\"],\n        \"context\": {\n            \"calls\": [\"datetime.now\", \"str.format\"],\n            \"called_by\": [\"daily_cron_job\", \"admin_dashboard_export\"]\n        }\n    })\n\n    add_item_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"description\": {\n            \"overall\": \"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to add.\"},\n                {\"name\": \"quantity\", \"type\": \"int\", \"description\": \"The number of units to add. Must be a positive integer.\"}\n            ],\n            \"returns\": [],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"process_shipment and restock_api_endpoint\" }\n        },\n        \"error\": None\n    })\n\n    check_stock_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"description\": {\n            \"overall\": \"Retrieves the current stock quantity for a given item.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to check.\"}\n            ],\n            \"returns\": [\n                {\"name\": \"stock_level\", \"type\": \"int\", \"description\": \"The quantity of the item in stock. Returns 0 if the item is not found.\"}\n            ],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"fulfill_order and ui_display_handler\" }\n        },\n        \"error\": None\n    })\n\n    generate_report_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"description\": {\n            \"overall\": \"Generates a timestamped string summary of the current inventory state.\",\n            \"parameters\": [ {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"} ],\n            \"returns\": [\n                {\"name\": \"report\", \"type\": \"str\", \"description\": \"A formatted string detailing all items and their quantities.\"}\n            ],\n            \"usage_context\": { \"calls\": \"datetime.now and str.format\", \"called_by\": \"daily_cron_job and admin_dashboard_export\" }\n        },\n        \"error\": None\n    })\n\n    class_input = ClassAnalysisInput(\n        mode=\"class_analysis\",\n        identifier=\"InventoryManager\",\n        # NOTE: The indentation of this string is now fixed. It starts at column 0.\n        source_code=\"\"\"class InventoryManager:\n    \\\"\\\"\\\"Manages stock levels for products in a warehouse.\\\"\\\"\\\"\n    def __init__(self, warehouse_id: str):\n        self.warehouse_id = warehouse_id\n        self.inventory = {}  # item_name: quantity\n\n    def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds an item to the inventory.\\\"\\\"\\\"\n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n\n    def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Checks the stock of a specific item.\\\"\\\"\\\"\n        return self.inventory.get(item_name, 0)\n\n    def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a summary report of the inventory.\\\"\\\"\\\"\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        report_lines = [f\"Inventory Report for {self.warehouse_id} at {timestamp}\"]\n        if not self.inventory:\n            report_lines.append(\"Inventory is empty.\")\n        else:\n            for item, quantity in self.inventory.items():\n                report_lines.append(f\"- {item}: {quantity}\")\n        return \"\\\\n\".join(report_lines)\n\"\"\",\n        imports=[\"from datetime import datetime\"],\n        context=ClassContextInput(\n            dependencies=[\"datetime\"],\n            instantiated_by=[\"main_app_startup\", \"warehouse_worker_script\"],\n            methods_analysis=[\n                add_item_analysis,\n                check_stock_analysis,\n                generate_report_analysis\n            ]\n        )\n    )\n\n    \n    # The helper methods expect a LIST of inputs, so we wrap our single object in a list.\n    input = [add_item_input, check_stock_input, generate_report_input]\n    analysis = [add_item_analysis, check_stock_analysis, generate_report_analysis] \n\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    llm_helper = LLMHelper(api_key=GEMINI_API_KEY, function_prompt_path=function_prompt_file, class_prompt_path=class_prompt_file)\n\n    # This will be the final JSON containing all documentation\n    final_documentation = {}\n\n    # The analysis of the methods is already provided in the context.\n    logging.info(\"\\n--- Generating documentation for classes ---\")\n    \n    # The `generate_for_classes` method returns a list of results.\n    analysis_results = llm_helper.generate_for_functions(input)\n\n    # --- Step 4: Process the results ---\n    \n    # We loop through the list of results (even though there's only one in this case).\n    for doc in analysis_results:\n        if doc:\n            logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n            if \"classes\" not in final_documentation:\n                final_documentation[\"classes\"] = {}\n            # Use .model_dump() to convert the Pydantic object back to a dict for JSON serialization\n            final_documentation[\"classes\"][doc.identifier] = doc.model_dump() \n        else:\n            logging.warning(f\"Failed to generate doc for a class\")\n\n    # --- Step 5: Display the final aggregated result ---\n    logging.info(\"\\n--- Final Generated Documentation ---\")\n    print(json.dumps(final_documentation, indent=2))",
              "start_line": 205,
              "end_line": 391,
              "context": {
                "calls": [
                  "backend/HelperLLM.py::ClassAnalysisInput",
                  "backend/HelperLLM.py::ClassContextInput",
                  "backend/HelperLLM.py::LLMHelper",
                  "backend/HelperLLM.py::dumps",
                  "backend/HelperLLM.py::generate_for_functions",
                  "backend/HelperLLM.py::info",
                  "backend/HelperLLM.py::model_dump",
                  "backend/HelperLLM.py::model_validate",
                  "backend/HelperLLM.py::print",
                  "backend/HelperLLM.py::warning"
                ],
                "called_by": [
                  {
                    "file": "HelperLLM.py",
                    "function": "backend.HelperLLM",
                    "mode": "module",
                    "line": 397
                  }
                ]
              }
            }
          ],
          "classes": [
            {
              "mode": "class_analysis",
              "identifier": "backend.HelperLLM.LLMHelper",
              "name": "LLMHelper",
              "docstring": "A class to interact with Google Gemini for generating code snippet documentation.\nIt centralizes API interaction, error handling, and validates I/O using Pydantic.",
              "source_code": "class LLMHelper:\n    \"\"\"\n    A class to interact with Google Gemini for generating code snippet documentation.\n    It centralizes API interaction, error handling, and validates I/O using Pydantic.\n    \"\"\"\n    def __init__(self, api_key: str, function_prompt_path: str, class_prompt_path: str, model_name: str = \"gemini-2.0-flash-lite\", ollama_base_url: str = None):\n        if not api_key:\n            raise ValueError(\"Gemini API Key must be set.\")\n        \n        try:\n            with open(function_prompt_path, 'r', encoding='utf-8') as f:\n                self.function_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Function system prompt file not found at: {function_prompt_path}\")\n            raise\n\n        # Handle the second file\n        try:\n            with open(class_prompt_path, 'r', encoding='utf-8') as f:\n                self.class_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Class system prompt file not found at: {class_prompt_path}\")\n            raise\n        \n        # Batch-Size config\n        self.model_name = model_name\n        self._configure_batch_settings(model_name)\n\n        if model_name.startswith(\"gemini-\"):\n            base_llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=0.3, \n            )\n        \n        elif model_name.startswith(\"gpt-\"):\n            base_llm = ChatOpenAI(\n                model_name=model_name,\n                api_key=api_key,\n                temperature=0.3,\n            )\n\n        else:\n            target_url = ollama_base_url if ollama_base_url else OLLAMA_BASE_URL\n            base_llm = ChatOllama(\n                model=model_name,\n                temperature=0.3,\n                base_url=target_url,\n            )\n\n        self.function_llm = base_llm.with_structured_output(FunctionAnalysis, method=\"json_schema\")\n        self.class_llm = base_llm.with_structured_output(ClassAnalysis, method=\"json_schema\")\n\n        self.raw_llm = base_llm\n\n        logging.info(f\"LLMHelper initialized with model '{model_name}'. Batch Size: {self.batch_size}\")\n\n    def _configure_batch_settings(self, model_name: str):\n\n        if model_name == \"gemini-2.0-flash-lite\":\n            self.batch_size = 30\n        \n        elif model_name == \"gemini-flash-latest\":\n            self.batch_size = 10\n            \n        elif model_name == \"gemini-2.5-pro\":\n            self.batch_size = 2\n        \n        elif model_name == \"gemini-2.0-flash\":\n            self.batch_size = 15\n\n        elif model_name == \"gemini-2.5-flash-lite\":\n            self.batch_size = 15\n\n        elif model_name == \"llama3\":\n            self.batch_size = 50\n\n        elif model_name == \"gpt-5.1\":\n            self.batch_size = 500\n\n        elif model_name == \"gpt-5-mini\":\n            self.batch_size = 500\n            \n        else:\n            logging.warning(f\"Unknown model '{model_name}', using conservative defaults.\")\n            self.batch_size = 2\n\n    def generate_for_functions(self, function_inputs: List[FunctionAnalysisInput]) -> List[Optional[FunctionAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of functions.\"\"\"\n\n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 61\n\n        if not function_inputs:\n            return []\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(function_input.model_dump(), indent=2) \n            for function_input in function_inputs\n        ]\n       \n        conversations = [[SystemMessage(content=self.function_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_functions = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n        \n            try:\n                batch_results = self.function_llm.batch(batch_conversations)\n                all_validated_functions.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                all_validated_functions.extend([None] * len(batch_conversations))\n            \n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_functions\n        \n            \n    \n\n    def generate_for_classes(self, class_inputs: List[ClassAnalysisInput]) -> List[Optional[ClassAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of classes.\"\"\"\n        if not class_inputs:\n            return []\n        \n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 61\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(class_input.model_dump(), indent=2)\n            for class_input in class_inputs\n        ]\n\n        conversations = [[SystemMessage(content=self.class_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_classes = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n\n            try:\n                batch_results = self.class_llm.batch(batch_conversations)\n                all_validated_classes.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                # Falls ein Fehler auftritt, f\u00fcllen wir die Liste mit None auf, um die Reihenfolge zu wahren\n                all_validated_classes.extend([None] * len(batch_conversations))\n\n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_classes",
              "start_line": 30,
              "end_line": 199,
              "context": {
                "dependencies": [],
                "instantiated_by": [
                  {
                    "file": "HelperLLM.py",
                    "function": "main_orchestrator",
                    "mode": "function",
                    "line": 365
                  },
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 263
                  }
                ],
                "method_context": [
                  {
                    "identifier": "backend.HelperLLM.LLMHelper.__init__",
                    "name": "__init__",
                    "calls": [
                      "backend/HelperLLM.py::LLMHelper::ChatGoogleGenerativeAI",
                      "backend/HelperLLM.py::LLMHelper::ChatOllama",
                      "backend/HelperLLM.py::LLMHelper::ChatOpenAI",
                      "backend/HelperLLM.py::LLMHelper::ValueError",
                      "backend/HelperLLM.py::LLMHelper::_configure_batch_settings",
                      "backend/HelperLLM.py::LLMHelper::error",
                      "backend/HelperLLM.py::LLMHelper::info",
                      "backend/HelperLLM.py::LLMHelper::open",
                      "backend/HelperLLM.py::LLMHelper::read",
                      "backend/HelperLLM.py::LLMHelper::startswith",
                      "backend/HelperLLM.py::LLMHelper::with_structured_output"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "api_key",
                      "function_prompt_path",
                      "class_prompt_path",
                      "model_name",
                      "ollama_base_url"
                    ],
                    "docstring": null,
                    "start_line": 35,
                    "end_line": 85
                  },
                  {
                    "identifier": "backend.HelperLLM.LLMHelper._configure_batch_settings",
                    "name": "_configure_batch_settings",
                    "calls": [
                      "backend/HelperLLM.py::LLMHelper::warning"
                    ],
                    "called_by": [
                      "backend/HelperLLM.py::LLMHelper::__init__"
                    ],
                    "args": [
                      "self",
                      "model_name"
                    ],
                    "docstring": null,
                    "start_line": 87,
                    "end_line": 115
                  },
                  {
                    "identifier": "backend.HelperLLM.LLMHelper.generate_for_functions",
                    "name": "generate_for_functions",
                    "calls": [
                      "backend/HelperLLM.py::LLMHelper::HumanMessage",
                      "backend/HelperLLM.py::LLMHelper::SystemMessage",
                      "backend/HelperLLM.py::LLMHelper::batch",
                      "backend/HelperLLM.py::LLMHelper::dumps",
                      "backend/HelperLLM.py::LLMHelper::error",
                      "backend/HelperLLM.py::LLMHelper::extend",
                      "backend/HelperLLM.py::LLMHelper::info",
                      "backend/HelperLLM.py::LLMHelper::len",
                      "backend/HelperLLM.py::LLMHelper::min",
                      "backend/HelperLLM.py::LLMHelper::model_dump",
                      "backend/HelperLLM.py::LLMHelper::range",
                      "backend/HelperLLM.py::LLMHelper::sleep"
                    ],
                    "called_by": [
                      {
                        "file": "main.py",
                        "function": "main_workflow",
                        "mode": "function",
                        "line": 288
                      }
                    ],
                    "args": [
                      "self",
                      "function_inputs"
                    ],
                    "docstring": "Generates and validates documentation for a batch of functions.",
                    "start_line": 117,
                    "end_line": 156
                  },
                  {
                    "identifier": "backend.HelperLLM.LLMHelper.generate_for_classes",
                    "name": "generate_for_classes",
                    "calls": [
                      "backend/HelperLLM.py::LLMHelper::HumanMessage",
                      "backend/HelperLLM.py::LLMHelper::SystemMessage",
                      "backend/HelperLLM.py::LLMHelper::batch",
                      "backend/HelperLLM.py::LLMHelper::dumps",
                      "backend/HelperLLM.py::LLMHelper::error",
                      "backend/HelperLLM.py::LLMHelper::extend",
                      "backend/HelperLLM.py::LLMHelper::info",
                      "backend/HelperLLM.py::LLMHelper::len",
                      "backend/HelperLLM.py::LLMHelper::min",
                      "backend/HelperLLM.py::LLMHelper::model_dump",
                      "backend/HelperLLM.py::LLMHelper::range",
                      "backend/HelperLLM.py::LLMHelper::sleep"
                    ],
                    "called_by": [
                      {
                        "file": "main.py",
                        "function": "main_workflow",
                        "mode": "function",
                        "line": 319
                      }
                    ],
                    "args": [
                      "self",
                      "class_inputs"
                    ],
                    "docstring": "Generates and validates documentation for a batch of classes.",
                    "start_line": 161,
                    "end_line": 199
                  }
                ]
              }
            }
          ]
        }
      },
      "backend/MainLLM.py": {
        "ast_nodes": {
          "imports": [
            "os",
            "logging",
            "sys",
            "dotenv.load_dotenv",
            "langchain_google_genai.ChatGoogleGenerativeAI",
            "langchain_ollama.ChatOllama",
            "langchain_openai.ChatOpenAI",
            "langchain.messages.HumanMessage",
            "langchain.messages.SystemMessage"
          ],
          "functions": [],
          "classes": [
            {
              "mode": "class_analysis",
              "identifier": "backend.MainLLM.MainLLM",
              "name": "MainLLM",
              "docstring": "Hauptklasse f\u00fcr die Interaktion mit dem LLM.",
              "source_code": "class MainLLM:\n    \"\"\"\n    Hauptklasse f\u00fcr die Interaktion mit dem LLM.\n    \"\"\"\n    def __init__(self, api_key: str, prompt_file_path: str, model_name: str = \"gemini-2.5-pro\", ollama_base_url: str = None):\n        if not api_key:\n            raise ValueError(\"Gemini API Key must be set.\")\n        \n        try:\n            with open(prompt_file_path, 'r', encoding='utf-8') as f:\n                self.system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"System prompt file not found at: {prompt_file_path}\")\n            raise\n        \n        self.model_name = model_name\n\n        if model_name.startswith(\"gemini-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n\n        elif model_name.startswith(\"gpt-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n\n        else:\n            target_url = ollama_base_url if ollama_base_url else OLLAMA_BASE_URL\n            self.llm = ChatOllama(\n                model=model_name,\n                temperature=1.0,\n                base_url=target_url,\n            )\n\n        logging.info(f\"Main LLM initialized with model '{model_name}'.\")\n    \n    def call_llm(self, user_input: str):\n\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with HelperLLM input...\")\n\n        try:\n            response = self.llm.invoke(messages)\n            logging.info(\"LLM call successful.\")\n            return response.content\n        except Exception as e:\n            logging.error(f\"Error during LLM call: {e}\")\n            return None\n\n    def stream_llm(self, user_input: str):\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with 'stream'...\")\n\n        try:\n            stream_iterator = self.llm.stream(messages)\n            \n            for chunk in stream_iterator:\n                yield chunk.content\n        except Exception as e:\n            error_message = f\"\\n--- Error during LLM stream call: {e} ---\"\n            logging.error(error_message)\n            yield error_message",
              "start_line": 20,
              "end_line": 92,
              "context": {
                "dependencies": [],
                "instantiated_by": [
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 377
                  }
                ],
                "method_context": [
                  {
                    "identifier": "backend.MainLLM.MainLLM.__init__",
                    "name": "__init__",
                    "calls": [
                      "backend/MainLLM.py::MainLLM::ChatGoogleGenerativeAI",
                      "backend/MainLLM.py::MainLLM::ChatOllama",
                      "backend/MainLLM.py::MainLLM::ValueError",
                      "backend/MainLLM.py::MainLLM::error",
                      "backend/MainLLM.py::MainLLM::info",
                      "backend/MainLLM.py::MainLLM::open",
                      "backend/MainLLM.py::MainLLM::read",
                      "backend/MainLLM.py::MainLLM::startswith"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "api_key",
                      "prompt_file_path",
                      "model_name",
                      "ollama_base_url"
                    ],
                    "docstring": null,
                    "start_line": 24,
                    "end_line": 59
                  },
                  {
                    "identifier": "backend.MainLLM.MainLLM.call_llm",
                    "name": "call_llm",
                    "calls": [
                      "backend/MainLLM.py::MainLLM::HumanMessage",
                      "backend/MainLLM.py::MainLLM::SystemMessage",
                      "backend/MainLLM.py::MainLLM::error",
                      "backend/MainLLM.py::MainLLM::info",
                      "backend/MainLLM.py::MainLLM::invoke"
                    ],
                    "called_by": [
                      {
                        "file": "main.py",
                        "function": "main_workflow",
                        "mode": "function",
                        "line": 396
                      }
                    ],
                    "args": [
                      "self",
                      "user_input"
                    ],
                    "docstring": null,
                    "start_line": 61,
                    "end_line": 75
                  },
                  {
                    "identifier": "backend.MainLLM.MainLLM.stream_llm",
                    "name": "stream_llm",
                    "calls": [
                      "backend/MainLLM.py::MainLLM::HumanMessage",
                      "backend/MainLLM.py::MainLLM::SystemMessage",
                      "backend/MainLLM.py::MainLLM::error",
                      "backend/MainLLM.py::MainLLM::info",
                      "backend/MainLLM.py::MainLLM::stream"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "user_input"
                    ],
                    "docstring": null,
                    "start_line": 77,
                    "end_line": 92
                  }
                ]
              }
            }
          ]
        }
      },
      "backend/basic_info.py": {
        "ast_nodes": {
          "imports": [
            "re",
            "os",
            "tomllib",
            "typing.List",
            "typing.Dict",
            "typing.Any",
            "typing.Optional"
          ],
          "functions": [],
          "classes": [
            {
              "mode": "class_analysis",
              "identifier": "backend.basic_info.ProjektInfoExtractor",
              "name": "ProjektInfoExtractor",
              "docstring": "Extrahiert grundlegende Projektinformationen aus g\u00e4ngigen Projektdateien\nwie README, pyproject.toml und requirements.txt.",
              "source_code": "class ProjektInfoExtractor:\n    \"\"\"\n    Extrahiert grundlegende Projektinformationen aus g\u00e4ngigen Projektdateien\n    wie README, pyproject.toml und requirements.txt.\n    \"\"\"\n    def __init__(self):\n        self.INFO_NICHT_GEFUNDEN = \"Information not found\"\n        # Initialisiert die Struktur mit Platzhaltern\n        self.info = {\n            \"projekt_uebersicht\": {\n                \"titel\": self.INFO_NICHT_GEFUNDEN,\n                \"beschreibung\": self.INFO_NICHT_GEFUNDEN,\n                \"aktueller_status\": self.INFO_NICHT_GEFUNDEN,\n                \"key_features\": self.INFO_NICHT_GEFUNDEN,\n                \"tech_stack\": self.INFO_NICHT_GEFUNDEN,\n            },\n            \"installation\": {\n                \"dependencies\": self.INFO_NICHT_GEFUNDEN,\n                \"setup_anleitung\": self.INFO_NICHT_GEFUNDEN,\n                \"quick_start_guide\": self.INFO_NICHT_GEFUNDEN,\n            }\n        }\n\n    def _finde_datei(self, patterns: List[str], dateien: List[Any]) -> Optional[Any]:\n        \"\"\"Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht.\"\"\"\n        for datei in dateien:\n            for pattern in patterns:\n                if datei.path.lower().endswith(pattern.lower()):\n                    return datei\n        return None\n\n    def _extrahiere_sektion_aus_markdown(self, inhalt: str, keywords: List[str]) -> Optional[str]:\n        \"\"\"\n        Extrahiert den Text unter einer Markdown-\u00dcberschrift (##).\n        \n        Args:\n            inhalt (str): Der gesamte Markdown-Text.\n            keywords (list): Eine Liste von alternativen Schl\u00fcsselw\u00f6rtern f\u00fcr den Titel \n                             der Sektion (z.B. [\"Installation\", \"Setup\"]).\n        \n        Returns:\n            str: Der extrahierte Textabschnitt oder None.\n        \"\"\"\n        # Erstellt ein Regex-Pattern, das auf jedes der Schl\u00fcsselw\u00f6rter reagiert\n        keyword_pattern = \"|\".join(re.escape(k) for k in keywords)\n        \n        # Sucht nach \"## Schl\u00fcsselwort\" und erfasst alles bis zur n\u00e4chsten \"##\" oder dem Dateiende\n        pattern = re.compile(\n            rf\"##\\s*({keyword_pattern})\\s*\\n(.*?)(?=\\n##|\\Z)\",\n            re.IGNORECASE | re.DOTALL\n        )\n        match = pattern.search(inhalt)\n        if match:\n            return match.group(2).strip()\n        return None\n\n    def _parse_readme(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer README-Datei.\"\"\"\n        if self.info[\"projekt_uebersicht\"][\"titel\"] == self.INFO_NICHT_GEFUNDEN:\n            title_match = re.search(r\"^\\s*#\\s*(.*)\", inhalt)\n            if title_match:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = title_match.group(1).strip()\n\n        # Beschreibung (Fallback)\n        # Nimmt den Text nach dem Titel bis zur n\u00e4chsten \u00dcberschrift\n        if self.info[\"projekt_uebersicht\"][\"beschreibung\"] == self.INFO_NICHT_GEFUNDEN:\n            desc_match = re.search(r\"^\\s*#\\s*.*\\n+([^#\\n].*)\", inhalt, re.DOTALL)\n            if desc_match:\n                 self.info[\"projekt_uebersicht\"][\"beschreibung\"] = desc_match.group(1).strip().split('\\n\\n')[0]\n\n\n        # Key Features\n        features = self._extrahiere_sektion_aus_markdown(inhalt, [\"Features\", \"Key Features\", \"Merkmale\"])\n        if features:\n            self.info[\"projekt_uebersicht\"][\"key_features\"] = features\n            \n        # Tech Stack\n        tech_stack = self._extrahiere_sektion_aus_markdown(inhalt, [\"Tech Stack\", \"Technology\", \"Technologien\"])\n        if tech_stack:\n            self.info[\"projekt_uebersicht\"][\"tech_stack\"] = tech_stack\n            \n        # Status\n        status = self._extrahiere_sektion_aus_markdown(inhalt, [\"Status\", \"Current Status\"])\n        if status:\n            self.info[\"projekt_uebersicht\"][\"aktueller_status\"] = status\n\n        # Setup-Anleitung\n        setup = self._extrahiere_sektion_aus_markdown(inhalt, [\"Installation\", \"Setup\", \"Getting Started\"])\n        if setup:\n            self.info[\"installation\"][\"setup_anleitung\"] = setup\n            \n        # Quick Start\n        quick_start = self._extrahiere_sektion_aus_markdown(inhalt, [\"Quick Start\", \"Schnellstart\"])\n        if quick_start:\n            self.info[\"installation\"][\"quick_start_guide\"] = quick_start\n\n    def _parse_toml(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer pyproject.toml-Datei.\"\"\"\n        if not tomllib:\n            print(\"Warnung: 'tomli' ist nicht installiert. pyproject.toml kann nicht analysiert werden.\")\n            return\n            \n        try:\n            data = tomllib.loads(inhalt)\n            project_data = data.get(\"project\", {})\n            \n            if \"name\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = project_data[\"name\"]\n            if \"description\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"beschreibung\"] = project_data[\"description\"]\n            if \"dependencies\" in project_data:\n                # \u00dcberschreibt 'dependencies' immer, da toml als prim\u00e4re Quelle gilt\n                self.info[\"installation\"][\"dependencies\"] = project_data[\"dependencies\"]\n        except tomllib.TOMLDecodeError as e:\n            print(f\"Warnung: Fehler beim Parsen der pyproject.toml: {e}\")\n\n    def _parse_requirements(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer requirements.txt-Datei.\"\"\"\n        # Nur f\u00fcllen, wenn noch keine Dependencies aus toml gefunden wurden\n        if self.info[\"installation\"][\"dependencies\"] == self.INFO_NICHT_GEFUNDEN:\n            lines = inhalt.splitlines()\n            dependencies = [\n                line.strip() for line in lines \n                if line.strip() and not line.strip().startswith('#')\n            ]\n            if dependencies:\n                self.info[\"installation\"][\"dependencies\"] = dependencies\n\n    def extrahiere_info(self, dateien: List[Any], repo_url: str) -> Dict[str, Any]:\n        \"\"\"\n        Orchestriert die Extraktion von Informationen aus einer Liste von RepoFile-Objekten.\n        \n        Die Reihenfolge der Verarbeitung ist wichtig, um Priorit\u00e4ten zu setzen:\n        1. pyproject.toml (h\u00f6chste Priorit\u00e4t f\u00fcr Metadaten)\n        2. requirements.txt (Fallback f\u00fcr Dependencies)\n        3. README (f\u00fcr beschreibende Texte und als Fallback)\n        4. Titel wird am Ende basierend auf der URL \u00fcberschrieben.\n        \"\"\"\n        # 1. Relevante Dateien finden\n        readme_datei = self._finde_datei([\"readme.md\", \"readme.rst\", \"readme.txt\", \"readme\"], dateien)\n        toml_datei = self._finde_datei([\"pyproject.toml\"], dateien)\n        req_datei = self._finde_datei([\"requirements.txt\"], dateien)\n\n        # 2. Dateien parsen (mit Priorisierung)\n        if toml_datei:\n            self._parse_toml(toml_datei.content)\n\n        if req_datei:\n            self._parse_requirements(req_datei.content)\n            \n        if readme_datei:\n            self._parse_readme(readme_datei.content)\n            \n        # 3. Finale Formatierung der Dependencies\n        deps = self.info[\"installation\"][\"dependencies\"]\n        if isinstance(deps, list):\n            if not deps:\n                self.info[\"installation\"][\"dependencies\"] = self.INFO_NICHT_GEFUNDEN\n            else:\n                self.info[\"installation\"][\"dependencies\"] = \"\\n\".join(f\"- {dep}\" for dep in deps)\n        \n        repo_name = os.path.basename(repo_url.removesuffix('.git'))\n        self.info[\"projekt_uebersicht\"][\"titel\"] = f\"{repo_name} documentation\"\n\n        return self.info",
              "start_line": 8,
              "end_line": 172,
              "context": {
                "dependencies": [],
                "instantiated_by": [
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 139
                  }
                ],
                "method_context": [
                  {
                    "identifier": "backend.basic_info.ProjektInfoExtractor.__init__",
                    "name": "__init__",
                    "calls": [],
                    "called_by": [],
                    "args": [
                      "self"
                    ],
                    "docstring": null,
                    "start_line": 13,
                    "end_line": 29
                  },
                  {
                    "identifier": "backend.basic_info.ProjektInfoExtractor._finde_datei",
                    "name": "_finde_datei",
                    "calls": [
                      "backend/basic_info.py::ProjektInfoExtractor::endswith",
                      "backend/basic_info.py::ProjektInfoExtractor::lower"
                    ],
                    "called_by": [
                      "backend/basic_info.py::ProjektInfoExtractor::extrahiere_info"
                    ],
                    "args": [
                      "self",
                      "patterns",
                      "dateien"
                    ],
                    "docstring": "Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht.",
                    "start_line": 31,
                    "end_line": 37
                  },
                  {
                    "identifier": "backend.basic_info.ProjektInfoExtractor._extrahiere_sektion_aus_markdown",
                    "name": "_extrahiere_sektion_aus_markdown",
                    "calls": [
                      "backend/basic_info.py::ProjektInfoExtractor::compile",
                      "backend/basic_info.py::ProjektInfoExtractor::escape",
                      "backend/basic_info.py::ProjektInfoExtractor::group",
                      "backend/basic_info.py::ProjektInfoExtractor::join",
                      "backend/basic_info.py::ProjektInfoExtractor::search",
                      "backend/basic_info.py::ProjektInfoExtractor::strip"
                    ],
                    "called_by": [
                      "backend/basic_info.py::ProjektInfoExtractor::_parse_readme"
                    ],
                    "args": [
                      "self",
                      "inhalt",
                      "keywords"
                    ],
                    "docstring": "Extrahiert den Text unter einer Markdown-\u00dcberschrift (##).\n\nArgs:\n    inhalt (str): Der gesamte Markdown-Text.\n    keywords (list): Eine Liste von alternativen Schl\u00fcsselw\u00f6rtern f\u00fcr den Titel \n                     der Sektion (z.B. [\"Installation\", \"Setup\"]).\n\nReturns:\n    str: Der extrahierte Textabschnitt oder None.",
                    "start_line": 39,
                    "end_line": 62
                  },
                  {
                    "identifier": "backend.basic_info.ProjektInfoExtractor._parse_readme",
                    "name": "_parse_readme",
                    "calls": [
                      "backend/basic_info.py::ProjektInfoExtractor::_extrahiere_sektion_aus_markdown",
                      "backend/basic_info.py::ProjektInfoExtractor::group",
                      "backend/basic_info.py::ProjektInfoExtractor::search",
                      "backend/basic_info.py::ProjektInfoExtractor::split",
                      "backend/basic_info.py::ProjektInfoExtractor::strip"
                    ],
                    "called_by": [
                      "backend/basic_info.py::ProjektInfoExtractor::extrahiere_info"
                    ],
                    "args": [
                      "self",
                      "inhalt"
                    ],
                    "docstring": "Parst den Inhalt einer README-Datei.",
                    "start_line": 64,
                    "end_line": 102
                  },
                  {
                    "identifier": "backend.basic_info.ProjektInfoExtractor._parse_toml",
                    "name": "_parse_toml",
                    "calls": [
                      "backend/basic_info.py::ProjektInfoExtractor::get",
                      "backend/basic_info.py::ProjektInfoExtractor::loads",
                      "backend/basic_info.py::ProjektInfoExtractor::print"
                    ],
                    "called_by": [
                      "backend/basic_info.py::ProjektInfoExtractor::extrahiere_info"
                    ],
                    "args": [
                      "self",
                      "inhalt"
                    ],
                    "docstring": "Parst den Inhalt einer pyproject.toml-Datei.",
                    "start_line": 104,
                    "end_line": 122
                  },
                  {
                    "identifier": "backend.basic_info.ProjektInfoExtractor._parse_requirements",
                    "name": "_parse_requirements",
                    "calls": [
                      "backend/basic_info.py::ProjektInfoExtractor::splitlines",
                      "backend/basic_info.py::ProjektInfoExtractor::startswith",
                      "backend/basic_info.py::ProjektInfoExtractor::strip"
                    ],
                    "called_by": [
                      "backend/basic_info.py::ProjektInfoExtractor::extrahiere_info"
                    ],
                    "args": [
                      "self",
                      "inhalt"
                    ],
                    "docstring": "Parst den Inhalt einer requirements.txt-Datei.",
                    "start_line": 124,
                    "end_line": 134
                  },
                  {
                    "identifier": "backend.basic_info.ProjektInfoExtractor.extrahiere_info",
                    "name": "extrahiere_info",
                    "calls": [
                      "backend/basic_info.py::ProjektInfoExtractor::_finde_datei",
                      "backend/basic_info.py::ProjektInfoExtractor::_parse_readme",
                      "backend/basic_info.py::ProjektInfoExtractor::_parse_requirements",
                      "backend/basic_info.py::ProjektInfoExtractor::_parse_toml",
                      "backend/basic_info.py::ProjektInfoExtractor::basename",
                      "backend/basic_info.py::ProjektInfoExtractor::isinstance",
                      "backend/basic_info.py::ProjektInfoExtractor::join",
                      "backend/basic_info.py::ProjektInfoExtractor::removesuffix"
                    ],
                    "called_by": [
                      {
                        "file": "main.py",
                        "function": "main_workflow",
                        "mode": "function",
                        "line": 140
                      }
                    ],
                    "args": [
                      "self",
                      "dateien",
                      "repo_url"
                    ],
                    "docstring": "Orchestriert die Extraktion von Informationen aus einer Liste von RepoFile-Objekten.\n\nDie Reihenfolge der Verarbeitung ist wichtig, um Priorit\u00e4ten zu setzen:\n1. pyproject.toml (h\u00f6chste Priorit\u00e4t f\u00fcr Metadaten)\n2. requirements.txt (Fallback f\u00fcr Dependencies)\n3. README (f\u00fcr beschreibende Texte und als Fallback)\n4. Titel wird am Ende basierend auf der URL \u00fcberschrieben.",
                    "start_line": 136,
                    "end_line": 172
                  }
                ]
              }
            }
          ]
        }
      },
      "backend/callgraph.py": {
        "ast_nodes": {
          "imports": [
            "ast",
            "networkx",
            "os",
            "typing.Dict",
            "getRepo.RepoFile",
            "getRepo.GitRepository",
            "getRepo.GitRepository",
            "basic_info.ProjektInfoExtractor",
            "os"
          ],
          "functions": [
            {
              "mode": "function_analysis",
              "identifier": "backend.callgraph.build_callGraph",
              "name": "build_callGraph",
              "args": [
                "tree",
                "filename"
              ],
              "docstring": "Erstellt einen Call-Graphen aus einem gegebenen Python-AST.\n\nDer Graph ist ein gerichteter Graph (networkx.DiGraph), in dem:\n  - Knoten: Funktionen, Methoden und der globale Scope (<module>) bzw. <main_block> f\u00fcr `if __name__ == \"__main__\"`-Code\n  - Kanten: Funktions-/Methodenaufrufe zwischen diesen Knoten\n\nArgs:\n    tree (ast.AST): Der AST der zu analysierenden Python-Datei.\n    filename (str, optional): Der Name der analysierten Datei, z. B. `\"main.py\"` oder `\"src/utils.py\"`.\n\nReturns:\n    nx.DiGraph: Der vollst\u00e4ndige Call-Graph.",
              "source_code": "def build_callGraph(tree: ast.AST, filename: str) -> nx.DiGraph:\n    \"\"\" \n    Erstellt einen Call-Graphen aus einem gegebenen Python-AST.\n\n    Der Graph ist ein gerichteter Graph (networkx.DiGraph), in dem:\n      - Knoten: Funktionen, Methoden und der globale Scope (<module>) bzw. <main_block> f\u00fcr `if __name__ == \"__main__\"`-Code\n      - Kanten: Funktions-/Methodenaufrufe zwischen diesen Knoten\n\n    Args:\n        tree (ast.AST): Der AST der zu analysierenden Python-Datei.\n        filename (str, optional): Der Name der analysierten Datei, z. B. `\"main.py\"` oder `\"src/utils.py\"`.\n\n    Returns:\n        nx.DiGraph: Der vollst\u00e4ndige Call-Graph.\n    \"\"\"\n    visitor = CallGraph(filename)\n    visitor.visit(tree)\n    graph = visitor.graph\n\n# add all edges from dictionary to the graph at once\n    for caller, callees in visitor.edges.items():\n        for callee in callees:\n            # if callee in visitor.function_set:\n                graph.add_edge(caller, callee)\n\n    return graph",
              "start_line": 150,
              "end_line": 175,
              "context": {
                "calls": [
                  "backend/callgraph.py::CallGraph",
                  "backend/callgraph.py::add_edge",
                  "backend/callgraph.py::items",
                  "backend/callgraph.py::visit"
                ],
                "called_by": [
                  {
                    "file": "AST_Schema.py",
                    "function": "analyze_repository",
                    "mode": "method",
                    "line": 183
                  },
                  {
                    "file": "callgraph.py",
                    "function": "build_global_callgraph",
                    "mode": "function",
                    "line": 211
                  }
                ]
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "backend.callgraph.graph_to_adj_list",
              "name": "graph_to_adj_list",
              "args": [
                "graph"
              ],
              "docstring": "Konvertiert einen networkx.DiGraph in eine Adjazenzliste (Dict),\ndie JSON-serialisierbar ist.\n\nArgs:\n    graph (nx.DiGraph): Der zu konvertierende Call-Graph.\n\nReturns:\n    Dict[str, List[str]]: Eine Adjazenzliste, bei der jeder Schl\u00fcssel\n                          ein aufrufender Knoten (caller) und der Wert\n                          eine Liste der aufgerufenen Knoten (callees) ist.",
              "source_code": "def graph_to_adj_list(graph: nx.DiGraph) -> Dict[str, list[str]]:\n    \"\"\"\n    Konvertiert einen networkx.DiGraph in eine Adjazenzliste (Dict),\n    die JSON-serialisierbar ist.\n\n    Args:\n        graph (nx.DiGraph): Der zu konvertierende Call-Graph.\n\n    Returns:\n        Dict[str, List[str]]: Eine Adjazenzliste, bei der jeder Schl\u00fcssel\n                              ein aufrufender Knoten (caller) und der Wert\n                              eine Liste der aufgerufenen Knoten (callees) ist.\n    \"\"\"\n    adj_list = {}\n    # Wir sortieren die Knoten f\u00fcr eine konsistente Ausgabe\n    for node in sorted(list(graph.nodes())):\n        # Wir holen alle Nachfolger (aufgerufene Funktionen) und sortieren sie ebenfalls\n        successors = sorted(list(graph.successors(node)))\n        if successors:  # Nur Knoten aufnehmen, die auch wirklich andere aufrufen\n            adj_list[node] = successors\n    return adj_list",
              "start_line": 178,
              "end_line": 198,
              "context": {
                "calls": [
                  "backend/callgraph.py::list",
                  "backend/callgraph.py::nodes",
                  "backend/callgraph.py::sorted",
                  "backend/callgraph.py::successors"
                ],
                "called_by": []
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "backend.callgraph.build_global_callgraph",
              "name": "build_global_callgraph",
              "args": [
                "repo"
              ],
              "docstring": null,
              "source_code": "def build_global_callgraph(repo: GitRepository)-> nx.DiGraph:\n    all_files = repository.get_all_files()\n    global_graph = nx.DiGraph()\n\n    for file in all_files: \n        if not file.path.endswith(\".py\"):\n            continue\n        filename = str(os.path.basename(file.path)).removesuffix(\".py\")\n        tree = ast.parse(file.content)\n        graph = build_callGraph(tree, filename)\n        \n        for node in graph.nodes:\n            global_graph.add_node(node)\n            \n        for caller, callee in graph.edges:\n            if callee:\n                global_graph.add_node(callee)\n                global_graph.add_edge(caller, callee)\n    \n    return global_graph",
              "start_line": 202,
              "end_line": 221,
              "context": {
                "calls": [
                  "backend/callgraph.py::DiGraph",
                  "backend/callgraph.py::add_edge",
                  "backend/callgraph.py::add_node",
                  "backend/callgraph.py::basename",
                  "backend/callgraph.py::build_callGraph",
                  "backend/callgraph.py::endswith",
                  "backend/callgraph.py::get_all_files",
                  "backend/callgraph.py::parse",
                  "backend/callgraph.py::removesuffix",
                  "backend/callgraph.py::str"
                ],
                "called_by": [
                  {
                    "file": "callgraph.py",
                    "function": "backend.callgraph",
                    "mode": "module",
                    "line": 262
                  }
                ]
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "backend.callgraph.make_safe_dot",
              "name": "make_safe_dot",
              "args": [
                "graph",
                "out_path"
              ],
              "docstring": null,
              "source_code": "def make_safe_dot(graph: nx.DiGraph, out_path: str):\n    mapping = {}\n    H = graph.copy()\n    for i, n in enumerate(list(graph.nodes())):\n        safe = f\"n{i}\"           \n        mapping[n] = safe\n\n    H = nx.relabel_nodes(H, mapping)\n    for orig, safe in mapping.items():\n        H.nodes[safe][\"label\"] = orig\n\n    nx.drawing.nx_pydot.write_dot(H, out_path)",
              "start_line": 225,
              "end_line": 236,
              "context": {
                "calls": [
                  "backend/callgraph.py::copy",
                  "backend/callgraph.py::enumerate",
                  "backend/callgraph.py::items",
                  "backend/callgraph.py::list",
                  "backend/callgraph.py::nodes",
                  "backend/callgraph.py::relabel_nodes",
                  "backend/callgraph.py::write_dot"
                ],
                "called_by": [
                  {
                    "file": "callgraph.py",
                    "function": "backend.callgraph",
                    "mode": "module",
                    "line": 263
                  }
                ]
              }
            }
          ],
          "classes": [
            {
              "mode": "class_analysis",
              "identifier": "backend.callgraph.CallGraph",
              "name": "CallGraph",
              "docstring": "Visitor, der Funktionsaufrufe im AST sammelt und Kanten f\u00fcr den Call-Graph erstellt.",
              "source_code": "class CallGraph(ast.NodeVisitor):\n    \"\"\"\n    Visitor, der Funktionsaufrufe im AST sammelt und Kanten f\u00fcr den Call-Graph erstellt.\n    \"\"\"\n    def __init__(self, filename: str):\n        \"\"\"\n        Initialisiert den Visitor mit einem Platzhalter f\u00fcr die aktuelle Funktion.\n        \"\"\"\n        self.filename = filename\n        self.current_function = None\n        self.current_class = None\n\n        self.graph = nx.DiGraph()\n        self.import_mapping: dict[str, str] = {}\n        self.function_set: set[str] = set()\n        self.edges: Dict[str, set[str]] = {}\n\n    def _recursive_call(self, node) -> list[str]:\n        all_calls = []\n        if isinstance(node, ast.Call):\n            return self._recursive_call(node.func)\n        elif isinstance(node, ast.Name):\n            all_calls.append(node.id)\n            return all_calls\n        elif isinstance(node, ast.Attribute):\n            all_calls.append(node.attr)\n            return all_calls\n        return all_calls\n            \n    def _resolve_all_callee_names(self, callee_nodes: list[str]) -> list[str]:\n        resolved_callees = []\n        for raw_callee in callee_nodes:\n            if not self.current_class:\n                resolved_callee = f\"{self.filename}::{raw_callee}\"\n            else:\n                resolved_callee = f\"{self.filename}::{self.current_class}::{raw_callee}\"\n            resolved_callees.append(resolved_callee)\n        return resolved_callees\n\n    def _make_full_name(self, basename: str, class_name: str | None = None)-> str:\n        if class_name:\n            return f\"{self.filename}::{class_name}::{basename}\"\n        return f\"{self.filename}::{basename}\"\n    \n    def _current_caller(self)-> str:\n        if self.current_function:\n            return self.current_function\n        return f\"<{self.filename}>\" if self.filename else\"<global-scope>\"\n\n    # def visit_Import(self, node):\n    #     for alias in node.names:\n    #         module_name = alias.name\n    #         module_asname = (alias.asname if alias.asname else module_name)\n    #         self.import_mapping[module_asname] = module_name\n    #     self.generic_visit(node)\n\n    # def visit_ImportFrom(self, node):\n    #     module_name = node.module\n    #     level_depth = node.level\n        \n    #     module_base = module_name.split(\".\")[0]\n    #     for alias in node.names:\n    #         import_name = (alias.asname if alias.asname else alias.name)\n    #         self.import_mapping[import_name] = module_base\n    #     # TODO: level depth und relative import From resolven\n\n    def visit_ClassDef(self, node: ast.ClassDef):\n        prev_class = self.current_class\n        self.current_class = node.name\n        for function in node.body:\n            self.visit(function)\n        self.current_class = prev_class\n        \n    def visit_FunctionDef(self, node):\n        \"\"\" \n        Besucht eine normale Funktionsdefinition.\n\n        Setzt `self.current_function`, erstellt den Knoten im Graphen und \n        traversiert rekursiv den Funktionsk\u00f6rper.\n        \"\"\"\n        if self.current_class:\n            self.current_function = self._make_full_name(node.name, class_name=self.current_class)\n        else:\n            self.current_function = self._make_full_name(node.name) \n        self.graph.add_node(self.current_function)\n        self.generic_visit(node)\n        self.function_set.add(self.current_function)\n        self.current_function = None\n\n    def visit_AsyncFunctionDef(self, node):\n        \"\"\"\n        Besucht eine asynchrone Funktionsdefinition (`async def`).\n\n        Funktioniert analog zu `visit_FunctionDef`.\n        \"\"\"\n        self.visit_FunctionDef(node)\n\n    def visit_Call(self, node):\n        \"\"\"\n        Besucht einen Funktions- oder Methodenaufruf und behandelt komplexe F\u00e4lle\n        mit einer detaillierten Warnung, anstatt abzust\u00fcrzen.\n        \"\"\"             \n        caller = self._current_caller()\n        raw_callees: list[str] = []\n        try:\n            # benutze die Helferfunktion, um Namen zu extrahieren\n            raw_callees = self._recursive_call(node)\n            # falls _recursive_call ein leeres array oder None zur\u00fcckgibt, sicherstellen, dass es list ist\n            if raw_callees is None:\n                raw_callees = []\n\n            resolved_callees = self._resolve_all_callee_names(raw_callees)\n\n            # sicherstellen, dass caller als Key existiert und ein Set ist\n            if caller not in self.edges:\n                self.edges[caller] = set()\n\n            for resolved_callee in resolved_callees:\n                if resolved_callee:\n                    self.edges[caller].add(resolved_callee)\n        except Exception as e:\n            print(f\"Unerwarteter Fehler bei der Verarbeitung eines Funktionsaufrufs: {e} in Datei {self.filename}\")\n        self.generic_visit(node)\n\n    def visit_If(self, node):\n        \"\"\"\n        Pr\u00fcft auf `if __name__ == \"__main__\"`-Bl\u00f6cke, um globale Aufrufe innerhalb\n        dieses Blocks separat als <main_block> im Call-Graphen darzustellen.\n\n        Alle Funktionsaufrufe innerhalb des Blocks werden dann von <main_block> aus\n        als Caller-Knoten erfasst.\n        \"\"\"\n        if (isinstance(node.test, ast.Compare) and\n            isinstance(node.test.left, ast.Name) and \n            node.test.left.id == \"__name__\"):\n            caller_backup = self.current_function\n            self.current_function = \"<main_block>\"\n            self.generic_visit(node)\n            self.current_function = caller_backup\n        else:\n            self.generic_visit(node)",
              "start_line": 8,
              "end_line": 148,
              "context": {
                "dependencies": [],
                "instantiated_by": [
                  {
                    "file": "callgraph.py",
                    "function": "build_callGraph",
                    "mode": "function",
                    "line": 165
                  }
                ],
                "method_context": [
                  {
                    "identifier": "backend.callgraph.CallGraph.__init__",
                    "name": "__init__",
                    "calls": [
                      "backend/callgraph.py::CallGraph::DiGraph",
                      "backend/callgraph.py::CallGraph::set"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "filename"
                    ],
                    "docstring": "Initialisiert den Visitor mit einem Platzhalter f\u00fcr die aktuelle Funktion.",
                    "start_line": 12,
                    "end_line": 23
                  },
                  {
                    "identifier": "backend.callgraph.CallGraph._recursive_call",
                    "name": "_recursive_call",
                    "calls": [
                      "backend/callgraph.py::CallGraph::_recursive_call",
                      "backend/callgraph.py::CallGraph::append",
                      "backend/callgraph.py::CallGraph::isinstance"
                    ],
                    "called_by": [
                      "backend/callgraph.py::CallGraph::_recursive_call",
                      "backend/callgraph.py::CallGraph::visit_Call"
                    ],
                    "args": [
                      "self",
                      "node"
                    ],
                    "docstring": null,
                    "start_line": 25,
                    "end_line": 35
                  },
                  {
                    "identifier": "backend.callgraph.CallGraph._resolve_all_callee_names",
                    "name": "_resolve_all_callee_names",
                    "calls": [
                      "backend/callgraph.py::CallGraph::append"
                    ],
                    "called_by": [
                      "backend/callgraph.py::CallGraph::visit_Call"
                    ],
                    "args": [
                      "self",
                      "callee_nodes"
                    ],
                    "docstring": null,
                    "start_line": 37,
                    "end_line": 45
                  },
                  {
                    "identifier": "backend.callgraph.CallGraph._make_full_name",
                    "name": "_make_full_name",
                    "calls": [],
                    "called_by": [
                      "backend/callgraph.py::CallGraph::visit_FunctionDef"
                    ],
                    "args": [
                      "self",
                      "basename",
                      "class_name"
                    ],
                    "docstring": null,
                    "start_line": 47,
                    "end_line": 50
                  },
                  {
                    "identifier": "backend.callgraph.CallGraph._current_caller",
                    "name": "_current_caller",
                    "calls": [],
                    "called_by": [
                      "backend/callgraph.py::CallGraph::visit_Call"
                    ],
                    "args": [
                      "self"
                    ],
                    "docstring": null,
                    "start_line": 52,
                    "end_line": 55
                  },
                  {
                    "identifier": "backend.callgraph.CallGraph.visit_ClassDef",
                    "name": "visit_ClassDef",
                    "calls": [
                      "backend/callgraph.py::CallGraph::visit"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "node"
                    ],
                    "docstring": null,
                    "start_line": 74,
                    "end_line": 79
                  },
                  {
                    "identifier": "backend.callgraph.CallGraph.visit_FunctionDef",
                    "name": "visit_FunctionDef",
                    "calls": [
                      "backend/callgraph.py::CallGraph::_make_full_name",
                      "backend/callgraph.py::CallGraph::add",
                      "backend/callgraph.py::CallGraph::add_node",
                      "backend/callgraph.py::CallGraph::generic_visit"
                    ],
                    "called_by": [
                      "backend/callgraph.py::CallGraph::visit_AsyncFunctionDef"
                    ],
                    "args": [
                      "self",
                      "node"
                    ],
                    "docstring": "Besucht eine normale Funktionsdefinition.\n\nSetzt `self.current_function`, erstellt den Knoten im Graphen und \ntraversiert rekursiv den Funktionsk\u00f6rper.",
                    "start_line": 81,
                    "end_line": 95
                  },
                  {
                    "identifier": "backend.callgraph.CallGraph.visit_AsyncFunctionDef",
                    "name": "visit_AsyncFunctionDef",
                    "calls": [
                      "backend/callgraph.py::CallGraph::visit_FunctionDef"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "node"
                    ],
                    "docstring": "Besucht eine asynchrone Funktionsdefinition (`async def`).\n\nFunktioniert analog zu `visit_FunctionDef`.",
                    "start_line": 97,
                    "end_line": 103
                  },
                  {
                    "identifier": "backend.callgraph.CallGraph.visit_Call",
                    "name": "visit_Call",
                    "calls": [
                      "backend/callgraph.py::CallGraph::_current_caller",
                      "backend/callgraph.py::CallGraph::_recursive_call",
                      "backend/callgraph.py::CallGraph::_resolve_all_callee_names",
                      "backend/callgraph.py::CallGraph::add",
                      "backend/callgraph.py::CallGraph::generic_visit",
                      "backend/callgraph.py::CallGraph::print",
                      "backend/callgraph.py::CallGraph::set"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "node"
                    ],
                    "docstring": "Besucht einen Funktions- oder Methodenaufruf und behandelt komplexe F\u00e4lle\nmit einer detaillierten Warnung, anstatt abzust\u00fcrzen.",
                    "start_line": 105,
                    "end_line": 130
                  },
                  {
                    "identifier": "backend.callgraph.CallGraph.visit_If",
                    "name": "visit_If",
                    "calls": [
                      "backend/callgraph.py::CallGraph::generic_visit",
                      "backend/callgraph.py::CallGraph::isinstance"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "node"
                    ],
                    "docstring": "Pr\u00fcft auf `if __name__ == \"__main__\"`-Bl\u00f6cke, um globale Aufrufe innerhalb\ndieses Blocks separat als <main_block> im Call-Graphen darzustellen.\n\nAlle Funktionsaufrufe innerhalb des Blocks werden dann von <main_block> aus\nals Caller-Knoten erfasst.",
                    "start_line": 132,
                    "end_line": 148
                  }
                ]
              }
            }
          ]
        }
      },
      "backend/getRepo.py": {
        "ast_nodes": {
          "imports": [
            "tempfile",
            "shutil",
            "git.Repo",
            "git.GitCommandError",
            "logging",
            "os"
          ],
          "functions": [],
          "classes": [
            {
              "mode": "class_analysis",
              "identifier": "backend.getRepo.RepoFile",
              "name": "RepoFile",
              "docstring": "Repr\u00e4sentiert eine einzelne Datei in einem Git-Repository.\n\nDer Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tats\u00e4chlichem Zugriff.",
              "source_code": "class RepoFile:\n    \"\"\"\n    Repr\u00e4sentiert eine einzelne Datei in einem Git-Repository.\n    \n    Der Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tats\u00e4chlichem Zugriff.\n    \"\"\"\n    def __init__(self, file_path, commit_tree):\n        \"\"\"\n        Initialisiert das RepoFile-Objekt.\n\n        Args:\n            file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n            commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt.\n        \"\"\"\n        self.path = file_path\n        self._tree = commit_tree\n        \n        # Attribute f\u00fcr Lazy Loading (werden erst bei Bedarf gef\u00fcllt)\n        self._blob = None\n        self._content = None\n        self._size = None\n\n    @property\n    def blob(self):\n        \"\"\"Lazy-l\u00e4dt das Git-Blob-Objekt.\"\"\"\n        if self._blob is None:\n            try:\n                self._blob = self._tree[self.path]\n            except KeyError:\n                raise FileNotFoundError(f\"Datei '{self.path}' konnte im Commit-Tree nicht gefunden werden.\")\n        return self._blob\n\n    @property\n    def content(self):\n        \"\"\"Lazy-l\u00e4dt und gibt den dekodierten Inhalt der Datei zur\u00fcck.\"\"\"\n        if self._content is None:\n            self._content = self.blob.data_stream.read().decode('utf-8', errors='ignore')\n        return self._content\n\n    @property\n    def size(self):\n        \"\"\"Lazy-l\u00e4dt und gibt die Gr\u00f6\u00dfe der Datei in Bytes zur\u00fcck.\"\"\"\n        if self._size is None:\n            self._size = self.blob.size\n        return self._size\n        \n    def analyze_word_count(self):\n        \"\"\"\n        Eine Beispiel-Analyse-Methode. Z\u00e4hlt die W\u00f6rter im Dateiinhalt.\n        \"\"\"\n        return len(self.content.split())\n\n    def __repr__(self):\n        \"\"\"Gibt eine n\u00fctzliche String-Repr\u00e4sentation des Objekts zur\u00fcck.\"\"\"\n        return f\"<RepoFile(path='{self.path}')>\"\n    \n    def to_dict(self, include_content=False):\n        data = {\n            \"path\": self.path,\n            \"name\": os.path.basename(self.path),\n            \"size\": self.size,\n            \"type\": \"file\"\n        }\n        if include_content:\n            data[\"content\"] = self.content\n        return data",
              "start_line": 10,
              "end_line": 75,
              "context": {
                "dependencies": [],
                "instantiated_by": [
                  {
                    "file": "getRepo.py",
                    "function": "get_all_files",
                    "mode": "method",
                    "line": 111
                  }
                ],
                "method_context": [
                  {
                    "identifier": "backend.getRepo.RepoFile.__init__",
                    "name": "__init__",
                    "calls": [],
                    "called_by": [],
                    "args": [
                      "self",
                      "file_path",
                      "commit_tree"
                    ],
                    "docstring": "Initialisiert das RepoFile-Objekt.\n\nArgs:\n    file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n    commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt.",
                    "start_line": 16,
                    "end_line": 30
                  },
                  {
                    "identifier": "backend.getRepo.RepoFile.blob",
                    "name": "blob",
                    "calls": [
                      "backend/getRepo.py::RepoFile::FileNotFoundError"
                    ],
                    "called_by": [],
                    "args": [
                      "self"
                    ],
                    "docstring": "Lazy-l\u00e4dt das Git-Blob-Objekt.",
                    "start_line": 33,
                    "end_line": 40
                  },
                  {
                    "identifier": "backend.getRepo.RepoFile.content",
                    "name": "content",
                    "calls": [
                      "backend/getRepo.py::RepoFile::decode",
                      "backend/getRepo.py::RepoFile::read"
                    ],
                    "called_by": [],
                    "args": [
                      "self"
                    ],
                    "docstring": "Lazy-l\u00e4dt und gibt den dekodierten Inhalt der Datei zur\u00fcck.",
                    "start_line": 43,
                    "end_line": 47
                  },
                  {
                    "identifier": "backend.getRepo.RepoFile.size",
                    "name": "size",
                    "calls": [],
                    "called_by": [],
                    "args": [
                      "self"
                    ],
                    "docstring": "Lazy-l\u00e4dt und gibt die Gr\u00f6\u00dfe der Datei in Bytes zur\u00fcck.",
                    "start_line": 50,
                    "end_line": 54
                  },
                  {
                    "identifier": "backend.getRepo.RepoFile.analyze_word_count",
                    "name": "analyze_word_count",
                    "calls": [
                      "backend/getRepo.py::RepoFile::len",
                      "backend/getRepo.py::RepoFile::split"
                    ],
                    "called_by": [],
                    "args": [
                      "self"
                    ],
                    "docstring": "Eine Beispiel-Analyse-Methode. Z\u00e4hlt die W\u00f6rter im Dateiinhalt.",
                    "start_line": 56,
                    "end_line": 60
                  },
                  {
                    "identifier": "backend.getRepo.RepoFile.__repr__",
                    "name": "__repr__",
                    "calls": [],
                    "called_by": [],
                    "args": [
                      "self"
                    ],
                    "docstring": "Gibt eine n\u00fctzliche String-Repr\u00e4sentation des Objekts zur\u00fcck.",
                    "start_line": 62,
                    "end_line": 64
                  },
                  {
                    "identifier": "backend.getRepo.RepoFile.to_dict",
                    "name": "to_dict",
                    "calls": [
                      "backend/getRepo.py::RepoFile::basename"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "include_content"
                    ],
                    "docstring": null,
                    "start_line": 66,
                    "end_line": 75
                  }
                ]
              }
            },
            {
              "mode": "class_analysis",
              "identifier": "backend.getRepo.GitRepository",
              "name": "GitRepository",
              "docstring": "Verwaltet ein Git-Repository, einschlie\u00dflich Klonen in ein tempor\u00e4res\nVerzeichnis und Bereitstellung von RepoFile-Objekten.",
              "source_code": "class GitRepository:\n    \"\"\"\n    Verwaltet ein Git-Repository, einschlie\u00dflich Klonen in ein tempor\u00e4res\n    Verzeichnis und Bereitstellung von RepoFile-Objekten.\n    \"\"\"\n    def __init__(self, repo_url):\n        self.repo_url = repo_url\n        self.temp_dir = tempfile.mkdtemp()\n        self.repo = None\n\n        self.files = []\n        \n        try:\n            logging.info(f\"Clone Repository {self.repo_url}...\")\n            self.repo = Repo.clone_from(self.repo_url, self.temp_dir)\n            self.latest_commit = self.repo.head.commit\n            self.commit_tree = self.latest_commit.tree\n            logging.info(\"Cloning successful.\")\n        except GitCommandError as e:\n            self.close()\n            raise RuntimeError(f\"Error cloning repository: {e}\") from e\n\n    def get_all_files(self):\n        \"\"\"\n        Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zur\u00fcck.\n\n        Returns:\n            list[RepoFile]: Eine Liste von RepoFile-Instanzen.\n        \"\"\"\n        file_paths = self.repo.git.ls_files().split('\\n')\n        self.files = [RepoFile(path, self.commit_tree) for path in file_paths if path]\n        return self.files\n\n    def close(self):\n        \"\"\"L\u00f6scht das tempor\u00e4re Verzeichnis und dessen Inhalt.\"\"\"\n        if self.temp_dir:\n            print(f\"\\nL\u00f6sche tempor\u00e4res Verzeichnis: {self.temp_dir}\")\n            #shutil.rmtree(self.temp_dir)\n            self.temp_dir = None\n            \n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def get_file_tree(self, include_content=False):\n\n        if not self.files:\n            self.get_all_files()\n\n        tree = {\"name\": \"root\", \"type\": \"directory\", \"children\": []}\n\n        for file_obj in self.files:\n            parts = file_obj.path.split('/')\n            current_level = tree[\"children\"]\n            \n            # Iteriere durch die Ordnerstruktur\n            for part in parts[:-1]:\n                # Suche, ob Ordner schon existiert\n                found = next((item for item in current_level if item[\"name\"] == part and item[\"type\"] == \"directory\"), None)\n                if not found:\n                    new_dir = {\"name\": part, \"type\": \"directory\", \"children\": []}\n                    current_level.append(new_dir)\n                    current_level = new_dir[\"children\"]\n                else:\n                    current_level = found[\"children\"]\n            \n            # Datei am Ende hinzuf\u00fcgen\n            current_level.append(file_obj.to_dict(include_content=include_content))\n\n        return tree",
              "start_line": 81,
              "end_line": 152,
              "context": {
                "dependencies": [],
                "instantiated_by": [
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 120
                  }
                ],
                "method_context": [
                  {
                    "identifier": "backend.getRepo.GitRepository.__init__",
                    "name": "__init__",
                    "calls": [
                      "backend/getRepo.py::GitRepository::RuntimeError",
                      "backend/getRepo.py::GitRepository::clone_from",
                      "backend/getRepo.py::GitRepository::close",
                      "backend/getRepo.py::GitRepository::info",
                      "backend/getRepo.py::GitRepository::mkdtemp"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "repo_url"
                    ],
                    "docstring": null,
                    "start_line": 86,
                    "end_line": 101
                  },
                  {
                    "identifier": "backend.getRepo.GitRepository.get_all_files",
                    "name": "get_all_files",
                    "calls": [
                      "backend/getRepo.py::GitRepository::RepoFile",
                      "backend/getRepo.py::GitRepository::ls_files",
                      "backend/getRepo.py::GitRepository::split"
                    ],
                    "called_by": [
                      "backend/getRepo.py::GitRepository::get_file_tree"
                    ],
                    "args": [
                      "self"
                    ],
                    "docstring": "Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zur\u00fcck.\n\nReturns:\n    list[RepoFile]: Eine Liste von RepoFile-Instanzen.",
                    "start_line": 103,
                    "end_line": 112
                  },
                  {
                    "identifier": "backend.getRepo.GitRepository.close",
                    "name": "close",
                    "calls": [
                      "backend/getRepo.py::GitRepository::print"
                    ],
                    "called_by": [
                      "backend/getRepo.py::GitRepository::__exit__",
                      "backend/getRepo.py::GitRepository::__init__"
                    ],
                    "args": [
                      "self"
                    ],
                    "docstring": "L\u00f6scht das tempor\u00e4re Verzeichnis und dessen Inhalt.",
                    "start_line": 114,
                    "end_line": 119
                  },
                  {
                    "identifier": "backend.getRepo.GitRepository.__enter__",
                    "name": "__enter__",
                    "calls": [],
                    "called_by": [],
                    "args": [
                      "self"
                    ],
                    "docstring": null,
                    "start_line": 121,
                    "end_line": 122
                  },
                  {
                    "identifier": "backend.getRepo.GitRepository.__exit__",
                    "name": "__exit__",
                    "calls": [
                      "backend/getRepo.py::GitRepository::close"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "exc_type",
                      "exc_val",
                      "exc_tb"
                    ],
                    "docstring": null,
                    "start_line": 124,
                    "end_line": 125
                  },
                  {
                    "identifier": "backend.getRepo.GitRepository.get_file_tree",
                    "name": "get_file_tree",
                    "calls": [
                      "backend/getRepo.py::GitRepository::append",
                      "backend/getRepo.py::GitRepository::get_all_files",
                      "backend/getRepo.py::GitRepository::next",
                      "backend/getRepo.py::GitRepository::split",
                      "backend/getRepo.py::GitRepository::to_dict"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "include_content"
                    ],
                    "docstring": null,
                    "start_line": 127,
                    "end_line": 152
                  }
                ]
              }
            }
          ]
        }
      },
      "backend/main.py": {
        "ast_nodes": {
          "imports": [
            "logging",
            "os",
            "re",
            "json",
            "time",
            "math",
            "datetime.datetime",
            "dotenv.load_dotenv",
            "getRepo.GitRepository",
            "getRepo.RepoFile",
            "AST_Schema.ASTAnalyzer",
            "MainLLM.MainLLM",
            "basic_info.ProjektInfoExtractor",
            "HelperLLM.LLMHelper",
            "relationship_analyzer.ProjectAnalyzer",
            "schemas.types.FunctionContextInput",
            "schemas.types.FunctionAnalysisInput",
            "schemas.types.ClassContextInput",
            "schemas.types.ClassAnalysisInput",
            "schemas.types.MethodContextInput"
          ],
          "functions": [
            {
              "mode": "function_analysis",
              "identifier": "backend.main.calculate_net_time",
              "name": "calculate_net_time",
              "args": [
                "start_time",
                "end_time",
                "total_items",
                "batch_size",
                "model_name"
              ],
              "docstring": "Berechnet die Dauer abz\u00fcglich der Sleep-Zeiten f\u00fcr Rate-Limits.",
              "source_code": "def calculate_net_time(start_time, end_time, total_items, batch_size, model_name):\n    \"\"\"Berechnet die Dauer abz\u00fcglich der Sleep-Zeiten f\u00fcr Rate-Limits.\"\"\"\n    total_duration = end_time - start_time\n    \n    if not model_name.startswith(\"gemini-\"):\n        return total_duration\n\n    if total_items == 0:\n        return 0\n\n    num_batches = math.ceil(total_items / batch_size)\n    sleep_count = max(0, num_batches - 1)\n    total_sleep_time = sleep_count * 61\n    \n    net_time = total_duration - total_sleep_time\n    return max(0, net_time)",
              "start_line": 23,
              "end_line": 38,
              "context": {
                "calls": [
                  "backend/main.py::ceil",
                  "backend/main.py::max",
                  "backend/main.py::startswith"
                ],
                "called_by": [
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 290
                  },
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 321
                  }
                ]
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "backend.main.main_workflow",
              "name": "main_workflow",
              "args": [
                "input",
                "api_keys",
                "model_names",
                "status_callback"
              ],
              "docstring": null,
              "source_code": "def main_workflow(input, api_keys: dict, model_names: dict, status_callback=None):\n    \n    def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)\n\n    update_status(\"\ud83d\udd0d Analysiere Input...\")\n    \n    user_input = input\n    \n    # API Key & Ollama Base URL aus Frontend holen\n    gemini_api_key = api_keys.get(\"gemini\")\n    openai_api_key = api_keys.get(\"gpt\")\n    ollama_base_url = api_keys.get(\"ollama\")\n\n    if model_names[\"helper\"].startswith(\"gpt-\"):\n        helper_api_key = openai_api_key\n    elif model_names[\"helper\"].startswith(\"gemini-\"):\n        helper_api_key = gemini_api_key\n    if model_names[\"main\"].startswith(\"gpt-\"):\n        api_key = openai_api_key\n    elif model_names[\"main\"].startswith(\"gemini-\"):\n        api_key = gemini_api_key\n\n    # Standardeinstellungen f\u00fcr Modelle\n    helper_model = model_names.get(\"helper\", \"gpt-5-mini\")\n    main_model = model_names.get(\"main\", \"gpt-5.1\")\n\n    # Error Handling f\u00fcr fehlende API Keys\n    if not gemini_api_key and \"gemini\" in helper_model:\n        raise ValueError(\"Gemini API Key was not provided in api_keys dictionary.\")\n\n    # URL Extraktion\n    repo_url = None\n    url_pattern = r\"https?://[^\\s]+\"\n    match = re.search(url_pattern, user_input)\n\n    if match:\n        repo_url = match.group(0)\n        logging.info(f\"Extracted repository URL: {repo_url}\")\n    else:\n        raise ValueError(\"Could not find a valid URL in the provided input.\")\n    \n    # Repo klonen und Dateien extrahieren\n    update_status(f\"\u2b07\ufe0f Klone Repository: {repo_url} ...\")\n    \n    repo_files = []\n    local_repo_path = \"\" \n\n    try: \n\n        with GitRepository(repo_url) as repo:\n            repo_files = repo.get_all_files()\n            if hasattr(repo, 'local_path'):\n                local_repo_path = repo.local_path\n            elif hasattr(repo, 'working_dir'):\n                local_repo_path = repo.working_dir\n            else:\n                local_repo_path = os.path.dirname(os.path.commonpath([f.path for f in repo_files]))\n\n            logging.info(f\"Total files retrieved: {len(repo_files)}\")\n\n    except Exception as e:\n        logging.error(f\"Error cloning repository: {e}\")\n        raise \n\n\n    # Extrahiere Basic Infos\n    update_status(\"\u2139\ufe0f Extrahiere Basis-Informationen...\")\n    try:\n        info_extractor = ProjektInfoExtractor()\n        basic_project_info = info_extractor.extrahiere_info(dateien=repo_files, repo_url=repo_url)\n        logging.info(\"Basic project info extracted\")\n    except Exception as e:\n        logging.error(f\"Error extracting basic project info: {e}\")\n\n    # Erstelle Repository Dateibaum\n    update_status(\"\ud83c\udf32 Erstelle Repository Dateibaum...\")\n    try:\n        repo_file_tree = repo.get_file_tree()\n        logging.info(\"Repository file tree constructed\")\n    except Exception as e:\n        logging.error(f\"Error constructing repository file tree: {e}\")\n\n    # Relationship Analyse durchf\u00fchren\n    update_status(\"\ud83d\udd17 Analysiere Beziehungen (Calls & Instanziierungen)...\")\n    try:\n        rel_analyzer = ProjectAnalyzer(project_root=local_repo_path)\n        relationship_results = rel_analyzer.analyze()\n        logging.info(f\"Relationships analyzed. Found definitions: {len(relationship_results)}\")\n    except Exception as e:\n        logging.error(f\"Error in relationship analyzer: {e}\")\n        relationship_results = []\n\n    # Erstelle AST Schema\n    update_status(\"\ud83c\udf33 Erstelle Abstract Syntax Tree (AST)...\")\n    try:        \n        ast_analyzer = ASTAnalyzer()   \n        ast_schema = ast_analyzer.analyze_repository(files=repo_files)\n        logging.info(\"AST schema created\")\n    except Exception as e:\n        logging.error(f\"Error retrieving repository files: {e}\")\n        raise\n\n    # Anreichern des AST Schemas mit Relationship Daten\n    update_status(\"\u2795 Reiche AST mit Beziehungsdaten an...\")            \n    try:   \n        ast_schema = ast_analyzer.merge_relationship_data(ast_schema, relationship_results)\n        logging.info(\"AST schema created and enriched\")\n\n    except Exception as e:\n        logging.error(f\"Error processing repository: {e}\")\n        raise\n\n    # Vorbereitung der HelperLLM Eingaben\n    update_status(\"\u2699\ufe0f Bereite Daten f\u00fcr Helper LLM vor...\")\n    \n    helper_llm_function_input = []\n    helper_llm_class_input = []\n\n    try:\n        for filename, file_data in ast_schema['files'].items():\n            ast_nodes = file_data.get('ast_nodes', {})\n            imports = ast_nodes.get('imports', [])\n            functions = ast_nodes.get('functions', [])\n            classes = ast_nodes.get('classes', [])\n\n            # --- 1. Funktionen verarbeiten ---\n            for function in functions:\n                context = function.get('context', {})\n                \n                # BEREINIGUNG: Nur Dictionaries (CallInfo) zulassen\n                raw_called_by = context.get('called_by', [])\n                clean_called_by = [cb for cb in raw_called_by if isinstance(cb, dict)]\n\n                function_context = FunctionContextInput(\n                    calls = context.get('calls', []),\n                    called_by = clean_called_by \n                )\n                \n                function_input = FunctionAnalysisInput(\n                    mode = function.get('mode', 'function_analysis'),\n                    identifier = function.get('identifier'),\n                    source_code = function.get('source_code'),\n                    imports = imports,\n                    context = function_context\n                )\n                \n                helper_llm_function_input.append(function_input)\n\n            # --- 2. Klassen verarbeiten ---\n            for _class in classes:\n                context = _class.get('context', {})\n                \n                # Method Context inputs erstellen und bereinigen\n                method_context_inputs = []\n                for method in context.get('method_context', []):\n                    \n                    # BEREINIGUNG: Nur Dictionaries (CallInfo) zulassen\n                    raw_method_called_by = method.get('called_by', [])\n                    clean_method_called_by = [cb for cb in raw_method_called_by if isinstance(cb, dict)]\n\n                    method_context_inputs.append(\n                        MethodContextInput(\n                            identifier=method.get('identifier'),\n                            calls=method.get('calls', []),\n                            called_by=clean_method_called_by, \n                            args=method.get('args', []),\n                            docstring=method.get('docstring')\n                        )\n                    )\n\n                # BEREINIGUNG: Instantiated_by filtern\n                raw_instantiated_by = context.get('instantiated_by', [])\n                clean_instantiated_by = [ib for ib in raw_instantiated_by if isinstance(ib, dict)]\n\n                class_context = ClassContextInput(\n                    dependencies = context.get('dependencies', []),\n                    instantiated_by = clean_instantiated_by, \n                    method_context = method_context_inputs\n                )\n\n                class_input = ClassAnalysisInput(\n                    mode = _class.get('mode', 'class_analysis'),\n                    identifier =_class.get('identifier'),\n                    source_code = _class.get('source_code'), \n                    imports = imports, \n                    context = class_context\n                )\n                \n                helper_llm_class_input.append(class_input)\n\n    except Exception as e:\n        logging.error(f\"Error preparing inputs for Helper LLM: {e}\")\n        raise\n    \n    logging.info(f\"Functions: {len(helper_llm_function_input)}, Classes: {len(helper_llm_class_input)}\")\n    \n    # Initialisiere HelperLLM\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    \n    llm_helper = LLMHelper(\n        api_key=helper_api_key, \n        function_prompt_path=function_prompt_file, \n        class_prompt_path=class_prompt_file,\n        model_name=helper_model,\n        ollama_base_url=ollama_base_url,\n    )\n    \n    if ollama_base_url:\n        os.environ[\"OLLAMA_BASE_URL\"] = ollama_base_url\n\n    # Initialisiere Ergebniscontainer\n    analysis_results = {}\n    function_analysis_results = []\n    class_analysis_results = []\n\n    # Call HelperLLM f\u00fcr Funktionen\n    update_status(f\"\ud83e\udd16 Helper LLM: Analysiere {len(helper_llm_function_input)} Funktionen ({helper_model})...\")\n\n    try:\n        net_time_func = 0\n        if len(helper_llm_function_input) > 0:\n\n            logging.info(\"\\n--- Generating documentation for Functions ---\")\n            t_start_func = time.time()    \n            function_analysis_results = llm_helper.generate_for_functions(helper_llm_function_input)    \n            t_end_func = time.time()\n            net_time_func = calculate_net_time(t_start_func, t_end_func, len(helper_llm_function_input), llm_helper.batch_size, helper_model)\n\n        if len(function_analysis_results) != 0:\n            for doc in function_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"functions\" not in analysis_results:\n                        analysis_results[\"functions\"] = {}\n                    analysis_results[\"functions\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(f\"Failed to generate doc for a function\") \n    except Exception as e:\n        logging.error(f\"Error during Helper LLM function analysis: {e}\")\n        raise\n    \n\n    # Call HelperLLM f\u00fcr Klassen\n    try:\n        net_time_class = 0\n        if len(helper_llm_class_input) > 0:\n            # Rate Limit Sleep f\u00fcr Gemini Modelle\n            if llm_helper.model_name.startswith(\"gemini-\") & (len(helper_llm_function_input) > 0):\n                update_status(\"\ud83d\udca4 Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n                time.sleep(61)\n            \n            update_status(f\"\ud83e\udd16 Helper LLM: Analysiere {len(helper_llm_class_input)} Klassen ({helper_model})...\")\n            \n            logging.info(\"\\n--- Generating documentation for Classes ---\")\n            t_start_class = time.time()\n            class_analysis_results = llm_helper.generate_for_classes(helper_llm_class_input)\n            t_end_class = time.time()\n            net_time_class = calculate_net_time(t_start_class, t_end_class, len(helper_llm_class_input), llm_helper.batch_size, helper_model)\n\n        if len(class_analysis_results) != 0:\n            for doc in class_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"classes\" not in analysis_results:\n                        analysis_results[\"classes\"] = {}\n                    analysis_results[\"classes\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(f\"Failed to generate doc for a class\")\n    except Exception as e:\n        logging.error(f\"Error during Helper LLM class analysis: {e}\")\n        raise\n\n    total_helper_time = net_time_func + net_time_class\n\n    # MainLLM Input Vorbereitung\n    main_llm_input = {\n        \"basic_info\": basic_project_info,\n        \"file_tree\": repo_file_tree,\n        \"ast_schema\": ast_schema,\n        \"analysis_results\": analysis_results\n    }\n    main_llm_input_json = json.dumps(main_llm_input, indent=2)\n    \n    # MainLLM Ausf\u00fchrung\n    main_llm = MainLLM(\n        api_key=api_key, \n        prompt_file_path=\"SystemPrompts/SystemPromptMainLLM.txt\",\n        model_name=main_model,\n        ollama_base_url=ollama_base_url,\n    )\n\n\n    # RPM Limit Sleep f\u00fcr Gemini Modelle\n    if llm_helper.model_name == main_llm.model_name and main_llm.model_name.startswith(\"gemini-\"):\n        time.sleep(61)\n        update_status(\"\ud83d\udca4 Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n\n    # Call MainLLM f\u00fcr finalen Report\n    update_status(f\"\ud83e\udde0 Main LLM: Generiere finalen Report ({main_model})...\")\n    try:\n        total_main_time = 0\n        logging.info(\"\\n--- Generating Final Report ---\")\n        t_start_main = time.time()\n        final_report = main_llm.call_llm(main_llm_input_json)\n        #for token in main_llm.stream_llm(main_llm_input_json):\n        #    full_response += token    \n        #    final_report = full_response\n        t_end_main = time.time()\n        total_main_time = t_end_main - t_start_main\n    except Exception as e:\n        logging.error(f\"Error during Main LLM final report generation: {e}\")\n        raise\n\n    \n    # Speichern\n    output_dir = \"result\"\n    os.makedirs(output_dir, exist_ok=True)  \n    total_active_time = total_helper_time + total_main_time\n    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M-%S\")\n    \n    report_filename = f\"report_{timestamp}_Helper_{llm_helper.model_name}_MainLLM_{main_llm.model_name}.md\"\n    report_filepath = os.path.join(output_dir, report_filename)\n    \n    if final_report:\n        with open(report_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(final_report)\n        logging.info(f\"Final report saved to '{report_filepath}'.\")\n    else:\n        final_report = \"Error: Report generation failed or returned empty.\"\n\n    metrics = {\n        \"helper_time\": round(total_helper_time, 2),\n        \"main_time\": round(total_main_time, 2),\n        \"total_time\": round(total_active_time, 2),\n        \"helper_model\": helper_model,\n        \"main_model\": main_model\n    }\n\n    return {\n        \"report\": final_report,\n        \"metrics\": metrics\n    }",
              "start_line": 41,
              "end_line": 386,
              "context": {
                "calls": [],
                "called_by": [
                  {
                    "file": "Frontend.py",
                    "function": "frontend.Frontend",
                    "mode": "module",
                    "line": 308
                  },
                  {
                    "file": "main.py",
                    "function": "backend.main",
                    "mode": "module",
                    "line": 458
                  }
                ]
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "backend.main.update_status",
              "name": "update_status",
              "args": [
                "msg"
              ],
              "docstring": null,
              "source_code": "def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)",
              "start_line": 43,
              "end_line": 46,
              "context": {
                "calls": [
                  "backend/main.py::info",
                  "backend/main.py::status_callback"
                ],
                "called_by": [
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 75
                  },
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 113
                  },
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 137
                  },
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 146
                  },
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 154
                  },
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 164
                  },
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 174
                  },
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 184
                  },
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 280
                  },
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 312
                  },
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 315
                  },
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 388
                  },
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 391
                  }
                ]
              }
            }
          ],
          "classes": []
        }
      },
      "backend/relationship_analyzer.py": {
        "ast_nodes": {
          "imports": [
            "ast",
            "os",
            "logging",
            "collections.defaultdict"
          ],
          "functions": [
            {
              "mode": "function_analysis",
              "identifier": "backend.relationship_analyzer.path_to_module",
              "name": "path_to_module",
              "args": [
                "filepath",
                "project_root"
              ],
              "docstring": "Wandelt einen Dateipfad in einen Python-Modulpfad um.",
              "source_code": "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    module_path = rel_path.replace(os.path.sep, '.')\n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n    return module_path",
              "start_line": 6,
              "end_line": 18,
              "context": {
                "calls": [
                  "backend/relationship_analyzer.py::basename",
                  "backend/relationship_analyzer.py::endswith",
                  "backend/relationship_analyzer.py::relpath",
                  "backend/relationship_analyzer.py::replace"
                ],
                "called_by": [
                  {
                    "file": "relationship_analyzer.py",
                    "function": "_collect_definitions",
                    "mode": "method",
                    "line": 60
                  },
                  {
                    "file": "relationship_analyzer.py",
                    "function": "__init__",
                    "mode": "method",
                    "line": 134
                  }
                ]
              }
            }
          ],
          "classes": [
            {
              "mode": "class_analysis",
              "identifier": "backend.relationship_analyzer.ProjectAnalyzer",
              "name": "ProjectAnalyzer",
              "docstring": null,
              "source_code": "class ProjectAnalyzer:\n    \n    def __init__(self, project_root):\n        self.project_root = os.path.abspath(project_root)\n        self.definitions = {}\n        self.call_graph = defaultdict(list)\n        self.file_asts = {} \n        self.ignore_dirs = {'.git', '.venv', 'venv', '__pycache__', 'node_modules', 'dist', 'build', 'docs'}\n\n    def analyze(self):\n        py_files = self._find_py_files()\n        \n        for filepath in py_files:\n            self._collect_definitions(filepath)\n            \n        for filepath in py_files:\n            self._resolve_calls(filepath)\n            \n        self.file_asts.clear()\n        \n        return self.get_formatted_results()\n\n    def _find_py_files(self):\n        py_files = []\n        for root, dirs, files in os.walk(self.project_root):\n            dirs[:] = [d for d in dirs if d not in self.ignore_dirs]\n            \n            for file in files:\n                if file.endswith(\".py\"):\n                    py_files.append(os.path.join(root, file))\n        return py_files\n\n    def _collect_definitions(self, filepath):\n        try:\n            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n                source = f.read()\n                tree = ast.parse(source, filename=filepath)\n                \n            self.file_asts[filepath] = tree\n            \n            module_path = path_to_module(filepath, self.project_root)\n            \n            for node in ast.walk(tree):\n                if isinstance(node, ast.FunctionDef):\n                    parent = self._get_parent(tree, node)\n                    if isinstance(parent, ast.ClassDef):\n                        path_name = f\"{module_path}.{parent.name}.{node.name}\"\n                        def_type = 'method'\n                    else:\n                        path_name = f\"{module_path}.{node.name}\"\n                        def_type = 'function'\n                    self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': def_type}\n                elif isinstance(node, ast.ClassDef):\n                    path_name = f\"{module_path}.{node.name}\"\n                    self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': 'class'}\n        except Exception as e:\n            logging.error(f\"Error collecting definitions in {filepath}: {e}\")\n            self.file_asts[filepath] = None\n            \n    def _get_parent(self, tree, node):\n        for parent in ast.walk(tree):\n            for child in ast.iter_child_nodes(parent):\n                if child is node:\n                    return parent\n        return None\n\n    def _resolve_calls(self, filepath):\n        tree = self.file_asts.get(filepath)\n        if not tree:\n            return\n\n        try:\n            resolver = CallResolverVisitor(filepath, self.project_root, self.definitions)\n            resolver.visit(tree)\n            for callee_pathname, caller_info in resolver.calls.items():\n                self.call_graph[callee_pathname].extend(caller_info)\n        except Exception as e:\n            logging.error(f\"Error resolving calls in {filepath}: {e}\")\n\n    def get_formatted_results(self):\n        output_list = []\n        \n        for callee_pathname, calls in self.call_graph.items():\n            if callee_pathname in self.definitions:\n                def_info = self.definitions[callee_pathname]\n                \n                definition_dict = {\n                    \"identifier\": callee_pathname,\n                    \"mode\": def_info.get('type', 'unknown'),\n                    \"origin\": os.path.basename(def_info['file']),\n                    \"origin_line\": def_info['line'],\n                    \"called_by\": [] \n                }\n                \n                unique_calls = {}\n                for call in calls:\n                    key = (call['file'], call['line'], call['caller'])\n                    if key not in unique_calls:\n                        unique_calls[key] = {\n                            \"file\": call['file'],\n                            \"function\": call['caller'],\n                            \"mode\": call.get('caller_type', 'unknown'),\n                            \"line\": call['line']\n                        }\n\n                if unique_calls:\n                    definition_dict[\"called_by\"] = sorted(unique_calls.values(), key=lambda x: (x['file'], x['line']))\n                    output_list.append(definition_dict)\n                    \n        return output_list",
              "start_line": 20,
              "end_line": 129,
              "context": {
                "dependencies": [],
                "instantiated_by": [
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 156
                  }
                ],
                "method_context": [
                  {
                    "identifier": "backend.relationship_analyzer.ProjectAnalyzer.__init__",
                    "name": "__init__",
                    "calls": [
                      "backend/relationship_analyzer.py::ProjectAnalyzer::abspath",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::defaultdict"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "project_root"
                    ],
                    "docstring": null,
                    "start_line": 22,
                    "end_line": 27
                  },
                  {
                    "identifier": "backend.relationship_analyzer.ProjectAnalyzer.analyze",
                    "name": "analyze",
                    "calls": [
                      "backend/relationship_analyzer.py::ProjectAnalyzer::_collect_definitions",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::_find_py_files",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::_resolve_calls",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::clear",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::get_formatted_results"
                    ],
                    "called_by": [
                      {
                        "file": "main.py",
                        "function": "main_workflow",
                        "mode": "function",
                        "line": 157
                      }
                    ],
                    "args": [
                      "self"
                    ],
                    "docstring": null,
                    "start_line": 29,
                    "end_line": 40
                  },
                  {
                    "identifier": "backend.relationship_analyzer.ProjectAnalyzer._find_py_files",
                    "name": "_find_py_files",
                    "calls": [
                      "backend/relationship_analyzer.py::ProjectAnalyzer::append",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::endswith",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::join",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::walk"
                    ],
                    "called_by": [
                      "backend/relationship_analyzer.py::ProjectAnalyzer::analyze"
                    ],
                    "args": [
                      "self"
                    ],
                    "docstring": null,
                    "start_line": 42,
                    "end_line": 50
                  },
                  {
                    "identifier": "backend.relationship_analyzer.ProjectAnalyzer._collect_definitions",
                    "name": "_collect_definitions",
                    "calls": [
                      "backend/relationship_analyzer.py::ProjectAnalyzer::_get_parent",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::error",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::isinstance",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::open",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::parse",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::path_to_module",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::read",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::walk"
                    ],
                    "called_by": [
                      "backend/relationship_analyzer.py::ProjectAnalyzer::analyze"
                    ],
                    "args": [
                      "self",
                      "filepath"
                    ],
                    "docstring": null,
                    "start_line": 52,
                    "end_line": 77
                  },
                  {
                    "identifier": "backend.relationship_analyzer.ProjectAnalyzer._get_parent",
                    "name": "_get_parent",
                    "calls": [
                      "backend/relationship_analyzer.py::ProjectAnalyzer::iter_child_nodes",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::walk"
                    ],
                    "called_by": [
                      "backend/relationship_analyzer.py::ProjectAnalyzer::_collect_definitions"
                    ],
                    "args": [
                      "self",
                      "tree",
                      "node"
                    ],
                    "docstring": null,
                    "start_line": 79,
                    "end_line": 84
                  },
                  {
                    "identifier": "backend.relationship_analyzer.ProjectAnalyzer._resolve_calls",
                    "name": "_resolve_calls",
                    "calls": [
                      "backend/relationship_analyzer.py::ProjectAnalyzer::CallResolverVisitor",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::error",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::extend",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::get",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::items",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::visit"
                    ],
                    "called_by": [
                      "backend/relationship_analyzer.py::ProjectAnalyzer::analyze"
                    ],
                    "args": [
                      "self",
                      "filepath"
                    ],
                    "docstring": null,
                    "start_line": 86,
                    "end_line": 97
                  },
                  {
                    "identifier": "backend.relationship_analyzer.ProjectAnalyzer.get_formatted_results",
                    "name": "get_formatted_results",
                    "calls": [
                      "backend/relationship_analyzer.py::ProjectAnalyzer::append",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::basename",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::get",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::items",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::sorted",
                      "backend/relationship_analyzer.py::ProjectAnalyzer::values"
                    ],
                    "called_by": [
                      "backend/relationship_analyzer.py::ProjectAnalyzer::analyze"
                    ],
                    "args": [
                      "self"
                    ],
                    "docstring": null,
                    "start_line": 99,
                    "end_line": 129
                  }
                ]
              }
            },
            {
              "mode": "class_analysis",
              "identifier": "backend.relationship_analyzer.CallResolverVisitor",
              "name": "CallResolverVisitor",
              "docstring": null,
              "source_code": "class CallResolverVisitor(ast.NodeVisitor):\n    def __init__(self, filepath, project_root, definitions):\n        self.filepath = filepath\n        self.module_path = path_to_module(filepath, project_root)\n        self.definitions = definitions\n        self.scope = {}\n        self.instance_types = {}\n        self.current_caller_name = self.module_path\n        self.current_class_name = None\n        self.calls = defaultdict(list)\n\n    def visit_ClassDef(self, node):\n        old_class_name, self.current_class_name = self.current_class_name, node.name\n        self.generic_visit(node)\n        self.current_class_name = old_class_name\n\n    def visit_FunctionDef(self, node):\n        old_caller_name, self.current_caller_name = self.current_caller_name, node.name\n        self.generic_visit(node)\n        self.current_caller_name = old_caller_name\n\n    def visit_Call(self, node):\n        callee_pathname = self._resolve_call_qname(node.func)\n        if callee_pathname and callee_pathname in self.definitions:\n            if self.current_caller_name == self.module_path:\n                caller_type = 'module'\n            elif self.current_class_name:\n                caller_type = 'method'\n            else:\n                caller_type = 'function'\n            \n            caller_info = {\n                'file': os.path.basename(self.filepath),\n                'line': node.lineno,\n                'caller': self.current_caller_name,\n                'caller_type': caller_type \n            }\n            self.calls[callee_pathname].append(caller_info)\n        self.generic_visit(node)\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.scope[alias.asname or alias.name] = alias.name\n        self.generic_visit(node)\n        \n    def visit_ImportFrom(self, node):\n        module = node.module or ''\n        for alias in node.names:\n            name = alias.asname or alias.name\n            if node.level > 0:\n                base = self.module_path.split('.')\n                prefix = '.'.join(base[:-node.level])\n                full_module_path = f\"{prefix}.{module}\" if module else prefix\n            else:\n                full_module_path = module\n            self.scope[name] = f\"{full_module_path}.{alias.name}\"\n        self.generic_visit(node)\n\n    def visit_Assign(self, node):\n        if isinstance(node.value, ast.Call) and isinstance(node.value.func, ast.Name):\n            class_name = node.value.func.id\n            if class_name in self.scope:\n                qualified_class_name = self.scope[class_name]\n                if qualified_class_name in self.definitions:\n                    for target in node.targets:\n                        if isinstance(target, ast.Name):\n                            self.instance_types[target.id] = qualified_class_name\n        self.generic_visit(node)\n\n    def _resolve_call_qname(self, func_node):\n        if isinstance(func_node, ast.Name):\n            name = func_node.id\n            if name in self.scope:\n                return self.scope[name]\n            local_pathname = f\"{self.module_path}.{name}\"\n            if local_pathname in self.definitions:\n                return local_pathname\n        elif isinstance(func_node, ast.Attribute) and isinstance(func_node.value, ast.Name):\n            var_name = func_node.value.id\n            method_name = func_node.attr\n            if var_name in self.instance_types:\n                class_pathname = self.instance_types[var_name]\n                return f\"{class_pathname}.{method_name}\"\n            if var_name in self.scope:\n                module_pathname = self.scope[var_name]\n                return f\"{module_pathname}.{method_name}\"\n        return None",
              "start_line": 131,
              "end_line": 217,
              "context": {
                "dependencies": [],
                "instantiated_by": [
                  {
                    "file": "relationship_analyzer.py",
                    "function": "_resolve_calls",
                    "mode": "method",
                    "line": 92
                  }
                ],
                "method_context": [
                  {
                    "identifier": "backend.relationship_analyzer.CallResolverVisitor.__init__",
                    "name": "__init__",
                    "calls": [
                      "backend/relationship_analyzer.py::CallResolverVisitor::defaultdict",
                      "backend/relationship_analyzer.py::CallResolverVisitor::path_to_module"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "filepath",
                      "project_root",
                      "definitions"
                    ],
                    "docstring": null,
                    "start_line": 132,
                    "end_line": 140
                  },
                  {
                    "identifier": "backend.relationship_analyzer.CallResolverVisitor.visit_ClassDef",
                    "name": "visit_ClassDef",
                    "calls": [
                      "backend/relationship_analyzer.py::CallResolverVisitor::generic_visit"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "node"
                    ],
                    "docstring": null,
                    "start_line": 142,
                    "end_line": 145
                  },
                  {
                    "identifier": "backend.relationship_analyzer.CallResolverVisitor.visit_FunctionDef",
                    "name": "visit_FunctionDef",
                    "calls": [
                      "backend/relationship_analyzer.py::CallResolverVisitor::generic_visit"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "node"
                    ],
                    "docstring": null,
                    "start_line": 147,
                    "end_line": 150
                  },
                  {
                    "identifier": "backend.relationship_analyzer.CallResolverVisitor.visit_Call",
                    "name": "visit_Call",
                    "calls": [
                      "backend/relationship_analyzer.py::CallResolverVisitor::_resolve_call_qname",
                      "backend/relationship_analyzer.py::CallResolverVisitor::append",
                      "backend/relationship_analyzer.py::CallResolverVisitor::basename",
                      "backend/relationship_analyzer.py::CallResolverVisitor::generic_visit"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "node"
                    ],
                    "docstring": null,
                    "start_line": 152,
                    "end_line": 169
                  },
                  {
                    "identifier": "backend.relationship_analyzer.CallResolverVisitor.visit_Import",
                    "name": "visit_Import",
                    "calls": [
                      "backend/relationship_analyzer.py::CallResolverVisitor::generic_visit"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "node"
                    ],
                    "docstring": null,
                    "start_line": 171,
                    "end_line": 174
                  },
                  {
                    "identifier": "backend.relationship_analyzer.CallResolverVisitor.visit_ImportFrom",
                    "name": "visit_ImportFrom",
                    "calls": [
                      "backend/relationship_analyzer.py::CallResolverVisitor::generic_visit",
                      "backend/relationship_analyzer.py::CallResolverVisitor::join",
                      "backend/relationship_analyzer.py::CallResolverVisitor::split"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "node"
                    ],
                    "docstring": null,
                    "start_line": 176,
                    "end_line": 187
                  },
                  {
                    "identifier": "backend.relationship_analyzer.CallResolverVisitor.visit_Assign",
                    "name": "visit_Assign",
                    "calls": [
                      "backend/relationship_analyzer.py::CallResolverVisitor::generic_visit",
                      "backend/relationship_analyzer.py::CallResolverVisitor::isinstance"
                    ],
                    "called_by": [],
                    "args": [
                      "self",
                      "node"
                    ],
                    "docstring": null,
                    "start_line": 189,
                    "end_line": 198
                  },
                  {
                    "identifier": "backend.relationship_analyzer.CallResolverVisitor._resolve_call_qname",
                    "name": "_resolve_call_qname",
                    "calls": [
                      "backend/relationship_analyzer.py::CallResolverVisitor::isinstance"
                    ],
                    "called_by": [
                      "backend/relationship_analyzer.py::CallResolverVisitor::visit_Call"
                    ],
                    "args": [
                      "self",
                      "func_node"
                    ],
                    "docstring": null,
                    "start_line": 200,
                    "end_line": 217
                  }
                ]
              }
            }
          ]
        }
      },
      "database/db.py": {
        "ast_nodes": {
          "imports": [
            "datetime.datetime",
            "pymongo.MongoClient",
            "dotenv.load_dotenv",
            "streamlit_authenticator",
            "cryptography.fernet.Fernet",
            "streamlit",
            "os"
          ],
          "functions": [
            {
              "mode": "function_analysis",
              "identifier": "database.db.encrypt_text",
              "name": "encrypt_text",
              "args": [
                "text"
              ],
              "docstring": null,
              "source_code": "def encrypt_text(text: str) -> str:\n    if not text or not cipher_suite: return text\n    return cipher_suite.encrypt(text.encode()).decode()",
              "start_line": 31,
              "end_line": 33,
              "context": {
                "calls": [
                  "database/db.py::decode",
                  "database/db.py::encode",
                  "database/db.py::encrypt"
                ],
                "called_by": [
                  {
                    "file": "db.py",
                    "function": "update_gemini_key",
                    "mode": "function",
                    "line": 71
                  }
                ]
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "database.db.decrypt_text",
              "name": "decrypt_text",
              "args": [
                "text"
              ],
              "docstring": null,
              "source_code": "def decrypt_text(text: str) -> str:\n    if not text or not cipher_suite: return text\n    try:\n        return cipher_suite.decrypt(text.encode()).decode()\n    except Exception:\n        \n        return text",
              "start_line": 35,
              "end_line": 41,
              "context": {
                "calls": [
                  "database/db.py::decode",
                  "database/db.py::decrypt",
                  "database/db.py::encode"
                ],
                "called_by": [
                  {
                    "file": "db.py",
                    "function": "get_decrypted_api_keys",
                    "mode": "function",
                    "line": 100
                  }
                ]
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "database.db.insert_user",
              "name": "insert_user",
              "args": [
                "username",
                "name",
                "password"
              ],
              "docstring": null,
              "source_code": "def insert_user(username: str, name: str,  password: str):\n    # Insert a new user into the database\n    user = {\n            \"_id\": username,\n            \"name\":name, \"hashed_password\": stauth.hasher.hash(password),\n            \"gemini_api_key\": \"\", \n            \"ollama_base_url\": \"\"\n    }\n    result = dbusers.insert_one(user)\n    return result.inserted_id",
              "start_line": 47,
              "end_line": 56,
              "context": {
                "calls": [
                  "database/db.py::hash",
                  "database/db.py::insert_one"
                ],
                "called_by": []
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "database.db.fetch_all_users",
              "name": "fetch_all_users",
              "args": [],
              "docstring": null,
              "source_code": "def fetch_all_users():\n    # Fetch all users from the database\n    users = list(dbusers.find())\n    return users",
              "start_line": 59,
              "end_line": 62,
              "context": {
                "calls": [
                  "database/db.py::find",
                  "database/db.py::list"
                ],
                "called_by": [
                  {
                    "file": "Frontend.py",
                    "function": "frontend.Frontend",
                    "mode": "module",
                    "line": 159
                  }
                ]
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "database.db.fetch_user",
              "name": "fetch_user",
              "args": [
                "username"
              ],
              "docstring": null,
              "source_code": "def fetch_user(username: str):\n    # Fetch a single user by username\n    user = dbusers.find_one({\"_id\": username})\n    return user",
              "start_line": 64,
              "end_line": 67,
              "context": {
                "calls": [
                  "database/db.py::find_one"
                ],
                "called_by": []
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "database.db.update_gemini_key",
              "name": "update_gemini_key",
              "args": [
                "username",
                "gemini_api_key"
              ],
              "docstring": null,
              "source_code": "def update_gemini_key(username: str, gemini_api_key: str ):\n    # Update the Gemini API key for a user\n    encrypted_key = encrypt_text(gemini_api_key)\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"gemini_api_key\": encrypted_key}})\n    return result.modified_count",
              "start_line": 69,
              "end_line": 73,
              "context": {
                "calls": [
                  "database/db.py::encrypt_text",
                  "database/db.py::update_one"
                ],
                "called_by": [
                  {
                    "file": "Frontend.py",
                    "function": "frontend.Frontend",
                    "mode": "module",
                    "line": 249
                  }
                ]
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "database.db.update_ollama_url",
              "name": "update_ollama_url",
              "args": [
                "username",
                "ollama_base_url"
              ],
              "docstring": null,
              "source_code": "def update_ollama_url(username: str, ollama_base_url: str ):\n    # Update the Ollama Base URL for a user\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"ollama_base_url\": ollama_base_url}})\n    return result.modified_count",
              "start_line": 75,
              "end_line": 78,
              "context": {
                "calls": [
                  "database/db.py::update_one"
                ],
                "called_by": [
                  {
                    "file": "Frontend.py",
                    "function": "frontend.Frontend",
                    "mode": "module",
                    "line": 268
                  }
                ]
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "database.db.fetch_gemini_key",
              "name": "fetch_gemini_key",
              "args": [
                "username"
              ],
              "docstring": null,
              "source_code": "def fetch_gemini_key(username: str):\n    # Fetch the Gemini API key for a user\n    user = dbusers.find_one({\"_id\": username}, {\"gemini_api_key\": 1, \"_id\": 0})\n    return user.get(\"gemini_api_key\")",
              "start_line": 80,
              "end_line": 83,
              "context": {
                "calls": [
                  "database/db.py::find_one",
                  "database/db.py::get"
                ],
                "called_by": []
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "database.db.fetch_ollama_url",
              "name": "fetch_ollama_url",
              "args": [
                "username"
              ],
              "docstring": null,
              "source_code": "def fetch_ollama_url(username: str):\n    # Fetch the Ollama Base URL for a user\n    user = dbusers.find_one({\"_id\": username}, {\"ollama_base_url\": 1, \"_id\": 0})\n    return user.get(\"ollama_base_url\")",
              "start_line": 85,
              "end_line": 88,
              "context": {
                "calls": [
                  "database/db.py::find_one",
                  "database/db.py::get"
                ],
                "called_by": []
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "database.db.delete_user",
              "name": "delete_user",
              "args": [
                "username"
              ],
              "docstring": null,
              "source_code": "def delete_user(username: str):\n    # Delete a user from the database\n    result = dbusers.delete_one({\"_id\": username})\n    return result.deleted_count",
              "start_line": 90,
              "end_line": 93,
              "context": {
                "calls": [
                  "database/db.py::delete_one"
                ],
                "called_by": []
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "database.db.get_decrypted_api_keys",
              "name": "get_decrypted_api_keys",
              "args": [
                "username"
              ],
              "docstring": "Holt User und entschl\u00fcsselt die Keys direkt",
              "source_code": "def get_decrypted_api_keys(username: str):\n    \"\"\"Holt User und entschl\u00fcsselt die Keys direkt\"\"\"\n    user = dbusers.find_one({\"_id\": username})\n    if not user: return None, None\n    \n    gemini_plain = decrypt_text(user.get(\"gemini_api_key\", \"\"))\n    ollama_plain = user.get(\"ollama_base_url\", \"\")\n    return gemini_plain, ollama_plain",
              "start_line": 95,
              "end_line": 102,
              "context": {
                "calls": [
                  "database/db.py::decrypt_text",
                  "database/db.py::find_one",
                  "database/db.py::get"
                ],
                "called_by": [
                  {
                    "file": "Frontend.py",
                    "function": "frontend.Frontend",
                    "mode": "module",
                    "line": 233
                  },
                  {
                    "file": "Frontend.py",
                    "function": "frontend.Frontend",
                    "mode": "module",
                    "line": 295
                  }
                ]
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "database.db.insert_exchange",
              "name": "insert_exchange",
              "args": [
                "question",
                "answer",
                "feedback",
                "username",
                "chat_name",
                "helper_used",
                "main_used",
                "total_time",
                "helper_time",
                "main_time"
              ],
              "docstring": null,
              "source_code": "def insert_exchange(question: str, answer: str, feedback: str, username: str, chat_name: str, helper_used: str=\"\", main_used: str=\"\", total_time: str=\"\", helper_time: str=\"\", main_time: str=\"\"):\n    # Insert a new exchange into the database\n    exchange = {\n        \"question\": question,\n        \"answer\": answer,\n        \"feedback\": feedback,\n        \"feedback_message\": \"\",\n        \"chat_name\": chat_name,\n        \"username\": username,\n        \"helper_used\": helper_used,\n        \"main_used\": main_used,\n        \"total_time\": total_time,\n        \"helper_time\": helper_time,\n        \"main_time\": main_time,\n        \"created_at\":  datetime.now()\n    }\n    result = dbexchanges.insert_one(exchange)\n    return result.inserted_id",
              "start_line": 108,
              "end_line": 125,
              "context": {
                "calls": [
                  "database/db.py::insert_one",
                  "database/db.py::now"
                ],
                "called_by": [
                  {
                    "file": "Frontend.py",
                    "function": "frontend.Frontend",
                    "mode": "module",
                    "line": 336
                  }
                ]
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "database.db.fetch_exchanges_by_user",
              "name": "fetch_exchanges_by_user",
              "args": [
                "username"
              ],
              "docstring": null,
              "source_code": "def fetch_exchanges_by_user(username: str):\n    exchanges = list(dbexchanges.find({\"username\": username}))\n    return exchanges",
              "start_line": 127,
              "end_line": 129,
              "context": {
                "calls": [
                  "database/db.py::find",
                  "database/db.py::list"
                ],
                "called_by": [
                  {
                    "file": "Frontend.py",
                    "function": "load_data_from_db",
                    "mode": "function",
                    "line": 29
                  }
                ]
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "database.db.fetch_exchanges_by_chat",
              "name": "fetch_exchanges_by_chat",
              "args": [
                "username",
                "chat_name"
              ],
              "docstring": null,
              "source_code": "def fetch_exchanges_by_chat(username: str, chat_name: str):\n    exchanges = list(dbexchanges.find({\"username\": username, \"chat_name\": chat_name}))\n    return exchanges",
              "start_line": 131,
              "end_line": 133,
              "context": {
                "calls": [
                  "database/db.py::find",
                  "database/db.py::list"
                ],
                "called_by": []
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "database.db.update_exchange_feedback",
              "name": "update_exchange_feedback",
              "args": [
                "exchange_id",
                "feedback"
              ],
              "docstring": null,
              "source_code": "def update_exchange_feedback(exchange_id, feedback: int):\n    # Update the feedback for a specific exchange\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback\": feedback}})\n    return result.modified_count",
              "start_line": 135,
              "end_line": 138,
              "context": {
                "calls": [
                  "database/db.py::update_one"
                ],
                "called_by": [
                  {
                    "file": "Frontend.py",
                    "function": "handle_feedback_change",
                    "mode": "function",
                    "line": 53
                  }
                ]
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "database.db.update_exchange_feedback_message",
              "name": "update_exchange_feedback_message",
              "args": [
                "exchange_id",
                "feedback_message"
              ],
              "docstring": null,
              "source_code": "def update_exchange_feedback_message(exchange_id, feedback_message: str):\n    # Update the feedback message for a specific exchange\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback_message\": feedback_message}})\n    return result.modified_count",
              "start_line": 140,
              "end_line": 143,
              "context": {
                "calls": [
                  "database/db.py::update_one"
                ],
                "called_by": [
                  {
                    "file": "Frontend.py",
                    "function": "render_exchange",
                    "mode": "function",
                    "line": 129
                  }
                ]
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "database.db.delete_chats_by_user",
              "name": "delete_chats_by_user",
              "args": [
                "username",
                "chat_name"
              ],
              "docstring": null,
              "source_code": "def delete_chats_by_user(username: str, chat_name: str):\n    # Delete all exchanges for a specific user and chat\n    result = dbexchanges.delete_many({\"username\": username, \"chat_name\": chat_name})\n    return result.deleted_count",
              "start_line": 145,
              "end_line": 148,
              "context": {
                "calls": [
                  "database/db.py::delete_many"
                ],
                "called_by": [
                  {
                    "file": "Frontend.py",
                    "function": "handle_delete_chat",
                    "mode": "function",
                    "line": 68
                  }
                ]
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "database.db.delete_exchange_by_id",
              "name": "delete_exchange_by_id",
              "args": [
                "exchange_id"
              ],
              "docstring": null,
              "source_code": "def delete_exchange_by_id(exchange_id: str):\n    # Delete a specific exchange by its ID\n    result = dbexchanges.delete_one({\"_id\": exchange_id})\n    return result.deleted_count",
              "start_line": 150,
              "end_line": 153,
              "context": {
                "calls": [
                  "database/db.py::delete_one"
                ],
                "called_by": [
                  {
                    "file": "Frontend.py",
                    "function": "handle_delete_exchange",
                    "mode": "function",
                    "line": 60
                  }
                ]
              }
            }
          ],
          "classes": []
        }
      },
      "frontend/Frontend.py": {
        "ast_nodes": {
          "imports": [
            "numpy",
            "datetime.datetime",
            "time",
            "pymongo.MongoClient",
            "dotenv.load_dotenv",
            "os",
            "sys",
            "logging",
            "traceback",
            "re",
            "streamlit_mermaid.st_mermaid",
            "backend.main",
            "database.db",
            "streamlit",
            "streamlit_authenticator"
          ],
          "functions": [
            {
              "mode": "function_analysis",
              "identifier": "frontend.Frontend.load_data_from_db",
              "name": "load_data_from_db",
              "args": [
                "username"
              ],
              "docstring": "L\u00e4dt existierende Chats aus der DB in den Session State",
              "source_code": "def load_data_from_db(username: str):\n    \"\"\"L\u00e4dt existierende Chats aus der DB in den Session State\"\"\"\n    if \"data_loaded\" not in st.session_state:\n        st.session_state.chats = {}\n        db_exchanges = db.fetch_exchanges_by_user(username)\n        for ex in db_exchanges:\n            c_name = ex.get(\"chat_name\", \"Unbenannt\")\n            if c_name not in st.session_state.chats:\n                st.session_state.chats[c_name] = {\"exchanges\": []}\n            if \"feedback\" not in ex or ex[\"feedback\"] is None:\n                ex[\"feedback\"] = np.nan\n            st.session_state.chats[c_name][\"exchanges\"].append(ex)\n            \n        if not st.session_state.chats:\n            st.session_state.chats[\"Chat 1\"] = {\"exchanges\": []}\n            st.session_state.active_chat = \"Chat 1\"\n        else:\n            first_chat = list(st.session_state.chats.keys())[0]\n            if \"active_chat\" not in st.session_state:\n                st.session_state.active_chat = first_chat\n        \n        st.session_state.data_loaded = True",
              "start_line": 25,
              "end_line": 46,
              "context": {
                "calls": [
                  "frontend/Frontend.py::append",
                  "frontend/Frontend.py::fetch_exchanges_by_user",
                  "frontend/Frontend.py::get",
                  "frontend/Frontend.py::keys",
                  "frontend/Frontend.py::list"
                ],
                "called_by": [
                  {
                    "file": "Frontend.py",
                    "function": "frontend.Frontend",
                    "mode": "module",
                    "line": 193
                  }
                ]
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "frontend.Frontend.handle_feedback_change",
              "name": "handle_feedback_change",
              "args": [
                "ex",
                "val"
              ],
              "docstring": "Update Feedback in State und DB",
              "source_code": "def handle_feedback_change(ex, val):\n    \"\"\"Update Feedback in State und DB\"\"\"\n    ex[\"feedback\"] = val\n    # DB Update\n    db.update_exchange_feedback(ex[\"_id\"], val)\n    st.rerun()",
              "start_line": 49,
              "end_line": 54,
              "context": {
                "calls": [
                  "frontend/Frontend.py::rerun",
                  "frontend/Frontend.py::update_exchange_feedback"
                ],
                "called_by": [
                  {
                    "file": "Frontend.py",
                    "function": "render_exchange",
                    "mode": "function",
                    "line": 117
                  },
                  {
                    "file": "Frontend.py",
                    "function": "render_exchange",
                    "mode": "function",
                    "line": 122
                  }
                ]
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "frontend.Frontend.handle_delete_exchange",
              "name": "handle_delete_exchange",
              "args": [
                "chat_name",
                "ex"
              ],
              "docstring": "L\u00f6scht Exchange aus State und DB",
              "source_code": "def handle_delete_exchange(chat_name, ex):\n    \"\"\"L\u00f6scht Exchange aus State und DB\"\"\"\n    # DB Delete\n    db.delete_exchange_by_id(ex[\"_id\"])\n    # State Delete\n    st.session_state.chats[chat_name][\"exchanges\"].remove(ex)\n    st.rerun()",
              "start_line": 57,
              "end_line": 63,
              "context": {
                "calls": [
                  "frontend/Frontend.py::delete_exchange_by_id",
                  "frontend/Frontend.py::remove",
                  "frontend/Frontend.py::rerun"
                ],
                "called_by": [
                  {
                    "file": "Frontend.py",
                    "function": "render_exchange",
                    "mode": "function",
                    "line": 146
                  }
                ]
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "frontend.Frontend.handle_delete_chat",
              "name": "handle_delete_chat",
              "args": [
                "username",
                "chat_name"
              ],
              "docstring": "L\u00f6scht kompletten Chat",
              "source_code": "def handle_delete_chat(username, chat_name):\n    \"\"\"L\u00f6scht kompletten Chat\"\"\"\n    # DB Delete\n    db.delete_chats_by_user(username, chat_name)\n    # State Delete\n    del st.session_state.chats[chat_name]\n    \n    # Neuen aktiven Chat setzen oder Default erstellen\n    if len(st.session_state.chats) > 0:\n        st.session_state.active_chat = list(st.session_state.chats.keys())[0]\n    else:\n        st.session_state.chats[\"Chat 1\"] = {\"exchanges\": []}\n        st.session_state.active_chat = \"Chat 1\"\n    st.rerun()",
              "start_line": 65,
              "end_line": 78,
              "context": {
                "calls": [
                  "frontend/Frontend.py::delete_chats_by_user",
                  "frontend/Frontend.py::keys",
                  "frontend/Frontend.py::len",
                  "frontend/Frontend.py::list",
                  "frontend/Frontend.py::rerun"
                ],
                "called_by": [
                  {
                    "file": "Frontend.py",
                    "function": "frontend.Frontend",
                    "mode": "module",
                    "line": 219
                  }
                ]
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "frontend.Frontend.render_text_with_mermaid",
              "name": "render_text_with_mermaid",
              "args": [
                "markdown_text"
              ],
              "docstring": "Splittet den Text bei ```mermaid Bl\u00f6cken und rendert Diagramme grafisch.",
              "source_code": "def render_text_with_mermaid(markdown_text):\n    \"\"\"\n    Splittet den Text bei ```mermaid Bl\u00f6cken und rendert Diagramme grafisch.\n    \"\"\"\n    if not markdown_text:\n        return\n\n    # Regex: Findet alles zwischen ```mermaid und ```\n    parts = re.split(r\"```mermaid\\s+(.*?)\\s+```\", markdown_text, flags=re.DOTALL)\n\n    for i, part in enumerate(parts):\n        # Gerade Indizes = Text\n        if i % 2 == 0:\n            if part.strip():\n                st.markdown(part)\n        # Ungerade Indizes = Mermaid Code\n        else:\n            try:\n                st_mermaid(part, key=f\"mermaid_{hash(part)}_{i}\")\n            except Exception:\n                st.code(part, language=\"mermaid\")",
              "start_line": 81,
              "end_line": 101,
              "context": {
                "calls": [
                  "frontend/Frontend.py::code",
                  "frontend/Frontend.py::enumerate",
                  "frontend/Frontend.py::hash",
                  "frontend/Frontend.py::markdown",
                  "frontend/Frontend.py::split",
                  "frontend/Frontend.py::st_mermaid",
                  "frontend/Frontend.py::strip"
                ],
                "called_by": [
                  {
                    "file": "Frontend.py",
                    "function": "render_exchange",
                    "mode": "function",
                    "line": 156
                  },
                  {
                    "file": "Frontend.py",
                    "function": "frontend.Frontend",
                    "mode": "module",
                    "line": 333
                  }
                ]
              }
            },
            {
              "mode": "function_analysis",
              "identifier": "frontend.Frontend.render_exchange",
              "name": "render_exchange",
              "args": [
                "ex",
                "current_chat_name"
              ],
              "docstring": "Anzeige einer Nachricht mit Toolbar f\u00fcr Feedback, Download, Nachricht und L\u00f6schen.",
              "source_code": "def render_exchange(ex, current_chat_name):\n    \"\"\"\n    Anzeige einer Nachricht mit Toolbar f\u00fcr Feedback, Download, Nachricht und L\u00f6schen.\n    \"\"\"\n    st.chat_message(\"user\").write(ex[\"question\"])\n    \n    with st.chat_message(\"assistant\"):\n        # Layout: Buttons kompakt links\n        cols = st.columns([3, 3, 3, 3, 3, 15])\n        \n        with cols[0]:\n            type_primary = ex.get(\"feedback\") == 1\n            if st.button(\"\ud83d\udc4d\", key=f\"up_{ex['_id']}\", type=\"primary\" if type_primary else \"secondary\", help=\"Positiv\"):\n                handle_feedback_change(ex, 1)\n\n        with cols[1]:\n            type_primary = ex.get(\"feedback\") == 0\n            if st.button(\"\ud83d\udc4e\", key=f\"down_{ex['_id']}\", type=\"primary\" if type_primary else \"secondary\", help=\"Negativ\"):\n                handle_feedback_change(ex, 0)\n\n        with cols[2]:\n            with st.popover(\"\ud83d\udcac\", help=\"Feedback schreiben\"):\n                msg = st.text_area(\"Feedback Nachricht:\", value=ex.get(\"feedback_message\", \"\"), key=f\"txt_{ex['_id']}\")\n                if st.button(\"Speichern\", key=f\"save_{ex['_id']}\"):\n                    ex[\"feedback_message\"] = msg\n                    db.update_exchange_feedback_message(ex[\"_id\"], msg)\n                    st.success(\"Gespeichert!\")\n                    time.sleep(1)\n                    st.rerun()\n\n        with cols[3]:\n            st.download_button(\n                label=\"\ud83d\udce5\",\n                data=ex[\"answer\"],\n                file_name=f\"response_{ex['_id']}.md\",\n                mime=\"text/markdown\",\n                key=f\"dl_{ex['_id']}\",\n                help=\"Download Markdown\"\n            )\n\n        with cols[4]:\n            if st.button(\"\ud83d\uddd1\ufe0f\", key=f\"del_{ex['_id']}\", help=\"Nachricht l\u00f6schen\"):\n                handle_delete_exchange(current_chat_name, ex)\n\n        with cols[5]:\n             if ex.get(\"feedback\") == 1:\n                 st.caption(\"Positiv bewertet\")\n             elif ex.get(\"feedback\") == 0:\n                 st.caption(\"Negativ bewertet\")\n\n        # Inhalt Scrollbar\n        with st.container(height=500):\n             render_text_with_mermaid(ex[\"answer\"])",
              "start_line": 104,
              "end_line": 156,
              "context": {
                "calls": [
                  "frontend/Frontend.py::button",
                  "frontend/Frontend.py::caption",
                  "frontend/Frontend.py::chat_message",
                  "frontend/Frontend.py::columns",
                  "frontend/Frontend.py::container",
                  "frontend/Frontend.py::download_button",
                  "frontend/Frontend.py::get",
                  "frontend/Frontend.py::handle_delete_exchange",
                  "frontend/Frontend.py::handle_feedback_change",
                  "frontend/Frontend.py::popover",
                  "frontend/Frontend.py::render_text_with_mermaid",
                  "frontend/Frontend.py::rerun",
                  "frontend/Frontend.py::sleep",
                  "frontend/Frontend.py::success",
                  "frontend/Frontend.py::text_area",
                  "frontend/Frontend.py::update_exchange_feedback_message",
                  "frontend/Frontend.py::write"
                ],
                "called_by": [
                  {
                    "file": "Frontend.py",
                    "function": "frontend.Frontend",
                    "mode": "module",
                    "line": 285
                  }
                ]
              }
            }
          ],
          "classes": []
        }
      },
      "schemas/types.py": {
        "ast_nodes": {
          "imports": [
            "typing.List",
            "typing.Optional",
            "typing.Literal",
            "pydantic.BaseModel",
            "pydantic.ValidationError"
          ],
          "functions": [],
          "classes": [
            {
              "mode": "class_analysis",
              "identifier": "schemas.types.ParameterDescription",
              "name": "ParameterDescription",
              "docstring": "Describes a single parameter of a function.",
              "source_code": "class ParameterDescription(BaseModel):\n    \"\"\"Describes a single parameter of a function.\"\"\"\n    name: str\n    type: str\n    description: str",
              "start_line": 6,
              "end_line": 10,
              "context": {
                "dependencies": [],
                "instantiated_by": [],
                "method_context": []
              }
            },
            {
              "mode": "class_analysis",
              "identifier": "schemas.types.ReturnDescription",
              "name": "ReturnDescription",
              "docstring": "Describes the return value of a function.",
              "source_code": "class ReturnDescription(BaseModel):\n    \"\"\"Describes the return value of a function.\"\"\"\n    name: str\n    type: str\n    description: str",
              "start_line": 12,
              "end_line": 16,
              "context": {
                "dependencies": [],
                "instantiated_by": [],
                "method_context": []
              }
            },
            {
              "mode": "class_analysis",
              "identifier": "schemas.types.UsageContext",
              "name": "UsageContext",
              "docstring": "Describes the calling context of a function.",
              "source_code": "class UsageContext(BaseModel):\n    \"\"\"Describes the calling context of a function.\"\"\"\n    calls: str\n    called_by: str",
              "start_line": 18,
              "end_line": 21,
              "context": {
                "dependencies": [],
                "instantiated_by": [],
                "method_context": []
              }
            },
            {
              "mode": "class_analysis",
              "identifier": "schemas.types.FunctionDescription",
              "name": "FunctionDescription",
              "docstring": "Contains the detailed analysis of a function's purpose and signature.",
              "source_code": "class FunctionDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a function's purpose and signature.\"\"\"\n    overall: str\n    parameters: List[ParameterDescription]\n    returns: List[ReturnDescription]\n    usage_context: UsageContext",
              "start_line": 23,
              "end_line": 28,
              "context": {
                "dependencies": [],
                "instantiated_by": [],
                "method_context": []
              }
            },
            {
              "mode": "class_analysis",
              "identifier": "schemas.types.FunctionAnalysis",
              "name": "FunctionAnalysis",
              "docstring": "The main model representing the entire JSON schema for a function.",
              "source_code": "class FunctionAnalysis(BaseModel):\n    \"\"\"The main model representing the entire JSON schema for a function.\"\"\"\n    identifier: str\n    description: FunctionDescription\n    error: Optional[str] = None",
              "start_line": 30,
              "end_line": 34,
              "context": {
                "dependencies": [],
                "instantiated_by": [],
                "method_context": []
              }
            },
            {
              "mode": "class_analysis",
              "identifier": "schemas.types.ConstructorDescription",
              "name": "ConstructorDescription",
              "docstring": "Describes the __init__ method of a class.",
              "source_code": "class ConstructorDescription(BaseModel):\n    \"\"\"Describes the __init__ method of a class.\"\"\"\n    description: str\n    parameters: List[ParameterDescription]",
              "start_line": 39,
              "end_line": 42,
              "context": {
                "dependencies": [],
                "instantiated_by": [],
                "method_context": []
              }
            },
            {
              "mode": "class_analysis",
              "identifier": "schemas.types.ClassContext",
              "name": "ClassContext",
              "docstring": "Describes the class's external dependencies and primary points of instantiation.",
              "source_code": "class ClassContext(BaseModel):\n    \"\"\"Describes the class's external dependencies and primary points of instantiation.\"\"\"\n    dependencies: str\n    instantiated_by: str",
              "start_line": 44,
              "end_line": 47,
              "context": {
                "dependencies": [],
                "instantiated_by": [],
                "method_context": []
              }
            },
            {
              "mode": "class_analysis",
              "identifier": "schemas.types.ClassDescription",
              "name": "ClassDescription",
              "docstring": "Contains the detailed analysis of a class's purpose, constructor, and methods.",
              "source_code": "class ClassDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a class's purpose, constructor, and methods.\"\"\"\n    overall: str\n    init_method: ConstructorDescription\n    methods: List[FunctionAnalysis]\n    usage_context: ClassContext",
              "start_line": 49,
              "end_line": 54,
              "context": {
                "dependencies": [],
                "instantiated_by": [],
                "method_context": []
              }
            },
            {
              "mode": "class_analysis",
              "identifier": "schemas.types.ClassAnalysis",
              "name": "ClassAnalysis",
              "docstring": "The main model for the entire JSON schema for a class.",
              "source_code": "class ClassAnalysis(BaseModel):\n    \"\"\"The main model for the entire JSON schema for a class.\"\"\"\n    identifier: str\n    description: ClassDescription\n    error: Optional[str] = None",
              "start_line": 56,
              "end_line": 60,
              "context": {
                "dependencies": [],
                "instantiated_by": [],
                "method_context": []
              }
            },
            {
              "mode": "class_analysis",
              "identifier": "schemas.types.CallInfo",
              "name": "CallInfo",
              "docstring": "Represents a specific call event from the relationship analyzer.\nUsed in 'called_by' and 'instantiated_by' lists.",
              "source_code": "class CallInfo(BaseModel):\n    \"\"\"\n    Represents a specific call event from the relationship analyzer.\n    Used in 'called_by' and 'instantiated_by' lists.\n    \"\"\"\n    file: str\n    function: str  # Name des Aufrufers\n    mode: str      # z.B. 'method', 'function', 'module'\n    line: int",
              "start_line": 65,
              "end_line": 73,
              "context": {
                "dependencies": [],
                "instantiated_by": [],
                "method_context": []
              }
            },
            {
              "mode": "class_analysis",
              "identifier": "schemas.types.FunctionContextInput",
              "name": "FunctionContextInput",
              "docstring": "Structured context for analyzing a function.",
              "source_code": "class FunctionContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a function.\"\"\"\n    calls: List[str]\n    called_by: List[CallInfo]",
              "start_line": 78,
              "end_line": 81,
              "context": {
                "dependencies": [],
                "instantiated_by": [
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 202
                  }
                ],
                "method_context": []
              }
            },
            {
              "mode": "class_analysis",
              "identifier": "schemas.types.FunctionAnalysisInput",
              "name": "FunctionAnalysisInput",
              "docstring": "The required input to generate a FunctionAnalysis object.",
              "source_code": "class FunctionAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a FunctionAnalysis object.\"\"\"\n    mode: Literal[\"function_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: FunctionContextInput",
              "start_line": 83,
              "end_line": 89,
              "context": {
                "dependencies": [],
                "instantiated_by": [
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 207
                  }
                ],
                "method_context": []
              }
            },
            {
              "mode": "class_analysis",
              "identifier": "schemas.types.MethodContextInput",
              "name": "MethodContextInput",
              "docstring": "Structured context for a classes methods",
              "source_code": "class MethodContextInput(BaseModel):\n    \"\"\"Structured context for a classes methods\"\"\"\n    identifier: str\n    calls: List[str]\n    called_by: List[CallInfo]\n    args: List[str]\n    docstring: Optional[str]",
              "start_line": 94,
              "end_line": 100,
              "context": {
                "dependencies": [],
                "instantiated_by": [
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 227
                  }
                ],
                "method_context": []
              }
            },
            {
              "mode": "class_analysis",
              "identifier": "schemas.types.ClassContextInput",
              "name": "ClassContextInput",
              "docstring": "Structured context for analyzing a class.",
              "source_code": "class ClassContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a class.\"\"\"\n    dependencies: List[str]\n    instantiated_by: List[CallInfo]\n    method_context: List[MethodContextInput]",
              "start_line": 102,
              "end_line": 106,
              "context": {
                "dependencies": [],
                "instantiated_by": [
                  {
                    "file": "HelperLLM.py",
                    "function": "main_orchestrator",
                    "mode": "function",
                    "line": 347
                  },
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 239
                  }
                ],
                "method_context": []
              }
            },
            {
              "mode": "class_analysis",
              "identifier": "schemas.types.ClassAnalysisInput",
              "name": "ClassAnalysisInput",
              "docstring": "The required input to generate a ClassAnalysis object.",
              "source_code": "class ClassAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a ClassAnalysis object.\"\"\"\n    mode: Literal[\"class_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: ClassContextInput",
              "start_line": 108,
              "end_line": 114,
              "context": {
                "dependencies": [],
                "instantiated_by": [
                  {
                    "file": "HelperLLM.py",
                    "function": "main_orchestrator",
                    "mode": "function",
                    "line": 316
                  },
                  {
                    "file": "main.py",
                    "function": "main_workflow",
                    "mode": "function",
                    "line": 245
                  }
                ],
                "method_context": []
              }
            }
          ]
        }
      }
    }
  },
  "analysis_results": {
    "functions": {
      "backend.AST_Schema.path_to_module": {
        "identifier": "backend.AST_Schema.path_to_module",
        "description": {
          "overall": "This function converts a given file path into a Python module path relative to a specified project root. It handles potential errors during relative path calculation and normalizes the path by replacing directory separators with dots. It also specifically handles the case of `__init__.py` files to correctly represent package modules.",
          "parameters": [
            {
              "name": "filepath",
              "type": "str",
              "description": "The absolute or relative path to the file."
            },
            {
              "name": "project_root",
              "type": "str",
              "description": "The root directory of the Python project."
            }
          ],
          "returns": [
            {
              "name": "module_path",
              "type": "str",
              "description": "The calculated Python module path."
            }
          ],
          "usage_context": {
            "calls": "This function calls `os.path.relpath` to get the relative path, `os.path.basename` if `relpath` fails, and uses string methods like `endswith` and `replace` for path manipulation.",
            "called_by": "This function is called by the `__init__` method within the `AST_Schema.py` file."
          }
        },
        "error": null
      },
      "backend.File_Dependency.build_file_dependency_graph": {
        "identifier": "backend.File_Dependency.build_file_dependency_graph",
        "description": {
          "overall": "This function constructs a directed graph representing file dependencies within a repository. It takes a filename, an Abstract Syntax Tree (AST) of the file, and the repository root as input. The function initializes a NetworkX directed graph and uses a custom visitor, `FileDependencyGraph`, to traverse the AST and identify import dependencies. It then populates the graph by adding nodes for each file and edges representing the import relationships. Finally, it returns the constructed graph.",
          "parameters": [
            {
              "name": "filename",
              "type": "str",
              "description": "The name of the file for which the dependency graph is being built."
            },
            {
              "name": "tree",
              "type": "AST",
              "description": "The Abstract Syntax Tree (AST) representing the structure of the input file."
            },
            {
              "name": "repo_root",
              "type": "str",
              "description": "The absolute path to the root directory of the repository."
            }
          ],
          "returns": [
            {
              "name": "graph",
              "type": "nx.DiGraph",
              "description": "A directed graph where nodes represent files and edges represent import dependencies."
            }
          ],
          "usage_context": {
            "calls": "This function initializes a NetworkX DiGraph, instantiates and uses a FileDependencyGraph visitor to process the AST, and then adds nodes and edges to the graph based on the identified import dependencies.",
            "called_by": "This function is called by the `build_repository_graph` function in the `File_Dependency.py` file."
          }
        },
        "error": null
      },
      "backend.File_Dependency.build_repository_graph": {
        "identifier": "backend.File_Dependency.build_repository_graph",
        "description": {
          "overall": "This function constructs a directed graph representing the dependencies between Python files within a Git repository. It iterates through all Python files, parses their content to build individual file dependency graphs, and then merges these into a single global graph. The function focuses on file-level dependencies, adding nodes for files and edges for calls between them. It filters out non-Python files and processes only files ending with '.py'.",
          "parameters": [
            {
              "name": "repository",
              "type": "GitRepository",
              "description": "An object representing the Git repository to analyze, providing methods to access files and repository information."
            }
          ],
          "returns": [
            {
              "name": "global_graph",
              "type": "nx.DiGraph",
              "description": "A NetworkX directed graph where nodes represent files and edges represent dependencies between them."
            }
          ],
          "usage_context": {
            "calls": "This function calls methods such as `get_all_files`, `basename`, `endswith`, `removesuffix`, `parse`, `build_file_dependency_graph`, `add_node`, and `add_edge` to process files and construct the dependency graph.",
            "called_by": "This function is called by the `backend.File_Dependency` class constructor."
          }
        },
        "error": null
      },
      "backend.File_Dependency.get_all_temp_files": {
        "identifier": "backend.File_Dependency.get_all_temp_files",
        "description": {
          "overall": "This function identifies and returns all Python files within a specified directory and its subdirectories. It resolves the root path of the given directory and then uses a recursive glob pattern to find all files ending with the '.py' extension. The function returns a list of these files, represented as paths relative to the resolved root directory.",
          "parameters": [
            {
              "name": "directory",
              "type": "str",
              "description": "The path to the directory to search for Python files."
            }
          ],
          "returns": [
            {
              "name": "all_files",
              "type": "list[Path]",
              "description": "A list of Path objects, where each Path represents a Python file found within the directory, relative to the root path."
            }
          ],
          "usage_context": {
            "calls": "This function calls Path, relative_to, resolve, and rglob methods, likely from the pathlib module, to manipulate and search for files within a directory structure.",
            "called_by": "This function is called by the _resolve_module_name method in File_Dependency.py."
          }
        },
        "error": null
      },
      "backend.File_Dependency.nx_to_mermaid_with_folders": {
        "identifier": "backend.File_Dependency.nx_to_mermaid_with_folders",
        "description": {
          "overall": "This function takes a NetworkX directed graph (G) representing file dependencies and converts it into a Mermaid.js graph definition string. It organizes nodes into subgraphs based on their folder structure, creating a visual representation of how files within folders relate to each other. The function iterates through the graph's nodes to map files to their respective folders, then constructs Mermaid syntax for subgraphs and individual files. Finally, it adds edges to represent the dependencies between files, outputting a single string that can be rendered by Mermaid.",
          "parameters": [
            {
              "name": "G",
              "type": "nx.DiGraph",
              "description": "A NetworkX directed graph where nodes represent file paths and edges represent dependencies between them."
            }
          ],
          "returns": [
            {
              "name": "mermaid_string",
              "type": "str",
              "description": "A string formatted for Mermaid.js, representing the graph with files organized into folders as subgraphs."
            }
          ],
          "usage_context": {
            "calls": "This function calls methods like append, defaultdict, items, join, replace, and split to process the graph data and construct the Mermaid string.",
            "called_by": "This function is called from the backend.File_Dependency module, specifically at line 238."
          }
        },
        "error": null
      },
      "backend.HelperLLM.main_orchestrator": {
        "identifier": "backend.HelperLLM.main_orchestrator",
        "description": {
          "overall": "This function serves as a testing orchestrator for the LLMHelper class, simulating the process of generating documentation for Python classes and their methods. It defines dummy input data for several functions ('add_item', 'check_stock', 'generate_report') and their corresponding pre-computed analyses. It then constructs a `ClassAnalysisInput` object for an 'InventoryManager' class, including the analyses of its methods. Finally, it utilizes an `LLMHelper` instance to generate documentation for these classes and prints the aggregated results in JSON format.",
          "parameters": [],
          "returns": [],
          "usage_context": {
            "calls": "This function calls backend.HelperLLM.py::ClassAnalysisInput, backend.HelperLLM.py::ClassContextInput, backend.HelperLLM.py::LLMHelper, backend.HelperLLM.py::dumps, backend.HelperLLM.py::generate_for_functions, backend.HelperLLM.py::info, backend.HelperLLM.py::model_dump, backend.HelperLLM.py::model_validate, backend.HelperLLM.py::print, and backend.HelperLLM.py::warning.",
            "called_by": "This function is called by backend.HelperLLM."
          }
        },
        "error": null
      },
      "backend.callgraph.build_callGraph": {
        "identifier": "backend.callgraph.build_callGraph",
        "description": {
          "overall": "This function constructs a call graph from a Python Abstract Syntax Tree (AST). The resulting graph is a directed graph (networkx.DiGraph) where nodes represent functions, methods, and scopes (global or main block), and edges represent function or method calls between them. It initializes a `CallGraph` visitor, traverses the AST to populate the visitor's data, and then adds edges to the graph based on the collected caller-callee relationships. Finally, it returns the complete call graph.",
          "parameters": [
            {
              "name": "tree",
              "type": "ast.AST",
              "description": "The Abstract Syntax Tree of the Python file to be analyzed."
            },
            {
              "name": "filename",
              "type": "str",
              "description": "The name of the analyzed file, used for context within the call graph."
            }
          ],
          "returns": [
            {
              "name": "graph",
              "type": "nx.DiGraph",
              "description": "The complete directed call graph representing function and method calls within the analyzed Python code."
            }
          ],
          "usage_context": {
            "calls": "This function utilizes a `CallGraph` visitor, calls its `visit` method, accesses its `edges` and `graph` attributes, and uses the `add_edge` method to build the graph.",
            "called_by": "This function is called by `analyze_repository` in `AST_Schema.py` and `build_global_callgraph` in `callgraph.py`."
          }
        },
        "error": null
      },
      "backend.callgraph.graph_to_adj_list": {
        "identifier": "backend.callgraph.graph_to_adj_list",
        "description": {
          "overall": "This function converts a directed graph (nx.DiGraph) from the networkx library into an adjacency list format, which is suitable for JSON serialization. It iterates through each node in the graph, retrieves its successors (functions it calls), and stores this information in a dictionary. The keys of the dictionary represent the calling nodes (callers), and the values are lists of the called nodes (callees). The function ensures a consistent output by sorting both the nodes and their successors before adding them to the adjacency list. Only nodes that actually call other functions are included in the final output.",
          "parameters": [
            {
              "name": "graph",
              "type": "nx.DiGraph",
              "description": "The directed graph representing the call graph to be converted."
            }
          ],
          "returns": [
            {
              "name": "adj_list",
              "type": "Dict[str, list[str]]",
              "description": "An adjacency list where keys are caller nodes (strings) and values are lists of callee nodes (strings)."
            }
          ],
          "usage_context": {
            "calls": "This function calls internal list operations, graph node and successor retrieval methods, and sorting functions.",
            "called_by": "This function is not called by any other function within the provided context."
          }
        },
        "error": null
      },
      "backend.callgraph.build_global_callgraph": {
        "identifier": "backend.callgraph.build_global_callgraph",
        "description": {
          "overall": "This function constructs a global call graph for a given Git repository. It iterates through all Python files in the repository, parses their Abstract Syntax Trees (ASTs), and builds a call graph for each file. These individual file call graphs are then merged into a single, comprehensive global call graph. The function uses the `networkx` library to represent the graph structure, where nodes represent functions or methods and edges represent calls between them. It handles file filtering to process only Python files and extracts filenames for context within the graph.",
          "parameters": [
            {
              "name": "repo",
              "type": "GitRepository",
              "description": "An object representing the Git repository to analyze, providing access to its files."
            }
          ],
          "returns": [
            {
              "name": "global_graph",
              "type": "nx.DiGraph",
              "description": "A directed graph representing the global call graph of the repository, where nodes are functions/methods and edges indicate calls."
            }
          ],
          "usage_context": {
            "calls": "This function calls methods for file system operations (like `endswith`, `basename`, `removesuffix`), AST parsing (`ast.parse`), building individual call graphs (`build_callGraph`), and graph manipulation (`nx.DiGraph`, `add_node`, `add_edge`). It also utilizes repository methods to retrieve all files (`get_all_files`).",
            "called_by": "This function is called from the `backend.callgraph` module at line 262."
          }
        },
        "error": null
      },
      "backend.callgraph.make_safe_dot": {
        "identifier": "backend.callgraph.make_safe_dot",
        "description": {
          "overall": "This function takes a NetworkX directed graph and an output path as input. It creates a safe representation of the graph by relabeling its nodes to be simple sequential identifiers (e.g., 'n0', 'n1'). This is likely done to avoid issues with special characters or complex names in the DOT format. The original node names are preserved as labels in the relabeled graph. Finally, it writes the modified graph to a DOT file at the specified output path.",
          "parameters": [
            {
              "name": "graph",
              "type": "nx.DiGraph",
              "description": "The input directed graph to be processed."
            },
            {
              "name": "out_path",
              "type": "str",
              "description": "The file path where the DOT representation of the graph will be saved."
            }
          ],
          "returns": [],
          "usage_context": {
            "calls": "This function calls methods for copying graphs, iterating through nodes, relabeling nodes, and writing the graph to a DOT file.",
            "called_by": "This function is called from the `backend.callgraph` module."
          }
        },
        "error": null
      },
      "backend.main.calculate_net_time": {
        "identifier": "backend.main.calculate_net_time",
        "description": {
          "overall": "This function calculates the net time spent on a task, excluding time spent sleeping due to rate limits. It takes the start and end times, total items processed, batch size, and model name as input. If the model name does not start with 'gemini-', the total duration is returned directly. For 'gemini-' models, it calculates the number of batches, the total sleep time based on the number of batches, and subtracts this sleep time from the total duration. The function ensures that the returned net time is not negative.",
          "parameters": [
            {
              "name": "start_time",
              "type": "Any",
              "description": "The timestamp when the task started."
            },
            {
              "name": "end_time",
              "type": "Any",
              "description": "The timestamp when the task ended."
            },
            {
              "name": "total_items",
              "type": "int",
              "description": "The total number of items processed."
            },
            {
              "name": "batch_size",
              "type": "int",
              "description": "The number of items processed in each batch."
            },
            {
              "name": "model_name",
              "type": "str",
              "description": "The name of the model being used."
            }
          ],
          "returns": [
            {
              "name": "net_time",
              "type": "Any",
              "description": "The calculated net time, excluding sleep durations, or the total duration if the model is not 'gemini-' based. Returns 0 if total_items is 0 for 'gemini-' models. The returned value is guaranteed to be non-negative."
            }
          ],
          "usage_context": {
            "calls": "This function calls math.ceil to determine the number of batches, math.max to ensure sleep count is non-negative and net time is non-negative, and the string method startswith to check the model name.",
            "called_by": "This function is called by the main_workflow function in main.py on lines 290 and 321."
          }
        },
        "error": null
      },
      "backend.main.main_workflow": {
        "identifier": "backend.main.main_workflow",
        "description": {
          "overall": "The main_workflow function orchestrates a complex process of analyzing a software repository to generate documentation. It begins by extracting a repository URL from user input and cloning the repository. It then proceeds to extract basic project information, construct a file tree, and perform relationship analysis (calls and instantiations) using specialized tools. The core of the workflow involves generating an Abstract Syntax Tree (AST) schema and enriching it with the relationship data. Finally, it prepares inputs for two Language Models (LLMs): a Helper LLM for analyzing individual functions and classes, and a Main LLM for generating a final report based on the combined analysis. The function handles API key management, model selection, and includes status callbacks for user feedback throughout the process. It also incorporates error handling and timing metrics for both LLM calls.",
          "parameters": [
            {
              "name": "input",
              "type": "Any",
              "description": "The raw user input, expected to contain a repository URL."
            },
            {
              "name": "api_keys",
              "type": "dict",
              "description": "A dictionary containing API keys for different services (e.g., 'gemini', 'gpt', 'ollama')."
            },
            {
              "name": "model_names",
              "type": "dict",
              "description": "A dictionary specifying the names of the models to be used for helper and main LLM tasks."
            },
            {
              "name": "status_callback",
              "type": "Callable",
              "description": "An optional callback function to provide status updates during the workflow execution."
            }
          ],
          "returns": [
            {
              "name": "report",
              "type": "str",
              "description": "The final generated documentation report as a string."
            },
            {
              "name": "metrics",
              "type": "dict",
              "description": "A dictionary containing performance metrics, including helper LLM time, main LLM time, total active time, and the models used."
            }
          ],
          "usage_context": {
            "calls": "This function calls numerous internal and external tools and LLMs, including GitRepository, ProjektInfoExtractor, ProjectAnalyzer, ASTAnalyzer, LLMHelper, and MainLLM, as well as various utility functions for file operations, logging, and time calculation.",
            "called_by": "This function is called by the Frontend.py script and the main.py module, indicating it serves as a central orchestrator for backend processing."
          }
        },
        "error": null
      },
      "backend.main.update_status": {
        "identifier": "backend.main.update_status",
        "description": {
          "overall": "This function is designed to report status updates. It first checks if a callback function named 'status_callback' is defined and, if so, it invokes this callback with the provided message. Subsequently, it logs the message using the 'logging.info' function. This dual approach allows for both custom handling of status messages via the callback and standard logging for monitoring purposes.",
          "parameters": [
            {
              "name": "msg",
              "type": "Any",
              "description": "The status message to be reported and logged."
            }
          ],
          "returns": [],
          "usage_context": {
            "calls": "This function calls the 'logging.info' function and conditionally calls a 'status_callback' function.",
            "called_by": "This function is called multiple times within the 'main_workflow' function in 'main.py'."
          }
        },
        "error": null
      },
      "backend.relationship_analyzer.path_to_module": {
        "identifier": "backend.relationship_analyzer.path_to_module",
        "description": {
          "overall": "This function converts a given file path into a Python module path. It first calculates the relative path of the file with respect to a project root. If the file is a Python file (ends with '.py'), the extension is removed. The path separators are then replaced with dots to form a module path. Special handling is included for '__init__.py' files, where the trailing '__init__' is removed from the module path.",
          "parameters": [
            {
              "name": "filepath",
              "type": "str",
              "description": "The absolute or relative path to the file."
            },
            {
              "name": "project_root",
              "type": "str",
              "description": "The root directory of the project from which the relative path is calculated."
            }
          ],
          "returns": [
            {
              "name": "module_path",
              "type": "str",
              "description": "The calculated Python module path derived from the filepath."
            }
          ],
          "usage_context": {
            "calls": "This function utilizes os.path.relpath, os.path.basename, str.endswith, and str.replace to manipulate file paths and convert them into module paths.",
            "called_by": "This function is called by '_collect_definitions' and '__init__' within the 'relationship_analyzer.py' file."
          }
        },
        "error": null
      },
      "database.db.encrypt_text": {
        "identifier": "database.db.encrypt_text",
        "description": {
          "overall": "This function encrypts a given string using a pre-defined cipher suite. It first checks if the input text is empty or if the cipher suite is not initialized. If either condition is true, it returns the original text without encryption. Otherwise, it encodes the input text into bytes, encrypts these bytes using the cipher suite, and then decodes the resulting encrypted bytes back into a string before returning it.",
          "parameters": [
            {
              "name": "text",
              "type": "str",
              "description": "The string to be encrypted."
            }
          ],
          "returns": [
            {
              "name": "encrypted_text",
              "type": "str",
              "description": "The encrypted string, or the original string if encryption could not be performed."
            }
          ],
          "usage_context": {
            "calls": "This function calls the encode and decode methods on the input string, and the encrypt method on the cipher_suite object.",
            "called_by": "This function is called by the update_gemini_key function in db.py."
          }
        },
        "error": null
      },
      "database.db.decrypt_text": {
        "identifier": "database.db.decrypt_text",
        "description": {
          "overall": "This function decrypts a given string using a cipher suite. It first checks if the input text or the cipher suite is invalid; if so, it returns the original text. Otherwise, it attempts to decrypt the text by encoding it, decrypting it, and then decoding the result. If any exception occurs during the decryption process, the original text is returned.",
          "parameters": [
            {
              "name": "text",
              "type": "str",
              "description": "The encrypted text string to be decrypted."
            }
          ],
          "returns": [
            {
              "name": "decrypted_text",
              "type": "str",
              "description": "The decrypted string, or the original string if decryption fails or is not possible."
            }
          ],
          "usage_context": {
            "calls": "This function calls encode, decrypt, and decode methods, likely related to cryptographic operations.",
            "called_by": "This function is called by the get_decrypted_api_keys function in db.py."
          }
        },
        "error": null
      },
      "database.db.insert_user": {
        "identifier": "database.db.insert_user",
        "description": {
          "overall": "This function inserts a new user record into a database collection named 'dbusers'. It takes a username, name, and password as input. The password is hashed using a provided hashing function before being stored along with other user details like API keys and base URLs. The function returns the unique identifier of the newly inserted user record.",
          "parameters": [
            {
              "name": "username",
              "type": "str",
              "description": "The unique username for the new user."
            },
            {
              "name": "name",
              "type": "str",
              "description": "The full name of the new user."
            },
            {
              "name": "password",
              "type": "str",
              "description": "The plain text password for the new user, which will be hashed."
            }
          ],
          "returns": [
            {
              "name": "inserted_id",
              "type": "Any",
              "description": "The unique identifier of the newly inserted user document in the database."
            }
          ],
          "usage_context": {
            "calls": "This function calls a hashing function (likely from 'stauth.hasher') to hash the password and 'dbusers.insert_one' to insert the user document into the database.",
            "called_by": "This function is not called by any other functions within the provided context."
          }
        },
        "error": null
      },
      "database.db.fetch_all_users": {
        "identifier": "database.db.fetch_all_users",
        "description": {
          "overall": "This function retrieves all user records from a database collection named 'dbusers'. It queries the collection and converts the result into a list. The function is designed to fetch all available user data without any filtering or specific selection criteria.",
          "parameters": [],
          "returns": [
            {
              "name": "users",
              "type": "list",
              "description": "A list containing all user documents fetched from the database."
            }
          ],
          "usage_context": {
            "calls": "This function calls the 'find' method on the 'dbusers' object and the 'list' constructor.",
            "called_by": "This function is called by the 'frontend.Frontend' constructor in the 'Frontend.py' file."
          }
        },
        "error": null
      },
      "database.db.fetch_user": {
        "identifier": "database.db.fetch_user",
        "description": {
          "overall": "This function retrieves a single user record from the database based on their username. It queries the `dbusers` collection using the provided username as the document's `_id`. The function then returns the found user document or None if no user matches the given username.",
          "parameters": [
            {
              "name": "username",
              "type": "str",
              "description": "The username of the user to fetch from the database."
            }
          ],
          "returns": [
            {
              "name": "user",
              "type": "Any",
              "description": "The user document found in the database, or None if the user does not exist."
            }
          ],
          "usage_context": {
            "calls": "This function calls `database/db.py::find_one` to perform the database query.",
            "called_by": "This function is not called by any other functions within the provided context."
          }
        },
        "error": null
      },
      "database.db.update_gemini_key": {
        "identifier": "database.db.update_gemini_key",
        "description": {
          "overall": "This function updates the Gemini API key for a given user in the database. It first encrypts the provided API key using a helper function `encrypt_text`. Then, it uses the `update_one` method to find the user by their username and set the encrypted API key in the database. The function returns the count of modified documents, indicating whether the update was successful.",
          "parameters": [
            {
              "name": "username",
              "type": "str",
              "description": "The username of the user whose Gemini API key needs to be updated."
            },
            {
              "name": "gemini_api_key",
              "type": "str",
              "description": "The new Gemini API key to be set for the user."
            }
          ],
          "returns": [
            {
              "name": "modified_count",
              "type": "int",
              "description": "The number of documents that were modified in the database. Typically 1 if the update was successful, 0 otherwise."
            }
          ],
          "usage_context": {
            "calls": "This function calls `encrypt_text` to encrypt the API key and `update_one` to modify the database record.",
            "called_by": "This function is called by the `frontend.Frontend` class in `Frontend.py`."
          }
        },
        "error": null
      },
      "database.db.update_ollama_url": {
        "identifier": "database.db.update_ollama_url",
        "description": {
          "overall": "This function updates the Ollama Base URL for a specific user in the database. It takes the username and the new Ollama base URL as input. The function then uses the `dbusers.update_one` method to find the user document by their username and set the `ollama_base_url` field to the provided value. Finally, it returns the count of modified documents, which should ideally be 1 if the update was successful.",
          "parameters": [
            {
              "name": "username",
              "type": "str",
              "description": "The unique identifier for the user whose Ollama URL needs to be updated."
            },
            {
              "name": "ollama_base_url",
              "type": "str",
              "description": "The new base URL for the Ollama service to be associated with the user."
            }
          ],
          "returns": [
            {
              "name": "modified_count",
              "type": "int",
              "description": "An integer representing the number of documents that were modified in the database. This is expected to be 1 if the update was successful for the specified user."
            }
          ],
          "usage_context": {
            "calls": "This function calls the `database/db.py::update_one` method to perform the database update operation.",
            "called_by": "This function is called by the `frontend.Frontend` constructor in the `Frontend.py` file."
          }
        },
        "error": null
      },
      "database.db.fetch_gemini_key": {
        "identifier": "database.db.fetch_gemini_key",
        "description": {
          "overall": "This function retrieves the Gemini API key associated with a given username from the database. It queries the database for a specific user's record and extracts the 'gemini_api_key' field. If the key is found, it is returned; otherwise, it returns None.",
          "parameters": [
            {
              "name": "username",
              "type": "str",
              "description": "The username of the user whose Gemini API key needs to be fetched."
            }
          ],
          "returns": [
            {
              "name": "gemini_api_key",
              "type": "str | None",
              "description": "The Gemini API key for the specified user, or None if not found."
            }
          ],
          "usage_context": {
            "calls": "This function calls `dbusers.find_one` to query the database and `user.get` to safely retrieve the API key from the result.",
            "called_by": "This function is not called by any other function within the provided context."
          }
        },
        "error": null
      },
      "database.db.fetch_ollama_url": {
        "identifier": "database.db.fetch_ollama_url",
        "description": {
          "overall": "This function retrieves the Ollama Base URL associated with a specific username from a database. It queries the database for a user document based on the provided username and extracts the 'ollama_base_url' field. If the user or the URL is not found, it returns None.",
          "parameters": [
            {
              "name": "username",
              "type": "str",
              "description": "The username to look up in the database."
            }
          ],
          "returns": [
            {
              "name": "ollama_base_url",
              "type": "str | None",
              "description": "The Ollama Base URL for the specified user, or None if not found."
            }
          ],
          "usage_context": {
            "calls": "This function calls `dbusers.find_one` to retrieve user data and then uses the `.get()` method on the result to safely access the 'ollama_base_url'.",
            "called_by": "This function is not called by any other functions within the provided context."
          }
        },
        "error": null
      },
      "database.db.delete_user": {
        "identifier": "database.db.delete_user",
        "description": {
          "overall": "This function deletes a user from the database based on their username. It interacts with a database collection named 'dbusers' to perform the deletion. The function returns the count of documents that were successfully deleted.",
          "parameters": [
            {
              "name": "username",
              "type": "str",
              "description": "The username of the user to be deleted from the database."
            }
          ],
          "returns": [
            {
              "name": "deleted_count",
              "type": "int",
              "description": "The number of documents that were deleted from the database. This is expected to be 1 if the user existed and was deleted, or 0 otherwise."
            }
          ],
          "usage_context": {
            "calls": "This function calls the `delete_one` method on the `dbusers` object.",
            "called_by": "This function is not called by any other function within the provided context."
          }
        },
        "error": null
      },
      "database.db.get_decrypted_api_keys": {
        "identifier": "database.db.get_decrypted_api_keys",
        "description": {
          "overall": "This function retrieves and decrypts API keys for a given username from the database. It first fetches the user's record using the provided username. If the user is not found, it returns None for both API keys. Otherwise, it decrypts the 'gemini_api_key' and retrieves the 'ollama_base_url' from the user's record, returning both decrypted values.",
          "parameters": [
            {
              "name": "username",
              "type": "str",
              "description": "The username of the user whose API keys are to be retrieved."
            }
          ],
          "returns": [
            {
              "name": "gemini_plain",
              "type": "str | None",
              "description": "The decrypted Gemini API key, or None if the user is not found or the key is missing."
            },
            {
              "name": "ollama_plain",
              "type": "str",
              "description": "The Ollama base URL, or an empty string if the user is not found or the URL is missing."
            }
          ],
          "usage_context": {
            "calls": "This function calls `database/db.py::decrypt_text` to decrypt the Gemini API key, `database/db.py::find_one` to retrieve the user record, and `database/db.py::get` to access user data.",
            "called_by": "This function is called by the `frontend.Frontend` class in `Frontend.py` at lines 233 and 295."
          }
        },
        "error": null
      },
      "database.db.insert_exchange": {
        "identifier": "database.db.insert_exchange",
        "description": {
          "overall": "This function inserts a new conversation exchange into a database. It takes various parameters related to a user's question, the AI's answer, feedback, user details, chat information, and performance metrics. The function constructs a dictionary containing all this information, including a timestamp of when the exchange was created, and then uses `dbexchanges.insert_one` to add this record to the database. Finally, it returns the unique identifier of the newly inserted record.",
          "parameters": [
            {
              "name": "question",
              "type": "str",
              "description": "The user's question."
            },
            {
              "name": "answer",
              "type": "str",
              "description": "The AI's answer to the question."
            },
            {
              "name": "feedback",
              "type": "str",
              "description": "User feedback on the answer."
            },
            {
              "name": "username",
              "type": "str",
              "description": "The username of the user."
            },
            {
              "name": "chat_name",
              "type": "str",
              "description": "The name of the chat session."
            },
            {
              "name": "helper_used",
              "type": "str",
              "description": "Information about whether a helper was used (optional, defaults to empty string)."
            },
            {
              "name": "main_used",
              "type": "str",
              "description": "Information about whether the main model was used (optional, defaults to empty string)."
            },
            {
              "name": "total_time",
              "type": "str",
              "description": "Total processing time (optional, defaults to empty string)."
            },
            {
              "name": "helper_time",
              "type": "str",
              "description": "Processing time for the helper model (optional, defaults to empty string)."
            },
            {
              "name": "main_time",
              "type": "str",
              "description": "Processing time for the main model (optional, defaults to empty string)."
            }
          ],
          "returns": [
            {
              "name": "result.inserted_id",
              "type": "Any",
              "description": "The unique identifier of the newly inserted exchange record."
            }
          ],
          "usage_context": {
            "calls": "This function calls `dbexchanges.insert_one` to insert a document and `datetime.now` to get the current timestamp.",
            "called_by": "This function is called by `Frontend.Frontend` in `Frontend.py`."
          }
        },
        "error": null
      },
      "database.db.fetch_exchanges_by_user": {
        "identifier": "database.db.fetch_exchanges_by_user",
        "description": {
          "overall": "This function retrieves exchange records associated with a specific username from a database. It queries a collection named 'dbexchanges' for documents matching the provided username and returns the results as a list. The function is designed to fetch user-specific transaction or exchange data.",
          "parameters": [
            {
              "name": "username",
              "type": "str",
              "description": "The username to filter exchange records by."
            }
          ],
          "returns": [
            {
              "name": "exchanges",
              "type": "list",
              "description": "A list of exchange documents found for the specified username."
            }
          ],
          "usage_context": {
            "calls": "This function calls the `find` method on `dbexchanges` and converts the result to a list.",
            "called_by": "This function is called by `load_data_from_db` in `Frontend.py`."
          }
        },
        "error": null
      },
      "database.db.fetch_exchanges_by_chat": {
        "identifier": "database.db.fetch_exchanges_by_chat",
        "description": {
          "overall": "This function retrieves a list of exchanges associated with a specific user and chat name from a database. It queries the database for documents matching the provided username and chat name, then converts the result into a list. The function is designed to fetch historical chat data.",
          "parameters": [
            {
              "name": "username",
              "type": "str",
              "description": "The username of the user whose exchanges are to be fetched."
            },
            {
              "name": "chat_name",
              "type": "str",
              "description": "The name of the chat for which exchanges are to be fetched."
            }
          ],
          "returns": [
            {
              "name": "exchanges",
              "type": "list",
              "description": "A list of dictionaries, where each dictionary represents an exchange document from the database."
            }
          ],
          "usage_context": {
            "calls": "This function calls the `find` method on `dbexchanges` and converts the result to a list.",
            "called_by": "This function is not called by any other function within the provided context."
          }
        },
        "error": null
      },
      "database.db.update_exchange_feedback": {
        "identifier": "database.db.update_exchange_feedback",
        "description": {
          "overall": "This function updates the feedback associated with a specific exchange in the database. It takes an exchange ID and an integer feedback value as input. The function then uses the `update_one` method to modify the 'feedback' field for the matching exchange document. Finally, it returns the count of documents that were modified.",
          "parameters": [
            {
              "name": "exchange_id",
              "type": "Any",
              "description": "The unique identifier of the exchange to be updated."
            },
            {
              "name": "feedback",
              "type": "int",
              "description": "The new feedback value to be set for the exchange. This should be an integer."
            }
          ],
          "returns": [
            {
              "name": "modified_count",
              "type": "int",
              "description": "An integer representing the number of documents that were successfully modified in the database. Typically, this will be 1 if the update was successful, or 0 if no document matched the provided exchange_id."
            }
          ],
          "usage_context": {
            "calls": "This function calls the `update_one` method, presumably from a database collection object named `dbexchanges`, to perform the update operation.",
            "called_by": "This function is called by the `handle_feedback_change` function in the `Frontend.py` file."
          }
        },
        "error": null
      },
      "database.db.update_exchange_feedback_message": {
        "identifier": "database.db.update_exchange_feedback_message",
        "description": {
          "overall": "This function updates the feedback message associated with a specific exchange in the database. It takes an exchange ID and a feedback message string as input. The function then uses the `dbexchanges.update_one` method to find the exchange by its ID and set the `feedback_message` field to the provided value. Finally, it returns the count of documents that were modified.",
          "parameters": [
            {
              "name": "exchange_id",
              "type": "any",
              "description": "The unique identifier of the exchange to be updated."
            },
            {
              "name": "feedback_message",
              "type": "str",
              "description": "The new feedback message to be associated with the exchange."
            }
          ],
          "returns": [
            {
              "name": "modified_count",
              "type": "int",
              "description": "The number of documents that were modified in the database. This is typically 1 if the update was successful, or 0 if the exchange was not found or the message was unchanged."
            }
          ],
          "usage_context": {
            "calls": "This function calls the `database/db.py::update_one` method to perform the database update.",
            "called_by": "This function is called by the `render_exchange` function in the `Frontend.py` file."
          }
        },
        "error": null
      },
      "database.db.delete_chats_by_user": {
        "identifier": "database.db.delete_chats_by_user",
        "description": {
          "overall": "This function deletes all exchanges associated with a specific user and chat name from the database. It utilizes a helper function `dbexchanges.delete_many` to perform the deletion operation. The function then returns the count of deleted documents.",
          "parameters": [
            {
              "name": "username",
              "type": "str",
              "description": "The username of the user whose chats are to be deleted."
            },
            {
              "name": "chat_name",
              "type": "str",
              "description": "The name of the chat from which exchanges should be deleted."
            }
          ],
          "returns": [
            {
              "name": "deleted_count",
              "type": "int",
              "description": "The number of documents that were deleted from the database."
            }
          ],
          "usage_context": {
            "calls": "This function calls the `database/db.py::delete_many` function to remove records from the database.",
            "called_by": "This function is invoked by the `handle_delete_chat` function in `Frontend.py`."
          }
        },
        "error": null
      },
      "database.db.delete_exchange_by_id": {
        "identifier": "database.db.delete_exchange_by_id",
        "description": {
          "overall": "This function deletes a specific exchange record from the database using its unique identifier. It interacts with a database collection named 'dbexchanges' to perform the deletion operation. The function returns the count of documents that were successfully deleted.",
          "parameters": [
            {
              "name": "exchange_id",
              "type": "str",
              "description": "The unique identifier of the exchange to be deleted."
            }
          ],
          "returns": [
            {
              "name": "deleted_count",
              "type": "int",
              "description": "The number of documents that were deleted from the database. This is typically 1 if the exchange was found and deleted, and 0 otherwise."
            }
          ],
          "usage_context": {
            "calls": "This function calls the 'delete_one' method on the 'dbexchanges' collection to remove a single document matching the provided exchange ID.",
            "called_by": "This function is called by the 'handle_delete_exchange' function in the 'Frontend.py' file."
          }
        },
        "error": null
      },
      "frontend.Frontend.load_data_from_db": {
        "identifier": "frontend.Frontend.load_data_from_db",
        "description": {
          "overall": "This function loads existing chat data from a database into the Streamlit session state. It initializes the session state for chats if 'data_loaded' is not already set. The function fetches exchanges for a given username from the database, organizing them by chat name. If a chat name does not exist, it is created. It also handles missing feedback data by setting it to NaN. Finally, it ensures that there is at least one chat available and sets the active chat, marking the data as loaded in the session state.",
          "parameters": [
            {
              "name": "username",
              "type": "str",
              "description": "The username for whom to load chat data."
            }
          ],
          "returns": [],
          "usage_context": {
            "calls": "This function calls methods such as append, fetch_exchanges_by_user, get, keys, and list.",
            "called_by": "This function is called by the frontend.Frontend constructor in Frontend.py."
          }
        },
        "error": null
      },
      "frontend.Frontend.handle_feedback_change": {
        "identifier": "frontend.Frontend.handle_feedback_change",
        "description": {
          "overall": "This function updates the feedback associated with an exchange in the application's state and database. It takes an exchange dictionary and a new feedback value as input. The function first modifies the 'feedback' key within the provided exchange dictionary to the new value. Subsequently, it calls a database update function to persist this change, using the exchange's ID and the new feedback value. Finally, it triggers a rerun of the Streamlit application to reflect the updated state.",
          "parameters": [
            {
              "name": "ex",
              "type": "dict",
              "description": "A dictionary representing the exchange, expected to contain at least '_id' and 'feedback' keys."
            },
            {
              "name": "val",
              "type": "any",
              "description": "The new feedback value to be assigned to the exchange."
            }
          ],
          "returns": [],
          "usage_context": {
            "calls": "This function calls the `rerun` function from the frontend and the `update_exchange_feedback` function from the database module.",
            "called_by": "This function is called by the `render_exchange` function in `Frontend.py` on lines 117 and 122."
          }
        },
        "error": null
      },
      "frontend.Frontend.handle_delete_exchange": {
        "identifier": "frontend.Frontend.handle_delete_exchange",
        "description": {
          "overall": "This function deletes an exchange from both the database and the application's state. It first removes the exchange from the database using its ID and then removes it from the session state's chat data. Finally, it triggers a rerun of the application to reflect the changes.",
          "parameters": [
            {
              "name": "chat_name",
              "type": "str",
              "description": "The name of the chat from which to delete the exchange."
            },
            {
              "name": "ex",
              "type": "dict",
              "description": "A dictionary representing the exchange to be deleted, containing at least an '_id' key."
            }
          ],
          "returns": [],
          "usage_context": {
            "calls": "This function calls `delete_exchange_by_id` to remove data from the database, `remove` to modify the session state, and `rerun` to refresh the application.",
            "called_by": "This function is called by the `render_exchange` function in `Frontend.py`."
          }
        },
        "error": null
      },
      "frontend.Frontend.handle_delete_chat": {
        "identifier": "frontend.Frontend.handle_delete_chat",
        "description": {
          "overall": "This function deletes a specified chat for a given username. It first removes the chat data from the database using `db.delete_chats_by_user`. Then, it removes the chat from the session state. Finally, it updates the `active_chat` session state to either the first remaining chat or a default new chat if no chats are left, and then reruns the application to reflect these changes.",
          "parameters": [
            {
              "name": "username",
              "type": "str",
              "description": "The username associated with the chat to be deleted."
            },
            {
              "name": "chat_name",
              "type": "str",
              "description": "The name of the chat to be deleted."
            }
          ],
          "returns": [],
          "usage_context": {
            "calls": "This function calls `db.delete_chats_by_user` to remove chat data from the database, and interacts with `st.session_state` to manage chat data and the active chat. It also calls `st.rerun()` to refresh the application state.",
            "called_by": "This function is called from within the `frontend.Frontend` class, specifically at line 219 in `Frontend.py`."
          }
        },
        "error": null
      },
      "frontend.Frontend.render_text_with_mermaid": {
        "identifier": "frontend.Frontend.render_text_with_mermaid",
        "description": {
          "overall": "This function takes a markdown text string as input and renders it, specifically handling blocks marked for Mermaid diagram visualization. It splits the input text based on ` ```mermaid ` delimiters. For regular text parts, it uses `st.markdown` for rendering. For the content within ` ```mermaid ` blocks, it attempts to render them as Mermaid diagrams using `st_mermaid`. If `st_mermaid` fails, it falls back to displaying the content as a code block with the language set to 'mermaid'. The function returns early if the input text is empty.",
          "parameters": [
            {
              "name": "markdown_text",
              "type": "str",
              "description": "The input markdown text which may contain Mermaid diagram blocks."
            }
          ],
          "returns": [],
          "usage_context": {
            "calls": "This function calls `re.split` to divide the text, `enumerate` to iterate through the parts, `hash` to create unique keys, `st.markdown` to render regular text, `st_mermaid` to render Mermaid diagrams, and `st.code` as a fallback for Mermaid rendering. It also uses `part.strip()` to remove leading/trailing whitespace.",
            "called_by": "This function is called by `render_exchange` in Frontend.py and also within the module `frontend.Frontend` at line 333."
          }
        },
        "error": null
      },
      "frontend.Frontend.render_exchange": {
        "identifier": "frontend.Frontend.render_exchange",
        "description": {
          "overall": "This function, `render_exchange`, is responsible for displaying a chat exchange within a Streamlit frontend. It takes an exchange object (`ex`) and the current chat name (`current_chat_name`) as input. The function first displays the user's question and then renders the assistant's answer within a chat message context. It includes a toolbar with buttons for positive/negative feedback, writing feedback messages, downloading the answer as a Markdown file, and deleting the exchange. The answer content is displayed in a scrollable container, with Mermaid diagrams rendered if present.",
          "parameters": [
            {
              "name": "ex",
              "type": "dict",
              "description": "A dictionary containing the chat exchange data, including 'question', 'answer', 'feedback', 'feedback_message', and '_id'."
            },
            {
              "name": "current_chat_name",
              "type": "str",
              "description": "The name of the current chat session."
            }
          ],
          "returns": [],
          "usage_context": {
            "calls": "This function calls various Streamlit components such as `st.chat_message`, `st.columns`, `st.button`, `st.popover`, `st.text_area`, `st.download_button`, `st.caption`, `st.container`, and `st.write`. It also calls helper functions like `handle_feedback_change`, `handle_delete_exchange`, `db.update_exchange_feedback_message`, `time.sleep`, `st.success`, `st.rerun`, and `render_text_with_mermaid`.",
            "called_by": "This function is called from the `frontend.Frontend` class, specifically from a method likely responsible for rendering the main chat interface."
          }
        },
        "error": null
      }
    },
    "classes": {
      "backend.AST_Schema.ASTVisitor": {
        "identifier": "backend.AST_Schema.ASTVisitor",
        "description": {
          "overall": "The ASTVisitor class is designed to traverse an Abstract Syntax Tree (AST) generated from Python source code. It specifically focuses on extracting information about imports, classes, and functions within the code. The visitor pattern is employed to systematically visit different node types in the AST, accumulating details into a structured schema. This schema is intended to represent the components of a Python project, including their identifiers, source code segments, and call relationships, facilitating further analysis or documentation generation.",
          "init_method": {
            "description": "Initializes the ASTVisitor with the source code, file path, and project root directory. It sets up instance variables to store this information and derives the module path. It also initializes an empty schema dictionary to store extracted information about imports, functions, and classes, and sets an internal variable `_current_class` to None.",
            "parameters": [
              {
                "name": "source_code",
                "type": "str",
                "description": "The raw source code of the file being analyzed."
              },
              {
                "name": "file_path",
                "type": "str",
                "description": "The absolute path to the file being analyzed."
              },
              {
                "name": "project_root",
                "type": "str",
                "description": "The root directory of the project."
              }
            ]
          },
          "methods": [
            {
              "identifier": "visit_Import",
              "description": {
                "overall": "This method visits an `ast.Import` node in the AST. It iterates through the imported module aliases and appends their names to the 'imports' list within the class's schema. After processing the import statement, it calls `generic_visit` to continue the traversal down the AST.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "ASTVisitor",
                    "description": "The instance of the ASTVisitor class."
                  },
                  {
                    "name": "node",
                    "type": "ast.Import",
                    "description": "The AST node representing an import statement."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method calls the `append` method to add import names to the schema and `generic_visit` to continue AST traversal.",
                  "called_by": "This method is called automatically by the `ast.NodeVisitor` when an `ast.Import` node is encountered during AST traversal."
                }
              },
              "error": null
            },
            {
              "identifier": "visit_ImportFrom",
              "description": {
                "overall": "This method handles `ast.ImportFrom` nodes, which represent imports from a specific module (e.g., `from os import path`). It iterates through the imported names, constructing a fully qualified name (module.name) and appending it to the 'imports' list in the schema. It then calls `generic_visit` to ensure further traversal of the AST.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "ASTVisitor",
                    "description": "The instance of the ASTVisitor class."
                  },
                  {
                    "name": "node",
                    "type": "ast.ImportFrom",
                    "description": "The AST node representing a 'from ... import ...' statement."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method calls the `append` method to add qualified import names to the schema and `generic_visit` to continue AST traversal.",
                  "called_by": "This method is called automatically by the `ast.NodeVisitor` when an `ast.ImportFrom` node is encountered during AST traversal."
                }
              },
              "error": null
            },
            {
              "identifier": "visit_ClassDef",
              "description": {
                "overall": "This method is responsible for visiting `ast.ClassDef` nodes, representing class definitions in the source code. It constructs a unique identifier for the class, including its module path. It then gathers information such as the class name, docstring, source code segment, start and end line numbers, and initializes a context dictionary for dependencies and method calls. This class information is appended to the 'classes' list in the schema. It also sets an internal `_current_class` attribute to store the information of the class currently being visited, and finally calls `generic_visit` to process nested nodes within the class definition before resetting `_current_class`.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "ASTVisitor",
                    "description": "The instance of the ASTVisitor class."
                  },
                  {
                    "name": "node",
                    "type": "ast.ClassDef",
                    "description": "The AST node representing a class definition."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method calls `ast.get_docstring` to retrieve the class docstring, `ast.get_source_segment` to get the source code of the class, `append` to add the class info to the schema, and `generic_visit` to continue AST traversal.",
                  "called_by": "This method is called automatically by the `ast.NodeVisitor` when an `ast.ClassDef` node is encountered during AST traversal."
                }
              },
              "error": null
            },
            {
              "identifier": "visit_FunctionDef",
              "description": {
                "overall": "This method handles `ast.FunctionDef` nodes, which represent function or method definitions. If the visitor is currently inside a class (`_current_class` is set), it treats the definition as a method, constructs a method identifier, and stores method-specific information (name, arguments, docstring, source code, line numbers) within the `method_context` of the current class in the schema. If not inside a class, it's treated as a standalone function, and its information is added to the 'functions' list in the schema. In both cases, it calls `generic_visit` to process any nested nodes within the function definition.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "ASTVisitor",
                    "description": "The instance of the ASTVisitor class."
                  },
                  {
                    "name": "node",
                    "type": "ast.FunctionDef",
                    "description": "The AST node representing a function or method definition."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method calls `ast.get_docstring` to retrieve the function/method docstring, `ast.get_source_segment` to get the source code, and `generic_visit` to continue AST traversal. It also appends information to either the current class's method context or the top-level functions list.",
                  "called_by": "This method is called automatically by the `ast.NodeVisitor` when an `ast.FunctionDef` node is encountered during AST traversal."
                }
              },
              "error": null
            },
            {
              "identifier": "visit_AsyncFunctionDef",
              "description": {
                "overall": "This method is an alias for `visit_FunctionDef`. It ensures that asynchronous function definitions (`async def`) are also processed correctly by delegating the visit operation to the `visit_FunctionDef` method. This allows the visitor to handle both regular and asynchronous function definitions uniformly.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "ASTVisitor",
                    "description": "The instance of the ASTVisitor class."
                  },
                  {
                    "name": "node",
                    "type": "ast.AsyncFunctionDef",
                    "description": "The AST node representing an asynchronous function definition."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method calls the `visit_FunctionDef` method to process the asynchronous function definition.",
                  "called_by": "This method is called automatically by the `ast.NodeVisitor` when an `ast.AsyncFunctionDef` node is encountered during AST traversal."
                }
              },
              "error": null
            }
          ],
          "usage_context": {
            "dependencies": "This class relies on the `ast` module for parsing Python code into an Abstract Syntax Tree and the `path_to_module` function (presumably defined elsewhere) to convert file paths into module paths. It also utilizes `ast.get_docstring` and `ast.get_source_segment` for extracting specific code details.",
            "instantiated_by": "The ASTVisitor class is instantiated within the `analyze_repository` function in the `AST_Schema.py` file, specifically on line 175."
          }
        },
        "error": null
      },
      "backend.AST_Schema.ASTAnalyzer": {
        "identifier": "backend.AST_Schema.ASTAnalyzer",
        "description": {
          "overall": "The ASTAnalyzer class is designed to process a collection of Python files, parse their Abstract Syntax Trees (ASTs), and build a comprehensive schema representing the code structure, including functions, classes, and their relationships. It enriches this schema with call graph information and merges relationship data to provide a detailed repository analysis.",
          "init_method": {
            "description": "Initializes the ASTAnalyzer. Currently, the constructor does not perform any specific setup or attribute initialization, as indicated by the 'pass' statement.",
            "parameters": []
          },
          "methods": [
            {
              "identifier": "_enrich_schema_with_callgraph",
              "description": {
                "overall": "This static method takes a schema dictionary and a NetworkX call graph, along with a filename, to enrich the schema with call graph information. It iterates through functions and methods within the schema, identifying their corresponding nodes in the call graph. For each identified function or method, it adds 'calls' and 'called_by' attributes to its context, populated with sorted lists of successors and predecessors from the call graph, respectively. This provides a detailed view of inter-function and inter-method call relationships within the context of a specific file.",
                "parameters": [
                  {
                    "name": "schema",
                    "type": "dict",
                    "description": "The schema dictionary to be enriched with call graph data."
                  },
                  {
                    "name": "call_graph",
                    "type": "nx.DiGraph",
                    "description": "A NetworkX directed graph representing the call graph of the code."
                  },
                  {
                    "name": "filename",
                    "type": "str",
                    "description": "The name of the file for which the call graph and schema are being processed."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method calls list, sorted, and successors/predecessors methods from the NetworkX graph object.",
                  "called_by": "This method is called internally by the analyze_repository method to enrich the schema with call graph data."
                }
              },
              "error": null
            },
            {
              "identifier": "merge_relationship_data",
              "description": {
                "overall": "This method merges relationship data, such as 'called_by' and 'instantiated_by' information, into a full schema. It first creates a lookup dictionary from the provided relationship data. Then, it iterates through the files and their corresponding AST nodes within the full schema. For each function, class, and method, it checks if its identifier exists in the relationship lookup and updates the schema with the relevant relationship information. This method is crucial for consolidating call and instantiation relationships across different parts of the analyzed code.",
                "parameters": [
                  {
                    "name": "full_schema",
                    "type": "dict",
                    "description": "The complete schema dictionary that will be updated with relationship data."
                  },
                  {
                    "name": "relationship_data",
                    "type": "list",
                    "description": "A list of dictionaries, where each dictionary contains relationship information like 'identifier' and 'called_by'."
                  }
                ],
                "returns": [
                  {
                    "name": "full_schema",
                    "type": "dict",
                    "description": "The updated full schema dictionary with merged relationship data."
                  }
                ],
                "usage_context": {
                  "calls": "This method utilizes the get and items methods for dictionary traversal and data retrieval.",
                  "called_by": "This method is called by the main_workflow function in main.py to merge relationship data into the full schema."
                }
              },
              "error": null
            },
            {
              "identifier": "analyze_repository",
              "description": {
                "overall": "This method orchestrates the analysis of an entire repository by processing a list of files. It initializes an empty schema and determines the project root directory. For each Python file, it parses the file content into an AST, creates an ASTVisitor to extract schema information, and builds a call graph. It then uses the _enrich_schema_with_callgraph method to add call graph details to the file's schema. Finally, it aggregates the processed file schemas into a comprehensive 'full_schema' dictionary, handling potential parsing errors gracefully. This method is the primary entry point for analyzing a collection of code files.",
                "parameters": [
                  {
                    "name": "files",
                    "type": "list",
                    "description": "A list of file objects, where each object contains file path and content."
                  }
                ],
                "returns": [
                  {
                    "name": "full_schema",
                    "type": "dict",
                    "description": "A dictionary containing the aggregated schema information for all processed files in the repository."
                  }
                ],
                "usage_context": {
                  "calls": "This method calls ast.parse, os.path.commonpath, os.path.dirname, os.path.isfile, str.endswith, str.strip, print, ASTVisitor, ASTVisitor.visit, build_callGraph, and _enrich_schema_with_callgraph.",
                  "called_by": "This method is called by the main_workflow function in main.py to initiate the repository analysis."
                }
              },
              "error": null
            }
          ],
          "usage_context": {
            "dependencies": "This class depends on the 'ast' module for parsing Python code, 'networkx' for graph manipulation (specifically for call graphs), 'os' for path operations, and a custom 'callgraph.build_callGraph' function. It also relies on an 'ASTVisitor' class for traversing the AST.",
            "instantiated_by": "The ASTAnalyzer class is instantiated within the main_workflow function in the main.py file."
          }
        },
        "error": null
      },
      "backend.File_Dependency.FileDependencyGraph": {
        "identifier": "backend.File_Dependency.FileDependencyGraph",
        "description": {
          "overall": "The FileDependencyGraph class is designed to analyze Python source code and build a graph representing file dependencies based on import statements. It traverses the Abstract Syntax Tree (AST) of a given file to identify direct and relative imports, resolving them to determine the relationships between different modules and files within a repository. The class stores these dependencies in a dictionary, mapping each file to a set of its imported modules or symbols.",
          "init_method": {
            "description": "Initializes the FileDependencyGraph with the filename to be analyzed and the root directory of the repository. This sets up the context for resolving imports within the specified file and repository.",
            "parameters": [
              {
                "name": "filename",
                "type": "str",
                "description": "The name of the file for which the dependency graph is being built."
              },
              {
                "name": "repo_root",
                "type": "str",
                "description": "The root path of the repository containing the file."
              }
            ]
          },
          "methods": [
            {
              "identifier": "_resolve_module_name",
              "description": {
                "overall": "This method is responsible for resolving relative import statements (e.g., `from .. import name`) within a Python file. It determines the actual module or symbol name based on the current file's location within the repository and the specified import level. The method searches for corresponding Python files or `__init__.py` files in the calculated base directory and checks if the imported symbol is exported. It raises an `ImportError` if the import cannot be resolved or if the import level is invalid for the file's location.",
                "parameters": [
                  {
                    "name": "node",
                    "type": "ImportFrom",
                    "description": "The AST node representing the import statement to be resolved."
                  }
                ],
                "returns": [
                  {
                    "name": "resolved",
                    "type": "list[str]",
                    "description": "A sorted list of resolved module or symbol names."
                  }
                ],
                "usage_context": {
                  "calls": "This method calls `get_all_temp_files` to get all files in the repository, `Path` to handle file paths, `len` and `sort` for path manipulation and ordering, `range` for loop control, `resolve` for absolute path resolution, and checks for file existence using `exists`.",
                  "called_by": "This method is called by `visit_ImportFrom` when a relative import needs to be resolved."
                }
              },
              "error": null
            },
            {
              "identifier": "module_file_exists",
              "description": {
                "overall": "A helper method to check if a Python module file or a package's `__init__.py` file exists at a given relative base path with a specified name. It constructs the potential file paths and returns `True` if either the `.py` file or the package directory with `__init__.py` exists, `False` otherwise.",
                "parameters": [
                  {
                    "name": "rel_base",
                    "type": "Path",
                    "description": "The relative base path within the repository."
                  },
                  {
                    "name": "name",
                    "type": "str",
                    "description": "The name of the module or package to check for."
                  }
                ],
                "returns": [
                  {
                    "name": "bool",
                    "type": "bool",
                    "description": "True if the module file or package exists, False otherwise."
                  }
                ],
                "usage_context": {
                  "calls": "This method calls the `exists` method on `Path` objects to check for file system entries.",
                  "called_by": "This method is called by `_resolve_module_name` to verify the existence of potential module files."
                }
              },
              "error": null
            },
            {
              "identifier": "init_exports_symbol",
              "description": {
                "overall": "This method checks if a given symbol is exported by an `__init__.py` file within a specified relative base path. It reads the `__init__.py` file, parses its content, and looks for the symbol either in the `__all__` list or as a defined function, class, or assignment. It returns `True` if the symbol is found and exported, and `False` otherwise, including cases where `__init__.py` does not exist or parsing fails.",
                "parameters": [
                  {
                    "name": "rel_base",
                    "type": "Path",
                    "description": "The relative base path to the directory containing the `__init__.py` file."
                  },
                  {
                    "name": "symbol",
                    "type": "str",
                    "description": "The name of the symbol (function, class, variable) to check for."
                  }
                ],
                "returns": [
                  {
                    "name": "bool",
                    "type": "bool",
                    "description": "True if the symbol is exported by the `__init__.py` file, False otherwise."
                  }
                ],
                "usage_context": {
                  "calls": "This method uses `exists` to check for `__init__.py`, `read_text` to get file content, `parse` to create an AST, `walk` to traverse the AST, `isinstance` to check node types, `literal_eval` to evaluate `__all__` contents, and `str` for path conversion.",
                  "called_by": "This method is called by `_resolve_module_name` to determine if a symbol imported from a package is valid."
                }
              },
              "error": null
            },
            {
              "identifier": "visit_Import",
              "description": {
                "overall": "This method is called when an `ast.Import` or `ast.ImportFrom` node is encountered during AST traversal. It processes the imported names and adds them to the `import_dependencies` dictionary, mapping the current filename to a set of imported module names. If a `base_name` is provided (typically from `visit_ImportFrom`), it's added; otherwise, the direct alias name is used. It ensures that the `import_dependencies` dictionary is initialized for the current filename and then calls `generic_visit` to continue traversal.",
                "parameters": [
                  {
                    "name": "node",
                    "type": "Import | ImportFrom",
                    "description": "The AST node representing the import statement."
                  },
                  {
                    "name": "base_name",
                    "type": "str | None",
                    "description": "An optional base name to add to the dependencies, often used for resolving specific parts of an import."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method calls `add` and `set` on Python sets, and `generic_visit` to continue the AST traversal.",
                  "called_by": "This method is called by `visit_ImportFrom` and potentially other visitor methods if they handle `ast.Import` nodes."
                }
              },
              "error": null
            },
            {
              "identifier": "visit_ImportFrom",
              "description": {
                "overall": "This method handles `ast.ImportFrom` nodes, which represent imports like `from module import name`. It first extracts the module name. If the module name is directly available, it uses the last part of the module name as the `base_name` and calls `visit_Import`. If the import is relative (module name is `None`), it calls `_resolve_module_name` to determine the actual module(s) and then calls `visit_Import` for each resolved name. Any `ImportError` during relative import resolution is caught and printed.",
                "parameters": [
                  {
                    "name": "node",
                    "type": "ImportFrom",
                    "description": "The AST node representing the 'from ... import ...' statement."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method calls `_resolve_module_name` for relative imports, `split` to parse module names, `visit_Import` to record the dependency, `print` to report errors, and `generic_visit` to continue AST traversal.",
                  "called_by": "This method is part of the AST visitor pattern and is invoked when the visitor encounters an `ImportFrom` node."
                }
              },
              "error": null
            }
          ],
          "usage_context": {
            "dependencies": "This class relies on several modules for its functionality, including `ast` for parsing Python code, `keyword` for checking Python keywords, `pathlib` for path manipulation, and potentially other modules like `getRepo.GitRepository` and `collections.defaultdict` for repository and data structure operations. It also uses `ast.literal_eval` and `ast.parse` for code analysis.",
            "instantiated_by": "The `FileDependencyGraph` class is instantiated within the `build_file_dependency_graph` function in the `File_Dependency.py` file, specifically at line 159."
          }
        },
        "error": null
      },
      "backend.HelperLLM.LLMHelper": {
        "identifier": "backend.HelperLLM.LLMHelper",
        "description": {
          "overall": "The LLMHelper class is designed to interact with various Large Language Models (LLMs) for generating documentation. It centralizes the configuration of LLM parameters, prompt management, and batch processing for both function and class documentation generation. The class supports different LLM providers like Google Generative AI, OpenAI, and Ollama, and handles API key management, prompt file loading, and model-specific batch size configurations. It ensures structured output by using Pydantic models for validation and includes error handling for API calls and file operations.",
          "init_method": {
            "description": "Initializes the LLMHelper with necessary API credentials, prompt file paths, and LLM configuration. It loads system prompts for function and class documentation, configures batch settings based on the model name, and sets up specific LLM clients (e.g., ChatGoogleGenerativeAI, ChatOpenAI, ChatOllama) with structured output capabilities for `FunctionAnalysis` and `ClassAnalysis`. It also initializes a raw LLM client for general use.",
            "parameters": [
              {
                "name": "api_key",
                "type": "str",
                "description": "The API key for authenticating with the LLM service. Raises ValueError if not provided."
              },
              {
                "name": "function_prompt_path",
                "type": "str",
                "description": "The file path to the system prompt used for function documentation generation. Raises FileNotFoundError if the file is not found."
              },
              {
                "name": "class_prompt_path",
                "type": "str",
                "description": "The file path to the system prompt used for class documentation generation. Raises FileNotFoundError if the file is not found."
              },
              {
                "name": "model_name",
                "type": "str",
                "description": "The name of the LLM model to use. Defaults to 'gemini-2.0-flash-lite'."
              },
              {
                "name": "ollama_base_url",
                "type": "str",
                "description": "The base URL for Ollama if using an Ollama model. Defaults to a predefined OLLAMA_BASE_URL if not provided."
              }
            ]
          },
          "methods": [
            {
              "identifier": "_configure_batch_settings",
              "description": {
                "overall": "Configures the batch size for LLM API calls based on the specified model name. It defines different batch sizes for various Gemini and OpenAI models, and a default conservative batch size for unknown models. It logs a warning if an unknown model is encountered.",
                "parameters": [
                  {
                    "name": "model_name",
                    "type": "str",
                    "description": "The name of the LLM model for which to configure batch settings."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "Logs a warning message using the logging module if an unknown model name is provided.",
                  "called_by": "This method is called internally by the __init__ method to set the batch size based on the selected LLM model."
                }
              },
              "error": null
            },
            {
              "identifier": "generate_for_functions",
              "description": {
                "overall": "Generates and validates documentation for a batch of function inputs using the configured LLM. It serializes the input data, constructs conversation prompts with system messages, and sends them to the LLM in batches. The method handles potential API errors by logging them and returning None for failed batches, while respecting rate limits by introducing a delay between batches. It returns a list of validated `FunctionAnalysis` objects or None for failed items.",
                "parameters": [
                  {
                    "name": "function_inputs",
                    "type": "List[FunctionAnalysisInput]",
                    "description": "A list of FunctionAnalysisInput objects, each containing the necessary information to generate documentation for a function."
                  }
                ],
                "returns": [
                  {
                    "name": "all_validated_functions",
                    "type": "List[Optional[FunctionAnalysis]]",
                    "description": "A list containing the generated and validated FunctionAnalysis objects for each input, or None if an error occurred during processing for a specific item."
                  }
                ],
                "usage_context": {
                  "calls": "This method calls `json.dumps` to serialize inputs, constructs `SystemMessage` and `HumanMessage` objects, uses the `batch` method of the `self.function_llm` client for API calls, logs information and errors using the `logging` module, and pauses execution using `time.sleep` to manage rate limits.",
                  "called_by": "This method is called by the `main_workflow` function in `main.py` to generate documentation for functions."
                }
              },
              "error": null
            },
            {
              "identifier": "generate_for_classes",
              "description": {
                "overall": "Generates and validates documentation for a batch of class inputs using the configured LLM. Similar to `generate_for_functions`, it serializes class input data, creates conversation prompts, and processes them in batches. It includes error handling for API calls and rate limit management with delays between batches. The method returns a list of validated `ClassAnalysis` objects or None for any items that failed processing.",
                "parameters": [
                  {
                    "name": "class_inputs",
                    "type": "List[ClassAnalysisInput]",
                    "description": "A list of ClassAnalysisInput objects, each containing the necessary information to generate documentation for a class."
                  }
                ],
                "returns": [
                  {
                    "name": "all_validated_classes",
                    "type": "List[Optional[ClassAnalysis]]",
                    "description": "A list containing the generated and validated ClassAnalysis objects for each input, or None if an error occurred during processing for a specific item."
                  }
                ],
                "usage_context": {
                  "calls": "This method calls `json.dumps` to serialize inputs, constructs `SystemMessage` and `HumanMessage` objects, uses the `batch` method of the `self.class_llm` client for API calls, logs information and errors using the `logging` module, and pauses execution using `time.sleep` to manage rate limits.",
                  "called_by": "This method is called by the `main_workflow` function in `main.py` to generate documentation for classes."
                }
              },
              "error": null
            }
          ],
          "usage_context": {
            "dependencies": "This class depends on various LLM provider libraries (Google Generative AI, Ollama, OpenAI), Pydantic for data validation, and standard Python libraries for file handling, logging, and time management. It also relies on custom schema types like `FunctionAnalysis`, `ClassAnalysis`, `FunctionAnalysisInput`, and `ClassAnalysisInput`.",
            "instantiated_by": "The LLMHelper class is instantiated in the `main_orchestrator` function within `HelperLLM.py` and in the `main_workflow` function within `main.py`."
          }
        },
        "error": null
      },
      "backend.MainLLM.MainLLM": {
        "identifier": "backend.MainLLM.MainLLM",
        "description": {
          "overall": "The MainLLM class serves as the primary interface for interacting with Large Language Models (LLMs). It handles the initialization of different LLM providers (Google Generative AI or Ollama) based on the provided model name and configuration. The class is responsible for loading a system prompt from a file and then provides methods to either get a direct response from the LLM or stream the response in chunks. It manages API keys and base URLs, ensuring proper connection to the chosen LLM service.",
          "init_method": {
            "description": "Initializes the MainLLM class by setting up the LLM client. It validates the provided API key, loads the system prompt from a specified file, and configures the LLM client based on the model name. It supports models starting with 'gemini-' or 'gpt-' using ChatGoogleGenerativeAI, and other models using ChatOllama, with configurable base URLs. Error handling is included for file not found exceptions during prompt loading.",
            "parameters": [
              {
                "name": "api_key",
                "type": "str",
                "description": "The API key required for authenticating with the LLM service (e.g., Google Generative AI)."
              },
              {
                "name": "prompt_file_path",
                "type": "str",
                "description": "The file path to the system prompt that will be used for all LLM interactions."
              },
              {
                "name": "model_name",
                "type": "str",
                "description": "The name of the LLM model to use. Defaults to 'gemini-2.5-pro'."
              },
              {
                "name": "ollama_base_url",
                "type": "str",
                "description": "The base URL for the Ollama service if using Ollama models. Defaults to a predefined OLLAMA_BASE_URL if not provided."
              }
            ]
          },
          "methods": [
            {
              "identifier": "call_llm",
              "description": {
                "overall": "This method sends a user's input to the configured LLM and returns the complete response content. It constructs a list of messages including the system prompt and the user's input, then invokes the LLM client. If the LLM call is successful, it returns the content of the response. In case of any exceptions during the LLM invocation, it logs the error and returns None.",
                "parameters": [
                  {
                    "name": "user_input",
                    "type": "str",
                    "description": "The input string provided by the user to be processed by the LLM."
                  }
                ],
                "returns": [
                  {
                    "name": "response.content",
                    "type": "str",
                    "description": "The text content generated by the LLM in response to the user input, or None if an error occurred."
                  }
                ],
                "usage_context": {
                  "calls": "This method calls SystemMessage and HumanMessage to format the input, and then uses the LLM client's invoke method to get a response.",
                  "called_by": "This method is called by the main_workflow function in main.py."
                }
              },
              "error": null
            },
            {
              "identifier": "stream_llm",
              "description": {
                "overall": "This method enables streaming of LLM responses, yielding content in chunks as it becomes available. It prepares messages similar to `call_llm` and then uses the LLM client's stream method to obtain an iterator. It iterates through the stream, yielding each chunk of content. If an error occurs during the streaming process, it logs the error and yields an error message.",
                "parameters": [
                  {
                    "name": "user_input",
                    "type": "str",
                    "description": "The input string provided by the user to be processed by the LLM for streaming."
                  }
                ],
                "returns": [
                  {
                    "name": "chunk.content",
                    "type": "str",
                    "description": "Yields chunks of text content as they are generated by the LLM, or an error message if an exception occurs."
                  }
                ],
                "usage_context": {
                  "calls": "This method calls SystemMessage and HumanMessage to format the input, and then uses the LLM client's stream method to get a response iterator.",
                  "called_by": "This method is not called by any other function within the provided context."
                }
              },
              "error": null
            }
          ],
          "usage_context": {
            "dependencies": "This class relies on external libraries for LLM interactions, specifically 'langchain_google_genai' for Google's models and 'langchain_ollama' for Ollama models. It also uses 'langchain.messages' for message formatting and 'logging' for output.",
            "instantiated_by": "This class is instantiated within the main_workflow function in the main.py file."
          }
        },
        "error": null
      },
      "backend.basic_info.ProjektInfoExtractor": {
        "identifier": "backend.basic_info.ProjektInfoExtractor",
        "description": {
          "overall": "The ProjektInfoExtractor class is designed to extract fundamental project information from common project files such as README, pyproject.toml, and requirements.txt. It initializes a structured dictionary to hold project overview and installation details, populating it by parsing these files in a prioritized order. The class aims to provide a consolidated view of project metadata, including title, description, key features, tech stack, installation instructions, and dependencies.",
          "init_method": {
            "description": "Initializes the ProjektInfoExtractor with a default structure for project information and defines a constant for indicating missing information. The `self.info` attribute is a dictionary pre-populated with nested dictionaries for 'projekt_uebersicht' (project overview) and 'installation', each containing placeholders for various details.",
            "parameters": [
              {
                "name": "self",
                "type": "ProjektInfoExtractor",
                "description": "The instance of the class."
              }
            ]
          },
          "methods": [
            {
              "identifier": "_finde_datei",
              "description": {
                "overall": "This private helper method searches through a list of file objects to find a file whose path matches one of the provided patterns. The search is case-insensitive, meaning it will find 'README.md' if the pattern is 'readme.md'. It iterates through each file and each pattern, returning the first file that matches a pattern.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "ProjektInfoExtractor",
                    "description": "The instance of the class."
                  },
                  {
                    "name": "patterns",
                    "type": "List[str]",
                    "description": "A list of file name patterns to search for (e.g., ['readme.md', 'pyproject.toml'])."
                  },
                  {
                    "name": "dateien",
                    "type": "List[Any]",
                    "description": "A list of file objects, where each object is expected to have a 'path' attribute."
                  }
                ],
                "returns": [
                  {
                    "name": "Optional[Any]",
                    "type": "Optional[Any]",
                    "description": "The first matching file object found, or None if no file matches any of the patterns."
                  }
                ],
                "usage_context": {
                  "calls": "This method calls the `endswith` and `lower` methods on file paths and patterns to perform a case-insensitive search.",
                  "called_by": "This method is called by the `extrahiere_info` method to locate specific project files like README, pyproject.toml, and requirements.txt."
                }
              },
              "error": null
            },
            {
              "identifier": "_extrahiere_sektion_aus_markdown",
              "description": {
                "overall": "This private helper method extracts a specific section of text from a Markdown content string. It uses regular expressions to find a section that starts with a Markdown heading (##) followed by one of the provided keywords. It captures all text following the heading until the next heading or the end of the file, returning the stripped content.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "ProjektInfoExtractor",
                    "description": "The instance of the class."
                  },
                  {
                    "name": "inhalt",
                    "type": "str",
                    "description": "The entire Markdown content as a string."
                  },
                  {
                    "name": "keywords",
                    "type": "List[str]",
                    "description": "A list of alternative keywords that can be used as section titles (e.g., ['Installation', 'Setup'])."
                  }
                ],
                "returns": [
                  {
                    "name": "Optional[str]",
                    "type": "Optional[str]",
                    "description": "The extracted text content of the section, stripped of leading/trailing whitespace, or None if the section is not found."
                  }
                ],
                "usage_context": {
                  "calls": "This method utilizes the `re` module for regular expression operations, specifically `re.escape`, `re.compile`, `re.search`, and `re.IGNORECASE | re.DOTALL` flags. It also uses `str.join` to construct the regex pattern and `str.strip` to clean the output.",
                  "called_by": "This method is called by `_parse_readme` to extract specific sections like 'Features', 'Tech Stack', 'Status', 'Installation', and 'Quick Start'."
                }
              },
              "error": null
            },
            {
              "identifier": "_parse_readme",
              "description": {
                "overall": "This private method parses the content of a README file to extract various project details. It attempts to find the project title from the first line, a description from the text following the title, key features, tech stack, current status, installation instructions, and quick start guide by calling the `_extrahiere_sektion_aus_markdown` helper method for specific sections. It updates the `self.info` dictionary with the extracted information.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "ProjektInfoExtractor",
                    "description": "The instance of the class."
                  },
                  {
                    "name": "inhalt",
                    "type": "str",
                    "description": "The content of the README file as a string."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method calls `_extrahiere_sektion_aus_markdown` multiple times to extract specific sections. It also uses `re.search` to find the title and description, and `str.strip` and `str.split` for text manipulation.",
                  "called_by": "This method is called by `extrahiere_info` after a README file has been found and its content is available."
                }
              },
              "error": null
            },
            {
              "identifier": "_parse_toml",
              "description": {
                "overall": "This private method parses the content of a `pyproject.toml` file using the `tomllib` library. It attempts to extract the project name, description, and dependencies from the `[project]` section of the TOML data. If the `tomllib` library is not available, it prints a warning. It handles potential `TOMLDecodeError` exceptions during parsing.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "ProjektInfoExtractor",
                    "description": "The instance of the class."
                  },
                  {
                    "name": "inhalt",
                    "type": "str",
                    "description": "The content of the pyproject.toml file as a string."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method uses `tomllib.loads` to parse the TOML content and `dict.get` to safely access nested dictionary keys. It also calls `print` to display warnings.",
                  "called_by": "This method is called by `extrahiere_info` if a `pyproject.toml` file is found."
                }
              },
              "error": null
            },
            {
              "identifier": "_parse_requirements",
              "description": {
                "overall": "This private method parses the content of a `requirements.txt` file to extract a list of dependencies. It splits the content into lines, strips whitespace from each line, and filters out empty lines and lines starting with a '#'. This method only populates the dependencies if they haven't already been found and set by `_parse_toml`, acting as a fallback.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "ProjektInfoExtractor",
                    "description": "The instance of the class."
                  },
                  {
                    "name": "inhalt",
                    "type": "str",
                    "description": "The content of the requirements.txt file as a string."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method uses `str.splitlines`, `str.strip`, and `str.startswith` to process the file content.",
                  "called_by": "This method is called by `extrahiere_info` if a `requirements.txt` file is found and `pyproject.toml` did not provide dependencies."
                }
              },
              "error": null
            },
            {
              "identifier": "extrahiere_info",
              "description": {
                "overall": "This is the main public method of the class that orchestrates the extraction of project information. It first identifies relevant project files (README, pyproject.toml, requirements.txt) using `_finde_datei`. Then, it parses these files in a specific order of priority: `pyproject.toml` first for metadata, followed by `requirements.txt` as a fallback for dependencies, and finally `README` for descriptive content. After parsing, it formats the dependencies and sets the project title based on the repository URL. The method returns the populated `self.info` dictionary.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "ProjektInfoExtractor",
                    "description": "The instance of the class."
                  },
                  {
                    "name": "dateien",
                    "type": "List[Any]",
                    "description": "A list of file objects representing the files in the repository."
                  },
                  {
                    "name": "repo_url",
                    "type": "str",
                    "description": "The URL of the repository, used to derive the project title."
                  }
                ],
                "returns": [
                  {
                    "name": "Dict[str, Any]",
                    "type": "Dict[str, Any]",
                    "description": "A dictionary containing the extracted project information, structured into 'projekt_uebersicht' and 'installation' sections."
                  }
                ],
                "usage_context": {
                  "calls": "This method calls `_finde_datei` to locate project files, `_parse_toml`, `_parse_requirements`, and `_parse_readme` to parse their content, and `os.path.basename`, `str.removesuffix`, `isinstance`, and `str.join` for final data formatting and title generation.",
                  "called_by": "This method is called by the `main_workflow` function in `main.py` to extract information for a given repository."
                }
              },
              "error": null
            }
          ],
          "usage_context": {
            "dependencies": "This class utilizes standard Python libraries such as `re` for regular expressions, `os` for operating system path operations, and `tomllib` for parsing TOML files. It also relies on type hinting modules like `typing.List`, `typing.Dict`, and `typing.Optional`.",
            "instantiated_by": "The `ProjektInfoExtractor` class is instantiated within the `main_workflow` function in the `main.py` file."
          }
        },
        "error": null
      },
      "backend.callgraph.CallGraph": {
        "identifier": "backend.callgraph.CallGraph",
        "description": {
          "overall": "The CallGraph class is an AST visitor designed to traverse Python code, identify function and method calls, and construct a directed graph representing these relationships. It processes different definition types like classes, functions, and asynchronous functions, and specifically handles call sites and conditional blocks like `if __name__ == '__main__'` to accurately map call chains within a given file. The class maintains internal state for the current file, class, and function being visited, along with data structures to store import mappings, function sets, and the call graph edges.",
          "init_method": {
            "description": "Initializes the CallGraph visitor. It sets up attributes to track the current filename, function, and class being processed. It also initializes data structures for the graph itself, including a dictionary for import mappings, a set for all encountered functions, and a dictionary to store the call graph edges, mapping callers to sets of callees.",
            "parameters": [
              {
                "name": "self",
                "type": "CallGraph",
                "description": "The instance of the CallGraph class."
              },
              {
                "name": "filename",
                "type": "str",
                "description": "The name of the file being analyzed."
              }
            ]
          },
          "methods": [
            {
              "identifier": "_recursive_call",
              "description": {
                "overall": "Recursively traverses an AST node to extract function call names. It handles different node types like `ast.Call`, `ast.Name`, and `ast.Attribute`. For `ast.Call`, it continues recursion on the function part. For `ast.Name` and `ast.Attribute`, it extracts the identifier or attribute name, respectively. It returns a list of extracted names, which are potential callees.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "CallGraph",
                    "description": "The instance of the CallGraph class."
                  },
                  {
                    "name": "node",
                    "type": "ast.AST",
                    "description": "The AST node to process."
                  }
                ],
                "returns": [
                  {
                    "name": "all_calls",
                    "type": "list[str]",
                    "description": "A list of strings representing the extracted callee names."
                  }
                ],
                "usage_context": {
                  "calls": "This method calls itself recursively and uses `isinstance` to check node types.",
                  "called_by": "This method is called by `visit_Call` to extract callee names from call nodes."
                }
              },
              "error": null
            },
            {
              "identifier": "_resolve_all_callee_names",
              "description": {
                "overall": "Resolves raw callee names into fully qualified names based on the current file and class context. If no class is currently being visited, it prefixes the callee name with the filename. If a class is active, it prefixes with both the filename and the class name. This ensures that callees are uniquely identified within the context of the analysis.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "CallGraph",
                    "description": "The instance of the CallGraph class."
                  },
                  {
                    "name": "callee_nodes",
                    "type": "list[str]",
                    "description": "A list of raw callee names to resolve."
                  }
                ],
                "returns": [
                  {
                    "name": "resolved_callees",
                    "type": "list[str]",
                    "description": "A list of fully qualified callee names."
                  }
                ],
                "usage_context": {
                  "calls": "This method uses `append` to build the list of resolved callees.",
                  "called_by": "This method is called by `visit_Call` to process the names extracted by `_recursive_call`."
                }
              },
              "error": null
            },
            {
              "identifier": "_make_full_name",
              "description": {
                "overall": "Constructs a fully qualified name for a function or method. It takes a base name and an optional class name. If a class name is provided, it formats the name as 'filename::className::basename'. Otherwise, it formats it as 'filename::basename'. This ensures consistent naming for nodes in the call graph.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "CallGraph",
                    "description": "The instance of the CallGraph class."
                  },
                  {
                    "name": "basename",
                    "type": "str",
                    "description": "The base name of the function or method."
                  },
                  {
                    "name": "class_name",
                    "type": "str | None",
                    "description": "The name of the class, if the function is a method. Defaults to None."
                  }
                ],
                "returns": [
                  {
                    "name": "full_name",
                    "type": "str",
                    "description": "The fully qualified name."
                  }
                ],
                "usage_context": {
                  "calls": "This method uses f-string formatting to construct the name.",
                  "called_by": "This method is called by `visit_FunctionDef` and `visit_ClassDef` to create full names for functions and methods."
                }
              },
              "error": null
            },
            {
              "identifier": "_current_caller",
              "description": {
                "overall": "Determines the current caller string for the call graph. If `self.current_function` is set, it returns that value. Otherwise, it returns a string representing the filename if available, or '<global-scope>' if the filename is also not available. This helps in identifying the context from which a call is made.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "CallGraph",
                    "description": "The instance of the CallGraph class."
                  }
                ],
                "returns": [
                  {
                    "name": "caller",
                    "type": "str",
                    "description": "The string representing the current caller."
                  }
                ],
                "usage_context": {
                  "calls": "This method uses conditional logic and f-string formatting.",
                  "called_by": "This method is called by `visit_Call` to determine the caller of a function or method."
                }
              },
              "error": null
            },
            {
              "identifier": "visit_ClassDef",
              "description": {
                "overall": "Visits an `ast.ClassDef` node, which represents a class definition. It temporarily sets `self.current_class` to the name of the class being visited, then iterates through the nodes in the class body, recursively visiting each one. After processing the class body, it restores the previous `self.current_class` value to maintain the correct context.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "CallGraph",
                    "description": "The instance of the CallGraph class."
                  },
                  {
                    "name": "node",
                    "type": "ast.ClassDef",
                    "description": "The AST node representing the class definition."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method calls `self.visit` recursively on the functions within the class body.",
                  "called_by": "This method is called by the AST traversal mechanism when it encounters a class definition."
                }
              },
              "error": null
            },
            {
              "identifier": "visit_FunctionDef",
              "description": {
                "overall": "Visits an `ast.FunctionDef` node, representing a standard function definition. It determines the fully qualified name of the function using `_make_full_name`, adds the function as a node to the graph, and then recursively visits the function's body using `generic_visit`. Finally, it adds the function to a set of all functions and resets `self.current_function` to `None`.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "CallGraph",
                    "description": "The instance of the CallGraph class."
                  },
                  {
                    "name": "node",
                    "type": "ast.FunctionDef",
                    "description": "The AST node representing the function definition."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method calls `_make_full_name`, `add_node` on the graph, `generic_visit` for recursive traversal, and `add` on a set.",
                  "called_by": "This method is called by the AST traversal mechanism when it encounters a function definition, and also by `visit_AsyncFunctionDef`."
                }
              },
              "error": null
            },
            {
              "identifier": "visit_AsyncFunctionDef",
              "description": {
                "overall": "Visits an `ast.AsyncFunctionDef` node, which represents an asynchronous function definition (defined with `async def`). This method simply delegates the visit operation to `visit_FunctionDef`, as the logic for handling asynchronous functions is identical to that of regular functions in terms of call graph construction.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "CallGraph",
                    "description": "The instance of the CallGraph class."
                  },
                  {
                    "name": "node",
                    "type": "ast.AsyncFunctionDef",
                    "description": "The AST node representing the asynchronous function definition."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method calls `visit_FunctionDef`.",
                  "called_by": "This method is called by the AST traversal mechanism when it encounters an asynchronous function definition."
                }
              },
              "error": null
            },
            {
              "identifier": "visit_Call",
              "description": {
                "overall": "Visits an `ast.Call` node, representing a function or method call. It determines the current caller, extracts raw callee names using `_recursive_call`, resolves them into fully qualified names using `_resolve_all_callee_names`, and then adds the corresponding edge to the call graph stored in `self.edges`. It includes error handling to gracefully manage unexpected issues during call processing.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "CallGraph",
                    "description": "The instance of the CallGraph class."
                  },
                  {
                    "name": "node",
                    "type": "ast.Call",
                    "description": "The AST node representing the function call."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method calls `_current_caller`, `_recursive_call`, `_resolve_all_callee_names`, `generic_visit`, `print` for error reporting, and `add` on a set.",
                  "called_by": "This method is called by the AST traversal mechanism when it encounters a function or method call."
                }
              },
              "error": null
            },
            {
              "identifier": "visit_If",
              "description": {
                "overall": "Visits an `ast.If` node, specifically checking for the common `if __name__ == \"__main__\"` construct. If this condition is met, it temporarily sets the `self.current_function` to a special marker '<main_block>' to distinguish calls made within this block. It then recursively visits the nodes within the `if` block and restores the original `self.current_function` afterwards. Otherwise, it simply proceeds with a generic visit.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "CallGraph",
                    "description": "The instance of the CallGraph class."
                  },
                  {
                    "name": "node",
                    "type": "ast.If",
                    "description": "The AST node representing the if statement."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method calls `generic_visit` and `isinstance`.",
                  "called_by": "This method is called by the AST traversal mechanism when it encounters an if statement."
                }
              },
              "error": null
            }
          ],
          "usage_context": {
            "dependencies": "This class relies on the `ast` module for parsing Python code into an Abstract Syntax Tree and `networkx` for graph manipulation. It also uses typing hints like `Dict` and `str`.",
            "instantiated_by": "The `CallGraph` class is instantiated by the `build_callGraph` function in `callgraph.py` at line 165."
          }
        },
        "error": null
      },
      "backend.getRepo.RepoFile": {
        "identifier": "backend.getRepo.RepoFile",
        "description": {
          "overall": "The RepoFile class represents a single file within a Git repository. It is designed for lazy loading of file content and metadata, meaning that attributes like the file's blob object, content, and size are only fetched when they are first accessed. This class provides methods to access file properties, analyze its content (like word count), and represent the file as a dictionary.",
          "init_method": {
            "description": "Initializes a RepoFile object by storing the file path and the commit tree it belongs to. It also sets up internal attributes for lazy loading of the blob, content, and size, initializing them to None.",
            "parameters": [
              {
                "name": "file_path",
                "type": "str",
                "description": "The path to the file within the repository."
              },
              {
                "name": "commit_tree",
                "type": "git.Tree",
                "description": "The Tree object of the commit from which the file originates."
              }
            ]
          },
          "methods": [
            {
              "identifier": "blob",
              "description": {
                "overall": "This property lazily loads and returns the Git blob object for the file. If the blob has not been loaded yet, it attempts to retrieve it from the associated commit tree using the file's path. If the file is not found in the tree, it raises a FileNotFoundError.",
                "parameters": [],
                "returns": [
                  {
                    "name": "self._blob",
                    "type": "git.Blob",
                    "description": "The Git blob object representing the file."
                  }
                ],
                "usage_context": {
                  "calls": "This method calls backend/getRepo.py::RepoFile::FileNotFoundError if the file is not found in the commit tree.",
                  "called_by": "This method is called by other methods within the RepoFile class that require access to the file's blob object, such as 'content' and 'size'."
                }
              },
              "error": null
            },
            {
              "identifier": "content",
              "description": {
                "overall": "This property lazily loads, decodes, and returns the content of the file. It first accesses the file's blob object (which itself might be lazily loaded) and then reads its data stream. The content is decoded from bytes to a UTF-8 string, with errors ignored during decoding.",
                "parameters": [],
                "returns": [
                  {
                    "name": "self._content",
                    "type": "str",
                    "description": "The decoded content of the file as a string."
                  }
                ],
                "usage_context": {
                  "calls": "This method calls the 'blob' property to get the file's blob object and then uses the read() method on the blob's data stream, followed by decode('utf-8', errors='ignore').",
                  "called_by": "This method is called by other methods within the RepoFile class that require access to the file's content, such as 'analyze_word_count' and 'to_dict'."
                }
              },
              "error": null
            },
            {
              "identifier": "size",
              "description": {
                "overall": "This property lazily loads and returns the size of the file in bytes. It retrieves the size directly from the file's blob object, which is loaded on demand if it hasn't been already.",
                "parameters": [],
                "returns": [
                  {
                    "name": "self._size",
                    "type": "int",
                    "description": "The size of the file in bytes."
                  }
                ],
                "usage_context": {
                  "calls": "This method calls the 'blob' property to access the file's blob object and then retrieves its 'size' attribute.",
                  "called_by": "This method is called by other methods within the RepoFile class that require the file's size, such as 'to_dict'."
                }
              },
              "error": null
            },
            {
              "identifier": "analyze_word_count",
              "description": {
                "overall": "This method performs a simple analysis on the file's content by counting the number of words. It accesses the file's content (which is lazily loaded) and then splits the content into words based on whitespace, returning the total count.",
                "parameters": [],
                "returns": [
                  {
                    "name": "word_count",
                    "type": "int",
                    "description": "The total number of words found in the file's content."
                  }
                ],
                "usage_context": {
                  "calls": "This method calls the 'content' property to get the file's content and then uses the split() method to separate words and len() to count them.",
                  "called_by": "This method is called when a word count analysis of a file's content is required."
                }
              },
              "error": null
            },
            {
              "identifier": "__repr__",
              "description": {
                "overall": "Provides a developer-friendly string representation of the RepoFile object. It includes the file's path, making it easy to identify the object when debugging or inspecting.",
                "parameters": [],
                "returns": [
                  {
                    "name": "representation",
                    "type": "str",
                    "description": "A string representation of the RepoFile object, e.g., \"<RepoFile(path='...')>\"."
                  }
                ],
                "usage_context": {
                  "calls": "This method uses an f-string for formatting.",
                  "called_by": "This method is called implicitly when the object is printed or inspected in an interactive session."
                }
              },
              "error": null
            },
            {
              "identifier": "to_dict",
              "description": {
                "overall": "Converts the RepoFile object into a dictionary representation. By default, it includes the file's path, name, size, and type. Optionally, if 'include_content' is set to True, it also includes the file's content in the dictionary.",
                "parameters": [
                  {
                    "name": "include_content",
                    "type": "bool",
                    "description": "A flag to determine if the file's content should be included in the dictionary."
                  }
                ],
                "returns": [
                  {
                    "name": "data",
                    "type": "dict",
                    "description": "A dictionary containing file metadata and optionally its content."
                  }
                ],
                "usage_context": {
                  "calls": "This method calls os.path.basename to extract the filename from the path, and accesses the 'path', 'size', and 'content' properties of the RepoFile object.",
                  "called_by": "This method is called when a dictionary representation of the file is needed, potentially for serialization or further processing."
                }
              },
              "error": null
            }
          ],
          "usage_context": {
            "dependencies": "This class depends on the 'git' library for interacting with Git repositories and the 'os' module for path manipulations.",
            "instantiated_by": "This class is instantiated by the 'get_all_files' function in the 'getRepo.py' file."
          }
        },
        "error": null
      },
      "backend.getRepo.GitRepository": {
        "identifier": "backend.getRepo.GitRepository",
        "description": {
          "overall": "The GitRepository class is designed to manage a Git repository by cloning it into a temporary directory. It provides methods to access files within the repository, retrieve them as RepoFile objects, and construct a hierarchical file tree. The class also handles cleanup by deleting the temporary directory upon completion, making it suitable for use in contexts where temporary access to a repository is needed.",
          "init_method": {
            "description": "Initializes the GitRepository by storing the repository URL, creating a temporary directory, and cloning the repository into it. It attempts to clone the repository using the provided URL and logs the process. If cloning fails, it cleans up and raises a RuntimeError. It also initializes attributes for files and the latest commit information.",
            "parameters": [
              {
                "name": "repo_url",
                "type": "str",
                "description": "The URL of the Git repository to be cloned."
              }
            ]
          },
          "methods": [
            {
              "identifier": "get_all_files",
              "description": {
                "overall": "Retrieves all files from the cloned Git repository and returns them as a list of RepoFile objects. It uses the Git command `ls-files` to get a list of file paths, then iterates through these paths to create RepoFile instances, associating each with the commit tree. This method populates the `self.files` attribute.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "GitRepository",
                    "description": "The instance of the GitRepository class."
                  }
                ],
                "returns": [
                  {
                    "name": "files",
                    "type": "list[RepoFile]",
                    "description": "A list containing RepoFile objects, each representing a file in the repository."
                  }
                ],
                "usage_context": {
                  "calls": "This method calls `self.repo.git.ls_files()` to list files, `split('\\n')` to parse the output, and uses a list comprehension to create `RepoFile` objects, passing the file path and `self.commit_tree` to the `RepoFile` constructor.",
                  "called_by": "This method is called by `get_file_tree` if no files have been loaded yet."
                }
              },
              "error": null
            },
            {
              "identifier": "close",
              "description": {
                "overall": "Cleans up by deleting the temporary directory that was created for the Git repository. It checks if `self.temp_dir` is set before attempting to remove the directory and its contents, and then sets `self.temp_dir` to None to indicate that cleanup has occurred. A message is printed to the console indicating the directory being deleted.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "GitRepository",
                    "description": "The instance of the GitRepository class."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method calls the built-in `print` function to display a message indicating the temporary directory being deleted.",
                  "called_by": "This method is called by the constructor (`__init__`) if a `GitCommandError` occurs during cloning, and by the `__exit__` method to ensure cleanup when exiting a context."
                }
              },
              "error": null
            },
            {
              "identifier": "__enter__",
              "description": {
                "overall": "Implements the context management protocol, returning the instance of the GitRepository itself. This allows the class to be used with the `with` statement, ensuring that resources are properly managed.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "GitRepository",
                    "description": "The instance of the GitRepository class."
                  }
                ],
                "returns": [
                  {
                    "name": "self",
                    "type": "GitRepository",
                    "description": "The current instance of the GitRepository."
                  }
                ],
                "usage_context": {
                  "calls": "This method does not call any other methods or functions.",
                  "called_by": "This method is called implicitly when entering a `with` statement block that uses a GitRepository instance."
                }
              },
              "error": null
            },
            {
              "identifier": "__exit__",
              "description": {
                "overall": "Implements the context management protocol's exit behavior. It ensures that the `close` method is called to clean up the temporary directory, regardless of whether an exception occurred within the `with` block. It receives exception details as arguments but does not explicitly handle them, allowing exceptions to propagate.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "GitRepository",
                    "description": "The instance of the GitRepository class."
                  },
                  {
                    "name": "exc_type",
                    "type": "type",
                    "description": "The type of the exception raised in the `with` block, if any."
                  },
                  {
                    "name": "exc_val",
                    "type": "Exception",
                    "description": "The exception instance raised in the `with` block, if any."
                  },
                  {
                    "name": "exc_tb",
                    "type": "traceback",
                    "description": "The traceback object associated with the exception, if any."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method calls `self.close()` to perform cleanup operations.",
                  "called_by": "This method is called implicitly when exiting a `with` statement block that uses a GitRepository instance."
                }
              },
              "error": null
            },
            {
              "identifier": "get_file_tree",
              "description": {
                "overall": "Constructs and returns a hierarchical tree structure representing the files and directories within the repository. If no files have been loaded, it first calls `get_all_files`. It then iterates through the file paths, building the directory structure and adding each file (optionally including its content) as a dictionary to the appropriate level in the tree. The root of the tree is always named 'root'.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "GitRepository",
                    "description": "The instance of the GitRepository class."
                  },
                  {
                    "name": "include_content",
                    "type": "bool",
                    "description": "A boolean flag indicating whether to include the content of each file in the returned tree structure. Defaults to False."
                  }
                ],
                "returns": [
                  {
                    "name": "tree",
                    "type": "dict",
                    "description": "A dictionary representing the hierarchical structure of the repository's files and directories."
                  }
                ],
                "usage_context": {
                  "calls": "This method calls `self.get_all_files()` if `self.files` is empty, and then iterates through `self.files`. Inside the loop, it calls `file_obj.path.split('/')` to parse paths, uses `next()` to find existing directory items, and `current_level.append()` to add new directories or files. Finally, it calls `file_obj.to_dict()` to convert file objects into dictionaries.",
                  "called_by": "This method is called directly to generate a file tree representation of the repository."
                }
              },
              "error": null
            }
          ],
          "usage_context": {
            "dependencies": "This class depends on external libraries such as `tempfile` for temporary directory creation, `shutil` for file operations (though commented out in `close`), `git.Repo` and `git.GitCommandError` for Git repository interactions, and `logging` for information output. It also relies on a `RepoFile` class which is not defined in the provided source code.",
            "instantiated_by": "The GitRepository class is instantiated within the `main_workflow` function in the `main.py` file on line 120."
          }
        },
        "error": null
      },
      "backend.relationship_analyzer.ProjectAnalyzer": {
        "identifier": "backend.relationship_analyzer.ProjectAnalyzer",
        "description": {
          "overall": "The ProjectAnalyzer class is designed to parse a Python project, identify its code definitions (functions, classes, methods), and build a call graph to understand relationships between these elements. It traverses the project's file system, parses Python files using the AST module, and stores information about definitions and calls. Finally, it formats these findings into a structured list representing the call graph.",
          "init_method": {
            "description": "The constructor initializes the ProjectAnalyzer with the root directory of the project. It sets up instance attributes to store project root, definitions, call graph, file ASTs, and a set of directories to ignore during file traversal. The project root path is normalized to an absolute path.",
            "parameters": [
              {
                "name": "project_root",
                "type": "string",
                "description": "The absolute path to the root directory of the project to be analyzed."
              }
            ]
          },
          "methods": [
            {
              "identifier": "analyze",
              "description": {
                "overall": "This is the main method to initiate the project analysis. It first finds all Python files within the project, then iterates through them to collect all code definitions (functions, classes, methods) and subsequently resolves the call relationships between them. After processing, it clears the stored ASTs to free up memory and returns the formatted results of the analysis.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "ProjectAnalyzer",
                    "description": "The instance of the ProjectAnalyzer class."
                  }
                ],
                "returns": [
                  {
                    "name": "output_list",
                    "type": "list",
                    "description": "A list of dictionaries, where each dictionary represents a definition and details the functions or methods that call it."
                  }
                ],
                "usage_context": {
                  "calls": "This method orchestrates the analysis by calling helper methods to find Python files, collect definitions, and resolve calls. It also calls methods to clear internal state and format the final results.",
                  "called_by": "This method is called by the main_workflow function in main.py."
                }
              },
              "error": null
            },
            {
              "identifier": "_find_py_files",
              "description": {
                "overall": "This private method recursively walks through the project directory starting from `self.project_root`. It identifies all Python files (`.py`) while excluding specified directories like `.git`, `venv`, `__pycache__`, etc. The method returns a list of absolute paths to all found Python files.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "ProjectAnalyzer",
                    "description": "The instance of the ProjectAnalyzer class."
                  }
                ],
                "returns": [
                  {
                    "name": "py_files",
                    "type": "list",
                    "description": "A list of strings, where each string is the absolute path to a Python file found in the project."
                  }
                ],
                "usage_context": {
                  "calls": "This method utilizes os.walk to traverse directories and os.path.join to construct file paths. It checks file extensions using endswith and appends found files to a list.",
                  "called_by": "This method is called by the analyze method to get a list of all Python files to be processed."
                }
              },
              "error": null
            },
            {
              "identifier": "_collect_definitions",
              "description": {
                "overall": "This private method parses a given Python file to extract code definitions such as functions, classes, and methods. It reads the source code, uses the `ast` module to build an Abstract Syntax Tree (AST), and then walks the tree to identify definition nodes. For each definition, it records its type, file path, line number, and constructs a unique path name. It stores the AST for later use and handles potential file reading or parsing errors by logging them.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "ProjectAnalyzer",
                    "description": "The instance of the ProjectAnalyzer class."
                  },
                  {
                    "name": "filepath",
                    "type": "string",
                    "description": "The path to the Python file to be parsed."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method reads file contents, parses them into an AST using ast.parse, and walks the AST. It calls _get_parent to determine if a function is a method within a class and uses path_to_module to create module paths. It also logs errors using the logging module.",
                  "called_by": "This method is called by the analyze method for each Python file found in the project."
                }
              },
              "error": null
            },
            {
              "identifier": "_get_parent",
              "description": {
                "overall": "This private helper method traverses the AST of a given tree to find the direct parent node of a specified child node. It iterates through all nodes in the tree and checks their child nodes to locate the target node, returning its parent. If the node is not found or has no parent within the tree, it returns None.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "ProjectAnalyzer",
                    "description": "The instance of the ProjectAnalyzer class."
                  },
                  {
                    "name": "tree",
                    "type": "ast.AST",
                    "description": "The Abstract Syntax Tree to search within."
                  },
                  {
                    "name": "node",
                    "type": "ast.AST",
                    "description": "The child node whose parent is to be found."
                  }
                ],
                "returns": [
                  {
                    "name": "parent",
                    "type": "ast.AST | None",
                    "description": "The parent node of the given node, or None if no parent is found."
                  }
                ],
                "usage_context": {
                  "calls": "This method uses ast.walk to iterate through the AST and ast.iter_child_nodes to examine children of each node.",
                  "called_by": "This method is called by _collect_definitions to determine the context (e.g., class or module level) of function definitions."
                }
              },
              "error": null
            },
            {
              "identifier": "_resolve_calls",
              "description": {
                "overall": "This private method analyzes the AST of a given Python file to identify function and method calls. It instantiates a `CallResolverVisitor` (presumably a custom visitor class) which traverses the AST and records call information. The collected call data, mapping callees to their callers, is then merged into the class's main `call_graph` attribute. Errors during the resolution process are logged.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "ProjectAnalyzer",
                    "description": "The instance of the ProjectAnalyzer class."
                  },
                  {
                    "name": "filepath",
                    "type": "string",
                    "description": "The path to the Python file whose calls are to be resolved."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method retrieves the AST from `self.file_asts`, instantiates `CallResolverVisitor`, visits the AST using the visitor, and extends `self.call_graph` with the results.",
                  "called_by": "This method is called by the analyze method for each Python file after definitions have been collected."
                }
              },
              "error": null
            },
            {
              "identifier": "get_formatted_results",
              "description": {
                "overall": "This method formats the collected call graph data into a user-friendly list of dictionaries. It iterates through the `self.call_graph`, and for each callee, it retrieves its definition information. It then processes the list of callers for that callee, ensuring uniqueness of call sites (file, line, caller) and structures the output to include the callee's identifier, mode, origin file and line, and a list of callers with their respective file, function, mode, and line number. Definitions not found in `self.definitions` are skipped.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "ProjectAnalyzer",
                    "description": "The instance of the ProjectAnalyzer class."
                  }
                ],
                "returns": [
                  {
                    "name": "output_list",
                    "type": "list",
                    "description": "A list of dictionaries, where each dictionary details a definition and the functions/methods that call it."
                  }
                ],
                "usage_context": {
                  "calls": "This method iterates through `self.call_graph` and `self.definitions`. It uses `os.path.basename` and `sorted` for formatting and `dict.values` and `dict.get` for data extraction.",
                  "called_by": "This method is called by the analyze method to produce the final output of the analysis."
                }
              },
              "error": null
            }
          ],
          "usage_context": {
            "dependencies": "This class relies on standard Python libraries such as `ast` for parsing code, `os` for file system operations, `logging` for error reporting, and `collections.defaultdict` for managing the call graph. It also implicitly depends on external components like `path_to_module` and `CallResolverVisitor` which are not defined within the provided source code.",
            "instantiated_by": "This class is instantiated within the main_workflow function in the main.py file."
          }
        },
        "error": null
      },
      "backend.relationship_analyzer.CallResolverVisitor": {
        "identifier": "backend.relationship_analyzer.CallResolverVisitor",
        "description": {
          "overall": "The CallResolverVisitor class is designed to traverse an Abstract Syntax Tree (AST) of Python code to identify and resolve function and method calls. It maintains scope information, tracks imported modules and aliases, and records the types of instantiated classes to accurately determine the fully qualified names of called functions or methods. This allows for the construction of a call graph, mapping where specific definitions are invoked from within the codebase.",
          "init_method": {
            "description": "Initializes the CallResolverVisitor with the file path, project root, and a dictionary of known definitions. It sets up internal state to track the current scope, instance types, caller information, and a defaultdict to store call relationships.",
            "parameters": [
              {
                "name": "filepath",
                "type": "str",
                "description": "The absolute path to the Python file being analyzed."
              },
              {
                "name": "project_root",
                "type": "str",
                "description": "The root directory of the project, used for resolving module paths."
              },
              {
                "name": "definitions",
                "type": "dict",
                "description": "A dictionary containing known definitions within the project, mapping qualified names to their details."
              }
            ]
          },
          "methods": [
            {
              "identifier": "visit_ClassDef",
              "description": {
                "overall": "Visits a class definition node in the AST. It temporarily stores the current class name, updates the `current_class_name` to the newly encountered class, recursively visits child nodes within the class definition, and then restores the previous class name upon exiting the class scope.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "CallResolverVisitor",
                    "description": "The instance of the visitor."
                  },
                  {
                    "name": "node",
                    "type": "ast.ClassDef",
                    "description": "The AST node representing a class definition."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method calls the generic_visit method to continue the AST traversal and updates the instance's current_class_name attribute.",
                  "called_by": "This method is called automatically by the ast.NodeVisitor when it encounters a ClassDef node during traversal."
                }
              },
              "error": null
            },
            {
              "identifier": "visit_FunctionDef",
              "description": {
                "overall": "Visits a function definition node in the AST. It temporarily stores the current caller name, updates `current_caller_name` to the name of the function being defined, recursively visits any nodes within the function body, and then restores the previous caller name.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "CallResolverVisitor",
                    "description": "The instance of the visitor."
                  },
                  {
                    "name": "node",
                    "type": "ast.FunctionDef",
                    "description": "The AST node representing a function definition."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method calls the generic_visit method to continue the AST traversal and updates the instance's current_caller_name attribute.",
                  "called_by": "This method is called automatically by the ast.NodeVisitor when it encounters a FunctionDef node during traversal."
                }
              },
              "error": null
            },
            {
              "identifier": "visit_Call",
              "description": {
                "overall": "Visits a call expression node in the AST. It attempts to resolve the fully qualified name of the called function or method using `_resolve_call_qname`. If the resolved name exists in the project's definitions, it records the call information, including the caller's file, line number, name, and type (module, method, or function), and appends it to the `calls` dictionary. It then continues the traversal.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "CallResolverVisitor",
                    "description": "The instance of the visitor."
                  },
                  {
                    "name": "node",
                    "type": "ast.Call",
                    "description": "The AST node representing a function or method call."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method calls the _resolve_call_qname method to determine the callee's identity, appends call information to the self.calls dictionary, uses os.path.basename to get the filename, and calls generic_visit to continue AST traversal.",
                  "called_by": "This method is called automatically by the ast.NodeVisitor when it encounters a Call node during traversal."
                }
              },
              "error": null
            },
            {
              "identifier": "visit_Import",
              "description": {
                "overall": "Visits an import statement node. It iterates through the imported aliases, updating the `scope` dictionary with the mapping from the imported name (or its alias) to its module path. It then continues the AST traversal.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "CallResolverVisitor",
                    "description": "The instance of the visitor."
                  },
                  {
                    "name": "node",
                    "type": "ast.Import",
                    "description": "The AST node representing an import statement."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method iterates through aliases and updates the self.scope dictionary, then calls generic_visit to continue AST traversal.",
                  "called_by": "This method is called automatically by the ast.NodeVisitor when it encounters an Import node during traversal."
                }
              },
              "error": null
            },
            {
              "identifier": "visit_ImportFrom",
              "description": {
                "overall": "Visits an import-from statement node. It processes each imported name, determining its full module path based on the current module's path and the import level. It then stores this mapping in the `scope` dictionary and continues the AST traversal.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "CallResolverVisitor",
                    "description": "The instance of the visitor."
                  },
                  {
                    "name": "node",
                    "type": "ast.ImportFrom",
                    "description": "The AST node representing an import-from statement."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method uses string manipulation (split, join) to construct module paths, updates the self.scope dictionary, and calls generic_visit to continue AST traversal.",
                  "called_by": "This method is called automatically by the ast.NodeVisitor when it encounters an ImportFrom node during traversal."
                }
              },
              "error": null
            },
            {
              "identifier": "visit_Assign",
              "description": {
                "overall": "Visits an assignment statement node. If the assignment involves calling a class constructor (e.g., `variable = ClassName(...)`), it identifies the class name, resolves its qualified name using the `scope`, and if the class is defined within the project, it records the mapping from the variable name to the qualified class name in the `instance_types` dictionary. It then continues the AST traversal.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "CallResolverVisitor",
                    "description": "The instance of the visitor."
                  },
                  {
                    "name": "node",
                    "type": "ast.Assign",
                    "description": "The AST node representing an assignment statement."
                  }
                ],
                "returns": [],
                "usage_context": {
                  "calls": "This method uses isinstance to check node types and checks for the presence of class names in the self.scope dictionary, updating self.instance_types, and calls generic_visit to continue AST traversal.",
                  "called_by": "This method is called automatically by the ast.NodeVisitor when it encounters an Assign node during traversal."
                }
              },
              "error": null
            },
            {
              "identifier": "_resolve_call_qname",
              "description": {
                "overall": "A helper method to resolve the fully qualified name (QName) of a called function or method from its AST node. It handles cases where the call is a direct name (function or module) or an attribute access (method call on an object or module attribute). It uses the `scope` and `instance_types` dictionaries to perform the resolution. Returns `None` if the QName cannot be resolved.",
                "parameters": [
                  {
                    "name": "self",
                    "type": "CallResolverVisitor",
                    "description": "The instance of the visitor."
                  },
                  {
                    "name": "func_node",
                    "type": "ast.expr",
                    "description": "The AST node representing the function or method being called (e.g., ast.Name or ast.Attribute)."
                  }
                ],
                "returns": [
                  {
                    "name": "qualified_name",
                    "type": "str | None",
                    "description": "The fully qualified name of the function or method, or None if it cannot be resolved."
                  }
                ],
                "usage_context": {
                  "calls": "This method uses isinstance to check the type of the func_node and accesses the self.scope and self.instance_types dictionaries.",
                  "called_by": "This method is called by the visit_Call method to determine the identity of the callee."
                }
              },
              "error": null
            }
          ],
          "usage_context": {
            "dependencies": "This class utilizes the 'ast' module for parsing Python code into an Abstract Syntax Tree, the 'os' module for path manipulations (specifically os.path.basename), and 'collections.defaultdict' for efficiently storing call information. It also relies on an external function `path_to_module` for converting file paths to module paths.",
            "instantiated_by": "This class is instantiated within the `_resolve_calls` method in the `relationship_analyzer.py` file on line 92."
          }
        },
        "error": null
      },
      "schemas.types.ParameterDescription": {
        "identifier": "schemas.types.ParameterDescription",
        "description": {
          "overall": "The ParameterDescription class is a Pydantic model used to define the structure for describing a single parameter within a function or method. It captures the parameter's name, its data type, and a textual explanation of its purpose.",
          "init_method": {
            "description": "Initializes a ParameterDescription object with the name, type, and description of a function parameter. This model inherits from Pydantic's BaseModel, ensuring data validation.",
            "parameters": [
              {
                "name": "name",
                "type": "str",
                "description": "The name of the parameter."
              },
              {
                "name": "type",
                "type": "str",
                "description": "The data type of the parameter."
              },
              {
                "name": "description",
                "type": "str",
                "description": "A textual explanation of the parameter's purpose."
              }
            ]
          },
          "methods": [],
          "usage_context": {
            "dependencies": "This class does not have any external dependencies beyond Pydantic's BaseModel.",
            "instantiated_by": "This class is intended to be instantiated directly when defining the structure of function parameters."
          }
        },
        "error": null
      },
      "schemas.types.ReturnDescription": {
        "identifier": "schemas.types.ReturnDescription",
        "description": {
          "overall": "This class, ReturnDescription, is a Pydantic model designed to structure information about a function's return value. It captures the name, type, and a textual description of what the function returns.",
          "init_method": {
            "description": "Initializes the ReturnDescription model with the name, type, and description of a function's return value. These attributes are directly assigned from the provided arguments.",
            "parameters": [
              {
                "name": "name",
                "type": "str",
                "description": "The name of the return value, if applicable."
              },
              {
                "name": "type",
                "type": "str",
                "description": "The data type of the return value."
              },
              {
                "name": "description",
                "type": "str",
                "description": "A textual explanation of the return value."
              }
            ]
          },
          "methods": [],
          "usage_context": {
            "dependencies": "This class depends on the BaseModel from the pydantic library for its structure and validation.",
            "instantiated_by": "This class is intended to be instantiated within systems that need to define and validate the structure of function return values, likely as part of API documentation or code analysis tools."
          }
        },
        "error": null
      },
      "schemas.types.UsageContext": {
        "identifier": "schemas.types.UsageContext",
        "description": {
          "overall": "The UsageContext class is a Pydantic model used to describe the calling context of a function. It specifically captures information about which other functions or methods a given function calls, and which functions or methods call it.",
          "init_method": {
            "description": "Initializes the UsageContext model with information about the functions called and the functions that call this one. It inherits from Pydantic's BaseModel for data validation.",
            "parameters": [
              {
                "name": "calls",
                "type": "str",
                "description": "A string describing the functions or methods called by this function."
              },
              {
                "name": "called_by",
                "type": "str",
                "description": "A string describing the functions or methods that call this function."
              }
            ]
          },
          "methods": [],
          "usage_context": {
            "dependencies": "This class depends on pydantic.BaseModel for its structure and validation.",
            "instantiated_by": "This class is typically instantiated within schemas or data structures that need to represent function call relationships."
          }
        },
        "error": null
      },
      "schemas.types.FunctionDescription": {
        "identifier": "schemas.types.FunctionDescription",
        "description": {
          "overall": "The FunctionDescription class is a Pydantic model designed to encapsulate a comprehensive analysis of a function. It details the function's overall purpose, its parameters, its return values, and its usage context within a larger system. This structure is intended for machine readability and facilitates the systematic documentation of code.",
          "init_method": {
            "description": "Initializes a FunctionDescription object. As this is a Pydantic model, initialization is handled by Pydantic's BaseModel, which validates and assigns the provided attributes. The attributes directly correspond to the components of a function's analysis: overall description, parameter details, return value details, and usage context.",
            "parameters": [
              {
                "name": "overall",
                "type": "str",
                "description": "A string providing an overall summary of the function's purpose and behavior."
              },
              {
                "name": "parameters",
                "type": "List[ParameterDescription]",
                "description": "A list of ParameterDescription objects, each detailing a parameter of the function."
              },
              {
                "name": "returns",
                "type": "List[ReturnDescription]",
                "description": "A list of ReturnDescription objects, each detailing a return value of the function."
              },
              {
                "name": "usage_context",
                "type": "UsageContext",
                "description": "A UsageContext object that describes how and where the function is called and what other functions it calls."
              }
            ]
          },
          "methods": [],
          "usage_context": {
            "dependencies": "This class depends on Pydantic's BaseModel for its structure and validation. It also relies on other defined types such as ParameterDescription, ReturnDescription, and UsageContext, which are assumed to be defined elsewhere.",
            "instantiated_by": "This class is intended to be instantiated by systems that perform code analysis and require a structured representation of function details. It is a data structure for holding analysis results."
          }
        },
        "error": null
      },
      "schemas.types.FunctionAnalysis": {
        "identifier": "schemas.types.FunctionAnalysis",
        "description": {
          "overall": "The FunctionAnalysis class is a Pydantic model designed to represent the structured analysis of a Python function. It serves as a primary data structure within a documentation generation system, encapsulating all relevant details about a function's signature, behavior, and usage context. This model is intended for machine readability, facilitating further processing by other AI components.",
          "init_method": {
            "description": "Initializes a FunctionAnalysis object. It takes the function's identifier, a detailed description object, and an optional error string as input. This constructor is used to create structured representations of function analyses, typically generated by an AI code analyst.",
            "parameters": [
              {
                "name": "identifier",
                "type": "str",
                "description": "The unique name or identifier of the function being analyzed."
              },
              {
                "name": "description",
                "type": "FunctionDescription",
                "description": "A complex object containing the detailed analysis of the function, including its overall purpose, parameters, return values, and usage context."
              },
              {
                "name": "error",
                "type": "Optional[str]",
                "description": "An optional field to store any error messages encountered during the analysis of the function. If no errors occurred, this will be None."
              }
            ]
          },
          "methods": [],
          "usage_context": {
            "dependencies": "This class has no external dependencies beyond Pydantic's BaseModel and typing.Optional.",
            "instantiated_by": "This class is instantiated by the system that performs code analysis and generates documentation, likely as part of a larger AI-driven documentation pipeline."
          }
        },
        "error": null
      },
      "schemas.types.ConstructorDescription": {
        "identifier": "schemas.types.ConstructorDescription",
        "description": {
          "overall": "This class, ConstructorDescription, is designed to encapsulate the details of a class's initialization method (__init__). It inherits from Pydantic's BaseModel, ensuring data validation for its fields. The class is intended to hold a textual description of the constructor and a list of its parameters, each described by a ParameterDescription object.",
          "init_method": {
            "description": "Initializes a ConstructorDescription object. It takes a textual description of the constructor and a list of ParameterDescription objects, which detail each parameter of the constructor.",
            "parameters": [
              {
                "name": "description",
                "type": "str",
                "description": "A string providing a textual summary of the __init__ method's purpose and behavior."
              },
              {
                "name": "parameters",
                "type": "List[ParameterDescription]",
                "description": "A list where each element is a ParameterDescription object, detailing a parameter of the __init__ method."
              }
            ]
          },
          "methods": [],
          "usage_context": {
            "dependencies": "This class has no external dependencies beyond Pydantic's BaseModel and typing.List.",
            "instantiated_by": "This class is likely instantiated by systems that need to represent and validate the structure of class constructors, such as documentation generators or code analysis tools."
          }
        },
        "error": null
      },
      "schemas.types.ClassContext": {
        "identifier": "schemas.types.ClassContext",
        "description": {
          "overall": "The ClassContext model is a Pydantic BaseModel used to describe a class's external dependencies and its primary points of instantiation. It serves as a structured way to document how a class interacts with other parts of a system and where it is typically created.",
          "init_method": {
            "description": "Initializes the ClassContext model with details about the class's dependencies and instantiation points. It directly assigns the provided values to the corresponding attributes.",
            "parameters": [
              {
                "name": "dependencies",
                "type": "str",
                "description": "A string describing the external dependencies of the class."
              },
              {
                "name": "instantiated_by",
                "type": "str",
                "description": "A string describing where the class is typically instantiated."
              }
            ]
          },
          "methods": [],
          "usage_context": {
            "dependencies": "This class does not appear to have any external dependencies defined within its scope.",
            "instantiated_by": "This class is not explicitly shown to be instantiated anywhere in the provided context."
          }
        },
        "error": null
      },
      "schemas.types.ClassDescription": {
        "identifier": "schemas.types.ClassDescription",
        "description": {
          "overall": "The ClassDescription class is a Pydantic model designed to encapsulate a comprehensive analysis of a Python class. It structures information about the class's overall purpose, its initialization method, a detailed breakdown of its individual methods, and its usage context including dependencies and instantiation points.",
          "init_method": {
            "description": "Initializes a ClassDescription object. It takes arguments that directly map to the attributes of the class, allowing for the structured representation of a class's analysis.",
            "parameters": [
              {
                "name": "overall",
                "type": "str",
                "description": "A string describing the overall purpose and responsibilities of the class."
              },
              {
                "name": "init_method",
                "type": "ConstructorDescription",
                "description": "An object detailing the constructor's behavior, including its description and parameters."
              },
              {
                "name": "methods",
                "type": "List[FunctionAnalysis]",
                "description": "A list of objects, where each object represents the analysis of a specific method within the class."
              },
              {
                "name": "usage_context",
                "type": "ClassContext",
                "description": "An object detailing the class's external dependencies and where it is instantiated."
              }
            ]
          },
          "methods": [],
          "usage_context": {
            "dependencies": "This class relies on other Pydantic models (BaseModel) and typing constructs (List) for its structure and definition.",
            "instantiated_by": "This class is intended to be instantiated by systems that perform code analysis and require a structured way to represent the findings."
          }
        },
        "error": null
      },
      "schemas.types.ClassAnalysis": {
        "identifier": "schemas.types.ClassAnalysis",
        "description": {
          "overall": "The ClassAnalysis model represents the overall structure for analyzing a Python class. It encapsulates the class's identifier, a detailed description of its components (constructor, methods, and overall purpose), and an optional error field for reporting analysis issues. This model is designed to be a comprehensive data structure for documenting and understanding Python classes.",
          "init_method": {
            "description": "Initializes a ClassAnalysis object. It takes the class identifier, a ClassDescription object containing the analysis details, and an optional error string. The error defaults to None, indicating successful analysis.",
            "parameters": [
              {
                "name": "identifier",
                "type": "str",
                "description": "The name of the class being analyzed."
              },
              {
                "name": "description",
                "type": "ClassDescription",
                "description": "An object containing the detailed analysis of the class, including its overall purpose, constructor, methods, and usage context."
              },
              {
                "name": "error",
                "type": "Optional[str]",
                "description": "An optional string that holds an error message if the analysis failed; otherwise, it is None."
              }
            ]
          },
          "methods": [],
          "usage_context": {
            "dependencies": "This class has no external dependencies explicitly listed in its context.",
            "instantiated_by": "This class is not instantiated by any other components according to the provided context."
          }
        },
        "error": null
      },
      "schemas.types.CallInfo": {
        "identifier": "schemas.types.CallInfo",
        "description": {
          "overall": "The CallInfo class is a Pydantic model used to represent detailed information about a specific call event, typically originating from a relationship analyzer. It serves to structure data for lists like 'called_by' and 'instantiated_by', providing context about the caller, the function or method being called, and the location of the call.",
          "init_method": {
            "description": "Initializes a CallInfo object with details about a call event. It takes the file path, function name, call mode, and line number as arguments, storing them as instance attributes.",
            "parameters": [
              {
                "name": "file",
                "type": "str",
                "description": "The path to the file where the call occurred."
              },
              {
                "name": "function",
                "type": "str",
                "description": "The name of the function or method that made the call."
              },
              {
                "name": "mode",
                "type": "str",
                "description": "The type of call, such as 'method', 'function', or 'module'."
              },
              {
                "name": "line",
                "type": "int",
                "description": "The line number in the file where the call occurred."
              }
            ]
          },
          "methods": [],
          "usage_context": {
            "dependencies": "This class does not appear to have any external dependencies beyond Pydantic's BaseModel.",
            "instantiated_by": "This class is instantiated in contexts where detailed call event information needs to be recorded, such as within relationship analysis systems for tracking 'called_by' and 'instantiated_by' relationships."
          }
        },
        "error": null
      },
      "schemas.types.FunctionContextInput": {
        "identifier": "schemas.types.FunctionContextInput",
        "description": {
          "overall": "The FunctionContextInput class is a Pydantic model designed to structure contextual information for analyzing a function. It specifically captures the functions or methods that a given function calls and the functions or methods that call it. This is useful for understanding call graphs and dependencies within a codebase.",
          "init_method": {
            "description": "Initializes the FunctionContextInput model with lists of calls and called_by information. This constructor is automatically generated by Pydantic based on the class attributes.",
            "parameters": [
              {
                "name": "calls",
                "type": "List[str]",
                "description": "A list of strings, where each string represents a function or method called by the function being analyzed."
              },
              {
                "name": "called_by",
                "type": "List[CallInfo]",
                "description": "A list of CallInfo objects, where each object represents a function or method that calls the function being analyzed."
              }
            ]
          },
          "methods": [],
          "usage_context": {
            "dependencies": "This class depends on Pydantic's BaseModel for its structure and validation, and it uses List from typing. It also references a CallInfo type which is assumed to be defined elsewhere.",
            "instantiated_by": "The FunctionContextInput class is instantiated within the main_workflow function in the main.py file on line 202."
          }
        },
        "error": null
      },
      "schemas.types.FunctionAnalysisInput": {
        "identifier": "schemas.types.FunctionAnalysisInput",
        "description": {
          "overall": "The FunctionAnalysisInput class is a Pydantic model designed to encapsulate all the necessary information required for analyzing a Python function. It serves as a structured input for a function analysis process, ensuring all relevant data like the function's identifier, source code, import statements, and contextual information are provided in a standardized format.",
          "init_method": {
            "description": "Initializes the FunctionAnalysisInput model with the necessary components for function analysis. It takes the mode, identifier, source code, a list of import statements, and a FunctionContextInput object as arguments, validating them according to Pydantic's BaseModel rules.",
            "parameters": [
              {
                "name": "mode",
                "type": "Literal[\"function_analysis\"]",
                "description": "Specifies the analysis mode, which must be 'function_analysis' for this input type."
              },
              {
                "name": "identifier",
                "type": "str",
                "description": "The unique name or identifier of the function to be analyzed."
              },
              {
                "name": "source_code",
                "type": "str",
                "description": "The raw source code of the function."
              },
              {
                "name": "imports",
                "type": "List[str]",
                "description": "A list of import statements relevant to the source code file where the function is defined."
              },
              {
                "name": "context",
                "type": "FunctionContextInput",
                "description": "An object containing contextual information about the function, such as its dependencies and call relationships."
              }
            ]
          },
          "methods": [],
          "usage_context": {
            "dependencies": "This class does not specify any external dependencies beyond Pydantic's BaseModel and typing modules.",
            "instantiated_by": "The FunctionAnalysisInput class is instantiated within the 'main.py' file, specifically in the 'main_workflow' function on line 207."
          }
        },
        "error": null
      },
      "schemas.types.MethodContextInput": {
        "identifier": "schemas.types.MethodContextInput",
        "description": {
          "overall": "The MethodContextInput class is a Pydantic model designed to structure contextual information about a class's methods. It captures details such as the method's identifier, lists of other methods or functions it calls and is called by, its arguments, and an optional docstring. This class serves as a data container for method-specific metadata within a larger system, likely for documentation generation or code analysis.",
          "init_method": {
            "description": "Initializes a MethodContextInput object, which is a Pydantic model. It takes the method's identifier, a list of calls it makes, a list of call information for methods that call it, a list of its arguments, and an optional docstring as input. These attributes are validated and stored as instance attributes.",
            "parameters": [
              {
                "name": "identifier",
                "type": "str",
                "description": "The unique name or identifier of the method."
              },
              {
                "name": "calls",
                "type": "List[str]",
                "description": "A list of strings, where each string represents a method or function that this method calls."
              },
              {
                "name": "called_by",
                "type": "List[CallInfo]",
                "description": "A list of CallInfo objects, where each object details a caller of this method."
              },
              {
                "name": "args",
                "type": "List[str]",
                "description": "A list of strings representing the arguments accepted by the method."
              },
              {
                "name": "docstring",
                "type": "Optional[str]",
                "description": "An optional string containing the docstring of the method."
              }
            ]
          },
          "methods": [],
          "usage_context": {
            "dependencies": "This class depends on `pydantic.BaseModel` for its structure and validation, and `typing.List` and `typing.Optional` for type hinting. It also relies on a `CallInfo` type, which is not defined in the provided source code but is used for the `called_by` attribute.",
            "instantiated_by": "The MethodContextInput class is instantiated within the `main.py` file, specifically in the `main_workflow` function on line 227."
          }
        },
        "error": null
      },
      "schemas.types.ClassContextInput": {
        "identifier": "schemas.types.ClassContextInput",
        "description": {
          "overall": "The ClassContextInput model is a Pydantic BaseModel designed to structure the contextual information required for analyzing a Python class. It encapsulates details about the class's dependencies, where it is instantiated, and the context of its individual methods. This model serves as a data container for facilitating comprehensive class analysis within a larger system.",
          "init_method": {
            "description": "Initializes the ClassContextInput model with lists of dependencies, instantiation information, and method contexts. This constructor sets up the data structure for holding all relevant information needed for a thorough class analysis.",
            "parameters": [
              {
                "name": "dependencies",
                "type": "List[str]",
                "description": "A list of strings representing external dependencies of the class being analyzed."
              },
              {
                "name": "instantiated_by",
                "type": "List[CallInfo]",
                "description": "A list of CallInfo objects detailing where instances of the class are created."
              },
              {
                "name": "method_context",
                "type": "List[MethodContextInput]",
                "description": "A list of MethodContextInput objects, each providing context for a specific method within the class."
              }
            ]
          },
          "methods": [],
          "usage_context": {
            "dependencies": "This class does not explicitly list any external dependencies within its definition.",
            "instantiated_by": "Instances of this class are created in the `main_orchestrator` function in `HelperLLM.py` and the `main_workflow` function in `main.py`."
          }
        },
        "error": null
      },
      "schemas.types.ClassAnalysisInput": {
        "identifier": "schemas.types.ClassAnalysisInput",
        "description": {
          "overall": "The ClassAnalysisInput class is a Pydantic model designed to structure the input required for a class analysis process. It defines the necessary fields for mode, identifier, source code, imports, and a nested context object, ensuring that the input data adheres to a specific schema for analysis.",
          "init_method": {
            "description": "Initializes the ClassAnalysisInput model with all the required fields for class analysis. It sets up the structure for the analysis mode, the identifier of the class to be analyzed, its source code, a list of relevant imports, and a context object containing additional information like dependencies and instantiation points.",
            "parameters": [
              {
                "name": "mode",
                "type": "Literal[\"class_analysis\"]",
                "description": "Specifies the analysis mode, which must be 'class_analysis' for this input type."
              },
              {
                "name": "identifier",
                "type": "str",
                "description": "The name or identifier of the class that is intended for analysis."
              },
              {
                "name": "source_code",
                "type": "str",
                "description": "The raw source code of the class definition to be analyzed."
              },
              {
                "name": "imports",
                "type": "List[str]",
                "description": "A list of import statements relevant to the source code, which may include unused imports."
              },
              {
                "name": "context",
                "type": "ClassContextInput",
                "description": "A nested object containing contextual information for the analysis, such as dependencies and instantiation details."
              }
            ]
          },
          "methods": [],
          "usage_context": {
            "dependencies": "This class does not have any explicit external code dependencies listed in its context.",
            "instantiated_by": "The ClassAnalysisInput class is instantiated within the 'main_orchestrator' function in 'HelperLLM.py' and the 'main_workflow' function in 'main.py'."
          }
        },
        "error": null
      }
    }
  }
}