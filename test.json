{
    "files": {
        "backend/AST_Schema.py": {
            "ast_nodes": {
                "imports": [
                    "ast",
                    "networkx",
                    "os",
                    "callgraph.build_filtered_callgraph",
                    "getRepo.GitRepository"
                ],
                "functions": [
                    {
                        "mode": "function_analysis",
                        "identifier": "backend.AST_Schema.path_to_module",
                        "name": "path_to_module",
                        "args": [
                            "filepath",
                            "project_root"
                        ],
                        "docstring": "Wandelt einen Dateipfad in einen Python-Modulpfad um.",
                        "source_code": "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    \n    module_path = rel_path.replace(os.path.sep, '.')\n    \n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n        \n    return module_path",
                        "start_line": 9,
                        "end_line": 24,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "backend.AST_Schema.ASTVisitor.__init__"
                            ]
                        }
                    }
                ],
                "classes": [
                    {
                        "mode": "class_analysis",
                        "identifier": "backend.AST_Schema.ASTVisitor",
                        "name": "ASTVisitor",
                        "docstring": null,
                        "source_code": "class ASTVisitor(ast.NodeVisitor):\n    def __init__(self, source_code: str, file_path: str, project_root: str):\n        self.source_code = source_code\n        self.file_path = file_path\n        self.project_root = project_root\n        self.module_path = path_to_module(self.file_path, self.project_root)\n        self.schema = {\"imports\": [], \"functions\": [], \"classes\": []}\n        self._current_class = None\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(f\"{node.module}.{alias.name}\")\n        self.generic_visit(node)\n\n    def visit_ClassDef(self, node):\n        class_identifier = f\"{self.module_path}.{node.name}\"\n\n        class_info = {\n            \"mode\": \"class_analysis\",\n            \"identifier\": class_identifier,\n            \"name\": node.name,\n            \"docstring\": ast.get_docstring(node),\n            \"source_code\": ast.get_source_segment(self.source_code, node),\n            \"start_line\": node.lineno,\n            \"end_line\": node.end_lineno,\n            \"context\": {\n                \"dependencies\": [],\n                \"instantiated_by\": [],\n                \"method_context\": []\n            },   \n        }\n        self.schema[\"classes\"].append(class_info)\n        \n        self._current_class = class_info \n        self.generic_visit(node)\n        self._current_class = None\n\n    def visit_FunctionDef(self, node):\n        if self._current_class:\n            method_identifier = f\"{self._current_class['identifier']}.{node.name}\"\n            method_context_info = {\n                \"identifier\": method_identifier,\n                \"name\": node.name,\n                \"calls\": [],\n                \"called_by\": [],\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n            }\n            self._current_class[\"context\"][\"method_context\"].append(method_context_info)\n        else:\n            func_identifier = f\"{self.module_path}.{node.name}\"\n            func_info = {\n                \"mode\": \"function_analysis\",\n                \"identifier\": func_identifier,\n                \"name\": node.name,\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"source_code\": ast.get_source_segment(self.source_code, node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n                \"context\": {\n                    \"calls\": [],\n                    \"called_by\": []\n                }\n            }\n            self.schema[\"functions\"].append(func_info)\n            \n        self.generic_visit(node)\n    \n    def visit_AsyncFunctionDef(self, node):\n        self.visit_FunctionDef(node)",
                        "start_line": 26,
                        "end_line": 103,
                        "context": {
                            "dependencies": [
                                "backend.AST_Schema.path_to_module"
                            ],
                            "instantiated_by": [
                                "backend.AST_Schema.ASTAnalyzer.analyze_repository"
                            ],
                            "method_context": [
                                {
                                    "identifier": "backend.AST_Schema.ASTVisitor.__init__",
                                    "name": "__init__",
                                    "calls": [
                                        "backend.AST_Schema.path_to_module"
                                    ],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "source_code",
                                        "file_path",
                                        "project_root"
                                    ],
                                    "docstring": null,
                                    "start_line": 27,
                                    "end_line": 33
                                },
                                {
                                    "identifier": "backend.AST_Schema.ASTVisitor.visit_Import",
                                    "name": "visit_Import",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "node"
                                    ],
                                    "docstring": null,
                                    "start_line": 35,
                                    "end_line": 38
                                },
                                {
                                    "identifier": "backend.AST_Schema.ASTVisitor.visit_ImportFrom",
                                    "name": "visit_ImportFrom",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "node"
                                    ],
                                    "docstring": null,
                                    "start_line": 40,
                                    "end_line": 43
                                },
                                {
                                    "identifier": "backend.AST_Schema.ASTVisitor.visit_ClassDef",
                                    "name": "visit_ClassDef",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "node"
                                    ],
                                    "docstring": null,
                                    "start_line": 45,
                                    "end_line": 66
                                },
                                {
                                    "identifier": "backend.AST_Schema.ASTVisitor.visit_FunctionDef",
                                    "name": "visit_FunctionDef",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "node"
                                    ],
                                    "docstring": null,
                                    "start_line": 68,
                                    "end_line": 100
                                },
                                {
                                    "identifier": "backend.AST_Schema.ASTVisitor.visit_AsyncFunctionDef",
                                    "name": "visit_AsyncFunctionDef",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "node"
                                    ],
                                    "docstring": null,
                                    "start_line": 102,
                                    "end_line": 103
                                }
                            ]
                        }
                    },
                    {
                        "mode": "class_analysis",
                        "identifier": "backend.AST_Schema.ASTAnalyzer",
                        "name": "ASTAnalyzer",
                        "docstring": null,
                        "source_code": "class ASTAnalyzer:\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def _enrich_schema_with_callgraph(schema: dict, call_graph: nx.DiGraph, filename: str):\n        for func in schema[\"functions\"]:\n            func_name_key = f\"{filename}::{func['name']}\"\n            if func_name_key in call_graph:\n                func['context']['calls'] = sorted(list(call_graph.successors(func_name_key)))\n                func['context']['called_by'] = sorted(list(call_graph.predecessors(func_name_key)))\n\n        for cls in schema[\"classes\"]:\n            for method_context in cls[\"context\"][\"method_context\"]:\n                func_name_key = f\"{filename}::{cls['name']}::{method_context['name']}\"\n                if func_name_key in call_graph:\n                    calls = sorted(list(call_graph.successors(func_name_key)))\n                    called_by = sorted(list(call_graph.predecessors(func_name_key)))\n                    \n                    method_context['calls'] = calls\n                    method_context['called_by'] = called_by\n\n    def merge_relationship_data(self, full_schema: dict, relationship_data: list) -> dict:\n        \n        rel_lookup = {item['identifier']: item.get('called_by', []) for item in relationship_data}\n\n        for file_path, file_data in full_schema.get(\"files\", {}).items():\n            ast_nodes = file_data.get(\"ast_nodes\", {})\n            \n            for func in ast_nodes.get(\"functions\", []):\n                func_id = func.get(\"identifier\")\n                if func_id and func_id in rel_lookup:\n                    func[\"context\"][\"called_by\"] = rel_lookup[func_id]\n\n            for cls in ast_nodes.get(\"classes\", []):\n                cls_id = cls.get(\"identifier\")\n                \n                if cls_id and cls_id in rel_lookup:\n                    cls[\"context\"][\"instantiated_by\"] = rel_lookup[cls_id]\n\n                for method in cls[\"context\"].get(\"method_context\", []):\n                    method_id = method.get(\"identifier\")\n                    if method_id and method_id in rel_lookup:\n                        method[\"called_by\"] = rel_lookup[method_id]\n        \n        return full_schema\n\n    def analyze_repository(self, files: list, repo: GitRepository) -> dict:\n        full_schema = {\n            \"files\": {}\n        }\n\n        all_paths = [file_obj.path for file_obj in files]\n        if not all_paths:\n            return full_schema\n        \n        project_root = os.path.commonpath(all_paths)\n        if os.path.isfile(project_root):\n            project_root = os.path.dirname(project_root)\n\n        try:\n            global_filtered_callgraph = build_filtered_callgraph(repo)\n        except ValueError as e:\n            print(\"Callgraph konnte nicht erstellt werden!\")\n\n        for file_obj in files:\n            if not file_obj.path.endswith('.py'):\n                continue\n            \n            file_content = file_obj.content\n            if not file_content.strip():\n                continue\n\n            try:\n                tree = ast.parse(file_content)\n                \n                visitor = ASTVisitor(\n                    source_code=file_content, \n                    file_path=file_obj.path, \n                    project_root=project_root\n                )\n                visitor.visit(tree)\n                file_schema_nodes = visitor.schema\n\n                self._enrich_schema_with_callgraph(\n                    file_schema_nodes, \n                    global_filtered_callgraph, \n                    file_obj.path\n                )\n\n                if file_schema_nodes[\"imports\"] or file_schema_nodes[\"functions\"] or file_schema_nodes[\"classes\"]:\n                    full_schema[\"files\"][file_obj.path] = {\n                        \"ast_nodes\": file_schema_nodes\n                    }\n\n            except (SyntaxError, ValueError) as e:\n                print(f\"Warnung: Konnte Datei '{file_obj.path}' nicht parsen. Fehler: {e}\")\n\n        return full_schema",
                        "start_line": 105,
                        "end_line": 204,
                        "context": {
                            "dependencies": [
                                "backend.AST_Schema.ASTVisitor"
                            ],
                            "instantiated_by": [
                                "backend.main.main_workflow"
                            ],
                            "method_context": [
                                {
                                    "identifier": "backend.AST_Schema.ASTAnalyzer.__init__",
                                    "name": "__init__",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self"
                                    ],
                                    "docstring": null,
                                    "start_line": 107,
                                    "end_line": 108
                                },
                                {
                                    "identifier": "backend.AST_Schema.ASTAnalyzer._enrich_schema_with_callgraph",
                                    "name": "_enrich_schema_with_callgraph",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "schema",
                                        "call_graph",
                                        "filename"
                                    ],
                                    "docstring": null,
                                    "start_line": 111,
                                    "end_line": 126
                                },
                                {
                                    "identifier": "backend.AST_Schema.ASTAnalyzer.merge_relationship_data",
                                    "name": "merge_relationship_data",
                                    "calls": [],
                                    "called_by": [
                                        "backend.main.main_workflow"
                                    ],
                                    "args": [
                                        "self",
                                        "full_schema",
                                        "relationship_data"
                                    ],
                                    "docstring": null,
                                    "start_line": 128,
                                    "end_line": 151
                                },
                                {
                                    "identifier": "backend.AST_Schema.ASTAnalyzer.analyze_repository",
                                    "name": "analyze_repository",
                                    "calls": [
                                        "backend.AST_Schema.ASTVisitor"
                                    ],
                                    "called_by": [
                                        "backend.main.main_workflow"
                                    ],
                                    "args": [
                                        "self",
                                        "files",
                                        "repo"
                                    ],
                                    "docstring": null,
                                    "start_line": 153,
                                    "end_line": 204
                                }
                            ]
                        }
                    }
                ]
            }
        },
        "backend/File_Dependency.py": {
            "ast_nodes": {
                "imports": [
                    "networkx",
                    "os",
                    "ast.Assign",
                    "ast.AST",
                    "ast.ClassDef",
                    "ast.FunctionDef",
                    "ast.Import",
                    "ast.ImportFrom",
                    "ast.Name",
                    "ast.NodeVisitor",
                    "ast.literal_eval",
                    "ast.parse",
                    "ast.walk",
                    "keyword.iskeyword",
                    "pathlib.Path",
                    "getRepo.GitRepository",
                    "callgraph.make_safe_dot"
                ],
                "functions": [
                    {
                        "mode": "function_analysis",
                        "identifier": "backend.File_Dependency.build_file_dependency_graph",
                        "name": "build_file_dependency_graph",
                        "args": [
                            "filename",
                            "tree",
                            "repo_root"
                        ],
                        "docstring": null,
                        "source_code": "def build_file_dependency_graph(filename: str, tree: AST, repo_root: str) -> nx.DiGraph:\n    graph = nx.DiGraph()\n\n    tree_visitor = FileDependencyGraph(filename, repo_root)\n    tree_visitor.visit(tree)\n\n    for caller, callees in tree_visitor.import_dependencies.items():\n        graph.add_node(caller)\n        graph.add_nodes_from(callees)\n        for callee in callees:\n            graph.add_edge(caller, callee)\n    \n    return graph",
                        "start_line": 153,
                        "end_line": 165,
                        "context": {
                            "calls": [
                                "backend.File_Dependency.FileDependencyGraph"
                            ],
                            "called_by": [
                                "backend.File_Dependency.build_repository_graph"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "backend.File_Dependency.build_repository_graph",
                        "name": "build_repository_graph",
                        "args": [
                            "repository"
                        ],
                        "docstring": null,
                        "source_code": "def build_repository_graph(repository: GitRepository) -> nx.DiGraph:\n    all_files = repository.get_all_files()\n    repo_root = repository.temp_dir\n    global_graph = nx.DiGraph()\n\n    for file in all_files: \n        if not file.path.endswith(\".py\"):\n            continue\n        filename = str(os.path.basename(file.path)).removesuffix(\".py\")\n        tree = parse(file.content)\n        graph = build_file_dependency_graph(filename, tree, repo_root)\n        \n        for node in graph.nodes:\n            global_graph.add_node(node)\n\n        for caller, callee in graph.edges:\n            if callee:\n                global_graph.add_node(callee)\n                global_graph.add_edge(caller, callee)\n    \n    return global_graph",
                        "start_line": 167,
                        "end_line": 187,
                        "context": {
                            "calls": [
                                "backend.File_Dependency.build_file_dependency_graph"
                            ],
                            "called_by": [
                                "backend.File_Dependency"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "backend.File_Dependency.get_all_temp_files",
                        "name": "get_all_temp_files",
                        "args": [
                            "directory"
                        ],
                        "docstring": null,
                        "source_code": "def get_all_temp_files(directory: str) -> list[Path]:\n    root_path = Path(directory).resolve()\n    all_files = [file.relative_to(root_path) for file in root_path.rglob(\"*.py\")]\n\n    return all_files",
                        "start_line": 189,
                        "end_line": 193,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "backend.File_Dependency.FileDependencyGraph._resolve_module_name"
                            ]
                        }
                    }
                ],
                "classes": [
                    {
                        "mode": "class_analysis",
                        "identifier": "backend.File_Dependency.FileDependencyGraph",
                        "name": "FileDependencyGraph",
                        "docstring": null,
                        "source_code": "class FileDependencyGraph(NodeVisitor):\n\n    import_dependencies: dict[str, set[str]] = {}\n    def __init__(self, filename: str, repo_root):\n        \"\"\"\n        Initialisiert den File Dependency Graphen\n\n        Args:\n\n        \"\"\"\n        self.filename = filename\n        self.repo_root = repo_root\n\n    def _resolve_module_name(self, node: ImportFrom) -> list[str]:\n        \"\"\"\n        Löst relative Imports der Form `from .. import name1, name2` auf.\n        Liefert die Liste der tatsächlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\n        Wirft ImportError, wenn nichts aufgelöst werden konnte.\n        \"\"\"\n        level_depth = node.level\n        names = [alias.name for alias in node.names]  \n        all_files = get_all_temp_files(self.repo_root)  \n\n        candidates = [p for p in all_files if p.stem == Path(self.filename).stem or p.name == f\"{self.filename}.py\"]\n\n        if not candidates:\n            raise ImportError(f\"Kann aktuelle Datei '{self.filename}' im Repo nicht finden.\")\n\n\n        candidates.sort(key=lambda p: len(p.parts))\n        current_rel_path = candidates[0]\n        base_dir = current_rel_path.parent\n        if level_depth <= 0:\n            raise ImportError(\"Erwarteter relativer Import (level >= 1).\")\n        for _ in range(level_depth - 1):\n            if base_dir == base_dir.parent and len(base_dir.parts) == 0:\n                raise ImportError(f\"Relative Import-Ebene ({level_depth}) zu groß für Datei '{current_rel_path}'.\")\n            base_dir = base_dir.parent\n\n        repo_root_path = Path(self.repo_root).resolve()\n        resolved: list[str] = []\n\n        def module_file_exists(rel_base: Path, name: str) -> bool:\n            file_path = repo_root_path / rel_base / f\"{name}.py\"\n            pkg_init = repo_root_path / rel_base / name / \"__init__.py\"\n            return file_path.exists() or pkg_init.exists()\n\n        def init_exports_symbol(rel_base: Path, symbol: str) -> bool:\n            \"\"\"\n            Prüft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\n            oder als Name (funktion/klasse/assign) definiert ist.\n            \"\"\"\n            init_path = repo_root_path / rel_base / \"__init__.py\"\n            if not init_path.exists():\n                return False\n            try:\n                src = init_path.read_text(encoding=\"utf-8\")\n                mod = parse(src, filename=str(init_path))\n            except Exception:\n                return False\n\n            for node_ in walk(mod):\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == \"__all__\":\n                            try:\n                                value = literal_eval(node_.value)\n                                if isinstance(value, (list, tuple)) and symbol in value:\n                                    return True\n                            except Exception:\n                                pass\n                if isinstance(node_, (FunctionDef, ClassDef)) and node_.name == symbol:\n                    return True\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == symbol:\n                            return True\n            return False\n\n        for name in names:\n            if not name.isidentifier() or iskeyword(name):\n                continue\n\n            if module_file_exists(base_dir, name):\n                resolved.append(name)\n                continue\n            if init_exports_symbol(base_dir, name):\n                resolved.append(name)\n                continue\n\n        resolved = sorted(set(resolved))\n\n        if not resolved:\n            raise ImportError(\n                f\"Kein passendes Modul/Symbol für relative Import-Auflösung gefunden \"\n                f\"(level={level_depth}, names={names}, base_dir={base_dir})\"\n            )\n\n        return resolved\n    \n    def visit_Import(self, node: Import | ImportFrom, base_name: str | None = None):\n        for alias in node.names:\n        \n            if self.filename not in self.import_dependencies:\n                self.import_dependencies[self.filename] = set()\n\n            if base_name:\n                self.import_dependencies[self.filename].add(base_name)\n            else:\n                self.import_dependencies[self.filename].add(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node: ImportFrom):\n        \"\"\"\n        Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\n        also c, und dieser wird als callee für den caller, das File, gesetzt.\n        \"\"\"\n        module_name = node.module\n        if module_name:\n            module_base = module_name.split(\".\")[-1]\n            self.visit_Import(node, module_base)\n        else:\n            try:\n                resolved = self._resolve_module_name(node)\n                for base in resolved:\n                    self.visit_Import(node, base)\n            except ImportError as e:\n                print(f\"Auflösung eines relativen Imports fehlgeschlagen: {e}\")\n\n        self.generic_visit(node)",
                        "start_line": 22,
                        "end_line": 151,
                        "context": {
                            "dependencies": [
                                "backend.File_Dependency.get_all_temp_files",
                                "backend.File_Dependency.init_exports_symbol",
                                "backend.File_Dependency.module_file_exists"
                            ],
                            "instantiated_by": [
                                "backend.File_Dependency.build_file_dependency_graph"
                            ],
                            "method_context": [
                                {
                                    "identifier": "backend.File_Dependency.FileDependencyGraph.__init__",
                                    "name": "__init__",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "filename",
                                        "repo_root"
                                    ],
                                    "docstring": "Initialisiert den File Dependency Graphen\n\nArgs:",
                                    "start_line": 25,
                                    "end_line": 33
                                },
                                {
                                    "identifier": "backend.File_Dependency.FileDependencyGraph._resolve_module_name",
                                    "name": "_resolve_module_name",
                                    "calls": [
                                        "backend.File_Dependency.get_all_temp_files",
                                        "backend.File_Dependency.init_exports_symbol",
                                        "backend.File_Dependency.module_file_exists"
                                    ],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "node"
                                    ],
                                    "docstring": "Löst relative Imports der Form `from .. import name1, name2` auf.\nLiefert die Liste der tatsächlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\nWirft ImportError, wenn nichts aufgelöst werden konnte.",
                                    "start_line": 35,
                                    "end_line": 120
                                },
                                {
                                    "identifier": "backend.File_Dependency.FileDependencyGraph.module_file_exists",
                                    "name": "module_file_exists",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "rel_base",
                                        "name"
                                    ],
                                    "docstring": null,
                                    "start_line": 64,
                                    "end_line": 67
                                },
                                {
                                    "identifier": "backend.File_Dependency.FileDependencyGraph.init_exports_symbol",
                                    "name": "init_exports_symbol",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "rel_base",
                                        "symbol"
                                    ],
                                    "docstring": "Prüft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\noder als Name (funktion/klasse/assign) definiert ist.",
                                    "start_line": 69,
                                    "end_line": 99
                                },
                                {
                                    "identifier": "backend.File_Dependency.FileDependencyGraph.visit_Import",
                                    "name": "visit_Import",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "node",
                                        "base_name"
                                    ],
                                    "docstring": null,
                                    "start_line": 122,
                                    "end_line": 132
                                },
                                {
                                    "identifier": "backend.File_Dependency.FileDependencyGraph.visit_ImportFrom",
                                    "name": "visit_ImportFrom",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "node"
                                    ],
                                    "docstring": "Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\nalso c, und dieser wird als callee für den caller, das File, gesetzt.",
                                    "start_line": 134,
                                    "end_line": 151
                                }
                            ]
                        }
                    }
                ]
            }
        },
        "backend/HelperLLM.py": {
            "ast_nodes": {
                "imports": [
                    "os",
                    "json",
                    "logging",
                    "time",
                    "typing.List",
                    "typing.Dict",
                    "typing.Any",
                    "typing.Optional",
                    "typing.Union",
                    "dotenv.load_dotenv",
                    "langchain_google_genai.ChatGoogleGenerativeAI",
                    "langchain_ollama.ChatOllama",
                    "langchain_openai.ChatOpenAI",
                    "langchain.messages.HumanMessage",
                    "langchain.messages.SystemMessage",
                    "langchain.messages.AIMessage",
                    "pydantic.ValidationError",
                    "schemas.types.FunctionAnalysis",
                    "schemas.types.ClassAnalysis",
                    "schemas.types.FunctionAnalysisInput",
                    "schemas.types.FunctionContextInput",
                    "schemas.types.ClassAnalysisInput",
                    "schemas.types.ClassContextInput"
                ],
                "functions": [
                    {
                        "mode": "function_analysis",
                        "identifier": "backend.HelperLLM.main_orchestrator",
                        "name": "main_orchestrator",
                        "args": [],
                        "docstring": "Dummy Data and processing loop for testing the LLMHelper class.\nThis version is syntactically correct and logically matches the Pydantic models.",
                        "source_code": "def main_orchestrator():\n    \"\"\"\n    Dummy Data and processing loop for testing the LLMHelper class.\n    This version is syntactically correct and logically matches the Pydantic models.\n    \"\"\"\n    \n    # --- Step 1: Define the pre-computed analysis for each method ---\n    \n    # Example Input 1: For the 'add_item' function\n    add_item_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"source_code\": \"\"\"def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\\\"\\\"\\\"\n        if not isinstance(quantity, int) or quantity <= 0:\n            raise ValueError(\"Quantity must be a positive integer.\")\n    \n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n        print(f\"Added {quantity} of {item_name}. New total: {self.inventory[item_name]}\")\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"process_shipment\", \"restock_api_endpoint\"]\n        }\n    })\n\n    # Example Input 2: For the 'check_stock' function\n    check_stock_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"source_code\": \"\"\"def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Retrieves the current stock quantity for a given item.\\\"\\\"\\\"\n        stock_level = self.inventory.get(item_name, 0)\n        return stock_level\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"fulfill_order\", \"ui_display_handler\"]\n        }\n    })\n\n    # Example Input 3: For the 'generate_report' function\n    generate_report_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"source_code\": \"\"\"def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a timestamped string summary of the current inventory state.\\\"\\\"\\\"\n        timestamp = datetime.now()\n        report_header = \"Inventory Report as of: {ts}\\\\n\".format(ts=timestamp)\n        report_lines = [report_header]\n        \n        for item, quantity in self.inventory.items():\n            line = \"- {name}: {qty}\".format(name=item, qty=quantity)\n            report_lines.append(line)\n            \n        return \"\\\\n\".join(report_lines)\"\"\",\n        \"imports\": [\"from datetime import datetime\"],\n        \"context\": {\n            \"calls\": [\"datetime.now\", \"str.format\"],\n            \"called_by\": [\"daily_cron_job\", \"admin_dashboard_export\"]\n        }\n    })\n\n    add_item_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"description\": {\n            \"overall\": \"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to add.\"},\n                {\"name\": \"quantity\", \"type\": \"int\", \"description\": \"The number of units to add. Must be a positive integer.\"}\n            ],\n            \"returns\": [],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"process_shipment and restock_api_endpoint\" }\n        },\n        \"error\": None\n    })\n\n    check_stock_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"description\": {\n            \"overall\": \"Retrieves the current stock quantity for a given item.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to check.\"}\n            ],\n            \"returns\": [\n                {\"name\": \"stock_level\", \"type\": \"int\", \"description\": \"The quantity of the item in stock. Returns 0 if the item is not found.\"}\n            ],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"fulfill_order and ui_display_handler\" }\n        },\n        \"error\": None\n    })\n\n    generate_report_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"description\": {\n            \"overall\": \"Generates a timestamped string summary of the current inventory state.\",\n            \"parameters\": [ {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"} ],\n            \"returns\": [\n                {\"name\": \"report\", \"type\": \"str\", \"description\": \"A formatted string detailing all items and their quantities.\"}\n            ],\n            \"usage_context\": { \"calls\": \"datetime.now and str.format\", \"called_by\": \"daily_cron_job and admin_dashboard_export\" }\n        },\n        \"error\": None\n    })\n\n    class_input = ClassAnalysisInput(\n        mode=\"class_analysis\",\n        identifier=\"InventoryManager\",\n        # NOTE: The indentation of this string is now fixed. It starts at column 0.\n        source_code=\"\"\"class InventoryManager:\n    \\\"\\\"\\\"Manages stock levels for products in a warehouse.\\\"\\\"\\\"\n    def __init__(self, warehouse_id: str):\n        self.warehouse_id = warehouse_id\n        self.inventory = {}  # item_name: quantity\n\n    def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds an item to the inventory.\\\"\\\"\\\"\n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n\n    def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Checks the stock of a specific item.\\\"\\\"\\\"\n        return self.inventory.get(item_name, 0)\n\n    def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a summary report of the inventory.\\\"\\\"\\\"\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        report_lines = [f\"Inventory Report for {self.warehouse_id} at {timestamp}\"]\n        if not self.inventory:\n            report_lines.append(\"Inventory is empty.\")\n        else:\n            for item, quantity in self.inventory.items():\n                report_lines.append(f\"- {item}: {quantity}\")\n        return \"\\\\n\".join(report_lines)\n\"\"\",\n        imports=[\"from datetime import datetime\"],\n        context=ClassContextInput(\n            dependencies=[\"datetime\"],\n            instantiated_by=[\"main_app_startup\", \"warehouse_worker_script\"],\n            methods_analysis=[\n                add_item_analysis,\n                check_stock_analysis,\n                generate_report_analysis\n            ]\n        )\n    )\n\n    \n    # The helper methods expect a LIST of inputs, so we wrap our single object in a list.\n    input = [add_item_input, check_stock_input, generate_report_input]\n    analysis = [add_item_analysis, check_stock_analysis, generate_report_analysis] \n\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    llm_helper = LLMHelper(api_key=GEMINI_API_KEY, function_prompt_path=function_prompt_file, class_prompt_path=class_prompt_file)\n\n    # This will be the final JSON containing all documentation\n    final_documentation = {}\n\n    # The analysis of the methods is already provided in the context.\n    logging.info(\"\\n--- Generating documentation for classes ---\")\n    \n    # The `generate_for_classes` method returns a list of results.\n    analysis_results = llm_helper.generate_for_functions(input)\n\n    # --- Step 4: Process the results ---\n    \n    # We loop through the list of results (even though there's only one in this case).\n    for doc in analysis_results:\n        if doc:\n            logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n            if \"classes\" not in final_documentation:\n                final_documentation[\"classes\"] = {}\n            # Use .model_dump() to convert the Pydantic object back to a dict for JSON serialization\n            final_documentation[\"classes\"][doc.identifier] = doc.model_dump() \n        else:\n            logging.warning(f\"Failed to generate doc for a class\")\n\n    # --- Step 5: Display the final aggregated result ---\n    logging.info(\"\\n--- Final Generated Documentation ---\")\n    print(json.dumps(final_documentation, indent=2))",
                        "start_line": 227,
                        "end_line": 413,
                        "context": {
                            "calls": [
                                "backend.HelperLLM.LLMHelper",
                                "schemas.types.ClassAnalysisInput",
                                "schemas.types.ClassContextInput"
                            ],
                            "called_by": [
                                "backend.HelperLLM"
                            ]
                        }
                    }
                ],
                "classes": [
                    {
                        "mode": "class_analysis",
                        "identifier": "backend.HelperLLM.LLMHelper",
                        "name": "LLMHelper",
                        "docstring": "A class to interact with Google Gemini for generating code snippet documentation.\nIt centralizes API interaction, error handling, and validates I/O using Pydantic.",
                        "source_code": "class LLMHelper:\n    \"\"\"\n    A class to interact with Google Gemini for generating code snippet documentation.\n    It centralizes API interaction, error handling, and validates I/O using Pydantic.\n    \"\"\"\n    def __init__(self, api_key: str, function_prompt_path: str, class_prompt_path: str, model_name: str = \"gemini-2.0-flash-lite\", base_url: str = None):\n        if not api_key:\n            raise ValueError(\"Gemini API Key must be set.\")\n        \n        try:\n            with open(function_prompt_path, 'r', encoding='utf-8') as f:\n                self.function_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Function system prompt file not found at: {function_prompt_path}\")\n            raise\n\n        # Handle the second file\n        try:\n            with open(class_prompt_path, 'r', encoding='utf-8') as f:\n                self.class_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Class system prompt file not found at: {class_prompt_path}\")\n            raise\n        \n        # Batch-Size config\n        self.model_name = model_name\n        self._configure_batch_settings(model_name)\n\n\n        if model_name.startswith(\"gemini-\"):\n            base_llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=0.3, \n            )\n        \n        elif model_name.startswith(\"gpt-\") and \"openGPT\" not in model_name:\n            base_llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=0.3,\n            )\n\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            if not SCADSLLM_URL:\n                raise ValueError(f\"SCADSLLM_URL environment variable is required for model {model_name}\")\n            \n            logging.info(f\"Connecting to Custom API at {SCADSLLM_URL} for model {model_name}\")\n            \n            base_llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                base_url=SCADSLLM_URL,\n                temperature=0.3,\n            )\n\n        else:\n            target_url = base_url if base_url else OLLAMA_BASE_URL\n            logging.info(f\"Using Ollama at {target_url} for model {model_name}\")\n            base_llm = ChatOllama(\n                model=model_name,\n                temperature=0.3,\n                base_url=target_url,\n            )\n\n        self.function_llm = base_llm.with_structured_output(FunctionAnalysis, method=\"json_schema\")\n        self.class_llm = base_llm.with_structured_output(ClassAnalysis, method=\"json_schema\")\n\n        self.raw_llm = base_llm\n\n        logging.info(f\"LLMHelper initialized with model '{model_name}'. Batch Size: {self.batch_size}\")\n\n    def _configure_batch_settings(self, model_name: str):\n\n        if model_name == \"gemini-2.0-flash-lite\":\n            self.batch_size = 30\n        \n        elif model_name == \"gemini-flash-latest\":\n            self.batch_size = 10\n\n        elif model_name == \"gemini-2.5-flash\":\n            self.batch_size = 10\n            \n        elif model_name == \"gemini-2.5-pro\":\n            self.batch_size = 2\n        \n        elif model_name == \"gemini-2.0-flash\":\n            self.batch_size = 15\n\n        elif model_name == \"gemini-2.5-flash-lite\":\n            self.batch_size = 15\n\n        elif model_name == \"llama3\":\n            self.batch_size = 50\n\n        elif model_name == \"gpt-5.1\":\n            self.batch_size = 500\n\n        elif model_name == \"gpt-5-mini\":\n            self.batch_size = 500\n\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            self.batch_size = 500\n            \n        else:\n            logging.warning(f\"Unknown model '{model_name}', using conservative defaults.\")\n            self.batch_size = 2\n\n    def generate_for_functions(self, function_inputs: List[FunctionAnalysisInput]) -> List[Optional[FunctionAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of functions.\"\"\"\n\n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 62\n\n        if not function_inputs:\n            return []\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(function_input.model_dump(), indent=2) \n            for function_input in function_inputs\n        ]\n       \n        conversations = [[SystemMessage(content=self.function_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_functions = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n        \n            try:\n                batch_results = self.function_llm.batch(batch_conversations, config={\"max_concurrency\": self.batch_size})\n                all_validated_functions.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                all_validated_functions.extend([None] * len(batch_conversations))\n            \n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_functions\n        \n            \n    \n\n    def generate_for_classes(self, class_inputs: List[ClassAnalysisInput]) -> List[Optional[ClassAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of classes.\"\"\"\n        if not class_inputs:\n            return []\n        \n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 62\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(class_input.model_dump(), indent=2)\n            for class_input in class_inputs\n        ]\n\n        conversations = [[SystemMessage(content=self.class_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_classes = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n\n            try:\n                batch_results = self.class_llm.batch(batch_conversations , config={\"max_concurrency\": self.batch_size})\n                all_validated_classes.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                # Falls ein Fehler auftritt, füllen wir die Liste mit None auf, um die Reihenfolge zu wahren\n                all_validated_classes.extend([None] * len(batch_conversations))\n\n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_classes",
                        "start_line": 31,
                        "end_line": 221,
                        "context": {
                            "dependencies": [],
                            "instantiated_by": [
                                "backend.HelperLLM.main_orchestrator",
                                "backend.main.main_workflow"
                            ],
                            "method_context": [
                                {
                                    "identifier": "backend.HelperLLM.LLMHelper.__init__",
                                    "name": "__init__",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "api_key",
                                        "function_prompt_path",
                                        "class_prompt_path",
                                        "model_name",
                                        "base_url"
                                    ],
                                    "docstring": null,
                                    "start_line": 36,
                                    "end_line": 101
                                },
                                {
                                    "identifier": "backend.HelperLLM.LLMHelper._configure_batch_settings",
                                    "name": "_configure_batch_settings",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "model_name"
                                    ],
                                    "docstring": null,
                                    "start_line": 103,
                                    "end_line": 137
                                },
                                {
                                    "identifier": "backend.HelperLLM.LLMHelper.generate_for_functions",
                                    "name": "generate_for_functions",
                                    "calls": [],
                                    "called_by": [
                                        "backend.main.main_workflow"
                                    ],
                                    "args": [
                                        "self",
                                        "function_inputs"
                                    ],
                                    "docstring": "Generates and validates documentation for a batch of functions.",
                                    "start_line": 139,
                                    "end_line": 178
                                },
                                {
                                    "identifier": "backend.HelperLLM.LLMHelper.generate_for_classes",
                                    "name": "generate_for_classes",
                                    "calls": [],
                                    "called_by": [
                                        "backend.main.main_workflow"
                                    ],
                                    "args": [
                                        "self",
                                        "class_inputs"
                                    ],
                                    "docstring": "Generates and validates documentation for a batch of classes.",
                                    "start_line": 183,
                                    "end_line": 221
                                }
                            ]
                        }
                    }
                ]
            }
        },
        "backend/MainLLM.py": {
            "ast_nodes": {
                "imports": [
                    "os",
                    "logging",
                    "sys",
                    "dotenv.load_dotenv",
                    "langchain_google_genai.ChatGoogleGenerativeAI",
                    "langchain_ollama.ChatOllama",
                    "langchain_openai.ChatOpenAI",
                    "langchain.messages.HumanMessage",
                    "langchain.messages.SystemMessage"
                ],
                "functions": [],
                "classes": [
                    {
                        "mode": "class_analysis",
                        "identifier": "backend.MainLLM.MainLLM",
                        "name": "MainLLM",
                        "docstring": "Hauptklasse für die Interaktion mit dem LLM.",
                        "source_code": "class MainLLM:\n    \"\"\"\n    Hauptklasse für die Interaktion mit dem LLM.\n    \"\"\"\n    def __init__(self, api_key: str, prompt_file_path: str, model_name: str = \"gemini-2.5-pro\", base_url: str = None):\n        if not api_key:\n            raise ValueError(\"Gemini API Key must be set.\")\n        \n        try:\n            with open(prompt_file_path, 'r', encoding='utf-8') as f:\n                self.system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"System prompt file not found at: {prompt_file_path}\")\n            raise\n        \n        self.model_name = model_name\n\n        if model_name.startswith(\"gemini-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n\n        elif model_name.startswith(\"gpt-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            if not SCADSLLM_URL:\n                raise ValueError(f\"SCADSLLM_URL environment variable is required for model {model_name}\")\n            \n            logging.info(f\"Connecting to Custom API at {SCADSLLM_URL} for model {model_name}\")\n            \n            self.llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                base_url=SCADSLLM_URL,\n                temperature=1.0,\n            )\n\n        else:\n            target_url = base_url if base_url else OLLAMA_BASE_URL\n            self.llm = ChatOllama(\n                model=model_name,\n                temperature=1.0,\n                base_url=target_url,\n            )\n\n        logging.info(f\"Main LLM initialized with model '{model_name}'.\")\n    \n    def call_llm(self, user_input: str):\n\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with HelperLLM input...\")\n\n        try:\n            response = self.llm.invoke(messages)\n            logging.info(\"LLM call successful.\")\n            return response.content\n        except Exception as e:\n            logging.error(f\"Error during LLM call: {e}\")\n            return None\n\n    def stream_llm(self, user_input: str):\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with 'stream'...\")\n\n        try:\n            stream_iterator = self.llm.stream(messages)\n            \n            for chunk in stream_iterator:\n                yield chunk.content\n        except Exception as e:\n            error_message = f\"\\n--- Error during LLM stream call: {e} ---\"\n            logging.error(error_message)\n            yield error_message",
                        "start_line": 22,
                        "end_line": 106,
                        "context": {
                            "dependencies": [],
                            "instantiated_by": [
                                "backend.main.main_workflow",
                                "backend.main.notebook_workflow"
                            ],
                            "method_context": [
                                {
                                    "identifier": "backend.MainLLM.MainLLM.__init__",
                                    "name": "__init__",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "api_key",
                                        "prompt_file_path",
                                        "model_name",
                                        "base_url"
                                    ],
                                    "docstring": null,
                                    "start_line": 26,
                                    "end_line": 73
                                },
                                {
                                    "identifier": "backend.MainLLM.MainLLM.call_llm",
                                    "name": "call_llm",
                                    "calls": [],
                                    "called_by": [
                                        "backend.main.main_workflow",
                                        "backend.main.notebook_workflow"
                                    ],
                                    "args": [
                                        "self",
                                        "user_input"
                                    ],
                                    "docstring": null,
                                    "start_line": 75,
                                    "end_line": 89
                                },
                                {
                                    "identifier": "backend.MainLLM.MainLLM.stream_llm",
                                    "name": "stream_llm",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "user_input"
                                    ],
                                    "docstring": null,
                                    "start_line": 91,
                                    "end_line": 106
                                }
                            ]
                        }
                    }
                ]
            }
        },
        "backend/basic_info.py": {
            "ast_nodes": {
                "imports": [
                    "re",
                    "os",
                    "tomllib",
                    "typing.List",
                    "typing.Dict",
                    "typing.Any",
                    "typing.Optional"
                ],
                "functions": [],
                "classes": [
                    {
                        "mode": "class_analysis",
                        "identifier": "backend.basic_info.ProjektInfoExtractor",
                        "name": "ProjektInfoExtractor",
                        "docstring": "Extrahiert grundlegende Projektinformationen aus gängigen Projektdateien\nwie README, pyproject.toml und requirements.txt.",
                        "source_code": "class ProjektInfoExtractor:\n    \"\"\"\n    Extrahiert grundlegende Projektinformationen aus gängigen Projektdateien\n    wie README, pyproject.toml und requirements.txt.\n    \"\"\"\n    def __init__(self):\n        self.INFO_NICHT_GEFUNDEN = \"Information not found\"\n        # Initialisiert die Struktur mit Platzhaltern\n        self.info = {\n            \"projekt_uebersicht\": {\n                \"titel\": self.INFO_NICHT_GEFUNDEN,\n                \"beschreibung\": self.INFO_NICHT_GEFUNDEN,\n                \"aktueller_status\": self.INFO_NICHT_GEFUNDEN,\n                \"key_features\": self.INFO_NICHT_GEFUNDEN,\n                \"tech_stack\": self.INFO_NICHT_GEFUNDEN,\n            },\n            \"installation\": {\n                \"dependencies\": self.INFO_NICHT_GEFUNDEN,\n                \"setup_anleitung\": self.INFO_NICHT_GEFUNDEN,\n                \"quick_start_guide\": self.INFO_NICHT_GEFUNDEN,\n            }\n        }\n\n    def _finde_datei(self, patterns: List[str], dateien: List[Any]) -> Optional[Any]:\n        \"\"\"Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht.\"\"\"\n        for datei in dateien:\n            for pattern in patterns:\n                if datei.path.lower().endswith(pattern.lower()):\n                    return datei\n        return None\n\n    def _extrahiere_sektion_aus_markdown(self, inhalt: str, keywords: List[str]) -> Optional[str]:\n        \"\"\"\n        Extrahiert den Text unter einer Markdown-Überschrift (##).\n        \n        Args:\n            inhalt (str): Der gesamte Markdown-Text.\n            keywords (list): Eine Liste von alternativen Schlüsselwörtern für den Titel \n                             der Sektion (z.B. [\"Installation\", \"Setup\"]).\n        \n        Returns:\n            str: Der extrahierte Textabschnitt oder None.\n        \"\"\"\n        # Erstellt ein Regex-Pattern, das auf jedes der Schlüsselwörter reagiert\n        keyword_pattern = \"|\".join(re.escape(k) for k in keywords)\n        \n        # Sucht nach \"## Schlüsselwort\" und erfasst alles bis zur nächsten \"##\" oder dem Dateiende\n        pattern = re.compile(\n            rf\"##\\s*({keyword_pattern})\\s*\\n(.*?)(?=\\n##|\\Z)\",\n            re.IGNORECASE | re.DOTALL\n        )\n        match = pattern.search(inhalt)\n        if match:\n            return match.group(2).strip()\n        return None\n\n    def _parse_readme(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer README-Datei.\"\"\"\n        if self.info[\"projekt_uebersicht\"][\"titel\"] == self.INFO_NICHT_GEFUNDEN:\n            title_match = re.search(r\"^\\s*#\\s*(.*)\", inhalt)\n            if title_match:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = title_match.group(1).strip()\n\n        # Beschreibung (Fallback)\n        # Nimmt den Text nach dem Titel bis zur nächsten Überschrift\n        if self.info[\"projekt_uebersicht\"][\"beschreibung\"] == self.INFO_NICHT_GEFUNDEN:\n            desc_match = re.search(r\"^\\s*#\\s*.*\\n+([^#\\n].*)\", inhalt, re.DOTALL)\n            if desc_match:\n                 self.info[\"projekt_uebersicht\"][\"beschreibung\"] = desc_match.group(1).strip().split('\\n\\n')[0]\n\n\n        # Key Features\n        features = self._extrahiere_sektion_aus_markdown(inhalt, [\"Features\", \"Key Features\", \"Merkmale\"])\n        if features:\n            self.info[\"projekt_uebersicht\"][\"key_features\"] = features\n            \n        # Tech Stack\n        tech_stack = self._extrahiere_sektion_aus_markdown(inhalt, [\"Tech Stack\", \"Technology\", \"Technologien\"])\n        if tech_stack:\n            self.info[\"projekt_uebersicht\"][\"tech_stack\"] = tech_stack\n            \n        # Status\n        status = self._extrahiere_sektion_aus_markdown(inhalt, [\"Status\", \"Current Status\"])\n        if status:\n            self.info[\"projekt_uebersicht\"][\"aktueller_status\"] = status\n\n        # Setup-Anleitung\n        setup = self._extrahiere_sektion_aus_markdown(inhalt, [\"Installation\", \"Setup\", \"Getting Started\"])\n        if setup:\n            self.info[\"installation\"][\"setup_anleitung\"] = setup\n            \n        # Quick Start\n        quick_start = self._extrahiere_sektion_aus_markdown(inhalt, [\"Quick Start\", \"Schnellstart\"])\n        if quick_start:\n            self.info[\"installation\"][\"quick_start_guide\"] = quick_start\n\n    def _parse_toml(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer pyproject.toml-Datei.\"\"\"\n        if not tomllib:\n            print(\"Warnung: 'tomli' ist nicht installiert. pyproject.toml kann nicht analysiert werden.\")\n            return\n            \n        try:\n            data = tomllib.loads(inhalt)\n            project_data = data.get(\"project\", {})\n            \n            if \"name\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = project_data[\"name\"]\n            if \"description\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"beschreibung\"] = project_data[\"description\"]\n            if \"dependencies\" in project_data:\n                # Überschreibt 'dependencies' immer, da toml als primäre Quelle gilt\n                self.info[\"installation\"][\"dependencies\"] = project_data[\"dependencies\"]\n        except tomllib.TOMLDecodeError as e:\n            print(f\"Warnung: Fehler beim Parsen der pyproject.toml: {e}\")\n\n    def _parse_requirements(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer requirements.txt-Datei.\"\"\"\n        # Nur füllen, wenn noch keine Dependencies aus toml gefunden wurden\n        if self.info[\"installation\"][\"dependencies\"] == self.INFO_NICHT_GEFUNDEN:\n            lines = inhalt.splitlines()\n            dependencies = [\n                line.strip() for line in lines \n                if line.strip() and not line.strip().startswith('#')\n            ]\n            if dependencies:\n                self.info[\"installation\"][\"dependencies\"] = dependencies\n\n    def extrahiere_info(self, dateien: List[Any], repo_url: str) -> Dict[str, Any]:\n        \"\"\"\n        Orchestriert die Extraktion von Informationen aus einer Liste von RepoFile-Objekten.\n        \n        Die Reihenfolge der Verarbeitung ist wichtig, um Prioritäten zu setzen:\n        1. pyproject.toml (höchste Priorität für Metadaten)\n        2. requirements.txt (Fallback für Dependencies)\n        3. README (für beschreibende Texte und als Fallback)\n        4. Titel wird am Ende basierend auf der URL überschrieben.\n        \"\"\"\n        # 1. Relevante Dateien finden\n        readme_datei = self._finde_datei([\"readme.md\", \"readme.rst\", \"readme.txt\", \"readme\"], dateien)\n        toml_datei = self._finde_datei([\"pyproject.toml\"], dateien)\n        req_datei = self._finde_datei([\"requirements.txt\"], dateien)\n\n        # 2. Dateien parsen (mit Priorisierung)\n        if toml_datei:\n            self._parse_toml(toml_datei.content)\n\n        if req_datei:\n            self._parse_requirements(req_datei.content)\n            \n        if readme_datei:\n            self._parse_readme(readme_datei.content)\n            \n        # 3. Finale Formatierung der Dependencies\n        deps = self.info[\"installation\"][\"dependencies\"]\n        if isinstance(deps, list):\n            if not deps:\n                self.info[\"installation\"][\"dependencies\"] = self.INFO_NICHT_GEFUNDEN\n            else:\n                self.info[\"installation\"][\"dependencies\"] = \"\\n\".join(f\"- {dep}\" for dep in deps)\n        \n        repo_name = os.path.basename(repo_url.removesuffix('.git'))\n        self.info[\"projekt_uebersicht\"][\"titel\"] = f\"{repo_name} documentation\"\n\n        return self.info",
                        "start_line": 8,
                        "end_line": 172,
                        "context": {
                            "dependencies": [],
                            "instantiated_by": [
                                "backend.main.main_workflow",
                                "backend.main.notebook_workflow"
                            ],
                            "method_context": [
                                {
                                    "identifier": "backend.basic_info.ProjektInfoExtractor.__init__",
                                    "name": "__init__",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self"
                                    ],
                                    "docstring": null,
                                    "start_line": 13,
                                    "end_line": 29
                                },
                                {
                                    "identifier": "backend.basic_info.ProjektInfoExtractor._finde_datei",
                                    "name": "_finde_datei",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "patterns",
                                        "dateien"
                                    ],
                                    "docstring": "Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht.",
                                    "start_line": 31,
                                    "end_line": 37
                                },
                                {
                                    "identifier": "backend.basic_info.ProjektInfoExtractor._extrahiere_sektion_aus_markdown",
                                    "name": "_extrahiere_sektion_aus_markdown",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "inhalt",
                                        "keywords"
                                    ],
                                    "docstring": "Extrahiert den Text unter einer Markdown-Überschrift (##).\n\nArgs:\n    inhalt (str): Der gesamte Markdown-Text.\n    keywords (list): Eine Liste von alternativen Schlüsselwörtern für den Titel \n                     der Sektion (z.B. [\"Installation\", \"Setup\"]).\n\nReturns:\n    str: Der extrahierte Textabschnitt oder None.",
                                    "start_line": 39,
                                    "end_line": 62
                                },
                                {
                                    "identifier": "backend.basic_info.ProjektInfoExtractor._parse_readme",
                                    "name": "_parse_readme",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "inhalt"
                                    ],
                                    "docstring": "Parst den Inhalt einer README-Datei.",
                                    "start_line": 64,
                                    "end_line": 102
                                },
                                {
                                    "identifier": "backend.basic_info.ProjektInfoExtractor._parse_toml",
                                    "name": "_parse_toml",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "inhalt"
                                    ],
                                    "docstring": "Parst den Inhalt einer pyproject.toml-Datei.",
                                    "start_line": 104,
                                    "end_line": 122
                                },
                                {
                                    "identifier": "backend.basic_info.ProjektInfoExtractor._parse_requirements",
                                    "name": "_parse_requirements",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "inhalt"
                                    ],
                                    "docstring": "Parst den Inhalt einer requirements.txt-Datei.",
                                    "start_line": 124,
                                    "end_line": 134
                                },
                                {
                                    "identifier": "backend.basic_info.ProjektInfoExtractor.extrahiere_info",
                                    "name": "extrahiere_info",
                                    "calls": [],
                                    "called_by": [
                                        "backend.main.main_workflow",
                                        "backend.main.notebook_workflow"
                                    ],
                                    "args": [
                                        "self",
                                        "dateien",
                                        "repo_url"
                                    ],
                                    "docstring": "Orchestriert die Extraktion von Informationen aus einer Liste von RepoFile-Objekten.\n\nDie Reihenfolge der Verarbeitung ist wichtig, um Prioritäten zu setzen:\n1. pyproject.toml (höchste Priorität für Metadaten)\n2. requirements.txt (Fallback für Dependencies)\n3. README (für beschreibende Texte und als Fallback)\n4. Titel wird am Ende basierend auf der URL überschrieben.",
                                    "start_line": 136,
                                    "end_line": 172
                                }
                            ]
                        }
                    }
                ]
            }
        },
        "backend/callgraph.py": {
            "ast_nodes": {
                "imports": [
                    "ast",
                    "networkx",
                    "os",
                    "pathlib.Path",
                    "typing.Dict",
                    "getRepo.GitRepository",
                    "getRepo.GitRepository",
                    "basic_info.ProjektInfoExtractor",
                    "os"
                ],
                "functions": [
                    {
                        "mode": "function_analysis",
                        "identifier": "backend.callgraph.make_safe_dot",
                        "name": "make_safe_dot",
                        "args": [
                            "graph",
                            "out_path"
                        ],
                        "docstring": null,
                        "source_code": "def make_safe_dot(graph: nx.DiGraph, out_path: str):\n    mapping = {}\n    H = graph.copy()\n    for i, n in enumerate(list(graph.nodes())):\n        safe = f\"n{i}\"           \n        mapping[n] = safe\n    H = nx.relabel_nodes(H, mapping)\n    for orig, safe in mapping.items():\n        H.nodes[safe][\"label\"] = orig\n    nx.drawing.nx_pydot.write_dot(H, out_path)",
                        "start_line": 173,
                        "end_line": 182,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "backend.callgraph"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "backend.callgraph.build_filtered_callgraph",
                        "name": "build_filtered_callgraph",
                        "args": [
                            "repo"
                        ],
                        "docstring": "Baut den globalen Call-Graphen und filtert ihn auf selbst geschriebene Funktionen.",
                        "source_code": "def build_filtered_callgraph(repo: GitRepository) -> nx.DiGraph:\n    \"\"\"\n    Baut den globalen Call-Graphen und filtert ihn auf selbst geschriebene Funktionen.\n    \"\"\"\n    all_file_objects = repo.get_all_files()\n    own_functions = set()\n    file_trees = {}\n\n    for file in all_file_objects:\n        if not file.path.endswith(\".py\"):\n            continue\n        filename = Path(file.path).stem\n        tree = ast.parse(file.content)\n        file_trees[filename] = tree\n        visitor = CallGraph(filename)\n        visitor.visit(tree)\n        own_functions.update(visitor.function_set)\n\n    # Globalen Call-Graph bauen\n    global_graph = nx.DiGraph()\n    for filename, tree in file_trees.items():\n        visitor = CallGraph(filename)\n        visitor.visit(tree)\n        for caller, callees in visitor.edges.items():\n            if caller not in own_functions:\n                continue \n            for callee in callees:\n                if callee in own_functions:\n                    global_graph.add_edge(caller, callee)\n                    global_graph.add_node(caller)\n                    global_graph.add_node(callee)\n    return global_graph",
                        "start_line": 185,
                        "end_line": 216,
                        "context": {
                            "calls": [
                                "backend.callgraph.CallGraph"
                            ],
                            "called_by": [
                                "backend.callgraph"
                            ]
                        }
                    }
                ],
                "classes": [
                    {
                        "mode": "class_analysis",
                        "identifier": "backend.callgraph.CallGraph",
                        "name": "CallGraph",
                        "docstring": null,
                        "source_code": "class CallGraph(ast.NodeVisitor):\n    def __init__(self, filename: str):\n        self.filename = filename\n        self.current_function = None\n        self.current_class = None\n        \n        self.local_defs: dict[str, str] = {}\n        self.graph = nx.DiGraph()\n        self.import_mapping: dict[str, str] = {}\n        self.function_set: set[str] = set()\n        self.edges: Dict[str, set[str]] = {}\n\n    def _recursive_call(self, node):\n        \"\"\"\n        Liefert eine Liste der Namenskomponenten als dotted string z.B. ['pkg', 'mod', 'Class', 'method']\n        \"\"\"\n        if isinstance(node, ast.Call):\n            return self._recursive_call(node.func)\n        if isinstance(node, ast.Name):\n            return [node.id]\n        if isinstance(node, ast.Attribute):\n            parts = self._recursive_call(node.value)\n            parts.append(node.attr)\n            return parts\n        return []\n\n    def _resolve_all_callee_names(self, callee_nodes: list[list[str]]) -> list[str]:\n        \"\"\"\n        callee_nodes ist jetzt eine Liste von Listen (Name-Steps).\n        Wir prüfen zuerst lokale Definitionen, dann import_mapping.\n        \"\"\"\n        resolved = []\n        for parts in callee_nodes:\n            if not parts:\n                continue\n            simple = parts[-1]\n            dotted = \".\".join(parts[-2:]) if len(parts) >= 2 else simple\n\n            if simple in self.local_defs:\n                resolved.append(self.local_defs[simple])\n                continue\n            if dotted in self.local_defs:\n                resolved.append(self.local_defs[dotted])\n                continue\n\n            first = parts[0]\n            if first in self.import_mapping:\n                mod = self.import_mapping[first]\n                rest = \"::\".join(parts[1:]) if len(parts) > 1 else simple\n                resolved.append(f\"{mod}::{rest}\")\n                continue\n\n            if self.current_class:\n                resolved.append(f\"{self.filename}::{parts[0]}::{simple}\" if len(parts)==1 else f\"{self.filename}::{'::'.join(parts)}\")\n            else:\n                resolved.append(f\"{self.filename}::{ '::'.join(parts)}\")\n        return resolved\n\n    def _make_full_name(self, basename: str, class_name: str | None = None) -> str:\n        if class_name:\n            return f\"{self.filename}::{class_name}::{basename}\"\n        return f\"{self.filename}::{basename}\"\n    \n    def _current_caller(self) -> str:\n        if self.current_function:\n            return self.current_function\n        return f\"<{self.filename}>\" if self.filename else \"<global-scope>\"\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            module_name = alias.name\n            module_asname = alias.asname if alias.asname else module_name\n            self.import_mapping[module_asname] = module_name\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        module_name = node.module.split(\".\")[-1] if node.module else \"\"\n        for alias in node.names:\n            self.import_mapping[alias.asname or alias.name] = f\"{module_name}\" if module_name else alias.name\n\n    def visit_ClassDef(self, node: ast.ClassDef):\n        prev_class = self.current_class\n        self.current_class = node.name\n        self.generic_visit(node)\n        self.current_class = prev_class\n        \n    def visit_FunctionDef(self, node):\n        prev_function = self.current_function\n        full_name = self._make_full_name(node.name, self.current_class)\n        self.local_defs[node.name] = full_name\n        if self.current_class:\n            self.local_defs[f\"{self.current_class}.{node.name}\"] = full_name\n\n        self.current_function = full_name\n        self.graph.add_node(self.current_function)\n        self.generic_visit(node)\n        self.function_set.add(self.current_function)\n        self.current_function = prev_function\n\n    def visit_AsyncFunctionDef(self, node):\n        self.visit_FunctionDef(node)\n\n    def visit_Call(self, node):\n        caller = self._current_caller()\n        parts = self._recursive_call(node)\n        resolved_callees = self._resolve_all_callee_names([parts])\n\n        if caller not in self.edges:\n            self.edges[caller] = set()\n\n        for callee in resolved_callees:\n            if callee:\n                self.edges[caller].add(callee)\n        self.generic_visit(node)\n\n    def visit_If(self, node):\n        if (isinstance(node.test, ast.Compare) and\n            isinstance(node.test.left, ast.Name) and \n            node.test.left.id == \"__name__\"):\n            caller_backup = self.current_function\n            self.current_function = \"<main_block>\"\n            self.generic_visit(node)\n            self.current_function = caller_backup\n        else:\n            self.generic_visit(node)",
                        "start_line": 10,
                        "end_line": 134,
                        "context": {
                            "dependencies": [],
                            "instantiated_by": [
                                "backend.callgraph.build_filtered_callgraph"
                            ],
                            "method_context": [
                                {
                                    "identifier": "backend.callgraph.CallGraph.__init__",
                                    "name": "__init__",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "filename"
                                    ],
                                    "docstring": null,
                                    "start_line": 11,
                                    "end_line": 20
                                },
                                {
                                    "identifier": "backend.callgraph.CallGraph._recursive_call",
                                    "name": "_recursive_call",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "node"
                                    ],
                                    "docstring": "Liefert eine Liste der Namenskomponenten als dotted string z.B. ['pkg', 'mod', 'Class', 'method']",
                                    "start_line": 22,
                                    "end_line": 34
                                },
                                {
                                    "identifier": "backend.callgraph.CallGraph._resolve_all_callee_names",
                                    "name": "_resolve_all_callee_names",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "callee_nodes"
                                    ],
                                    "docstring": "callee_nodes ist jetzt eine Liste von Listen (Name-Steps).\nWir prüfen zuerst lokale Definitionen, dann import_mapping.",
                                    "start_line": 36,
                                    "end_line": 66
                                },
                                {
                                    "identifier": "backend.callgraph.CallGraph._make_full_name",
                                    "name": "_make_full_name",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "basename",
                                        "class_name"
                                    ],
                                    "docstring": null,
                                    "start_line": 68,
                                    "end_line": 71
                                },
                                {
                                    "identifier": "backend.callgraph.CallGraph._current_caller",
                                    "name": "_current_caller",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self"
                                    ],
                                    "docstring": null,
                                    "start_line": 73,
                                    "end_line": 76
                                },
                                {
                                    "identifier": "backend.callgraph.CallGraph.visit_Import",
                                    "name": "visit_Import",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "node"
                                    ],
                                    "docstring": null,
                                    "start_line": 78,
                                    "end_line": 83
                                },
                                {
                                    "identifier": "backend.callgraph.CallGraph.visit_ImportFrom",
                                    "name": "visit_ImportFrom",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "node"
                                    ],
                                    "docstring": null,
                                    "start_line": 85,
                                    "end_line": 88
                                },
                                {
                                    "identifier": "backend.callgraph.CallGraph.visit_ClassDef",
                                    "name": "visit_ClassDef",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "node"
                                    ],
                                    "docstring": null,
                                    "start_line": 90,
                                    "end_line": 94
                                },
                                {
                                    "identifier": "backend.callgraph.CallGraph.visit_FunctionDef",
                                    "name": "visit_FunctionDef",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "node"
                                    ],
                                    "docstring": null,
                                    "start_line": 96,
                                    "end_line": 107
                                },
                                {
                                    "identifier": "backend.callgraph.CallGraph.visit_AsyncFunctionDef",
                                    "name": "visit_AsyncFunctionDef",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "node"
                                    ],
                                    "docstring": null,
                                    "start_line": 109,
                                    "end_line": 110
                                },
                                {
                                    "identifier": "backend.callgraph.CallGraph.visit_Call",
                                    "name": "visit_Call",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "node"
                                    ],
                                    "docstring": null,
                                    "start_line": 112,
                                    "end_line": 123
                                },
                                {
                                    "identifier": "backend.callgraph.CallGraph.visit_If",
                                    "name": "visit_If",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "node"
                                    ],
                                    "docstring": null,
                                    "start_line": 125,
                                    "end_line": 134
                                }
                            ]
                        }
                    }
                ]
            }
        },
        "backend/converter.py": {
            "ast_nodes": {
                "imports": [
                    "logging",
                    "nbformat",
                    "base64",
                    "nbformat.reader.NotJSONError"
                ],
                "functions": [
                    {
                        "mode": "function_analysis",
                        "identifier": "backend.converter.wrap_cdata",
                        "name": "wrap_cdata",
                        "args": [
                            "content"
                        ],
                        "docstring": "Wraps content in CDATA tags.",
                        "source_code": "def wrap_cdata(content):\n    \"\"\"Wraps content in CDATA tags.\"\"\"\n    return f\"<![CDATA[\\n{content}\\n]]>\"",
                        "start_line": 8,
                        "end_line": 10,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "backend.converter.convert_notebook_to_xml"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "backend.converter.extract_output_content",
                        "name": "extract_output_content",
                        "args": [
                            "outputs",
                            "image_list"
                        ],
                        "docstring": "Extracts text and handles images by decoding Base64 to bytes.\nReturns: A list of text strings or placeholders.",
                        "source_code": "def extract_output_content(outputs, image_list):\n    \"\"\"\n    Extracts text and handles images by decoding Base64 to bytes.\n    Returns: A list of text strings or placeholders.\n    \"\"\"\n    extracted_xml_snippets = []\n    \n    for output in outputs:\n\n        if output.output_type in ('display_data', 'execute_result') and hasattr(output, 'data'):\n            data = output.data\n            \n            # Helper to process image\n            def process_image(mime_type):\n                if mime_type in data:\n                    try:\n                        b64_str = data[mime_type]\n                        b64_str = b64_str.replace('\\n', '')\n\n                        image_index = len(image_list)\n                        image_list.append({\n                            \"mime_type\": mime_type,\n                            \"data\": b64_str\n                        })\n\n                        return f'\\n<IMAGE_PLACEHOLDER index=\"{image_index}\" mime=\"{mime_type}\"/>\\n'\n                    except Exception as e:\n                        return f\"<ERROR>Could not decode image: {e}</ERROR>\"\n                return None\n\n            # Ignore jpeg if png is found\n            img_xml = process_image('image/png')\n            if not img_xml:\n                img_xml = process_image('image/jpeg')\n            \n            if img_xml:\n                extracted_xml_snippets.append(img_xml)\n            elif 'text/plain' in data:\n                extracted_xml_snippets.append(data['text/plain'])\n\n        elif output.output_type == 'stream':\n            extracted_xml_snippets.append(output.text)\n            \n        elif output.output_type == 'error':\n            extracted_xml_snippets.append(f\"{output.ename}: {output.evalue}\")\n\n    return extracted_xml_snippets",
                        "start_line": 12,
                        "end_line": 58,
                        "context": {
                            "calls": [
                                "backend.converter.process_image"
                            ],
                            "called_by": [
                                "backend.converter.convert_notebook_to_xml"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "backend.converter.process_image",
                        "name": "process_image",
                        "args": [
                            "mime_type"
                        ],
                        "docstring": null,
                        "source_code": "def process_image(mime_type):\n                if mime_type in data:\n                    try:\n                        b64_str = data[mime_type]\n                        b64_str = b64_str.replace('\\n', '')\n\n                        image_index = len(image_list)\n                        image_list.append({\n                            \"mime_type\": mime_type,\n                            \"data\": b64_str\n                        })\n\n                        return f'\\n<IMAGE_PLACEHOLDER index=\"{image_index}\" mime=\"{mime_type}\"/>\\n'\n                    except Exception as e:\n                        return f\"<ERROR>Could not decode image: {e}</ERROR>\"\n                return None",
                        "start_line": 25,
                        "end_line": 40,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "backend.converter.extract_output_content"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "backend.converter.convert_notebook_to_xml",
                        "name": "convert_notebook_to_xml",
                        "args": [
                            "file_content"
                        ],
                        "docstring": null,
                        "source_code": "def convert_notebook_to_xml(file_content):\n    try:\n        nb = nbformat.reads(file_content, as_version=4)\n    except NotJSONError:\n        return \"<ERROR>Could not parse file as JSON/Notebook</ERROR>\", []\n\n    xml_parts = []\n    extracted_images = [] \n\n    for cell in nb.cells:\n        if cell.cell_type == 'markdown':\n            cell_xml = f'<CELL type=\"markdown\">\\n{cell.source}\\n</CELL>'\n            xml_parts.append(cell_xml)\n\n        elif cell.cell_type == 'code':\n            source_code = wrap_cdata(cell.source)\n            xml_parts.append(f'<CELL type=\"code\">\\n{source_code}\\n</CELL>')\n\n            if hasattr(cell, 'outputs') and cell.outputs:\n                snippets = extract_output_content(cell.outputs, extracted_images)\n                \n                content = \"\\n\".join(snippets)\n                \n                if content.strip():\n                    wrapped_output = wrap_cdata(content)\n                    xml_parts.append(f'<CELL type=\"output\">\\n{wrapped_output}\\n</CELL>')\n\n    return \"\\n\\n\".join(xml_parts), extracted_images",
                        "start_line": 60,
                        "end_line": 87,
                        "context": {
                            "calls": [
                                "backend.converter.extract_output_content",
                                "backend.converter.wrap_cdata"
                            ],
                            "called_by": [
                                "backend.converter.process_repo_notebooks"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "backend.converter.process_repo_notebooks",
                        "name": "process_repo_notebooks",
                        "args": [
                            "repo_files"
                        ],
                        "docstring": null,
                        "source_code": "def process_repo_notebooks(repo_files):\n    notebook_files = [f for f in repo_files if f.path.endswith('.ipynb')]\n    logging.info(f\"Found {len(notebook_files)} notebooks.\")\n        \n    results = {}\n\n    for nb_file in notebook_files:\n        logging.info(f\"Processing: {nb_file.path}\")\n\n        xml_output, images = convert_notebook_to_xml(nb_file.content)\n        results[nb_file.path] = {\n            \"xml\": xml_output,\n            \"images\": images\n        }\n            \n    return results",
                        "start_line": 90,
                        "end_line": 105,
                        "context": {
                            "calls": [
                                "backend.converter.convert_notebook_to_xml"
                            ],
                            "called_by": [
                                "backend.main.notebook_workflow"
                            ]
                        }
                    }
                ],
                "classes": []
            }
        },
        "backend/getRepo.py": {
            "ast_nodes": {
                "imports": [
                    "tempfile",
                    "git.Repo",
                    "git.GitCommandError",
                    "logging",
                    "os"
                ],
                "functions": [],
                "classes": [
                    {
                        "mode": "class_analysis",
                        "identifier": "backend.getRepo.RepoFile",
                        "name": "RepoFile",
                        "docstring": "Repräsentiert eine einzelne Datei in einem Git-Repository.\n\nDer Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tatsächlichem Zugriff.",
                        "source_code": "class RepoFile:\n    \"\"\"\n    Repräsentiert eine einzelne Datei in einem Git-Repository.\n    \n    Der Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tatsächlichem Zugriff.\n    \"\"\"\n    def __init__(self, file_path, commit_tree):\n        \"\"\"\n        Initialisiert das RepoFile-Objekt.\n\n        Args:\n            file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n            commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt.\n        \"\"\"\n        self.path = file_path\n        self._tree = commit_tree\n        \n\n        self._blob = None\n        self._content = None\n        self._size = None\n\n    @property\n    def blob(self):\n        \"\"\"Lazy-lädt das Git-Blob-Objekt.\"\"\"\n        if self._blob is None:\n            try:\n                self._blob = self._tree[self.path]\n            except KeyError:\n                raise FileNotFoundError(f\"Datei '{self.path}' konnte im Commit-Tree nicht gefunden werden.\")\n        return self._blob\n\n    @property\n    def content(self):\n        \"\"\"Lazy-lädt und gibt den dekodierten Inhalt der Datei zurück.\"\"\"\n        if self._content is None:\n            self._content = self.blob.data_stream.read().decode('utf-8', errors='ignore')\n        return self._content\n\n    @property\n    def size(self):\n        \"\"\"Lazy-lädt und gibt die Größe der Datei in Bytes zurück.\"\"\"\n        if self._size is None:\n            self._size = self.blob.size\n        return self._size\n        \n    def analyze_word_count(self):\n        \"\"\"\n        Eine Beispiel-Analyse-Methode. Zählt die Wörter im Dateiinhalt.\n        \"\"\"\n        return len(self.content.split())\n\n    def __repr__(self):\n        \"\"\"Gibt eine nützliche String-Repräsentation des Objekts zurück.\"\"\"\n        return f\"<RepoFile(path='{self.path}')>\"\n    \n    def to_dict(self, include_content=False):\n        data = {\n            \"path\": self.path,\n            \"name\": os.path.basename(self.path),\n            \"size\": self.size,\n            \"type\": \"file\"\n        }\n        if include_content:\n            data[\"content\"] = self.content\n        return data",
                        "start_line": 7,
                        "end_line": 72,
                        "context": {
                            "dependencies": [],
                            "instantiated_by": [
                                "backend.getRepo.GitRepository.get_all_files"
                            ],
                            "method_context": [
                                {
                                    "identifier": "backend.getRepo.RepoFile.__init__",
                                    "name": "__init__",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "file_path",
                                        "commit_tree"
                                    ],
                                    "docstring": "Initialisiert das RepoFile-Objekt.\n\nArgs:\n    file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n    commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt.",
                                    "start_line": 13,
                                    "end_line": 27
                                },
                                {
                                    "identifier": "backend.getRepo.RepoFile.blob",
                                    "name": "blob",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self"
                                    ],
                                    "docstring": "Lazy-lädt das Git-Blob-Objekt.",
                                    "start_line": 30,
                                    "end_line": 37
                                },
                                {
                                    "identifier": "backend.getRepo.RepoFile.content",
                                    "name": "content",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self"
                                    ],
                                    "docstring": "Lazy-lädt und gibt den dekodierten Inhalt der Datei zurück.",
                                    "start_line": 40,
                                    "end_line": 44
                                },
                                {
                                    "identifier": "backend.getRepo.RepoFile.size",
                                    "name": "size",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self"
                                    ],
                                    "docstring": "Lazy-lädt und gibt die Größe der Datei in Bytes zurück.",
                                    "start_line": 47,
                                    "end_line": 51
                                },
                                {
                                    "identifier": "backend.getRepo.RepoFile.analyze_word_count",
                                    "name": "analyze_word_count",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self"
                                    ],
                                    "docstring": "Eine Beispiel-Analyse-Methode. Zählt die Wörter im Dateiinhalt.",
                                    "start_line": 53,
                                    "end_line": 57
                                },
                                {
                                    "identifier": "backend.getRepo.RepoFile.__repr__",
                                    "name": "__repr__",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self"
                                    ],
                                    "docstring": "Gibt eine nützliche String-Repräsentation des Objekts zurück.",
                                    "start_line": 59,
                                    "end_line": 61
                                },
                                {
                                    "identifier": "backend.getRepo.RepoFile.to_dict",
                                    "name": "to_dict",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "include_content"
                                    ],
                                    "docstring": null,
                                    "start_line": 63,
                                    "end_line": 72
                                }
                            ]
                        }
                    },
                    {
                        "mode": "class_analysis",
                        "identifier": "backend.getRepo.GitRepository",
                        "name": "GitRepository",
                        "docstring": "Verwaltet ein Git-Repository, einschließlich Klonen in ein temporäres\nVerzeichnis und Bereitstellung von RepoFile-Objekten.",
                        "source_code": "class GitRepository:\n    \"\"\"\n    Verwaltet ein Git-Repository, einschließlich Klonen in ein temporäres\n    Verzeichnis und Bereitstellung von RepoFile-Objekten.\n    \"\"\"\n    def __init__(self, repo_url):\n        self.repo_url = repo_url\n        self.temp_dir = tempfile.mkdtemp()\n        self.repo = None\n\n        self.files = []\n        \n        try:\n            logging.info(f\"Clone Repository {self.repo_url}...\")\n            self.repo = Repo.clone_from(self.repo_url, self.temp_dir)\n            self.latest_commit = self.repo.head.commit\n            self.commit_tree = self.latest_commit.tree\n            logging.info(\"Cloning successful.\")\n        except GitCommandError as e:\n            self.close()\n            raise RuntimeError(f\"Error cloning repository: {e}\") from e\n\n    def get_all_files(self):\n        \"\"\"\n        Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zurück.\n\n        Returns:\n            list[RepoFile]: Eine Liste von RepoFile-Instanzen.\n        \"\"\"\n        file_paths = self.repo.git.ls_files().split('\\n')\n        self.files = [RepoFile(path, self.commit_tree) for path in file_paths if path]\n        return self.files\n\n    def close(self):\n        \"\"\"Löscht das temporäre Verzeichnis und dessen Inhalt.\"\"\"\n        if self.temp_dir:\n            print(f\"\\nLösche temporäres Verzeichnis: {self.temp_dir}\")\n            self.temp_dir = None\n            \n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def get_file_tree(self, include_content=False):\n\n        if not self.files:\n            self.get_all_files()\n\n        tree = {\"name\": \"root\", \"type\": \"directory\", \"children\": []}\n\n        for file_obj in self.files:\n            parts = file_obj.path.split('/')\n            current_level = tree[\"children\"]\n            \n            # Iteriere durch die Ordnerstruktur\n            for part in parts[:-1]:\n                # Suche, ob Ordner schon existiert\n                found = next((item for item in current_level if item[\"name\"] == part and item[\"type\"] == \"directory\"), None)\n                if not found:\n                    new_dir = {\"name\": part, \"type\": \"directory\", \"children\": []}\n                    current_level.append(new_dir)\n                    current_level = new_dir[\"children\"]\n                else:\n                    current_level = found[\"children\"]\n            \n            # Datei am Ende hinzufügen\n            current_level.append(file_obj.to_dict(include_content=include_content))\n\n        return tree",
                        "start_line": 78,
                        "end_line": 148,
                        "context": {
                            "dependencies": [
                                "backend.getRepo.RepoFile"
                            ],
                            "instantiated_by": [
                                "backend.main.main_workflow",
                                "backend.main.notebook_workflow"
                            ],
                            "method_context": [
                                {
                                    "identifier": "backend.getRepo.GitRepository.__init__",
                                    "name": "__init__",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "repo_url"
                                    ],
                                    "docstring": null,
                                    "start_line": 83,
                                    "end_line": 98
                                },
                                {
                                    "identifier": "backend.getRepo.GitRepository.get_all_files",
                                    "name": "get_all_files",
                                    "calls": [
                                        "backend.getRepo.RepoFile"
                                    ],
                                    "called_by": [],
                                    "args": [
                                        "self"
                                    ],
                                    "docstring": "Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zurück.\n\nReturns:\n    list[RepoFile]: Eine Liste von RepoFile-Instanzen.",
                                    "start_line": 100,
                                    "end_line": 109
                                },
                                {
                                    "identifier": "backend.getRepo.GitRepository.close",
                                    "name": "close",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self"
                                    ],
                                    "docstring": "Löscht das temporäre Verzeichnis und dessen Inhalt.",
                                    "start_line": 111,
                                    "end_line": 115
                                },
                                {
                                    "identifier": "backend.getRepo.GitRepository.__enter__",
                                    "name": "__enter__",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self"
                                    ],
                                    "docstring": null,
                                    "start_line": 117,
                                    "end_line": 118
                                },
                                {
                                    "identifier": "backend.getRepo.GitRepository.__exit__",
                                    "name": "__exit__",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "exc_type",
                                        "exc_val",
                                        "exc_tb"
                                    ],
                                    "docstring": null,
                                    "start_line": 120,
                                    "end_line": 121
                                },
                                {
                                    "identifier": "backend.getRepo.GitRepository.get_file_tree",
                                    "name": "get_file_tree",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "include_content"
                                    ],
                                    "docstring": null,
                                    "start_line": 123,
                                    "end_line": 148
                                }
                            ]
                        }
                    }
                ]
            }
        },
        "backend/main.py": {
            "ast_nodes": {
                "imports": [
                    "json",
                    "math",
                    "logging",
                    "os",
                    "re",
                    "time",
                    "math",
                    "datetime.datetime",
                    "matplotlib.pyplot",
                    "datetime.datetime",
                    "pathlib.Path",
                    "dotenv.load_dotenv",
                    "getRepo.GitRepository",
                    "AST_Schema.ASTAnalyzer",
                    "MainLLM.MainLLM",
                    "basic_info.ProjektInfoExtractor",
                    "HelperLLM.LLMHelper",
                    "relationship_analyzer.ProjectAnalyzer",
                    "schemas.types.FunctionContextInput",
                    "schemas.types.FunctionAnalysisInput",
                    "schemas.types.ClassContextInput",
                    "schemas.types.ClassAnalysisInput",
                    "schemas.types.MethodContextInput",
                    "toon_format.encode",
                    "toon_format.count_tokens",
                    "toon_format.estimate_savings",
                    "toon_format.compare_formats",
                    "converter.process_repo_notebooks"
                ],
                "functions": [
                    {
                        "mode": "function_analysis",
                        "identifier": "backend.main.create_savings_chart",
                        "name": "create_savings_chart",
                        "args": [
                            "json_tokens",
                            "toon_tokens",
                            "savings_percent",
                            "output_path"
                        ],
                        "docstring": "Erstellt ein Balkendiagramm für den Token-Vergleich und speichert es.",
                        "source_code": "def create_savings_chart(json_tokens, toon_tokens, savings_percent, output_path):\n    \"\"\"Erstellt ein Balkendiagramm für den Token-Vergleich und speichert es.\"\"\"\n    labels = ['JSON', 'TOON']\n    values = [json_tokens, toon_tokens]\n    colors = ['#ff9999', '#66b3ff']\n\n    plt.figure(figsize=(8, 6))\n    bars = plt.bar(labels, values, color=colors, width=0.5)\n\n    # Titel und Beschriftungen\n    plt.title(f'Token-Vergleich: {savings_percent:.2f}% Einsparung', fontsize=14)\n    plt.ylabel('Anzahl Token')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n\n    # Werte über den Balken anzeigen\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width() / 2.0, height, \n                 f'{int(height):,}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n\n    # Speichern\n    plt.savefig(output_path)\n    plt.close()",
                        "start_line": 33,
                        "end_line": 55,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "backend.main.main_workflow"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "backend.main.calculate_net_time",
                        "name": "calculate_net_time",
                        "args": [
                            "start_time",
                            "end_time",
                            "total_items",
                            "batch_size",
                            "model_name"
                        ],
                        "docstring": "Berechnet die Dauer abzüglich der Sleep-Zeiten für Rate-Limits.",
                        "source_code": "def calculate_net_time(start_time, end_time, total_items, batch_size, model_name):\n    \"\"\"Berechnet die Dauer abzüglich der Sleep-Zeiten für Rate-Limits.\"\"\"\n    total_duration = end_time - start_time\n    \n    if not model_name.startswith(\"gemini-\"):\n        return total_duration\n\n    if total_items == 0:\n        return 0\n\n    num_batches = math.ceil(total_items / batch_size)\n    sleep_count = max(0, num_batches - 1)\n    total_sleep_time = sleep_count * 61\n    \n    net_time = total_duration - total_sleep_time\n    return max(0, net_time)",
                        "start_line": 57,
                        "end_line": 72,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "backend.main.main_workflow"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "backend.main.main_workflow",
                        "name": "main_workflow",
                        "args": [
                            "input",
                            "api_keys",
                            "model_names",
                            "status_callback"
                        ],
                        "docstring": null,
                        "source_code": "def main_workflow(input, api_keys: dict, model_names: dict, status_callback=None):\n    \n    def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)\n\n    update_status(\"🔍 Analysiere Input...\")\n    \n    user_input = input\n    \n    # API Key & Ollama Base URL aus Frontend holen\n    gemini_api_key = api_keys.get(\"gemini\")\n    openai_api_key = api_keys.get(\"gpt\")\n    scadsllm_api_key = api_keys.get(\"scadsllm\")\n    scadsllm_base_url = api_keys.get(\"scadsllm_base_url\")\n    ollama_base_url = api_keys.get(\"ollama\")\n    base_url = None\n\n    if model_names[\"helper\"].startswith(\"gpt-\"):\n        helper_api_key = openai_api_key\n    elif model_names[\"helper\"].startswith(\"gemini-\"):\n        helper_api_key = gemini_api_key\n    elif \"/\" in model_names[\"helper\"] or model_names[\"helper\"].startswith(\"alias-\") or any(x in model_names[\"helper\"] for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        helper_api_key = scadsllm_api_key\n        base_url = scadsllm_base_url\n    else:\n        helper_api_key = None\n        base_url = ollama_base_url\n    if model_names[\"main\"].startswith(\"gpt-\"):\n        main_api_key = openai_api_key\n    elif model_names[\"main\"].startswith(\"gemini-\"):\n        main_api_key = gemini_api_key\n    elif \"/\" in model_names[\"main\"] or model_names[\"main\"].startswith(\"alias-\") or any(x in model_names[\"main\"] for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        main_api_key = scadsllm_api_key\n        base_url = scadsllm_base_url\n    else:\n        main_api_key = None\n        base_url = ollama_base_url\n\n    # Standardeinstellungen für Modelle\n    helper_model = model_names.get(\"helper\", \"gpt-5-mini\")\n    main_model = model_names.get(\"main\", \"gpt-5.1\")\n\n    # Error Handling für fehlende API Keys\n    if not gemini_api_key and \"gemini\" in helper_model:\n        raise ValueError(\"Gemini API Key was not provided in api_keys dictionary.\")\n\n    # URL Extraktion\n    repo_url = None\n    url_pattern = r\"https?://(?:www\\.)?github\\.com/[^\\s]+\"\n    match = re.search(url_pattern, user_input)\n\n    if match:\n        repo_url = match.group(0)\n        logging.info(f\"Extracted repository URL: {repo_url}\")\n    else:\n        raise ValueError(\"Could not find a valid URL in the provided input.\")\n    \n    # Repo klonen und Dateien extrahieren\n    update_status(f\"⬇️ Klone Repository: {repo_url} ...\")\n    \n    repo_files = []\n    local_repo_path = \"\" \n\n    try: \n\n        with GitRepository(repo_url) as repo:\n            repo_files = repo.get_all_files()\n            if hasattr(repo, 'local_path'):\n                local_repo_path = repo.local_path\n            elif hasattr(repo, 'working_dir'):\n                local_repo_path = repo.working_dir\n            else:\n                local_repo_path = os.path.dirname(os.path.commonpath([f.path for f in repo_files]))\n\n            logging.info(f\"Total files retrieved: {len(repo_files)}\")\n\n    except Exception as e:\n        logging.error(f\"Error cloning repository: {e}\")\n        raise \n\n\n    # Extrahiere Basic Infos\n    update_status(\"ℹ️ Extrahiere Basis-Informationen...\")\n    try:\n        info_extractor = ProjektInfoExtractor()\n        basic_project_info = info_extractor.extrahiere_info(dateien=repo_files, repo_url=repo_url)\n        logging.info(\"Basic project info extracted\")\n    except Exception as e:\n        logging.error(f\"Error extracting basic project info: {e}\")\n        basic_project_info = \"Could not extract basic info.\"\n\n    # Erstelle Repository Dateibaum\n    update_status(\"🌲 Erstelle Repository Dateibaum...\")\n    try:\n        repo_file_tree = repo.get_file_tree()\n        logging.info(\"Repository file tree constructed\")\n    except Exception as e:\n        logging.error(f\"Error constructing repository file tree: {e}\")\n        repo_file_tree = \"Could not create file tree.\"\n\n    # Relationship Analyse durchführen\n    update_status(\"🔗 Analysiere Beziehungen (Calls & Instanziierungen)...\")\n    try:\n        rel_analyzer = ProjectAnalyzer(project_root=local_repo_path)\n        relationship_results = rel_analyzer.analyze()\n        logging.info(f\"Relationships analyzed. Found definitions: {len(relationship_results)}\")\n    except Exception as e:\n        logging.error(f\"Error in relationship analyzer: {e}\")\n        relationship_results = []\n\n    # Erstelle AST Schema\n    update_status(\"🌳 Erstelle Abstract Syntax Tree (AST)...\")\n    try:        \n        ast_analyzer = ASTAnalyzer()   \n        ast_schema = ast_analyzer.analyze_repository(files=repo_files, repo=repo)\n        logging.info(\"AST schema created\")\n    except Exception as e:\n        logging.error(f\"Error retrieving repository files: {e}\")\n        raise\n\n    # Anreichern des AST Schemas mit Relationship Daten\n    update_status(\"➕ Reiche AST mit Beziehungsdaten an...\")            \n    try:   \n        ast_schema = ast_analyzer.merge_relationship_data(ast_schema, relationship_results)\n        logging.info(\"AST schema created and enriched\")\n\n    except Exception as e:\n        logging.error(f\"Error processing repository: {e}\")\n        raise\n\n    # Vorbereitung der HelperLLM Eingaben\n    update_status(\"⚙️ Bereite Daten für Helper LLM vor...\")\n    \n    helper_llm_function_input = []\n    helper_llm_class_input = []\n\n    try:\n        for filename, file_data in ast_schema['files'].items():\n            ast_nodes = file_data.get('ast_nodes', {})\n            imports = ast_nodes.get('imports', [])\n            functions = ast_nodes.get('functions', [])\n            classes = ast_nodes.get('classes', [])\n\n            for function in functions:\n                context = function.get('context', {})\n                \n                raw_called_by = context.get('called_by', [])\n                clean_called_by = [cb for cb in raw_called_by if isinstance(cb, dict)]\n\n                function_context = FunctionContextInput(\n                    calls = context.get('calls', []),\n                    called_by = clean_called_by \n                )\n                \n                function_input = FunctionAnalysisInput(\n                    mode = function.get('mode', 'function_analysis'),\n                    identifier = function.get('identifier'),\n                    source_code = function.get('source_code'),\n                    imports = imports,\n                    context = function_context\n                )\n                \n                helper_llm_function_input.append(function_input)\n\n            for _class in classes:\n                context = _class.get('context', {})\n                \n                method_context_inputs = []\n                for method in context.get('method_context', []):\n                    \n                    raw_method_called_by = method.get('called_by', [])\n                    clean_method_called_by = [cb for cb in raw_method_called_by if isinstance(cb, dict)]\n\n                    method_context_inputs.append(\n                        MethodContextInput(\n                            identifier=method.get('identifier'),\n                            calls=method.get('calls', []),\n                            called_by=clean_method_called_by, \n                            args=method.get('args', []),\n                            docstring=method.get('docstring')\n                        )\n                    )\n\n                raw_instantiated_by = context.get('instantiated_by', [])\n                clean_instantiated_by = [ib for ib in raw_instantiated_by if isinstance(ib, dict)]\n\n                class_context = ClassContextInput(\n                    dependencies = context.get('dependencies', []),\n                    instantiated_by = clean_instantiated_by, \n                    method_context = method_context_inputs\n                )\n\n                class_input = ClassAnalysisInput(\n                    mode = _class.get('mode', 'class_analysis'),\n                    identifier =_class.get('identifier'),\n                    source_code = _class.get('source_code'), \n                    imports = imports, \n                    context = class_context\n                )\n                \n                helper_llm_class_input.append(class_input)\n\n    except Exception as e:\n        logging.error(f\"Error preparing inputs for Helper LLM: {e}\")\n        raise\n    \n    # Initialisiere HelperLLM\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    \n    llm_helper = LLMHelper(\n        api_key=helper_api_key, \n        function_prompt_path=function_prompt_file, \n        class_prompt_path=class_prompt_file,\n        model_name=helper_model,\n        base_url=base_url,\n    )\n    \n    if ollama_base_url:\n        os.environ[\"OLLAMA_BASE_URL\"] = ollama_base_url\n\n    # Initialisiere Ergebniscontainer\n    analysis_results = {}\n    function_analysis_results = []\n    class_analysis_results = []\n\n    # Call HelperLLM für Funktionen\n    update_status(f\"🤖 Helper LLM: Analysiere {len(helper_llm_function_input)} Funktionen ({helper_model})...\")\n\n    try:\n        net_time_func = 0\n        if len(helper_llm_function_input) > 0:\n\n            logging.info(\"\\n--- Generating documentation for Functions ---\")\n            t_start_func = time.time()    \n            function_analysis_results = llm_helper.generate_for_functions(helper_llm_function_input)    \n            t_end_func = time.time()\n            net_time_func = calculate_net_time(t_start_func, t_end_func, len(helper_llm_function_input), llm_helper.batch_size, helper_model)\n\n        if len(function_analysis_results) != 0:\n            for doc in function_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"functions\" not in analysis_results:\n                        analysis_results[\"functions\"] = {}\n                    analysis_results[\"functions\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(f\"Failed to generate doc for a function\") \n    except Exception as e:\n        logging.error(f\"Error during Helper LLM function analysis: {e}\")\n        raise\n    \n\n    # Call HelperLLM für Klassen\n    try:\n        net_time_class = 0\n        if len(helper_llm_class_input) > 0:\n            # Rate Limit Sleep für Gemini Modelle\n            if llm_helper.model_name.startswith(\"gemini-\") & (len(helper_llm_function_input) > 0):\n                update_status(\"💤 Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n                time.sleep(65)\n            \n            update_status(f\"🤖 Helper LLM: Analysiere {len(helper_llm_class_input)} Klassen ({helper_model})...\")\n            \n            logging.info(\"\\n--- Generating documentation for Classes ---\")\n            t_start_class = time.time()\n            class_analysis_results = llm_helper.generate_for_classes(helper_llm_class_input)\n            t_end_class = time.time()\n            net_time_class = calculate_net_time(t_start_class, t_end_class, len(helper_llm_class_input), llm_helper.batch_size, helper_model)\n\n        if len(class_analysis_results) != 0:\n            for doc in class_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"classes\" not in analysis_results:\n                        analysis_results[\"classes\"] = {}\n                    analysis_results[\"classes\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(f\"Failed to generate doc for a class\")\n    except Exception as e:\n        logging.error(f\"Error during Helper LLM class analysis: {e}\")\n        raise\n\n    total_helper_time = net_time_func + net_time_class\n\n    # MainLLM Input Vorbereitung\n    main_llm_input = {\n        \"basic_info\": basic_project_info,\n        \"file_tree\": repo_file_tree,\n        \"ast_schema\": ast_schema,\n        \"analysis_results\": analysis_results\n    }\n\n    # Speichern als JSON (Optional)\n    main_llm_input_json = json.dumps(main_llm_input, indent=2)\n    # with open(\"output.json\", \"w\", encoding=\"utf-8\") as f:\n    #     f.write(main_llm_input_json)\n    #     logging.info(\"JSON-Datei wurde gespeichert.\")\n\n    # Konvertiere Input in Toon Format\n    main_llm_input_toon = encode(main_llm_input)\n\n    #Speichern in TOON Format (Optional)\n    # with open(\"output.toon\", \"w\", encoding=\"utf-8\") as f:\n    #     f.write(main_llm_input_toon)\n    #     logging.info(\"output.toon erfolgreich gespeichert.\")\n    \n    # Token Evaluation\n    savings_data = None\n    try:\n        logging.info(\"--- Evaluierung der Token-Ersparnis ---\")\n        savings_data = estimate_savings(main_llm_input)\n        logging.info(f\"JSON Tokens: {savings_data['json_tokens']}\")\n        logging.info(f\"TOON Tokens: {savings_data['toon_tokens']}\")\n        logging.info(f\"Ersparnis:   {savings_data['savings_percent']:.2f}%\")\n        \n    except Exception as e:\n        logging.warning(f\"Token evaluation could not be performed: {e}\")    \n\n\n\n    prompt_file_mainllm = \"SystemPrompts/SystemPromptMainLLM.txt\"\n    prompt_file_mainllm_toon = \"SystemPrompts/SystemPromptMainLLMToon.txt\"\n    # MainLLM Ausführung\n    main_llm = MainLLM(\n        api_key=main_api_key, \n        prompt_file_path=prompt_file_mainllm_toon,\n        model_name=main_model,\n        base_url=base_url,\n    )\n\n\n    # RPM Limit Sleep für Gemini Modelle\n    if llm_helper.model_name == main_llm.model_name and main_llm.model_name.startswith(\"gemini-\"):\n        time.sleep(65)\n        update_status(\"💤 Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n\n    # Call MainLLM für finalen Report\n    update_status(f\"🧠 Main LLM: Generiere finalen Report ({main_model})...\")\n    try:\n        total_main_time = 0\n        logging.info(\"\\n--- Generating Final Report ---\")\n        t_start_main = time.time()\n        final_report = main_llm.call_llm(main_llm_input_toon)\n        t_end_main = time.time()\n        total_main_time = t_end_main - t_start_main\n    except Exception as e:\n        logging.error(f\"Error during Main LLM final report generation: {e}\")\n        raise\n\n    \n    # --- Speichern der Ergebnisse ---\n    output_dir = \"result\"\n    stats_dir = \"Statistics\" # Neuer Ordner\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(stats_dir, exist_ok=True) # Stelle sicher, dass Statistics Ordner existiert\n\n    total_active_time = total_helper_time + total_main_time\n    \n    # Dateiname für Report\n    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M-%S\")\n    report_filename = f\"report_{timestamp}_Helper_{llm_helper.model_name}_MainLLM_{main_llm.model_name}.md\"\n    report_filepath = os.path.join(output_dir, report_filename)\n    \n    if final_report:\n        with open(report_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(final_report)\n        logging.info(f\"Final report saved to '{report_filepath}'.\")\n        \n        if savings_data:\n            try:\n                savings_filename = report_filename.replace(\"report_\", \"savings_\").replace(\".md\", \".png\")\n                savings_filepath = os.path.join(stats_dir, savings_filename)\n                \n                logging.info(f\"Erstelle Token-Chart: {savings_filepath}\")\n                create_savings_chart(\n                    json_tokens=savings_data['json_tokens'], \n                    toon_tokens=savings_data['toon_tokens'], \n                    savings_percent=savings_data['savings_percent'],\n                    output_path=savings_filepath\n                )\n            except Exception as chart_error:\n                logging.error(f\"Konnte Diagramm nicht erstellen: {chart_error}\")\n\n    else:\n        final_report = \"Error: Report generation failed or returned empty.\"\n\n    metrics = {\n        \"helper_time\": round(total_helper_time, 2),\n        \"main_time\": round(total_main_time, 2),\n        \"total_time\": round(total_active_time, 2),\n        \"helper_model\": helper_model,\n        \"main_model\": main_model,\n        \"json_tokens\": savings_data['json_tokens'] if savings_data else None,\n        \"toon_tokens\": savings_data['toon_tokens'] if savings_data else None,\n        \"savings_percent\": round(savings_data['savings_percent'], 2) if savings_data else None\n    \n    }\n\n    return {\n        \"report\": final_report,\n        \"metrics\": metrics\n    }",
                        "start_line": 75,
                        "end_line": 479,
                        "context": {
                            "calls": [
                                "backend.AST_Schema.ASTAnalyzer",
                                "backend.AST_Schema.ASTAnalyzer.analyze_repository",
                                "backend.AST_Schema.ASTAnalyzer.merge_relationship_data",
                                "backend.HelperLLM.LLMHelper",
                                "backend.HelperLLM.LLMHelper.generate_for_classes",
                                "backend.HelperLLM.LLMHelper.generate_for_functions",
                                "backend.MainLLM.MainLLM",
                                "backend.MainLLM.MainLLM.call_llm",
                                "backend.basic_info.ProjektInfoExtractor",
                                "backend.basic_info.ProjektInfoExtractor.extrahiere_info",
                                "backend.getRepo.GitRepository",
                                "backend.main.calculate_net_time",
                                "backend.main.create_savings_chart",
                                "backend.main.update_status",
                                "backend.relationship_analyzer.ProjectAnalyzer",
                                "backend.relationship_analyzer.ProjectAnalyzer.analyze",
                                "backend.relationship_analyzer.ProjectAnalyzer.get_raw_relationships",
                                "schemas.types.ClassAnalysisInput",
                                "schemas.types.ClassContextInput",
                                "schemas.types.FunctionAnalysisInput",
                                "schemas.types.FunctionContextInput",
                                "schemas.types.MethodContextInput"
                            ],
                            "called_by": [
                                "backend.main",
                                "frontend.frontend"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "backend.main.update_status",
                        "name": "update_status",
                        "args": [
                            "msg"
                        ],
                        "docstring": null,
                        "source_code": "def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)",
                        "start_line": 77,
                        "end_line": 80,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "backend.main.main_workflow",
                                "backend.main.notebook_workflow"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "backend.main.notebook_workflow",
                        "name": "notebook_workflow",
                        "args": [
                            "input",
                            "api_key",
                            "model",
                            "status_callback"
                        ],
                        "docstring": null,
                        "source_code": "def notebook_workflow(input, api_key, model, status_callback=None):\n\n    def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)\n\n    def gemini_payload(basic_info, nb_path, xml_content, images):\n        \n        intro_json = json.dumps({\n            \"basic_info\": basic_info,\n            \"current_notebook_path\": nb_path\n        }, indent=2)\n        \n        payload_content = []\n\n        payload_content.append({\n            \"type\": \"text\",\n            \"text\": f\"Context Information:\\n{intro_json}\\n\\nNotebook XML Structure:\\n\"\n        })\n\n        # Regex to find: <IMAGE_PLACEHOLDER index=\"0\" mime=\"image/png\"/>\n        pattern = r'(<IMAGE_PLACEHOLDER index=\"(\\d+)\" mime=\"([^\"]+)\"/>)'\n        last_pos = 0\n        \n        for match in re.finditer(pattern, xml_content):\n            text_segment = xml_content[last_pos:match.start()]\n            if text_segment.strip():\n                payload_content.append({\n                    \"type\": \"text\",\n                    \"text\": text_segment\n                })\n                \n            image_index = int(match.group(2))\n            mime_type = match.group(3)\n            \n            if image_index < len(images):\n                img_data = images[image_index]\n                b64_string = img_data['data']\n                \n                payload_content.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:{mime_type};base64,{b64_string}\"\n                    }\n                })\n                \n            last_pos = match.end()\n\n        # Add any remaining text after the last image\n        remaining_text = xml_content[last_pos:]\n        if remaining_text.strip():\n            payload_content.append({\n                \"type\": \"text\",\n                \"text\": remaining_text\n            })\n\n        return payload_content\n\n\n    base_url = None\n\n    if model.startswith(\"gpt-\"):\n        input_api_key = api_key.get(\"gpt\")\n    elif model.startswith(\"gemini-\"):\n        input_api_key = api_key.get(\"gemini\")\n    elif \"/\" in model or model.startswith(\"alias-\") or any(x in model for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        input_api_key = api_key.get(\"scadsllm\")\n        base_url = api_key.get(\"scadsllm_base_url\")\n    else:\n        input_api_key = None\n        base_url = api_key.get(\"ollama\")\n\n    update_status(\"🔍 Analysiere Input...\")\n\n    # URL Extraktion\n    repo_url = None\n    url_pattern = r\"https?://(?:www\\.)?github\\.com/[^\\s]+\"\n    match = re.search(url_pattern, input)\n\n    if match:\n        repo_url = match.group(0)\n        logging.info(f\"Extracted repository URL: {repo_url}\")\n    else:\n        raise ValueError(\"Could not find a valid URL in the provided input.\")\n    \n    update_status(f\"⬇️ Klone Repository: {repo_url} ...\")\n\n    try: \n        with GitRepository(repo_url) as repo:\n            repo_files = repo.get_all_files()\n    except Exception as e:\n        logging.error(f\"Error cloning repository: {e}\")\n        raise\n    \n    update_status(f\"🔗 Bereite Input vor...\")\n\n    # convert to XML\n    processed_data = process_repo_notebooks(repo_files)\n\n    \n    # Extrahiere Basic Infos\n    update_status(\"ℹ️ Extrahiere Basis-Informationen...\")\n    try:\n        info_extractor = ProjektInfoExtractor()\n        basic_project_info = info_extractor.extrahiere_info(dateien=repo_files, repo_url=repo_url)\n        logging.info(\"Basic project info extracted\")\n    except Exception as e:\n        logging.error(f\"Error extracting basic project info: {e}\")\n        basic_project_info = \"Could not extract basic info.\"\n\n\n\n    prompt_file_notebook_llm = \"SystemPrompts/SystemPromptNotebookLLM.txt\"\n    notebook_llm = MainLLM(\n        api_key=input_api_key, \n        prompt_file_path=prompt_file_notebook_llm,\n        model_name=model,\n        base_url=base_url,\n    )\n\n    notebook_reports = []\n    total_notebooks = len(processed_data)\n    \n    logging.info(f\"Starting sequential processing of {total_notebooks} notebooks...\")\n\n    # Iterate over each notebook file individually\n    for index, (nb_path, nb_data) in enumerate(processed_data.items(), 1):\n        \n        update_status(f\"🧠 Generiere Report ({index}/{total_notebooks}): {os.path.basename(nb_path)}\")\n        \n        nb_xml = nb_data['xml']\n        nb_images = nb_data['images']\n\n        llm_payload = gemini_payload(basic_project_info, nb_path, nb_xml, nb_images)\n\n        try:\n            single_report = notebook_llm.call_llm(llm_payload)\n            notebook_reports.append(single_report)\n            \n        except Exception as e:\n            logging.error(f\"Error generating report for {nb_path}: {e}\")\n            notebook_reports.append(f\"# Error processing {nb_path}\\n\\nError: {str(e)}\\n\\n---\\n\")\n\n    # Concatenate all reports\n    final_report = \"\\n\\n<br>\\n<br>\\n<br>\\n\\n\".join(notebook_reports)\n\n    \n    # --- Speichern der Ergebnisse ---\n    output_dir = \"result\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Dateiname für Report\n    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M-%S\")\n    report_filename = f\"notebook_report_{timestamp}_NotebookLLM_{notebook_llm.model_name}.md\"\n    report_filepath = os.path.join(output_dir, report_filename)\n    \n    if final_report:\n        with open(report_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(final_report)\n        logging.info(f\"Final report saved to '{report_filepath}'.\")",
                        "start_line": 485,
                        "end_line": 645,
                        "context": {
                            "calls": [
                                "backend.MainLLM.MainLLM",
                                "backend.MainLLM.MainLLM.call_llm",
                                "backend.basic_info.ProjektInfoExtractor",
                                "backend.basic_info.ProjektInfoExtractor.extrahiere_info",
                                "backend.converter.process_repo_notebooks",
                                "backend.getRepo.GitRepository",
                                "backend.main.gemini_payload",
                                "backend.main.update_status"
                            ],
                            "called_by": []
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "backend.main.update_status",
                        "name": "update_status",
                        "args": [
                            "msg"
                        ],
                        "docstring": null,
                        "source_code": "def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)",
                        "start_line": 487,
                        "end_line": 490,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "backend.main.main_workflow",
                                "backend.main.notebook_workflow"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "backend.main.gemini_payload",
                        "name": "gemini_payload",
                        "args": [
                            "basic_info",
                            "nb_path",
                            "xml_content",
                            "images"
                        ],
                        "docstring": null,
                        "source_code": "def gemini_payload(basic_info, nb_path, xml_content, images):\n        \n        intro_json = json.dumps({\n            \"basic_info\": basic_info,\n            \"current_notebook_path\": nb_path\n        }, indent=2)\n        \n        payload_content = []\n\n        payload_content.append({\n            \"type\": \"text\",\n            \"text\": f\"Context Information:\\n{intro_json}\\n\\nNotebook XML Structure:\\n\"\n        })\n\n        # Regex to find: <IMAGE_PLACEHOLDER index=\"0\" mime=\"image/png\"/>\n        pattern = r'(<IMAGE_PLACEHOLDER index=\"(\\d+)\" mime=\"([^\"]+)\"/>)'\n        last_pos = 0\n        \n        for match in re.finditer(pattern, xml_content):\n            text_segment = xml_content[last_pos:match.start()]\n            if text_segment.strip():\n                payload_content.append({\n                    \"type\": \"text\",\n                    \"text\": text_segment\n                })\n                \n            image_index = int(match.group(2))\n            mime_type = match.group(3)\n            \n            if image_index < len(images):\n                img_data = images[image_index]\n                b64_string = img_data['data']\n                \n                payload_content.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:{mime_type};base64,{b64_string}\"\n                    }\n                })\n                \n            last_pos = match.end()\n\n        # Add any remaining text after the last image\n        remaining_text = xml_content[last_pos:]\n        if remaining_text.strip():\n            payload_content.append({\n                \"type\": \"text\",\n                \"text\": remaining_text\n            })\n\n        return payload_content",
                        "start_line": 492,
                        "end_line": 542,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "backend.main.notebook_workflow"
                            ]
                        }
                    }
                ],
                "classes": []
            }
        },
        "backend/relationship_analyzer.py": {
            "ast_nodes": {
                "imports": [
                    "ast",
                    "os",
                    "logging",
                    "collections.defaultdict"
                ],
                "functions": [
                    {
                        "mode": "function_analysis",
                        "identifier": "backend.relationship_analyzer.path_to_module",
                        "name": "path_to_module",
                        "args": [
                            "filepath",
                            "project_root"
                        ],
                        "docstring": "Wandelt einen Dateipfad in einen Python-Modulpfad um.",
                        "source_code": "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    module_path = rel_path.replace(os.path.sep, '.')\n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n    return module_path",
                        "start_line": 6,
                        "end_line": 18,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "backend.relationship_analyzer.CallResolverVisitor.__init__",
                                "backend.relationship_analyzer.ProjectAnalyzer._collect_definitions"
                            ]
                        }
                    }
                ],
                "classes": [
                    {
                        "mode": "class_analysis",
                        "identifier": "backend.relationship_analyzer.ProjectAnalyzer",
                        "name": "ProjectAnalyzer",
                        "docstring": null,
                        "source_code": "class ProjectAnalyzer:\n    \n    def __init__(self, project_root):\n        self.project_root = os.path.abspath(project_root)\n        self.definitions = {}\n        self.call_graph = defaultdict(list)\n        self.file_asts = {} \n        self.ignore_dirs = {'.git', '.venv', 'venv', '__pycache__', 'node_modules', 'dist', 'build', 'docs'}\n\n    def analyze(self):\n        py_files = self._find_py_files()\n        \n        for filepath in py_files:\n            self._collect_definitions(filepath)\n            \n        for filepath in py_files:\n            self._resolve_calls(filepath)\n            \n        self.file_asts.clear()\n        \n        return self.get_formatted_results()\n\n    def _find_py_files(self):\n        py_files = []\n        for root, dirs, files in os.walk(self.project_root):\n            dirs[:] = [d for d in dirs if d not in self.ignore_dirs]\n            \n            for file in files:\n                if file.endswith(\".py\"):\n                    py_files.append(os.path.join(root, file))\n        return py_files\n\n    def _collect_definitions(self, filepath):\n        try:\n            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n                source = f.read()\n                tree = ast.parse(source, filename=filepath)\n                \n            self.file_asts[filepath] = tree\n            \n            module_path = path_to_module(filepath, self.project_root)\n            \n            for node in ast.walk(tree):\n                if isinstance(node, ast.FunctionDef):\n                    parent = self._get_parent(tree, node)\n                    if isinstance(parent, ast.ClassDef):\n                        path_name = f\"{module_path}.{parent.name}.{node.name}\"\n                        def_type = 'method'\n                    else:\n                        path_name = f\"{module_path}.{node.name}\"\n                        def_type = 'function'\n                    self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': def_type}\n                elif isinstance(node, ast.ClassDef):\n                    path_name = f\"{module_path}.{node.name}\"\n                    self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': 'class'}\n        except Exception as e:\n            logging.error(f\"Error collecting definitions in {filepath}: {e}\")\n            self.file_asts[filepath] = None\n            \n    def _get_parent(self, tree, node):\n        for parent in ast.walk(tree):\n            for child in ast.iter_child_nodes(parent):\n                if child is node:\n                    return parent\n        return None\n\n    def _resolve_calls(self, filepath):\n        tree = self.file_asts.get(filepath)\n        if not tree:\n            return\n\n        try:\n            resolver = CallResolverVisitor(filepath, self.project_root, self.definitions)\n            resolver.visit(tree)\n            for callee_pathname, caller_info in resolver.calls.items():\n                self.call_graph[callee_pathname].extend(caller_info)\n        except Exception as e:\n            logging.error(f\"Error resolving calls in {filepath}: {e}\")\n\n    def get_formatted_results(self):\n        output_list = []\n        \n        for callee_pathname, calls in self.call_graph.items():\n            if callee_pathname in self.definitions:\n                def_info = self.definitions[callee_pathname]\n                \n                definition_dict = {\n                    \"identifier\": callee_pathname,\n                    \"mode\": def_info.get('type', 'unknown'),\n                    \"origin\": os.path.basename(def_info['file']),\n                    \"origin_line\": def_info['line'],\n                    \"called_by\": [] \n                }\n                \n                unique_calls = {}\n                for call in calls:\n                    key = (call['file'], call['line'], call['caller'])\n                    if key not in unique_calls:\n                        unique_calls[key] = {\n                            \"file\": call['file'],\n                            \"function\": call['caller'],\n                            \"mode\": call.get('caller_type', 'unknown'),\n                            \"line\": call['line']\n                        }\n\n                if unique_calls:\n                    definition_dict[\"called_by\"] = sorted(unique_calls.values(), key=lambda x: (x['file'], x['line']))\n                    output_list.append(definition_dict)\n                    \n        return output_list",
                        "start_line": 20,
                        "end_line": 129,
                        "context": {
                            "dependencies": [
                                "backend.relationship_analyzer.CallResolverVisitor",
                                "backend.relationship_analyzer.path_to_module"
                            ],
                            "instantiated_by": [
                                "backend.main.main_workflow"
                            ],
                            "method_context": [
                                {
                                    "identifier": "backend.relationship_analyzer.ProjectAnalyzer.__init__",
                                    "name": "__init__",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "project_root"
                                    ],
                                    "docstring": null,
                                    "start_line": 22,
                                    "end_line": 27
                                },
                                {
                                    "identifier": "backend.relationship_analyzer.ProjectAnalyzer.analyze",
                                    "name": "analyze",
                                    "calls": [],
                                    "called_by": [
                                        "backend.main.main_workflow"
                                    ],
                                    "args": [
                                        "self"
                                    ],
                                    "docstring": null,
                                    "start_line": 29,
                                    "end_line": 40
                                },
                                {
                                    "identifier": "backend.relationship_analyzer.ProjectAnalyzer._find_py_files",
                                    "name": "_find_py_files",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self"
                                    ],
                                    "docstring": null,
                                    "start_line": 42,
                                    "end_line": 50
                                },
                                {
                                    "identifier": "backend.relationship_analyzer.ProjectAnalyzer._collect_definitions",
                                    "name": "_collect_definitions",
                                    "calls": [
                                        "backend.relationship_analyzer.path_to_module"
                                    ],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "filepath"
                                    ],
                                    "docstring": null,
                                    "start_line": 52,
                                    "end_line": 77
                                },
                                {
                                    "identifier": "backend.relationship_analyzer.ProjectAnalyzer._get_parent",
                                    "name": "_get_parent",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "tree",
                                        "node"
                                    ],
                                    "docstring": null,
                                    "start_line": 79,
                                    "end_line": 84
                                },
                                {
                                    "identifier": "backend.relationship_analyzer.ProjectAnalyzer._resolve_calls",
                                    "name": "_resolve_calls",
                                    "calls": [
                                        "backend.relationship_analyzer.CallResolverVisitor"
                                    ],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "filepath"
                                    ],
                                    "docstring": null,
                                    "start_line": 86,
                                    "end_line": 97
                                },
                                {
                                    "identifier": "backend.relationship_analyzer.ProjectAnalyzer.get_formatted_results",
                                    "name": "get_formatted_results",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self"
                                    ],
                                    "docstring": null,
                                    "start_line": 99,
                                    "end_line": 129
                                }
                            ]
                        }
                    },
                    {
                        "mode": "class_analysis",
                        "identifier": "backend.relationship_analyzer.CallResolverVisitor",
                        "name": "CallResolverVisitor",
                        "docstring": null,
                        "source_code": "class CallResolverVisitor(ast.NodeVisitor):\n    def __init__(self, filepath, project_root, definitions):\n        self.filepath = filepath\n        self.module_path = path_to_module(filepath, project_root)\n        self.definitions = definitions\n        self.scope = {}\n        self.instance_types = {}\n        self.current_caller_name = self.module_path\n        self.current_class_name = None\n        self.calls = defaultdict(list)\n\n    def visit_ClassDef(self, node):\n        old_class_name, self.current_class_name = self.current_class_name, node.name\n        self.generic_visit(node)\n        self.current_class_name = old_class_name\n\n    def visit_FunctionDef(self, node):\n        old_caller_name, self.current_caller_name = self.current_caller_name, node.name\n        self.generic_visit(node)\n        self.current_caller_name = old_caller_name\n\n    def visit_Call(self, node):\n        callee_pathname = self._resolve_call_qname(node.func)\n        if callee_pathname and callee_pathname in self.definitions:\n            if self.current_caller_name == self.module_path:\n                caller_type = 'module'\n            elif self.current_class_name:\n                caller_type = 'method'\n            else:\n                caller_type = 'function'\n            \n            caller_info = {\n                'file': os.path.basename(self.filepath),\n                'line': node.lineno,\n                'caller': self.current_caller_name,\n                'caller_type': caller_type \n            }\n            self.calls[callee_pathname].append(caller_info)\n        self.generic_visit(node)\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.scope[alias.asname or alias.name] = alias.name\n        self.generic_visit(node)\n        \n    def visit_ImportFrom(self, node):\n        module = node.module or ''\n        for alias in node.names:\n            name = alias.asname or alias.name\n            if node.level > 0:\n                base = self.module_path.split('.')\n                prefix = '.'.join(base[:-node.level])\n                full_module_path = f\"{prefix}.{module}\" if module else prefix\n            else:\n                full_module_path = module\n            self.scope[name] = f\"{full_module_path}.{alias.name}\"\n        self.generic_visit(node)\n\n    def visit_Assign(self, node):\n        if isinstance(node.value, ast.Call) and isinstance(node.value.func, ast.Name):\n            class_name = node.value.func.id\n            if class_name in self.scope:\n                qualified_class_name = self.scope[class_name]\n                if qualified_class_name in self.definitions:\n                    for target in node.targets:\n                        if isinstance(target, ast.Name):\n                            self.instance_types[target.id] = qualified_class_name\n        self.generic_visit(node)\n\n    def _resolve_call_qname(self, func_node):\n        if isinstance(func_node, ast.Name):\n            name = func_node.id\n            if name in self.scope:\n                return self.scope[name]\n            local_pathname = f\"{self.module_path}.{name}\"\n            if local_pathname in self.definitions:\n                return local_pathname\n        elif isinstance(func_node, ast.Attribute) and isinstance(func_node.value, ast.Name):\n            var_name = func_node.value.id\n            method_name = func_node.attr\n            if var_name in self.instance_types:\n                class_pathname = self.instance_types[var_name]\n                return f\"{class_pathname}.{method_name}\"\n            if var_name in self.scope:\n                module_pathname = self.scope[var_name]\n                return f\"{module_pathname}.{method_name}\"\n        return None",
                        "start_line": 131,
                        "end_line": 217,
                        "context": {
                            "dependencies": [
                                "backend.relationship_analyzer.path_to_module"
                            ],
                            "instantiated_by": [
                                "backend.relationship_analyzer.ProjectAnalyzer._resolve_calls"
                            ],
                            "method_context": [
                                {
                                    "identifier": "backend.relationship_analyzer.CallResolverVisitor.__init__",
                                    "name": "__init__",
                                    "calls": [
                                        "backend.relationship_analyzer.path_to_module"
                                    ],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "filepath",
                                        "project_root",
                                        "definitions"
                                    ],
                                    "docstring": null,
                                    "start_line": 132,
                                    "end_line": 140
                                },
                                {
                                    "identifier": "backend.relationship_analyzer.CallResolverVisitor.visit_ClassDef",
                                    "name": "visit_ClassDef",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "node"
                                    ],
                                    "docstring": null,
                                    "start_line": 142,
                                    "end_line": 145
                                },
                                {
                                    "identifier": "backend.relationship_analyzer.CallResolverVisitor.visit_FunctionDef",
                                    "name": "visit_FunctionDef",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "node"
                                    ],
                                    "docstring": null,
                                    "start_line": 147,
                                    "end_line": 150
                                },
                                {
                                    "identifier": "backend.relationship_analyzer.CallResolverVisitor.visit_Call",
                                    "name": "visit_Call",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "node"
                                    ],
                                    "docstring": null,
                                    "start_line": 152,
                                    "end_line": 169
                                },
                                {
                                    "identifier": "backend.relationship_analyzer.CallResolverVisitor.visit_Import",
                                    "name": "visit_Import",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "node"
                                    ],
                                    "docstring": null,
                                    "start_line": 171,
                                    "end_line": 174
                                },
                                {
                                    "identifier": "backend.relationship_analyzer.CallResolverVisitor.visit_ImportFrom",
                                    "name": "visit_ImportFrom",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "node"
                                    ],
                                    "docstring": null,
                                    "start_line": 176,
                                    "end_line": 187
                                },
                                {
                                    "identifier": "backend.relationship_analyzer.CallResolverVisitor.visit_Assign",
                                    "name": "visit_Assign",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "node"
                                    ],
                                    "docstring": null,
                                    "start_line": 189,
                                    "end_line": 198
                                },
                                {
                                    "identifier": "backend.relationship_analyzer.CallResolverVisitor._resolve_call_qname",
                                    "name": "_resolve_call_qname",
                                    "calls": [],
                                    "called_by": [],
                                    "args": [
                                        "self",
                                        "func_node"
                                    ],
                                    "docstring": null,
                                    "start_line": 200,
                                    "end_line": 217
                                }
                            ]
                        }
                    }
                ]
            }
        },
        "backend/scads_key_test.py": {
            "ast_nodes": {
                "imports": [
                    "os",
                    "dotenv.load_dotenv",
                    "openai.OpenAI"
                ],
                "functions": [],
                "classes": []
            }
        },
        "database/db.py": {
            "ast_nodes": {
                "imports": [
                    "datetime.datetime",
                    "pymongo.MongoClient",
                    "dotenv.load_dotenv",
                    "streamlit_authenticator",
                    "cryptography.fernet.Fernet",
                    "streamlit",
                    "uuid",
                    "os"
                ],
                "functions": [
                    {
                        "mode": "function_analysis",
                        "identifier": "database.db.encrypt_text",
                        "name": "encrypt_text",
                        "args": [
                            "text"
                        ],
                        "docstring": null,
                        "source_code": "def encrypt_text(text: str) -> str:\n    if not text or not cipher_suite: return text\n    return cipher_suite.encrypt(text.strip().encode()).decode()",
                        "start_line": 32,
                        "end_line": 34,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "database.db.update_gemini_key",
                                "database.db.update_gpt_key"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "database.db.decrypt_text",
                        "name": "decrypt_text",
                        "args": [
                            "text"
                        ],
                        "docstring": null,
                        "source_code": "def decrypt_text(text: str) -> str:\n    if not text or not cipher_suite: return text\n    try:\n        return cipher_suite.decrypt(text.strip().encode()).decode()\n    except Exception:\n        return text",
                        "start_line": 36,
                        "end_line": 41,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "database.db.get_decrypted_api_keys"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "database.db.insert_user",
                        "name": "insert_user",
                        "args": [
                            "username",
                            "name",
                            "password"
                        ],
                        "docstring": null,
                        "source_code": "def insert_user(username: str, name: str, password: str):\n    user = {\n        \"_id\": username,\n        \"name\": name, \n        \"hashed_password\": stauth.Hasher.hash(password),\n        \"gemini_api_key\": \"\",\n        \"ollama_base_url\": \"\",\n        \"gpt_api_key\": \"\"\n    }\n    result = dbusers.insert_one(user)\n    return result.inserted_id",
                        "start_line": 47,
                        "end_line": 57,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "frontend.frontend"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "database.db.fetch_all_users",
                        "name": "fetch_all_users",
                        "args": [],
                        "docstring": null,
                        "source_code": "def fetch_all_users():\n    return list(dbusers.find())",
                        "start_line": 59,
                        "end_line": 60,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "frontend.frontend"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "database.db.fetch_user",
                        "name": "fetch_user",
                        "args": [
                            "username"
                        ],
                        "docstring": null,
                        "source_code": "def fetch_user(username: str):\n    return dbusers.find_one({\"_id\": username})",
                        "start_line": 62,
                        "end_line": 63,
                        "context": {
                            "calls": [],
                            "called_by": []
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "database.db.update_user_name",
                        "name": "update_user_name",
                        "args": [
                            "username",
                            "new_name"
                        ],
                        "docstring": null,
                        "source_code": "def update_user_name(username: str, new_name: str):\n    # Achtung: _id kann in Mongo nicht einfach so geändert werden. \n    # Hier wird nur das Name-Feld geupdated.\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"name\": new_name}})\n    return result.modified_count",
                        "start_line": 65,
                        "end_line": 69,
                        "context": {
                            "calls": [],
                            "called_by": []
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "database.db.update_gemini_key",
                        "name": "update_gemini_key",
                        "args": [
                            "username",
                            "gemini_api_key"
                        ],
                        "docstring": null,
                        "source_code": "def update_gemini_key(username: str, gemini_api_key: str):\n    encrypted_key = encrypt_text(gemini_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"gemini_api_key\": encrypted_key}})\n    return result.modified_count",
                        "start_line": 71,
                        "end_line": 74,
                        "context": {
                            "calls": [
                                "database.db.encrypt_text"
                            ],
                            "called_by": [
                                "frontend.frontend",
                                "frontend.frontend.save_gemini_cb"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "database.db.update_gpt_key",
                        "name": "update_gpt_key",
                        "args": [
                            "username",
                            "gpt_api_key"
                        ],
                        "docstring": null,
                        "source_code": "def update_gpt_key(username: str, gpt_api_key: str):\n    encrypted_key = encrypt_text(gpt_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"gpt_api_key\": encrypted_key}})\n    return result.modified_count",
                        "start_line": 76,
                        "end_line": 79,
                        "context": {
                            "calls": [
                                "database.db.encrypt_text"
                            ],
                            "called_by": [
                                "frontend.frontend"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "database.db.update_ollama_url",
                        "name": "update_ollama_url",
                        "args": [
                            "username",
                            "ollama_base_url"
                        ],
                        "docstring": null,
                        "source_code": "def update_ollama_url(username: str, ollama_base_url: str):\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"ollama_base_url\": ollama_base_url.strip()}})\n    return result.modified_count",
                        "start_line": 81,
                        "end_line": 83,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "frontend.frontend",
                                "frontend.frontend.save_ollama_cb"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "database.db.fetch_gemini_key",
                        "name": "fetch_gemini_key",
                        "args": [
                            "username"
                        ],
                        "docstring": null,
                        "source_code": "def fetch_gemini_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"gemini_api_key\": 1, \"_id\": 0})\n    return user.get(\"gemini_api_key\") if user else None",
                        "start_line": 85,
                        "end_line": 87,
                        "context": {
                            "calls": [],
                            "called_by": []
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "database.db.fetch_ollama_url",
                        "name": "fetch_ollama_url",
                        "args": [
                            "username"
                        ],
                        "docstring": null,
                        "source_code": "def fetch_ollama_url(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"ollama_base_url\": 1, \"_id\": 0})\n    return user.get(\"ollama_base_url\") if user else None",
                        "start_line": 89,
                        "end_line": 91,
                        "context": {
                            "calls": [],
                            "called_by": []
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "database.db.delete_user",
                        "name": "delete_user",
                        "args": [
                            "username"
                        ],
                        "docstring": null,
                        "source_code": "def delete_user(username: str):\n    return dbusers.delete_one({\"_id\": username}).deleted_count",
                        "start_line": 93,
                        "end_line": 94,
                        "context": {
                            "calls": [],
                            "called_by": []
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "database.db.get_decrypted_api_keys",
                        "name": "get_decrypted_api_keys",
                        "args": [
                            "username"
                        ],
                        "docstring": null,
                        "source_code": "def get_decrypted_api_keys(username: str):\n    user = dbusers.find_one({\"_id\": username})\n    if not user: return None, None\n    gemini_plain = decrypt_text(user.get(\"gemini_api_key\", \"\"))\n    ollama_plain = user.get(\"ollama_base_url\", \"\")\n    gpt_plain = decrypt_text(user.get(\"gpt_api_key\", \"\"))\n    return gemini_plain, ollama_plain, gpt_plain",
                        "start_line": 96,
                        "end_line": 102,
                        "context": {
                            "calls": [
                                "database.db.decrypt_text"
                            ],
                            "called_by": [
                                "frontend.frontend"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "database.db.insert_chat",
                        "name": "insert_chat",
                        "args": [
                            "username",
                            "chat_name"
                        ],
                        "docstring": "Erstellt einen neuen Chat-Eintrag.",
                        "source_code": "def insert_chat(username: str, chat_name: str):\n    \"\"\"Erstellt einen neuen Chat-Eintrag.\"\"\"\n    chat = {\n        \"_id\": str(uuid.uuid4()),\n        \"username\": username,\n        \"chat_name\": chat_name,\n        \"created_at\": datetime.now()\n    }\n    result = dbchats.insert_one(chat)\n    return result.inserted_id",
                        "start_line": 108,
                        "end_line": 117,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "frontend.frontend",
                                "frontend.frontend.handle_delete_chat",
                                "frontend.frontend.load_data_from_db"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "database.db.fetch_chats_by_user",
                        "name": "fetch_chats_by_user",
                        "args": [
                            "username"
                        ],
                        "docstring": "Holt alle definierten Chats eines Users.",
                        "source_code": "def fetch_chats_by_user(username: str):\n    \"\"\"Holt alle definierten Chats eines Users.\"\"\"\n    # Sortieren nach Erstellung macht Sinn\n    chats = list(dbchats.find({\"username\": username}).sort(\"created_at\", 1))\n    return chats",
                        "start_line": 119,
                        "end_line": 123,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "frontend.frontend.load_data_from_db"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "database.db.check_chat_exists",
                        "name": "check_chat_exists",
                        "args": [
                            "username",
                            "chat_name"
                        ],
                        "docstring": null,
                        "source_code": "def check_chat_exists(username: str, chat_name: str):\n    return dbchats.find_one({\"username\": username, \"chat_name\": chat_name}) is not None",
                        "start_line": 125,
                        "end_line": 126,
                        "context": {
                            "calls": [],
                            "called_by": []
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "database.db.rename_chat_fully",
                        "name": "rename_chat_fully",
                        "args": [
                            "username",
                            "old_name",
                            "new_name"
                        ],
                        "docstring": "Benennt einen Chat und alle zugehörigen Exchanges um.",
                        "source_code": "def rename_chat_fully(username: str, old_name: str, new_name: str):\n    \"\"\"\n    Benennt einen Chat und alle zugehörigen Exchanges um.\n    \"\"\"\n    # 1. Chat-Eintrag umbenennen\n    result = dbchats.update_one(\n        {\"username\": username, \"chat_name\": old_name}, \n        {\"$set\": {\"chat_name\": new_name}}\n    )\n    \n    # 2. Alle Messages (Exchanges) umhängen\n    dbexchanges.update_many(\n        {\"username\": username, \"chat_name\": old_name},\n        {\"$set\": {\"chat_name\": new_name}}\n    )\n    return result.modified_count",
                        "start_line": 129,
                        "end_line": 144,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "frontend.frontend"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "database.db.insert_exchange",
                        "name": "insert_exchange",
                        "args": [
                            "question",
                            "answer",
                            "feedback",
                            "username",
                            "chat_name",
                            "helper_used",
                            "main_used",
                            "total_time",
                            "helper_time",
                            "main_time",
                            "json_tokens",
                            "toon_tokens",
                            "savings_percent"
                        ],
                        "docstring": null,
                        "source_code": "def insert_exchange(question: str, answer: str, feedback: str, username: str, chat_name: str, \n                    helper_used: str=\"\", main_used: str=\"\", total_time: str=\"\", helper_time: str=\"\", main_time: str=\"\",\n                    json_tokens=0, toon_tokens=0, savings_percent=0.0):\n    new_id = str(uuid.uuid4())\n    exchange = {\n        \"_id\": new_id,\n        \"question\": question,\n        \"answer\": answer,\n        \"feedback\": feedback,\n        \"feedback_message\": \"\",\n        \"chat_name\": chat_name,\n        \"username\": username,\n        \"helper_used\": helper_used,\n        \"main_used\": main_used,\n        \"total_time\": total_time,\n        \"helper_time\": helper_time,\n        \"main_time\": main_time,\n        \"json_tokens\": json_tokens,\n        \"toon_tokens\": toon_tokens,\n        \"savings_percent\": savings_percent,\n        \"created_at\": datetime.now()\n    }\n    try:\n        dbexchanges.insert_one(exchange)\n        return new_id\n    except Exception as e:\n        print(f\"DB Error: {e}\")\n        return None",
                        "start_line": 150,
                        "end_line": 177,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "frontend.frontend"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "database.db.fetch_exchanges_by_user",
                        "name": "fetch_exchanges_by_user",
                        "args": [
                            "username"
                        ],
                        "docstring": null,
                        "source_code": "def fetch_exchanges_by_user(username: str):\n    # Sortieren nach Zeitstempel wichtig für die Anzeige\n    exchanges = list(dbexchanges.find({\"username\": username}).sort(\"created_at\", 1))\n    return exchanges",
                        "start_line": 179,
                        "end_line": 182,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "frontend.frontend.load_data_from_db"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "database.db.fetch_exchanges_by_chat",
                        "name": "fetch_exchanges_by_chat",
                        "args": [
                            "username",
                            "chat_name"
                        ],
                        "docstring": null,
                        "source_code": "def fetch_exchanges_by_chat(username: str, chat_name: str):\n    exchanges = list(dbexchanges.find({\"username\": username, \"chat_name\": chat_name}).sort(\"created_at\", 1))\n    return exchanges",
                        "start_line": 184,
                        "end_line": 186,
                        "context": {
                            "calls": [],
                            "called_by": []
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "database.db.update_exchange_feedback",
                        "name": "update_exchange_feedback",
                        "args": [
                            "exchange_id",
                            "feedback"
                        ],
                        "docstring": null,
                        "source_code": "def update_exchange_feedback(exchange_id, feedback: int):\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback\": feedback}})\n    return result.modified_count",
                        "start_line": 188,
                        "end_line": 190,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "frontend.frontend.handle_feedback_change"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "database.db.update_exchange_feedback_message",
                        "name": "update_exchange_feedback_message",
                        "args": [
                            "exchange_id",
                            "feedback_message"
                        ],
                        "docstring": null,
                        "source_code": "def update_exchange_feedback_message(exchange_id, feedback_message: str):\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback_message\": feedback_message}})\n    return result.modified_count",
                        "start_line": 192,
                        "end_line": 194,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "frontend.frontend.render_exchange"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "database.db.delete_exchange_by_id",
                        "name": "delete_exchange_by_id",
                        "args": [
                            "exchange_id"
                        ],
                        "docstring": null,
                        "source_code": "def delete_exchange_by_id(exchange_id: str):\n    result = dbexchanges.delete_one({\"_id\": exchange_id})\n    return result.deleted_count",
                        "start_line": 196,
                        "end_line": 198,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "frontend.frontend.handle_delete_exchange"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "database.db.delete_full_chat",
                        "name": "delete_full_chat",
                        "args": [
                            "username",
                            "chat_name"
                        ],
                        "docstring": "Löscht den Chat UND alle zugehörigen Exchanges.\nDas sorgt für Konsistenz zwischen Frontend und Backend.",
                        "source_code": "def delete_full_chat(username: str, chat_name: str):\n    \"\"\"\n    Löscht den Chat UND alle zugehörigen Exchanges.\n    Das sorgt für Konsistenz zwischen Frontend und Backend.\n    \"\"\"\n    # 1. Alle Nachrichten in diesem Chat löschen\n    del_exchanges = dbexchanges.delete_many({\"username\": username, \"chat_name\": chat_name})\n    \n    # 2. Den Chat selbst aus der Chat-Liste löschen\n    del_chat = dbchats.delete_one({\"username\": username, \"chat_name\": chat_name})\n    \n    return del_chat.deleted_count",
                        "start_line": 202,
                        "end_line": 213,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "frontend.frontend.handle_delete_chat"
                            ]
                        }
                    }
                ],
                "classes": []
            }
        },
        "frontend/frontend.py": {
            "ast_nodes": {
                "imports": [
                    "numpy",
                    "datetime.datetime",
                    "time",
                    "pymongo.MongoClient",
                    "dotenv.load_dotenv",
                    "os",
                    "sys",
                    "urllib.parse.urlparse",
                    "logging",
                    "traceback",
                    "re",
                    "streamlit_mermaid.st_mermaid",
                    "backend.main",
                    "database.db",
                    "streamlit",
                    "streamlit_authenticator"
                ],
                "functions": [
                    {
                        "mode": "function_analysis",
                        "identifier": "frontend.frontend.clean_names",
                        "name": "clean_names",
                        "args": [
                            "model_list"
                        ],
                        "docstring": null,
                        "source_code": "def clean_names(model_list):\n    return [m.split(\"/\")[-1] for m in model_list]",
                        "start_line": 54,
                        "end_line": 55,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "frontend.frontend"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "frontend.frontend.get_filtered_models",
                        "name": "get_filtered_models",
                        "args": [
                            "source_list",
                            "category_name"
                        ],
                        "docstring": "Filtert eine Liste basierend auf der gewählten Kategorie.",
                        "source_code": "def get_filtered_models(source_list, category_name):\n    \"\"\"Filtert eine Liste basierend auf der gewählten Kategorie.\"\"\"\n    keywords = CATEGORY_KEYWORDS.get(category_name, [\"\"])\n    \n    if \"STANDARD\" in keywords:\n        # Nur Modelle zurückgeben, die auch in der Standard-Liste sind\n        return [m for m in source_list if m in STANDARD_MODELS]\n    \n    filtered = []\n    for model in source_list:\n        # Prüfen ob ein Keyword im Namen steckt\n        if any(k in model.lower() for k in keywords):\n            filtered.append(model)\n            \n    return filtered if filtered else source_list",
                        "start_line": 83,
                        "end_line": 97,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "frontend.frontend"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "frontend.frontend.save_gemini_cb",
                        "name": "save_gemini_cb",
                        "args": [],
                        "docstring": null,
                        "source_code": "def save_gemini_cb():\n    new_key = st.session_state.get(\"in_gemini_key\", \"\")\n    if new_key:\n        db.update_gemini_key(st.session_state[\"username\"], new_key)\n        st.session_state[\"in_gemini_key\"] = \"\"\n        st.toast(\"Gemini Key erfolgreich gespeichert! ✅\")",
                        "start_line": 139,
                        "end_line": 144,
                        "context": {
                            "calls": [
                                "database.db.update_gemini_key"
                            ],
                            "called_by": []
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "frontend.frontend.save_ollama_cb",
                        "name": "save_ollama_cb",
                        "args": [],
                        "docstring": null,
                        "source_code": "def save_ollama_cb():\n    new_url = st.session_state.get(\"in_ollama_url\", \"\")\n    if new_url:\n        db.update_ollama_url(st.session_state[\"username\"], new_url)\n        st.toast(\"Ollama URL gespeichert! ✅\")",
                        "start_line": 146,
                        "end_line": 150,
                        "context": {
                            "calls": [
                                "database.db.update_ollama_url"
                            ],
                            "called_by": []
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "frontend.frontend.load_data_from_db",
                        "name": "load_data_from_db",
                        "args": [
                            "username"
                        ],
                        "docstring": "Lädt Chats und Exchanges konsistent aus der DB.",
                        "source_code": "def load_data_from_db(username: str):\n    \"\"\"Lädt Chats und Exchanges konsistent aus der DB.\"\"\"\n    \n    # Nur laden, wenn neuer User oder noch nicht geladen\n    if \"loaded_user\" not in st.session_state or st.session_state.loaded_user != username:\n        st.session_state.chats = {}\n        \n        # 1. Erst die definierten Chats laden (damit auch leere Chats da sind)\n        db_chats = db.fetch_chats_by_user(username)\n        for c in db_chats:\n            c_name = c.get(\"chat_name\")\n            if c_name:\n                st.session_state.chats[c_name] = {\"exchanges\": []}\n\n        # 2. Dann die Exchanges laden und einsortieren\n        db_exchanges = db.fetch_exchanges_by_user(username)\n        for ex in db_exchanges:\n            c_name = ex.get(\"chat_name\", \"Unbenannt\")\n            \n            # Falls Exchanges existieren für Chats, die nicht in dbchats sind (Legacy Support)\n            if c_name not in st.session_state.chats:\n                st.session_state.chats[c_name] = {\"exchanges\": []}\n            \n            if \"feedback\" not in ex or ex[\"feedback\"] is None:\n                ex[\"feedback\"] = np.nan\n            \n            st.session_state.chats[c_name][\"exchanges\"].append(ex)\n\n        # 3. Default Chat erstellen, falls gar nichts existiert\n        if not st.session_state.chats:\n            initial_name = \"Chat 1\"\n            # Konsistent in DB anlegen\n            db.insert_chat(username, initial_name)\n            st.session_state.chats[initial_name] = {\"exchanges\": []}\n            st.session_state.active_chat = initial_name\n        else:\n            # Active Chat setzen, falls nötig\n            if \"active_chat\" not in st.session_state or st.session_state.active_chat not in st.session_state.chats:\n                # Nimm den ersten verfügbaren Chat\n                st.session_state.active_chat = list(st.session_state.chats.keys())[0]\n        \n        st.session_state.loaded_user = username",
                        "start_line": 156,
                        "end_line": 197,
                        "context": {
                            "calls": [
                                "database.db.fetch_chats_by_user",
                                "database.db.fetch_exchanges_by_user",
                                "database.db.insert_chat"
                            ],
                            "called_by": [
                                "frontend.frontend"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "frontend.frontend.handle_feedback_change",
                        "name": "handle_feedback_change",
                        "args": [
                            "ex",
                            "val"
                        ],
                        "docstring": null,
                        "source_code": "def handle_feedback_change(ex, val):\n    ex[\"feedback\"] = val\n    db.update_exchange_feedback(ex[\"_id\"], val)\n    st.rerun()",
                        "start_line": 203,
                        "end_line": 206,
                        "context": {
                            "calls": [
                                "database.db.update_exchange_feedback"
                            ],
                            "called_by": [
                                "frontend.frontend.render_exchange"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "frontend.frontend.handle_delete_exchange",
                        "name": "handle_delete_exchange",
                        "args": [
                            "chat_name",
                            "ex"
                        ],
                        "docstring": null,
                        "source_code": "def handle_delete_exchange(chat_name, ex):\n    db.delete_exchange_by_id(ex[\"_id\"])\n    if chat_name in st.session_state.chats:\n        if ex in st.session_state.chats[chat_name][\"exchanges\"]:\n            st.session_state.chats[chat_name][\"exchanges\"].remove(ex)\n    st.rerun()",
                        "start_line": 208,
                        "end_line": 213,
                        "context": {
                            "calls": [
                                "database.db.delete_exchange_by_id"
                            ],
                            "called_by": [
                                "frontend.frontend.render_exchange"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "frontend.frontend.handle_delete_chat",
                        "name": "handle_delete_chat",
                        "args": [
                            "username",
                            "chat_name"
                        ],
                        "docstring": null,
                        "source_code": "def handle_delete_chat(username, chat_name):\n    # KONSISTENZ: Ruft jetzt delete_full_chat in DB auf\n    db.delete_full_chat(username, chat_name)\n    \n    # State bereinigen\n    if chat_name in st.session_state.chats:\n        del st.session_state.chats[chat_name]\n    \n    # Active Chat neu setzen\n    if len(st.session_state.chats) > 0:\n        st.session_state.active_chat = list(st.session_state.chats.keys())[0]\n    else:\n        # Wenn alles weg ist, neuen leeren Chat anlegen\n        new_name = \"Chat 1\"\n        db.insert_chat(username, new_name)\n        st.session_state.chats[new_name] = {\"exchanges\": []}\n        st.session_state.active_chat = new_name\n        \n    st.rerun()",
                        "start_line": 215,
                        "end_line": 233,
                        "context": {
                            "calls": [
                                "database.db.delete_full_chat",
                                "database.db.insert_chat"
                            ],
                            "called_by": [
                                "frontend.frontend"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "frontend.frontend.extract_repo_name",
                        "name": "extract_repo_name",
                        "args": [
                            "text"
                        ],
                        "docstring": null,
                        "source_code": "def extract_repo_name(text):\n    url_match = re.search(r'(https?://[^\\s]+)', text)\n    if url_match:\n        url = url_match.group(0)\n        parsed = urlparse(url)\n        path = parsed.path.strip(\"/\")\n        if path:\n            repo_name = path.split(\"/\")[-1]\n            if repo_name.endswith(\".git\"):\n                repo_name = repo_name[:-4]\n            return repo_name\n    return None",
                        "start_line": 239,
                        "end_line": 250,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "frontend.frontend"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "frontend.frontend.stream_text_generator",
                        "name": "stream_text_generator",
                        "args": [
                            "text"
                        ],
                        "docstring": null,
                        "source_code": "def stream_text_generator(text):\n    for word in text.split(\" \"):\n        yield word + \" \"\n        time.sleep(0.01)",
                        "start_line": 252,
                        "end_line": 255,
                        "context": {
                            "calls": [],
                            "called_by": [
                                "frontend.frontend.render_text_with_mermaid"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "frontend.frontend.render_text_with_mermaid",
                        "name": "render_text_with_mermaid",
                        "args": [
                            "markdown_text",
                            "should_stream"
                        ],
                        "docstring": null,
                        "source_code": "def render_text_with_mermaid(markdown_text, should_stream=False):\n    if not markdown_text:\n        return\n\n    parts = re.split(r\"```mermaid\\s+(.*?)\\s+```\", markdown_text, flags=re.DOTALL)\n\n    for i, part in enumerate(parts):\n        if i % 2 == 0:\n            if part.strip():\n                if should_stream:\n                    st.write_stream(stream_text_generator(part))\n                else:\n                    st.markdown(part)\n        else:\n            try:\n                st_mermaid(part, key=f\"mermaid_{hash(part)}_{i}\")\n            except Exception:\n                st.code(part, language=\"mermaid\")",
                        "start_line": 257,
                        "end_line": 274,
                        "context": {
                            "calls": [
                                "frontend.frontend.stream_text_generator"
                            ],
                            "called_by": [
                                "frontend.frontend",
                                "frontend.frontend.render_exchange"
                            ]
                        }
                    },
                    {
                        "mode": "function_analysis",
                        "identifier": "frontend.frontend.render_exchange",
                        "name": "render_exchange",
                        "args": [
                            "ex",
                            "current_chat_name"
                        ],
                        "docstring": null,
                        "source_code": "def render_exchange(ex, current_chat_name):\n    st.chat_message(\"user\").write(ex[\"question\"])\n\n    with st.chat_message(\"assistant\"):\n        answer_text = ex.get(\"answer\", \"\")\n        is_error = answer_text.startswith(\"Fehler\") or answer_text.startswith(\"Error\")\n\n        # --- 1. TOOLBAR CONTAINER ---\n        # Nutzt die neuen Parameter horizontal=True und alignment\n        # border=True rahmt die Toolbar ein\n        # horizontal_alignment=\"right\" schiebt alles nach rechts\n        with st.container(horizontal=True, horizontal_alignment=\"right\"):\n            \n            if not is_error:\n                # 1. Status Text (wird jetzt links von den Buttons angezeigt, aber rechtsbündig im Container)\n                st.write(\"hier die Antwort:\")\n                if ex.get(\"feedback\") == 1:\n                    st.caption(\"✅ Hilfreich\")\n                elif ex.get(\"feedback\") == 0:\n                    st.caption(\"❌ Nicht hilfreich\")\n                \n                # 2. Die Buttons (einfach untereinander im Code, erscheinen nebeneinander im UI)\n                \n                # Like\n                type_primary_up = ex.get(\"feedback\") == 1\n                if st.button(\"👍\", key=f\"up_{ex['_id']}\", type=\"primary\" if type_primary_up else \"secondary\", help=\"Hilfreich\"):\n                    handle_feedback_change(ex, 1)\n\n                # Dislike\n                type_primary_down = ex.get(\"feedback\") == 0\n                if st.button(\"👎\", key=f\"down_{ex['_id']}\", type=\"primary\" if type_primary_down else \"secondary\", help=\"Nicht hilfreich\"):\n                    handle_feedback_change(ex, 0)\n\n                # Comment Popover\n                with st.popover(\"💬\", help=\"Notiz hinzufügen\"):\n                    msg = st.text_area(\"Notiz:\", value=ex.get(\"feedback_message\", \"\"), key=f\"txt_{ex['_id']}\")\n                    if st.button(\"Speichern\", key=f\"save_{ex['_id']}\"):\n                        ex[\"feedback_message\"] = msg\n                        db.update_exchange_feedback_message(ex[\"_id\"], msg)\n                        st.success(\"Gespeichert!\")\n                        time.sleep(0.5)\n                        st.rerun()\n\n                # Download\n                st.download_button(\n                    label=\"📥\",\n                    data=ex[\"answer\"],\n                    file_name=f\"response_{ex['_id']}.md\",\n                    mime=\"text/markdown\",\n                    key=f\"dl_{ex['_id']}\",\n                    help=\"Als Markdown herunterladen\"\n                )\n\n                # Delete\n                if st.button(\"🗑️\", key=f\"del_{ex['_id']}\", help=\"Nachricht löschen\", type=\"secondary\"):\n                    handle_delete_exchange(current_chat_name, ex)\n\n            else:\n                # Fehlerfall\n                st.error(\"⚠️ Fehler\")\n                if st.button(\"🗑️ Löschen\", key=f\"del_err_{ex['_id']}\"):\n                    handle_delete_exchange(current_chat_name, ex)\n\n        # --- 2. CONTENT (Antwort) ---\n        with st.container(height=500, border=True):\n             render_text_with_mermaid(ex[\"answer\"], should_stream=False)",
                        "start_line": 280,
                        "end_line": 345,
                        "context": {
                            "calls": [
                                "database.db.update_exchange_feedback_message",
                                "frontend.frontend.handle_delete_exchange",
                                "frontend.frontend.handle_feedback_change",
                                "frontend.frontend.render_text_with_mermaid"
                            ],
                            "called_by": [
                                "frontend.frontend"
                            ]
                        }
                    }
                ],
                "classes": []
            }
        },
        "schemas/types.py": {
            "ast_nodes": {
                "imports": [
                    "typing.List",
                    "typing.Optional",
                    "typing.Literal",
                    "pydantic.BaseModel",
                    "pydantic.ValidationError"
                ],
                "functions": [],
                "classes": [
                    {
                        "mode": "class_analysis",
                        "identifier": "schemas.types.ParameterDescription",
                        "name": "ParameterDescription",
                        "docstring": "Describes a single parameter of a function.",
                        "source_code": "class ParameterDescription(BaseModel):\n    \"\"\"Describes a single parameter of a function.\"\"\"\n    name: str\n    type: str\n    description: str",
                        "start_line": 6,
                        "end_line": 10,
                        "context": {
                            "dependencies": [],
                            "instantiated_by": [],
                            "method_context": []
                        }
                    },
                    {
                        "mode": "class_analysis",
                        "identifier": "schemas.types.ReturnDescription",
                        "name": "ReturnDescription",
                        "docstring": "Describes the return value of a function.",
                        "source_code": "class ReturnDescription(BaseModel):\n    \"\"\"Describes the return value of a function.\"\"\"\n    name: str\n    type: str\n    description: str",
                        "start_line": 12,
                        "end_line": 16,
                        "context": {
                            "dependencies": [],
                            "instantiated_by": [],
                            "method_context": []
                        }
                    },
                    {
                        "mode": "class_analysis",
                        "identifier": "schemas.types.UsageContext",
                        "name": "UsageContext",
                        "docstring": "Describes the calling context of a function.",
                        "source_code": "class UsageContext(BaseModel):\n    \"\"\"Describes the calling context of a function.\"\"\"\n    calls: str\n    called_by: str",
                        "start_line": 18,
                        "end_line": 21,
                        "context": {
                            "dependencies": [],
                            "instantiated_by": [],
                            "method_context": []
                        }
                    },
                    {
                        "mode": "class_analysis",
                        "identifier": "schemas.types.FunctionDescription",
                        "name": "FunctionDescription",
                        "docstring": "Contains the detailed analysis of a function's purpose and signature.",
                        "source_code": "class FunctionDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a function's purpose and signature.\"\"\"\n    overall: str\n    parameters: List[ParameterDescription]\n    returns: List[ReturnDescription]\n    usage_context: UsageContext",
                        "start_line": 23,
                        "end_line": 28,
                        "context": {
                            "dependencies": [],
                            "instantiated_by": [],
                            "method_context": []
                        }
                    },
                    {
                        "mode": "class_analysis",
                        "identifier": "schemas.types.FunctionAnalysis",
                        "name": "FunctionAnalysis",
                        "docstring": "The main model representing the entire JSON schema for a function.",
                        "source_code": "class FunctionAnalysis(BaseModel):\n    \"\"\"The main model representing the entire JSON schema for a function.\"\"\"\n    identifier: str\n    description: FunctionDescription\n    error: Optional[str] = None",
                        "start_line": 30,
                        "end_line": 34,
                        "context": {
                            "dependencies": [],
                            "instantiated_by": [],
                            "method_context": []
                        }
                    },
                    {
                        "mode": "class_analysis",
                        "identifier": "schemas.types.ConstructorDescription",
                        "name": "ConstructorDescription",
                        "docstring": "Describes the __init__ method of a class.",
                        "source_code": "class ConstructorDescription(BaseModel):\n    \"\"\"Describes the __init__ method of a class.\"\"\"\n    description: str\n    parameters: List[ParameterDescription]",
                        "start_line": 39,
                        "end_line": 42,
                        "context": {
                            "dependencies": [],
                            "instantiated_by": [],
                            "method_context": []
                        }
                    },
                    {
                        "mode": "class_analysis",
                        "identifier": "schemas.types.ClassContext",
                        "name": "ClassContext",
                        "docstring": "Describes the class's external dependencies and primary points of instantiation.",
                        "source_code": "class ClassContext(BaseModel):\n    \"\"\"Describes the class's external dependencies and primary points of instantiation.\"\"\"\n    dependencies: str\n    instantiated_by: str",
                        "start_line": 44,
                        "end_line": 47,
                        "context": {
                            "dependencies": [],
                            "instantiated_by": [],
                            "method_context": []
                        }
                    },
                    {
                        "mode": "class_analysis",
                        "identifier": "schemas.types.ClassDescription",
                        "name": "ClassDescription",
                        "docstring": "Contains the detailed analysis of a class's purpose, constructor, and methods.",
                        "source_code": "class ClassDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a class's purpose, constructor, and methods.\"\"\"\n    overall: str\n    init_method: ConstructorDescription\n    methods: List[FunctionAnalysis]\n    usage_context: ClassContext",
                        "start_line": 49,
                        "end_line": 54,
                        "context": {
                            "dependencies": [],
                            "instantiated_by": [],
                            "method_context": []
                        }
                    },
                    {
                        "mode": "class_analysis",
                        "identifier": "schemas.types.ClassAnalysis",
                        "name": "ClassAnalysis",
                        "docstring": "The main model for the entire JSON schema for a class.",
                        "source_code": "class ClassAnalysis(BaseModel):\n    \"\"\"The main model for the entire JSON schema for a class.\"\"\"\n    identifier: str\n    description: ClassDescription\n    error: Optional[str] = None",
                        "start_line": 56,
                        "end_line": 60,
                        "context": {
                            "dependencies": [],
                            "instantiated_by": [],
                            "method_context": []
                        }
                    },
                    {
                        "mode": "class_analysis",
                        "identifier": "schemas.types.CallInfo",
                        "name": "CallInfo",
                        "docstring": "Represents a specific call event from the relationship analyzer.\nUsed in 'called_by' and 'instantiated_by' lists.",
                        "source_code": "class CallInfo(BaseModel):\n    \"\"\"\n    Represents a specific call event from the relationship analyzer.\n    Used in 'called_by' and 'instantiated_by' lists.\n    \"\"\"\n    file: str\n    function: str  # Name des Aufrufers\n    mode: str      # z.B. 'method', 'function', 'module'\n    line: int",
                        "start_line": 65,
                        "end_line": 73,
                        "context": {
                            "dependencies": [],
                            "instantiated_by": [],
                            "method_context": []
                        }
                    },
                    {
                        "mode": "class_analysis",
                        "identifier": "schemas.types.FunctionContextInput",
                        "name": "FunctionContextInput",
                        "docstring": "Structured context for analyzing a function.",
                        "source_code": "class FunctionContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a function.\"\"\"\n    calls: List[str]\n    called_by: List[CallInfo]",
                        "start_line": 78,
                        "end_line": 81,
                        "context": {
                            "dependencies": [],
                            "instantiated_by": [
                                "backend.main.main_workflow"
                            ],
                            "method_context": []
                        }
                    },
                    {
                        "mode": "class_analysis",
                        "identifier": "schemas.types.FunctionAnalysisInput",
                        "name": "FunctionAnalysisInput",
                        "docstring": "The required input to generate a FunctionAnalysis object.",
                        "source_code": "class FunctionAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a FunctionAnalysis object.\"\"\"\n    mode: Literal[\"function_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: FunctionContextInput",
                        "start_line": 83,
                        "end_line": 89,
                        "context": {
                            "dependencies": [],
                            "instantiated_by": [
                                "backend.main.main_workflow"
                            ],
                            "method_context": []
                        }
                    },
                    {
                        "mode": "class_analysis",
                        "identifier": "schemas.types.MethodContextInput",
                        "name": "MethodContextInput",
                        "docstring": "Structured context for a classes methods",
                        "source_code": "class MethodContextInput(BaseModel):\n    \"\"\"Structured context for a classes methods\"\"\"\n    identifier: str\n    calls: List[str]\n    called_by: List[CallInfo]\n    args: List[str]\n    docstring: Optional[str]",
                        "start_line": 94,
                        "end_line": 100,
                        "context": {
                            "dependencies": [],
                            "instantiated_by": [
                                "backend.main.main_workflow"
                            ],
                            "method_context": []
                        }
                    },
                    {
                        "mode": "class_analysis",
                        "identifier": "schemas.types.ClassContextInput",
                        "name": "ClassContextInput",
                        "docstring": "Structured context for analyzing a class.",
                        "source_code": "class ClassContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a class.\"\"\"\n    dependencies: List[str]\n    instantiated_by: List[CallInfo]\n    method_context: List[MethodContextInput]",
                        "start_line": 102,
                        "end_line": 106,
                        "context": {
                            "dependencies": [],
                            "instantiated_by": [
                                "backend.HelperLLM.main_orchestrator",
                                "backend.main.main_workflow"
                            ],
                            "method_context": []
                        }
                    },
                    {
                        "mode": "class_analysis",
                        "identifier": "schemas.types.ClassAnalysisInput",
                        "name": "ClassAnalysisInput",
                        "docstring": "The required input to generate a ClassAnalysis object.",
                        "source_code": "class ClassAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a ClassAnalysis object.\"\"\"\n    mode: Literal[\"class_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: ClassContextInput",
                        "start_line": 108,
                        "end_line": 114,
                        "context": {
                            "dependencies": [],
                            "instantiated_by": [
                                "backend.HelperLLM.main_orchestrator",
                                "backend.main.main_workflow"
                            ],
                            "method_context": []
                        }
                    }
                ]
            }
        }
    }
}