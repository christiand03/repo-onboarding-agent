{
  "functions": [
    {
      "mode": "function_analysis",
      "identifier": "backend.AST_Schema.path_to_module",
      "source_code": "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    \n    module_path = rel_path.replace(os.path.sep, '.')\n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n    return module_path",
      "imports": [
        "ast",
        "os",
        "getRepo.GitRepository"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.File_Dependency.build_file_dependency_graph",
      "source_code": "def build_file_dependency_graph(filename: str, tree: AST, repo_root: str) -> nx.DiGraph:\n    graph = nx.DiGraph()\n\n    tree_visitor = FileDependencyGraph(filename, repo_root)\n    tree_visitor.visit(tree)\n\n    for caller, callees in tree_visitor.import_dependencies.items():\n        graph.add_node(caller)\n        graph.add_nodes_from(callees)\n        for callee in callees:\n            graph.add_edge(caller, callee)\n    \n    return graph",
      "imports": [
        "networkx",
        "os",
        "ast.Assign",
        "ast.AST",
        "ast.ClassDef",
        "ast.FunctionDef",
        "ast.Import",
        "ast.ImportFrom",
        "ast.Name",
        "ast.NodeVisitor",
        "ast.literal_eval",
        "ast.parse",
        "ast.walk",
        "keyword.iskeyword",
        "pathlib.Path",
        "getRepo.GitRepository",
        "callgraph.make_safe_dot",
        "pathlib.Path"
      ],
      "context": {
        "calls": [
          "backend.File_Dependency.FileDependencyGraph"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.File_Dependency.build_repository_graph",
      "source_code": "def build_repository_graph(repository: GitRepository) -> nx.DiGraph:\n    all_files = repository.get_all_files()\n    repo_root = repository.temp_dir\n    global_graph = nx.DiGraph()\n    filtered_files = [file for file in all_files\n                      if (file.path.endswith(\".py\")\n                      or \"backend\" in file.path)]\n    for file in filtered_files: \n        filename = str(os.path.basename(file.path)).removesuffix(\".py\")\n        tree = parse(file.content)\n        graph = build_file_dependency_graph(filename, tree, repo_root)\n        \n        for node in graph.nodes:\n            global_graph.add_node(node)\n\n        for caller, callee in graph.edges:\n            if callee:\n                global_graph.add_node(callee)\n                global_graph.add_edge(caller, callee)\n    \n    return global_graph",
      "imports": [
        "networkx",
        "os",
        "ast.Assign",
        "ast.AST",
        "ast.ClassDef",
        "ast.FunctionDef",
        "ast.Import",
        "ast.ImportFrom",
        "ast.Name",
        "ast.NodeVisitor",
        "ast.literal_eval",
        "ast.parse",
        "ast.walk",
        "keyword.iskeyword",
        "pathlib.Path",
        "getRepo.GitRepository",
        "callgraph.make_safe_dot",
        "pathlib.Path"
      ],
      "context": {
        "calls": [
          "backend.File_Dependency.build_file_dependency_graph"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.File_Dependency.get_all_temp_files",
      "source_code": "def get_all_temp_files(directory: str) -> list[Path]:\n    root_path = Path(directory).resolve()\n    all_files = [file.relative_to(root_path) for file in root_path.rglob(\"*.py\")]\n\n    return all_files",
      "imports": [
        "networkx",
        "os",
        "ast.Assign",
        "ast.AST",
        "ast.ClassDef",
        "ast.FunctionDef",
        "ast.Import",
        "ast.ImportFrom",
        "ast.Name",
        "ast.NodeVisitor",
        "ast.literal_eval",
        "ast.parse",
        "ast.walk",
        "keyword.iskeyword",
        "pathlib.Path",
        "getRepo.GitRepository",
        "callgraph.make_safe_dot",
        "pathlib.Path"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.HelperLLM.main_orchestrator",
      "source_code": "def main_orchestrator():\n    \"\"\"\n    Dummy Data and processing loop for testing the LLMHelper class.\n    This version is syntactically correct and logically matches the Pydantic models.\n    \"\"\"\n    \n    # --- Step 1: Define the pre-computed analysis for each method ---\n    \n    # Example Input 1: For the 'add_item' function\n    add_item_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"source_code\": \"\"\"def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\\\"\\\"\\\"\n        if not isinstance(quantity, int) or quantity <= 0:\n            raise ValueError(\"Quantity must be a positive integer.\")\n    \n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n        print(f\"Added {quantity} of {item_name}. New total: {self.inventory[item_name]}\")\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"process_shipment\", \"restock_api_endpoint\"]\n        }\n    })\n\n    # Example Input 2: For the 'check_stock' function\n    check_stock_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"source_code\": \"\"\"def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Retrieves the current stock quantity for a given item.\\\"\\\"\\\"\n        stock_level = self.inventory.get(item_name, 0)\n        return stock_level\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"fulfill_order\", \"ui_display_handler\"]\n        }\n    })\n\n    # Example Input 3: For the 'generate_report' function\n    generate_report_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"source_code\": \"\"\"def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a timestamped string summary of the current inventory state.\\\"\\\"\\\"\n        timestamp = datetime.now()\n        report_header = \"Inventory Report as of: {ts}\\\\n\".format(ts=timestamp)\n        report_lines = [report_header]\n        \n        for item, quantity in self.inventory.items():\n            line = \"- {name}: {qty}\".format(name=item, qty=quantity)\n            report_lines.append(line)\n            \n        return \"\\\\n\".join(report_lines)\"\"\",\n        \"imports\": [\"from datetime import datetime\"],\n        \"context\": {\n            \"calls\": [\"datetime.now\", \"str.format\"],\n            \"called_by\": [\"daily_cron_job\", \"admin_dashboard_export\"]\n        }\n    })\n\n    add_item_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"description\": {\n            \"overall\": \"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to add.\"},\n                {\"name\": \"quantity\", \"type\": \"int\", \"description\": \"The number of units to add. Must be a positive integer.\"}\n            ],\n            \"returns\": [],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"process_shipment and restock_api_endpoint\" }\n        },\n        \"error\": None\n    })\n\n    check_stock_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"description\": {\n            \"overall\": \"Retrieves the current stock quantity for a given item.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to check.\"}\n            ],\n            \"returns\": [\n                {\"name\": \"stock_level\", \"type\": \"int\", \"description\": \"The quantity of the item in stock. Returns 0 if the item is not found.\"}\n            ],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"fulfill_order and ui_display_handler\" }\n        },\n        \"error\": None\n    })\n\n    generate_report_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"description\": {\n            \"overall\": \"Generates a timestamped string summary of the current inventory state.\",\n            \"parameters\": [ {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"} ],\n            \"returns\": [\n                {\"name\": \"report\", \"type\": \"str\", \"description\": \"A formatted string detailing all items and their quantities.\"}\n            ],\n            \"usage_context\": { \"calls\": \"datetime.now and str.format\", \"called_by\": \"daily_cron_job and admin_dashboard_export\" }\n        },\n        \"error\": None\n    })\n\n    class_input = ClassAnalysisInput(\n        mode=\"class_analysis\",\n        identifier=\"InventoryManager\",\n        # NOTE: The indentation of this string is now fixed. It starts at column 0.\n        source_code=\"\"\"class InventoryManager:\n    \\\"\\\"\\\"Manages stock levels for products in a warehouse.\\\"\\\"\\\"\n    def __init__(self, warehouse_id: str):\n        self.warehouse_id = warehouse_id\n        self.inventory = {}  # item_name: quantity\n\n    def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds an item to the inventory.\\\"\\\"\\\"\n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n\n    def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Checks the stock of a specific item.\\\"\\\"\\\"\n        return self.inventory.get(item_name, 0)\n\n    def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a summary report of the inventory.\\\"\\\"\\\"\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        report_lines = [f\"Inventory Report for {self.warehouse_id} at {timestamp}\"]\n        if not self.inventory:\n            report_lines.append(\"Inventory is empty.\")\n        else:\n            for item, quantity in self.inventory.items():\n                report_lines.append(f\"- {item}: {quantity}\")\n        return \"\\\\n\".join(report_lines)\n\"\"\",\n        imports=[\"from datetime import datetime\"],\n        context=ClassContextInput(\n            dependencies=[\"datetime\"],\n            instantiated_by=[\"main_app_startup\", \"warehouse_worker_script\"],\n            methods_analysis=[\n                add_item_analysis,\n                check_stock_analysis,\n                generate_report_analysis\n            ]\n        )\n    )\n\n    \n    # The helper methods expect a LIST of inputs, so we wrap our single object in a list.\n    input = [add_item_input, check_stock_input, generate_report_input]\n    # analysis = [add_item_analysis, check_stock_analysis, generate_report_analysis] \n\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    llm_helper = LLMHelper(api_key=GEMINI_API_KEY, function_prompt_path=function_prompt_file, class_prompt_path=class_prompt_file)\n\n    # This will be the final JSON containing all documentation\n    final_documentation = {}\n\n    # The analysis of the methods is already provided in the context.\n    logging.info(\"\\n--- Generating documentation for classes ---\")\n    \n    # The `generate_for_classes` method returns a list of results.\n    analysis_results = llm_helper.generate_for_functions(input)\n\n    # --- Step 4: Process the results ---\n    \n    # We loop through the list of results (even though there's only one in this case).\n    for doc in analysis_results:\n        if doc:\n            logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n            if \"classes\" not in final_documentation:\n                final_documentation[\"classes\"] = {}\n            # Use .model_dump() to convert the Pydantic object back to a dict for JSON serialization\n            final_documentation[\"classes\"][doc.identifier] = doc.model_dump() \n        else:\n            logging.warning(\"Failed to generate doc for a class\")\n\n    # --- Step 5: Display the final aggregated result ---\n    logging.info(\"\\n--- Final Generated Documentation ---\")\n    print(json.dumps(final_documentation, indent=2))",
      "imports": [
        "os",
        "json",
        "logging",
        "time",
        "typing.List",
        "typing.Optional",
        "dotenv.load_dotenv",
        "langchain_google_genai.ChatGoogleGenerativeAI",
        "langchain_ollama.ChatOllama",
        "langchain_openai.ChatOpenAI",
        "langchain_core.messages.HumanMessage",
        "langchain_core.messages.SystemMessage",
        "schemas.types.FunctionAnalysis",
        "schemas.types.ClassAnalysis",
        "schemas.types.FunctionAnalysisInput",
        "schemas.types.ClassAnalysisInput",
        "schemas.types.ClassContextInput"
      ],
      "context": {
        "calls": [
          "backend.HelperLLM.LLMHelper",
          "schemas.types.ClassAnalysisInput",
          "schemas.types.ClassContextInput"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.callgraph.build_callGraph",
      "source_code": "def build_callGraph(tree: ast.AST, filename: str) -> nx.DiGraph:\n    visitor = CallGraph(filename)\n    visitor.visit(tree)\n    graph = visitor.graph\n    own_functions = visitor.function_set\n\n    for caller, callees in visitor.edges.items():\n        if caller in own_functions:\n            graph.add_node(caller)\n        for callee in callees:\n            if callee in own_functions:\n                graph.add_node(callee)\n                graph.add_edge(caller, callee)\n\n    return graph",
      "imports": [
        "ast",
        "networkx",
        "pathlib.Path",
        "getRepo.GitRepository",
        "getRepo.GitRepository"
      ],
      "context": {
        "calls": [
          "backend.callgraph.CallGraph"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.callgraph.make_safe_dot",
      "source_code": "def make_safe_dot(graph: nx.DiGraph, out_path: str):\n    mapping = {}\n    H = graph.copy()\n    for i, n in enumerate(list(graph.nodes())):\n        safe = f\"n{i}\"           \n        mapping[n] = safe\n    H = nx.relabel_nodes(H, mapping)\n    for orig, safe in mapping.items():\n        H.nodes[safe][\"label\"] = orig\n    nx.drawing.nx_pydot.write_dot(H, out_path)",
      "imports": [
        "ast",
        "networkx",
        "pathlib.Path",
        "getRepo.GitRepository",
        "getRepo.GitRepository"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.callgraph.build_filtered_callgraph",
      "source_code": "def build_filtered_callgraph(repo: GitRepository) -> nx.DiGraph:\n    \"\"\"\n    Baut den globalen Call-Graphen und filtert ihn auf selbst geschriebene Funktionen.\n    \"\"\"\n    all_file_objects = repo.get_all_files()\n    own_functions = set()\n    file_trees = {}\n\n    for file in all_file_objects:\n        if not file.path.endswith(\".py\"):\n            continue\n        filename = Path(file.path).stem\n        tree = ast.parse(file.content)\n        file_trees[filename] = tree\n        visitor = CallGraph(filename)\n        visitor.visit(tree)\n        own_functions.update(visitor.function_set)\n\n    # Globalen Call-Graph bauen\n    global_graph = nx.DiGraph()\n    for filename, tree in file_trees.items():\n        visitor = CallGraph(filename)\n        visitor.visit(tree)\n        for caller, callees in visitor.edges.items():\n            if caller not in own_functions:\n                continue \n            for callee in callees:\n                if callee in own_functions:\n                    global_graph.add_edge(caller, callee)\n                    global_graph.add_node(caller)\n                    global_graph.add_node(callee)\n    return global_graph",
      "imports": [
        "ast",
        "networkx",
        "pathlib.Path",
        "getRepo.GitRepository",
        "getRepo.GitRepository"
      ],
      "context": {
        "calls": [
          "backend.callgraph.CallGraph"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.converter.wrap_cdata",
      "source_code": "def wrap_cdata(content):\n    \"\"\"Wraps content in CDATA tags.\"\"\"\n    return f\"<![CDATA[\\n{content}\\n]]>\"",
      "imports": [
        "logging",
        "nbformat",
        "nbformat.reader.NotJSONError"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.converter.extract_output_content",
      "source_code": "def extract_output_content(outputs, image_list):\n    \"\"\"\n    Extracts text and handles images by decoding Base64 to bytes.\n    Returns: A list of text strings or placeholders.\n    \"\"\"\n    extracted_xml_snippets = []\n    \n    for output in outputs:\n\n        if output.output_type in ('display_data', 'execute_result') and hasattr(output, 'data'):\n            data = output.data\n            \n            # Helper to process image\n            def process_image(mime_type):\n                if mime_type in data:\n                    try:\n                        b64_str = data[mime_type]\n                        b64_str = b64_str.replace('\\n', '')\n\n                        image_index = len(image_list)\n                        image_list.append({\n                            \"mime_type\": mime_type,\n                            \"data\": b64_str\n                        })\n\n                        return f'\\n<IMAGE_PLACEHOLDER index=\"{image_index}\" mime=\"{mime_type}\"/>\\n'\n                    except Exception as e:\n                        return f\"<ERROR>Could not decode image: {e}</ERROR>\"\n                return None\n\n            # Ignore jpeg if png is found\n            img_xml = process_image('image/png')\n            if not img_xml:\n                img_xml = process_image('image/jpeg')\n            \n            if img_xml:\n                extracted_xml_snippets.append(img_xml)\n            elif 'text/plain' in data:\n                extracted_xml_snippets.append(data['text/plain'])\n\n        elif output.output_type == 'stream':\n            extracted_xml_snippets.append(output.text)\n            \n        elif output.output_type == 'error':\n            extracted_xml_snippets.append(f\"{output.ename}: {output.evalue}\")\n\n    return extracted_xml_snippets",
      "imports": [
        "logging",
        "nbformat",
        "nbformat.reader.NotJSONError"
      ],
      "context": {
        "calls": [
          "backend.converter.process_image"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.converter.process_image",
      "source_code": "def process_image(mime_type):\n                if mime_type in data:\n                    try:\n                        b64_str = data[mime_type]\n                        b64_str = b64_str.replace('\\n', '')\n\n                        image_index = len(image_list)\n                        image_list.append({\n                            \"mime_type\": mime_type,\n                            \"data\": b64_str\n                        })\n\n                        return f'\\n<IMAGE_PLACEHOLDER index=\"{image_index}\" mime=\"{mime_type}\"/>\\n'\n                    except Exception as e:\n                        return f\"<ERROR>Could not decode image: {e}</ERROR>\"\n                return None",
      "imports": [
        "logging",
        "nbformat",
        "nbformat.reader.NotJSONError"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.converter.convert_notebook_to_xml",
      "source_code": "def convert_notebook_to_xml(file_content):\n    try:\n        nb = nbformat.reads(file_content, as_version=4)\n    except NotJSONError:\n        return \"<ERROR>Could not parse file as JSON/Notebook</ERROR>\", []\n\n    xml_parts = []\n    extracted_images = [] \n\n    for cell in nb.cells:\n        if cell.cell_type == 'markdown':\n            cell_xml = f'<CELL type=\"markdown\">\\n{cell.source}\\n</CELL>'\n            xml_parts.append(cell_xml)\n\n        elif cell.cell_type == 'code':\n            source_code = wrap_cdata(cell.source)\n            xml_parts.append(f'<CELL type=\"code\">\\n{source_code}\\n</CELL>')\n\n            if hasattr(cell, 'outputs') and cell.outputs:\n                snippets = extract_output_content(cell.outputs, extracted_images)\n                \n                content = \"\\n\".join(snippets)\n                \n                if content.strip():\n                    wrapped_output = wrap_cdata(content)\n                    xml_parts.append(f'<CELL type=\"output\">\\n{wrapped_output}\\n</CELL>')\n\n    return \"\\n\\n\".join(xml_parts), extracted_images",
      "imports": [
        "logging",
        "nbformat",
        "nbformat.reader.NotJSONError"
      ],
      "context": {
        "calls": [
          "backend.converter.extract_output_content",
          "backend.converter.wrap_cdata"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.converter.process_repo_notebooks",
      "source_code": "def process_repo_notebooks(repo_files):\n    notebook_files = [f for f in repo_files if f.path.endswith('.ipynb')]\n    logging.info(f\"Found {len(notebook_files)} notebooks.\")\n        \n    results = {}\n\n    for nb_file in notebook_files:\n        logging.info(f\"Processing: {nb_file.path}\")\n\n        xml_output, images = convert_notebook_to_xml(nb_file.content)\n        results[nb_file.path] = {\n            \"xml\": xml_output,\n            \"images\": images\n        }\n            \n    return results",
      "imports": [
        "logging",
        "nbformat",
        "nbformat.reader.NotJSONError"
      ],
      "context": {
        "calls": [
          "backend.converter.convert_notebook_to_xml"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.diagram_generation.emitter.mermaid_id",
      "source_code": "def mermaid_id(name: str) -> str:\n    \"\"\"Mermaid identifier d\u00fcrfen keine Punkte im Namen enthalten.\"\"\"\n    return name.replace(\".\", \"_\")",
      "imports": [
        "diagram_generation.data_types.ModuleSymbol",
        "diagram_generation.data_types.ResolvedCall"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.diagram_generation.generator.analyze_project",
      "source_code": "def analyze_project(all_files: list[RepoFile]):\n    project = ProjectIndex(modules={})\n    trees: dict[str, ast.AST] = {}\n\n    py_files: list[RepoFile] = []\n    for file in all_files:\n        if file.path.endswith(\".py\"):\n            py_files.append(file)\n\n\n    for file in py_files:\n        module_name = file.path.split(\"/\")[-1].removesuffix(\".py\")\n        packages: list[str] = file.path.split(\"/\")[:-1]\n        tree = ast.parse(file.content)\n        attach_with_parents(tree)\n\n        collector = SymbolCollector(module_name, packages)\n        collector.visit(tree)\n\n        project.modules[module_name] = collector.module\n        trees[module_name] = tree\n\n    raw_calls = {}\n\n    for module_name, tree in trees.items():\n        module = project.modules[module_name]\n        visitor = TreeVisitor(module, project)\n        visitor.visit(tree)\n        raw_calls[module_name] = (visitor.calls)\n\n    resolver = CallResolver(project)\n    resolved_calls = resolver.resolve_all(raw_calls)\n\n    return project, resolved_calls",
      "imports": [
        "ast",
        "pathlib.Path",
        "sys",
        "re",
        "getRepo.RepoFile",
        "diagram_generation.call_resolver.CallResolver",
        "diagram_generation.callgraph.TreeVisitor",
        "diagram_generation.data_types.ProjectIndex",
        "diagram_generation.data_types.ResolvedCall",
        "diagram_generation.emitter.MermaidSequenceEmitter",
        "diagram_generation.emitter.MermaidClassDiagramEmitter",
        "diagram_generation.emitter.MermaidOverviewEmitter",
        "diagram_generation.symbol_collector.SymbolCollector",
        "diagram_generation.symbol_collector.attach_with_parents",
        "getRepo.GitRepository",
        "getRepo.RepoFile",
        "pathlib.Path"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.diagram_generation.generator.main_diagram_generation",
      "source_code": "def main_diagram_generation(py_files: list[str]) -> tuple[dict, dict, str]:\n    project, resolved_calls = analyze_project(py_files)\n    call_from_same_function: dict[str, list[ResolvedCall]] = {}\n\n    for res_calls in resolved_calls.values():\n        if len(res_calls) == 0:\n            continue\n        for res_call in res_calls:\n            caller_name = res_call.caller.name\n            if caller_name not in call_from_same_function:\n                call_from_same_function[caller_name] = []\n            call_from_same_function[caller_name].append(res_call)\n\n    class_diagrams = MermaidClassDiagramEmitter().emit(project.modules)\n    component_diagram = MermaidOverviewEmitter().emit(project.modules)\n\n    seqs: dict[str, str] = {}\n    for function, calls in call_from_same_function.items():\n        seq = MermaidSequenceEmitter().emit(calls)\n        seqs[function] = seq\n\n    return seqs, class_diagrams, component_diagram",
      "imports": [
        "ast",
        "pathlib.Path",
        "sys",
        "re",
        "getRepo.RepoFile",
        "diagram_generation.call_resolver.CallResolver",
        "diagram_generation.callgraph.TreeVisitor",
        "diagram_generation.data_types.ProjectIndex",
        "diagram_generation.data_types.ResolvedCall",
        "diagram_generation.emitter.MermaidSequenceEmitter",
        "diagram_generation.emitter.MermaidClassDiagramEmitter",
        "diagram_generation.emitter.MermaidOverviewEmitter",
        "diagram_generation.symbol_collector.SymbolCollector",
        "diagram_generation.symbol_collector.attach_with_parents",
        "getRepo.GitRepository",
        "getRepo.RepoFile",
        "pathlib.Path"
      ],
      "context": {
        "calls": [
          "backend.diagram_generation.generator.analyze_project"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.diagram_generation.generator.enrich_report_with_diagrams",
      "source_code": "def enrich_report_with_diagrams(final_report: str, diagrams: dict, component_diagram: str, class_diagrams: dict) -> str:\n    \"\"\"F\u00fcgt Diagramme aus dem `diagrams`-Dictionary in den `final_report` ein.\"\"\"\n    report_lines = final_report.splitlines()\n    enriched_report = []\n\n    for line in report_lines:\n        enriched_report.append(line)\n        if \"#### Function:\" in line:\n            for filename, seq_diagram in diagrams.items():\n                    if filename in line:\n                        enriched_report.append(f\"   **Sequence diagram for {filename}**\")\n                        enriched_report.append(seq_diagram)\n        \n        if \"## 4. Architecture\" in line:\n            enriched_report.append(component_diagram)\n        \n        \n        if \"#### Class:\" in line:\n            for class_name, class_diagram in class_diagrams.items():\n                if re.search(rf\"\\b{re.escape(class_name)}\\b\", line):\n                    enriched_report.append(class_diagram)\n\n        \n    return \"\\n\".join(enriched_report)",
      "imports": [
        "ast",
        "pathlib.Path",
        "sys",
        "re",
        "getRepo.RepoFile",
        "diagram_generation.call_resolver.CallResolver",
        "diagram_generation.callgraph.TreeVisitor",
        "diagram_generation.data_types.ProjectIndex",
        "diagram_generation.data_types.ResolvedCall",
        "diagram_generation.emitter.MermaidSequenceEmitter",
        "diagram_generation.emitter.MermaidClassDiagramEmitter",
        "diagram_generation.emitter.MermaidOverviewEmitter",
        "diagram_generation.symbol_collector.SymbolCollector",
        "diagram_generation.symbol_collector.attach_with_parents",
        "getRepo.GitRepository",
        "getRepo.RepoFile",
        "pathlib.Path"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.diagram_generation.main.analyze_project",
      "source_code": "def analyze_project(py_files: list[RepoFile]):\n    project = ProjectIndex(modules={})\n    trees: dict[str, ast.AST] = {}\n\n    for file in py_files:\n        module_name = file.path.split(\"/\")[-1].removesuffix(\".py\")\n        packages: list[str] = file.path.split(\"/\")[:-1]\n        tree = ast.parse(file.content)\n        attach_with_parents(tree)\n\n        collector = SymbolCollector(module_name, packages)\n        collector.visit(tree)\n\n        project.modules[module_name] = collector.module\n        trees[module_name] = tree\n\n    raw_calls = {}\n\n    for module_name, tree in trees.items():\n        module = project.modules[module_name]\n        visitor = TreeVisitor(module)\n        visitor.visit(tree)\n        raw_calls[module_name] = (visitor.calls)\n\n    resolver = CallResolver(project)\n    resolved_calls = resolver.resolve_all(raw_calls)\n\n    return project, resolved_calls",
      "imports": [
        "ast",
        "pathlib.Path",
        "sys",
        "getRepo.GitRepository",
        "getRepo.RepoFile",
        "call_resolver.CallResolver",
        "callgraph.TreeVisitor",
        "data_types.ProjectIndex",
        "data_types.ResolvedCall",
        "data_types.FunctionSymbol",
        "emitter.MermaidClassDiagramEmitter",
        "emitter.MermaidSequenceEmitter",
        "symbol_collector.SymbolCollector",
        "symbol_collector.attach_with_parents"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.diagram_generation.main.construct_overview_diagram",
      "source_code": "def construct_overview_diagram(project: ProjectIndex) -> list[list[str]]:\n    packages = []\n    for mod in project.modules.values():\n        packages.extend([mod.overlying_packages])\n\n    overlying_packages: set[str] = set(p[0] for p in packages)\n\n    module_imports = [{m: m.imports.keys() } for m in project.modules.values()]\n                      \n    \n    # sorted_packages = sorted(packages, key=lambda x: len(x), reverse=True)\n    \n    # for idx, p in enumerate(sorted_packages[:]):\n    #     if len(packages[idx + 1]) < len(p):\n    #         packages = packages[:idx + 1]\n\n    return packages",
      "imports": [
        "ast",
        "pathlib.Path",
        "sys",
        "getRepo.GitRepository",
        "getRepo.RepoFile",
        "call_resolver.CallResolver",
        "callgraph.TreeVisitor",
        "data_types.ProjectIndex",
        "data_types.ResolvedCall",
        "data_types.FunctionSymbol",
        "emitter.MermaidClassDiagramEmitter",
        "emitter.MermaidSequenceEmitter",
        "symbol_collector.SymbolCollector",
        "symbol_collector.attach_with_parents"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.diagram_generation.main.main_diagram_generation",
      "source_code": "def main_diagram_generation():\n    # repo_url = \"https://github.com/christiand03/repo-onboarding-agent\"\n    # repo_url = \"https://github.com/brkahmed/taskly\"\n    repo_url = \"https://github.com/pallets/flask\"\n    repo = GitRepository(repo_url)\n    all_files = repo.get_all_files()\n    py_files = [f for f in all_files if f.path.endswith(\".py\")]\n\n\n    project, resolved_calls = analyze_project(py_files)\n        \n    # packages = construct_overview_diagram(project)\n\n    # print(packages)\n    call_from_same_function: dict[FunctionSymbol, list[ResolvedCall]] = {}\n\n    for res_calls in resolved_calls.values():\n        if len(res_calls) == 0:\n            continue\n        for res_call in res_calls:\n            caller_name = res_call.caller.qualname\n            if caller_name not in call_from_same_function:\n                call_from_same_function[caller_name] = []\n            call_from_same_function[caller_name].append(res_call)\n\n    seqs: list[str] = []\n    for calls in call_from_same_function.values():\n        seq = MermaidSequenceEmitter().emit(calls)\n        seqs.append(seq)\n    # cls = MermaidClassDiagramEmitter().emit(project.modules)\n\n    print(\"\\n=== SEQUENCE DIAGRAMS ===\\n\")\n    for s in seqs:\n        with open(\"SequenceDiagrams_Flask_3.md\", \"a\") as f:\n            f.write(f\"{s}\\n\")",
      "imports": [
        "ast",
        "pathlib.Path",
        "sys",
        "getRepo.GitRepository",
        "getRepo.RepoFile",
        "call_resolver.CallResolver",
        "callgraph.TreeVisitor",
        "data_types.ProjectIndex",
        "data_types.ResolvedCall",
        "data_types.FunctionSymbol",
        "emitter.MermaidClassDiagramEmitter",
        "emitter.MermaidSequenceEmitter",
        "symbol_collector.SymbolCollector",
        "symbol_collector.attach_with_parents"
      ],
      "context": {
        "calls": [
          "backend.diagram_generation.main.analyze_project"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.diagram_generation.symbol_collector.attach_with_parents",
      "source_code": "def attach_with_parents(tree: AST) -> None:\n    for parent in walk(tree):\n        for child in iter_child_nodes(parent):\n            child.parent = parent",
      "imports": [
        "ast.AST",
        "ast.AsyncFunctionDef",
        "ast.ClassDef",
        "ast.FunctionDef",
        "ast.Import",
        "ast.ImportFrom",
        "ast.NodeVisitor",
        "ast.Return",
        "ast.iter_child_nodes",
        "ast.parse",
        "ast.walk",
        "diagram_generation.data_types.ModuleSymbol",
        "diagram_generation.data_types.FunctionSymbol",
        "diagram_generation.data_types.ClassSymbol",
        "diagram_generation.callgraph.TreeVisitor",
        "sys",
        "pathlib.Path",
        "getRepo.GitRepository"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.main.create_savings_chart",
      "source_code": "def create_savings_chart(json_tokens, toon_tokens, savings_percent, output_path):\n    \"\"\"Erstellt ein Balkendiagramm f\u00fcr den Token-Vergleich und speichert es.\"\"\"\n    labels = ['JSON', 'TOON']\n    values = [json_tokens, toon_tokens]\n    colors = ['#ff9999', '#66b3ff']\n\n    plt.figure(figsize=(8, 6))\n    bars = plt.bar(labels, values, color=colors, width=0.5)\n\n    # Titel und Beschriftungen\n    plt.title(f'Token-Vergleich: {savings_percent:.2f}% Einsparung', fontsize=14)\n    plt.ylabel('Anzahl Token')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n\n    # Werte \u00fcber den Balken anzeigen\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width() / 2.0, height, \n                 f'{int(height):,}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n\n    # Speichern\n    plt.savefig(output_path)\n    plt.close()",
      "imports": [
        "json",
        "math",
        "logging",
        "os",
        "re",
        "time",
        "datetime.datetime",
        "matplotlib.pyplot",
        "sys",
        "pathlib.Path",
        "dotenv.load_dotenv",
        "diagram_generation.generator",
        "getRepo.GitRepository",
        "AST_Schema.ASTAnalyzer",
        "MainLLM.MainLLM",
        "basic_info.ProjektInfoExtractor",
        "HelperLLM.LLMHelper",
        "relationship_analyzer.ProjectAnalyzer",
        "schemas.types.FunctionContextInput",
        "schemas.types.FunctionAnalysisInput",
        "schemas.types.ClassContextInput",
        "schemas.types.ClassAnalysisInput",
        "schemas.types.MethodContextInput",
        "ptoon.encode",
        "ptoon.estimate_savings",
        "converter.process_repo_notebooks"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.main.calculate_net_time",
      "source_code": "def calculate_net_time(start_time, end_time, total_items, batch_size, model_name):\n    \"\"\"Berechnet die Dauer abz\u00fcglich der Sleep-Zeiten f\u00fcr Rate-Limits.\"\"\"\n    total_duration = end_time - start_time\n    \n    if not model_name.startswith(\"gemini-\"):\n        return total_duration\n\n    if total_items == 0:\n        return 0\n\n    num_batches = math.ceil(total_items / batch_size)\n    sleep_count = max(0, num_batches - 1)\n    total_sleep_time = sleep_count * 61\n    \n    net_time = total_duration - total_sleep_time\n    return max(0, net_time)",
      "imports": [
        "json",
        "math",
        "logging",
        "os",
        "re",
        "time",
        "datetime.datetime",
        "matplotlib.pyplot",
        "sys",
        "pathlib.Path",
        "dotenv.load_dotenv",
        "diagram_generation.generator",
        "getRepo.GitRepository",
        "AST_Schema.ASTAnalyzer",
        "MainLLM.MainLLM",
        "basic_info.ProjektInfoExtractor",
        "HelperLLM.LLMHelper",
        "relationship_analyzer.ProjectAnalyzer",
        "schemas.types.FunctionContextInput",
        "schemas.types.FunctionAnalysisInput",
        "schemas.types.ClassContextInput",
        "schemas.types.ClassAnalysisInput",
        "schemas.types.MethodContextInput",
        "ptoon.encode",
        "ptoon.estimate_savings",
        "converter.process_repo_notebooks"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.main.main_workflow",
      "source_code": "def main_workflow(user_input, api_keys: dict, model_names: dict, status_callback=None, check_stop=None):\n    \n    def update_status(msg):\n        # --- NEU: Pr\u00fcfen ob abgebrochen wurde ---\n        if check_stop and check_stop():\n            logging.info(\"Interrupt empfangen. Breche main_workflow ab.\")\n            raise InterruptedError(\"Analyse wurde vom User gestoppt.\")\n        \n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)\n\n    update_status(\"\ud83d\udd0d Analysiere Input...\")\n    \n    # API Keys & URLs extrahieren\n    gemini_api_key = api_keys.get(\"gemini\")\n    openai_api_key = api_keys.get(\"gpt\")\n    scadsllm_api_key = api_keys.get(\"scadsllm\")\n    scadsllm_base_url = api_keys.get(\"scadsllm_base_url\")\n    ollama_base_url = api_keys.get(\"ollama\")\n    \n    # User-eigene Open Source Konfiguration\n    user_opensrc_key = api_keys.get(\"opensrc_key\")\n    user_opensrc_url = api_keys.get(\"opensrc_url\")\n\n    helper_model = model_names.get(\"helper\", \"gemini-2.0-flash-lite\")\n    main_model = model_names.get(\"main\", \"gemini-2.0-pro\")\n\n    def get_key_and_url(model_name):\n        \"\"\"Bestimmt pro Modell den richtigen Key und die richtige Base URL.\"\"\"\n        if model_name.startswith(\"gpt-\"):\n            return openai_api_key, None\n        \n        if model_name.startswith(\"gemini-\"):\n            # NUR wenn es wirklich mit gemini- anf\u00e4ngt, wird der Gemini Key erzwungen\n            if not gemini_api_key:\n                raise ValueError(f\"Gemini API Key fehlt f\u00fcr Modell {model_name}\")\n            return gemini_api_key, None\n        \n        if model_name == \"llama3\":\n            return None, ollama_base_url\n        \n        # Logik f\u00fcr Open Source / ScadsLLM / Aliasse\n        # Falls der User in der UI einen eigenen OS-Key/URL hinterlegt hat, nimm diesen\n        if user_opensrc_url:\n            return user_opensrc_key, user_opensrc_url\n            \n        # Sonst Fallback auf System-ScadsLLM\n        return scadsllm_api_key, scadsllm_base_url\n\n    # Zuweisung f\u00fcr Helper und Main\n    helper_api_key, helper_base_url = get_key_and_url(helper_model)\n    main_api_key, main_base_url = get_key_and_url(main_model)    \n\n    # Error Handling f\u00fcr fehlende API Keys\n    if not gemini_api_key and \"gemini\" in helper_model:\n        raise ValueError(\"Gemini API Key was not provided in api_keys dictionary.\")\n\n    # URL Extraktion\n    repo_url = None\n    url_pattern = r\"https?://(?:www\\.)?github\\.com/[^\\s]+\"\n    match = re.search(url_pattern, user_input)\n\n    if match:\n        repo_url = match.group(0)\n        logging.info(f\"Extracted repository URL: {repo_url}\")\n    else:\n        raise ValueError(\"Could not find a valid URL in the provided input.\")\n    \n    # Repo klonen und Dateien extrahieren\n    update_status(f\"\u2b07\ufe0f Klone Repository: {repo_url} ...\")\n    \n    repo_files = []\n    local_repo_path = \"\" \n\n    try: \n\n        with GitRepository(repo_url) as repo:\n            repo_files = repo.get_all_files()\n            local_repo_path = repo.temp_dir\n\n            logging.info(f\"Total files retrieved: {len(repo_files)}\")\n\n    except Exception as e:\n        logging.error(f\"Error cloning repository: {e}\")\n        raise \n\n\n    # Extrahiere Basic Infos\n    update_status(\"\u2139\ufe0f Extrahiere Basis-Informationen...\")\n    try:\n        info_extractor = ProjektInfoExtractor()\n        basic_project_info = info_extractor.extrahiere_info(dateien=repo_files, repo_url=repo_url)\n        logging.info(\"Basic project info extracted\")\n    except Exception as e:\n        logging.error(f\"Error extracting basic project info: {e}\")\n        basic_project_info = \"Could not extract basic info.\"\n\n    # Erstelle Repository Dateibaum\n    update_status(\"\ud83c\udf32 Erstelle Repository Dateibaum...\")\n    try:\n        repo_file_tree = repo.get_file_tree()\n        logging.info(\"Repository file tree constructed\")\n    except Exception as e:\n        logging.error(f\"Error constructing repository file tree: {e}\")\n        repo_file_tree = \"Could not create file tree.\"\n\n    # Relationship Analyse durchf\u00fchren\n    update_status(\"\ud83d\udd17 Analysiere Beziehungen (Calls & Instanziierungen)...\")\n    try:\n        rel_analyzer = ProjectAnalyzer(project_root=local_repo_path)\n        rel_analyzer.analyze()\n        \n        raw_relationships = rel_analyzer.get_raw_relationships()\n        \n        logging.info(\"Relationships analyzed.\")\n    except Exception as e:\n        logging.error(f\"Error in relationship analyzer: {e}\")\n        raw_relationships = {\"outgoing\": {}, \"incoming\": {}}\n\n    # Erstelle AST Schema\n    update_status(\"\ud83c\udf33 Erstelle Abstract Syntax Tree (AST)...\")\n    try:        \n        ast_analyzer = ASTAnalyzer()   \n        ast_schema = ast_analyzer.analyze_repository(files=repo_files, repo=repo)\n        logging.info(\"AST schema created\")\n    except Exception as e:\n        logging.error(f\"Error retrieving repository files: {e}\")\n        raise\n\n    # Anreichern des AST Schemas mit Relationship Daten\n    update_status(\"\u2795 Reiche AST mit Beziehungsdaten an...\")            \n    try:   \n        ast_schema = ast_analyzer.merge_relationship_data(ast_schema, raw_relationships)\n        logging.info(\"AST schema created and enriched\")\n\n    except Exception as e:\n        logging.error(f\"Error processing repository: {e}\")\n        raise\n\n    # Vorbereitung der HelperLLM Eingaben\n    update_status(\"\u2699\ufe0f Bereite Daten f\u00fcr Helper LLM vor...\")\n    \n    helper_llm_function_input = []\n    helper_llm_class_input = []\n\n    try:\n        for filename, file_data in ast_schema['files'].items():\n            ast_nodes = file_data.get('ast_nodes', {})\n            imports = ast_nodes.get('imports', [])\n            functions = ast_nodes.get('functions', [])\n            classes = ast_nodes.get('classes', [])\n\n            for function in functions:\n                context = function.get('context', {})\n                \n                raw_called_by = context.get('called_by', [])\n                clean_called_by = [cb for cb in raw_called_by if isinstance(cb, dict)]\n\n                function_context = FunctionContextInput(\n                    calls = context.get('calls', []),\n                    called_by = clean_called_by \n                )\n                \n                function_input = FunctionAnalysisInput(\n                    mode = function.get('mode', 'function_analysis'),\n                    identifier = function.get('identifier'),\n                    source_code = function.get('source_code'),\n                    imports = imports,\n                    context = function_context\n                )\n                \n                helper_llm_function_input.append(function_input)\n\n            for _class in classes:\n                context = _class.get('context', {})\n                \n                method_context_inputs = []\n                for method in context.get('method_context', []):\n                    \n                    raw_method_called_by = method.get('called_by', [])\n                    clean_method_called_by = [cb for cb in raw_method_called_by if isinstance(cb, dict)]\n\n                    method_context_inputs.append(\n                        MethodContextInput(\n                            identifier=method.get('identifier'),\n                            calls=method.get('calls', []),\n                            called_by=clean_method_called_by, \n                            args=method.get('args', []),\n                            docstring=method.get('docstring')\n                        )\n                    )\n\n                raw_instantiated_by = context.get('instantiated_by', [])\n                clean_instantiated_by = [ib for ib in raw_instantiated_by if isinstance(ib, dict)]\n\n                class_context = ClassContextInput(\n                    dependencies = context.get('dependencies', []),\n                    instantiated_by = clean_instantiated_by, \n                    method_context = method_context_inputs\n                )\n\n                class_input = ClassAnalysisInput(\n                    mode = _class.get('mode', 'class_analysis'),\n                    identifier =_class.get('identifier'),\n                    source_code = _class.get('source_code'), \n                    imports = imports, \n                    context = class_context\n                )\n                \n                helper_llm_class_input.append(class_input)\n\n    except Exception as e:\n        logging.error(f\"Error preparing inputs for Helper LLM: {e}\")\n        raise\n    \n    # Initialisiere HelperLLM\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    \n    llm_helper = LLMHelper(\n        api_key=helper_api_key, \n        function_prompt_path=function_prompt_file, \n        class_prompt_path=class_prompt_file,\n        model_name=helper_model,\n        base_url=helper_base_url,\n    )\n    \n    if ollama_base_url:\n        os.environ[\"OLLAMA_BASE_URL\"] = ollama_base_url\n\n    # Initialisiere Ergebniscontainer\n    analysis_results = {}\n    function_analysis_results = []\n    class_analysis_results = []\n\n    # Call HelperLLM f\u00fcr Funktionen\n    update_status(f\"\ud83e\udd16 Helper LLM: Analysiere {len(helper_llm_function_input)} Funktionen ({helper_model})...\")\n\n    try:\n        net_time_func = 0\n        if len(helper_llm_function_input) > 0:\n\n            logging.info(\"\\n--- Generating documentation for Functions ---\")\n            t_start_func = time.time()    \n            function_analysis_results = llm_helper.generate_for_functions(helper_llm_function_input)    \n            t_end_func = time.time()\n            net_time_func = calculate_net_time(t_start_func, t_end_func, len(helper_llm_function_input), llm_helper.batch_size, helper_model)\n\n        if len(function_analysis_results) != 0:\n            for doc in function_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"functions\" not in analysis_results:\n                        analysis_results[\"functions\"] = {}\n                    analysis_results[\"functions\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(\"Failed to generate doc for a function\") \n    except Exception as e:\n        logging.error(f\"Error during Helper LLM function analysis: {e}\")\n        raise\n    \n\n    # Call HelperLLM f\u00fcr Klassen\n    try:\n        net_time_class = 0\n        if len(helper_llm_class_input) > 0:\n            # Rate Limit Sleep f\u00fcr Gemini Modelle\n            if llm_helper.model_name.startswith(\"gemini-\") & (len(helper_llm_function_input) > 0):\n                update_status(\"\ud83d\udca4 Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n                time.sleep(65)\n            \n            update_status(f\"\ud83e\udd16 Helper LLM: Analysiere {len(helper_llm_class_input)} Klassen ({helper_model})...\")\n            \n            logging.info(\"\\n--- Generating documentation for Classes ---\")\n            t_start_class = time.time()\n            class_analysis_results = llm_helper.generate_for_classes(helper_llm_class_input)\n            t_end_class = time.time()\n            net_time_class = calculate_net_time(t_start_class, t_end_class, len(helper_llm_class_input), llm_helper.batch_size, helper_model)\n\n        if len(class_analysis_results) != 0:\n            for doc in class_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"classes\" not in analysis_results:\n                        analysis_results[\"classes\"] = {}\n                    analysis_results[\"classes\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(\"Failed to generate doc for a class\")\n    except Exception as e:\n        logging.error(f\"Error during Helper LLM class analysis: {e}\")\n        raise\n\n    total_helper_time = net_time_func + net_time_class\n\n    diagrams, class_diagrams, component_diagram = generator.main_diagram_generation(repo_files)\n    # MainLLM Input Vorbereitung\n    main_llm_input = {\n        \"basic_info\": basic_project_info,\n        \"file_tree\": repo_file_tree,\n        \"ast_schema\": ast_schema,\n        \"analysis_results\": analysis_results,\n    }\n\n    # Konvertiere Input in Toon Format\n    main_llm_input_toon = encode(main_llm_input)\n    # Token Evaluation\n    savings_data = None\n    try:\n        logging.info(\"--- Evaluierung der Token-Ersparnis ---\")\n        savings_data = estimate_savings(main_llm_input)\n        logging.info(f\"JSON Tokens: {savings_data['json_tokens']}\")\n        logging.info(f\"TOON Tokens: {savings_data['toon_tokens']}\")\n        logging.info(f\"Ersparnis:   {savings_data['savings_percent']:.2f}%\")\n        \n    except Exception as e:\n        logging.warning(f\"Token evaluation could not be performed: {e}\")    \n\n    # prompt_file_mainllm = \"SystemPrompts/SystemPromptMainLLM.txt\"\n    prompt_file_mainllm_toon = \"SystemPrompts/SystemPromptMainLLMToon.txt\"\n    # MainLLM Ausf\u00fchrung\n    main_llm = MainLLM(\n        api_key=main_api_key, \n        prompt_file_path=prompt_file_mainllm_toon,\n        model_name=main_model,\n        base_url=main_base_url,\n    )\n\n    # RPM Limit Sleep f\u00fcr Gemini Modelle\n    if llm_helper.model_name == main_llm.model_name and main_llm.model_name.startswith(\"gemini-\"):\n        time.sleep(65)\n        update_status(\"\ud83d\udca4 Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n\n    # Call MainLLM f\u00fcr finalen Report\n    update_status(f\"\ud83e\udde0 Main LLM: Generiere finalen Report ({main_model})...\")\n    try:\n        total_main_time = 0\n        logging.info(\"\\n--- Generating Final Report ---\")\n        t_start_main = time.time()\n        final_report = main_llm.call_llm(main_llm_input_toon)\n        t_end_main = time.time()\n        total_main_time = t_end_main - t_start_main\n    except Exception as e:\n        logging.error(f\"Error during Main LLM final report generation: {e}\")\n        raise\n\n    enriched_final_report = enrich_report_with_diagrams(final_report, diagrams, component_diagram, class_diagrams)\n    # --- Speichern der Ergebnisse ---\n    output_dir = \"result\"\n    stats_dir = \"Statistics\" # Neuer Ordner\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(stats_dir, exist_ok=True) # Stelle sicher, dass Statistics Ordner existiert\n\n    total_active_time = total_helper_time + total_main_time\n    \n    # Dateiname f\u00fcr Report\n    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M-%S\")\n    report_filename = f\"report_{timestamp}_Helper_{llm_helper.model_name}_MainLLM_{main_llm.model_name}.md\"\n    report_filename = report_filename.replace(\"/\", \"-\")\n    report_filepath: str = os.path.join(output_dir, report_filename)\n    if final_report:\n        with open(report_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(enriched_final_report)\n        logging.info(f\"Final report saved to '{report_filepath}'.\")\n        \n        if savings_data:\n            try:\n                savings_filename = report_filename.replace(\"report_\", \"savings_\").replace(\".md\", \".png\")\n                savings_filepath = os.path.join(stats_dir, savings_filename)\n                \n                logging.info(f\"Erstelle Token-Chart: {savings_filepath}\")\n                create_savings_chart(\n                    json_tokens=savings_data['json_tokens'], \n                    toon_tokens=savings_data['toon_tokens'], \n                    savings_percent=savings_data['savings_percent'],\n                    output_path=savings_filepath\n                )\n            except Exception as chart_error:\n                logging.error(f\"Konnte Diagramm nicht erstellen: {chart_error}\")\n\n    else:\n        final_report = \"Error: Report generation failed or returned empty.\"\n\n    metrics = {\n        \"helper_time\": round(total_helper_time, 2),\n        \"main_time\": round(total_main_time, 2),\n        \"total_time\": round(total_active_time, 2),\n        \"helper_model\": helper_model,\n        \"main_model\": main_model,\n        \"json_tokens\": savings_data['json_tokens'] if savings_data else None,\n        \"toon_tokens\": savings_data['toon_tokens'] if savings_data else None,\n        \"savings_percent\": round(savings_data['savings_percent'], 2) if savings_data else None\n    \n    }\n\n    return {\n        \"report\": enriched_final_report,\n        \"metrics\": metrics\n    }",
      "imports": [
        "json",
        "math",
        "logging",
        "os",
        "re",
        "time",
        "datetime.datetime",
        "matplotlib.pyplot",
        "sys",
        "pathlib.Path",
        "dotenv.load_dotenv",
        "diagram_generation.generator",
        "getRepo.GitRepository",
        "AST_Schema.ASTAnalyzer",
        "MainLLM.MainLLM",
        "basic_info.ProjektInfoExtractor",
        "HelperLLM.LLMHelper",
        "relationship_analyzer.ProjectAnalyzer",
        "schemas.types.FunctionContextInput",
        "schemas.types.FunctionAnalysisInput",
        "schemas.types.ClassContextInput",
        "schemas.types.ClassAnalysisInput",
        "schemas.types.MethodContextInput",
        "ptoon.encode",
        "ptoon.estimate_savings",
        "converter.process_repo_notebooks"
      ],
      "context": {
        "calls": [
          "backend.AST_Schema.ASTAnalyzer",
          "backend.AST_Schema.ASTAnalyzer.analyze_repository",
          "backend.AST_Schema.ASTAnalyzer.merge_relationship_data",
          "backend.HelperLLM.LLMHelper",
          "backend.HelperLLM.LLMHelper.generate_for_classes",
          "backend.HelperLLM.LLMHelper.generate_for_functions",
          "backend.MainLLM.MainLLM",
          "backend.MainLLM.MainLLM.call_llm",
          "backend.basic_info.ProjektInfoExtractor",
          "backend.basic_info.ProjektInfoExtractor.extrahiere_info",
          "backend.diagram_generation.generator.main_diagram_generation",
          "backend.getRepo.GitRepository",
          "backend.main.calculate_net_time",
          "backend.main.create_savings_chart",
          "backend.main.enrich_report_with_diagrams",
          "backend.main.get_key_and_url",
          "backend.main.update_status",
          "backend.relationship_analyzer.ProjectAnalyzer",
          "backend.relationship_analyzer.ProjectAnalyzer.analyze",
          "backend.relationship_analyzer.ProjectAnalyzer.get_raw_relationships",
          "schemas.types.ClassAnalysisInput",
          "schemas.types.ClassContextInput",
          "schemas.types.FunctionAnalysisInput",
          "schemas.types.FunctionContextInput",
          "schemas.types.MethodContextInput"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.main.update_status",
      "source_code": "def update_status(msg):\n        # --- NEU: Pr\u00fcfen ob abgebrochen wurde ---\n        if check_stop and check_stop():\n            logging.info(\"Interrupt empfangen. Breche main_workflow ab.\")\n            raise InterruptedError(\"Analyse wurde vom User gestoppt.\")\n        \n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)",
      "imports": [
        "json",
        "math",
        "logging",
        "os",
        "re",
        "time",
        "datetime.datetime",
        "matplotlib.pyplot",
        "sys",
        "pathlib.Path",
        "dotenv.load_dotenv",
        "diagram_generation.generator",
        "getRepo.GitRepository",
        "AST_Schema.ASTAnalyzer",
        "MainLLM.MainLLM",
        "basic_info.ProjektInfoExtractor",
        "HelperLLM.LLMHelper",
        "relationship_analyzer.ProjectAnalyzer",
        "schemas.types.FunctionContextInput",
        "schemas.types.FunctionAnalysisInput",
        "schemas.types.ClassContextInput",
        "schemas.types.ClassAnalysisInput",
        "schemas.types.MethodContextInput",
        "ptoon.encode",
        "ptoon.estimate_savings",
        "converter.process_repo_notebooks"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.main.get_key_and_url",
      "source_code": "def get_key_and_url(model_name):\n        \"\"\"Bestimmt pro Modell den richtigen Key und die richtige Base URL.\"\"\"\n        if model_name.startswith(\"gpt-\"):\n            return openai_api_key, None\n        \n        if model_name.startswith(\"gemini-\"):\n            # NUR wenn es wirklich mit gemini- anf\u00e4ngt, wird der Gemini Key erzwungen\n            if not gemini_api_key:\n                raise ValueError(f\"Gemini API Key fehlt f\u00fcr Modell {model_name}\")\n            return gemini_api_key, None\n        \n        if model_name == \"llama3\":\n            return None, ollama_base_url\n        \n        # Logik f\u00fcr Open Source / ScadsLLM / Aliasse\n        # Falls der User in der UI einen eigenen OS-Key/URL hinterlegt hat, nimm diesen\n        if user_opensrc_url:\n            return user_opensrc_key, user_opensrc_url\n            \n        # Sonst Fallback auf System-ScadsLLM\n        return scadsllm_api_key, scadsllm_base_url",
      "imports": [
        "json",
        "math",
        "logging",
        "os",
        "re",
        "time",
        "datetime.datetime",
        "matplotlib.pyplot",
        "sys",
        "pathlib.Path",
        "dotenv.load_dotenv",
        "diagram_generation.generator",
        "getRepo.GitRepository",
        "AST_Schema.ASTAnalyzer",
        "MainLLM.MainLLM",
        "basic_info.ProjektInfoExtractor",
        "HelperLLM.LLMHelper",
        "relationship_analyzer.ProjectAnalyzer",
        "schemas.types.FunctionContextInput",
        "schemas.types.FunctionAnalysisInput",
        "schemas.types.ClassContextInput",
        "schemas.types.ClassAnalysisInput",
        "schemas.types.MethodContextInput",
        "ptoon.encode",
        "ptoon.estimate_savings",
        "converter.process_repo_notebooks"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.main.enrich_report_with_diagrams",
      "source_code": "def enrich_report_with_diagrams(final_report: str, diagrams: dict, component_diagram: str, class_diagrams: dict) -> str:\n    \"\"\"F\u00fcgt Diagramme aus dem `diagrams`-Dictionary in den `final_report` ein.\"\"\"\n    report_lines = final_report.splitlines()\n    enriched_report = []\n\n    for line in report_lines:\n        enriched_report.append(line)\n        if \"#### Function:\" in line:\n            for filename, seq_diagram in diagrams.items():\n                    if filename in line:\n                        enriched_report.append(f\"   **Sequence diagram for {filename}**\")\n                        enriched_report.append(seq_diagram)\n        \n        if \"## 4. Architecture\" in line:\n            enriched_report.append(component_diagram)\n        \n        if \"#### Class:\" in line:\n            for class_name, class_diagram in class_diagrams.items():\n                if re.search(rf\"\\b{re.escape(class_name)}\\b\", line):\n                    enriched_report.append(class_diagram)\n\n    return \"\\n\".join(enriched_report)",
      "imports": [
        "json",
        "math",
        "logging",
        "os",
        "re",
        "time",
        "datetime.datetime",
        "matplotlib.pyplot",
        "sys",
        "pathlib.Path",
        "dotenv.load_dotenv",
        "diagram_generation.generator",
        "getRepo.GitRepository",
        "AST_Schema.ASTAnalyzer",
        "MainLLM.MainLLM",
        "basic_info.ProjektInfoExtractor",
        "HelperLLM.LLMHelper",
        "relationship_analyzer.ProjectAnalyzer",
        "schemas.types.FunctionContextInput",
        "schemas.types.FunctionAnalysisInput",
        "schemas.types.ClassContextInput",
        "schemas.types.ClassAnalysisInput",
        "schemas.types.MethodContextInput",
        "ptoon.encode",
        "ptoon.estimate_savings",
        "converter.process_repo_notebooks"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.main.notebook_workflow",
      "source_code": "def notebook_workflow(input, api_keys, model, status_callback=None, check_stop=None):\n    t_start = time.time()\n    final_report = \"keine Notebooks gefunden\"\n    \n    def update_status(msg):\n        # --- NEU: Pr\u00fcfen ob abgebrochen wurde ---\n        if check_stop and check_stop():\n            logging.info(\"Interrupt empfangen. Breche notebook_workflow ab.\")\n            raise InterruptedError(\"Analyse wurde vom User gestoppt.\")\n            \n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)\n\n    # --- API KEY & URL LOGIK (KONSISTENT ZU MAIN_WORKFLOW) ---\n    gemini_api_key = api_keys.get(\"gemini\")\n    openai_api_key = api_keys.get(\"gpt\")\n    scadsllm_api_key = api_keys.get(\"scadsllm\")\n    scadsllm_base_url = api_keys.get(\"scadsllm_base_url\")\n    ollama_base_url = api_keys.get(\"ollama\")\n    \n    # User-eigene Open Source Konfiguration aus der UI\n    user_opensrc_key = api_keys.get(\"opensrc_key\")\n    user_opensrc_url = api_keys.get(\"opensrc_url\")\n\n    input_api_key = None\n    base_url = None\n\n    # Pr\u00e4zise Bestimmung des Providers\n    if model.startswith(\"gpt-\"):\n        input_api_key = openai_api_key\n        base_url = None\n    elif model.startswith(\"gemini-\"):\n        if not gemini_api_key:\n            raise ValueError(f\"Gemini API Key fehlt f\u00fcr Modell {model}\")\n        input_api_key = gemini_api_key\n        base_url = None\n    elif model == \"llama3\":\n        input_api_key = None\n        base_url = ollama_base_url\n    else:\n        # Logik f\u00fcr Open Source / ScadsLLM / Aliasse\n        if user_opensrc_url:\n            input_api_key = user_opensrc_key\n            base_url = user_opensrc_url\n        else:\n            input_api_key = scadsllm_api_key\n            base_url = scadsllm_base_url\n\n    update_status(\"\ud83d\udd0d Analysiere Input...\")\n\n    # URL Extraktion\n    repo_url = None\n    url_pattern = r\"https?://(?:www\\.)?github\\.com/[^\\s]+\"\n    match = re.search(url_pattern, input)\n\n    if match:\n        repo_url = match.group(0)\n    else:\n        raise ValueError(\"Keine g\u00fcltige GitHub-URL im Input gefunden.\")\n    \n    update_status(f\"\u2b07\ufe0f Klone Repository: {repo_url} ...\")\n\n    try: \n        with GitRepository(repo_url) as repo:\n            repo_files = repo.get_all_files()\n            \n            # Extrahiere Basic Infos (w\u00e4hrend Repo noch offen ist)\n            update_status(\"\u2139\ufe0f Extrahiere Basis-Informationen...\")\n            info_extractor = ProjektInfoExtractor()\n            basic_project_info = info_extractor.extrahiere_info(dateien=repo_files, repo_url=repo_url)\n            \n            update_status(\"\ud83d\udd17 Bereite Notebooks vor...\")\n            # convert to XML/Toon (erfordert repo_files)\n            processed_data = process_repo_notebooks(repo_files)\n            \n    except Exception as e:\n        logging.error(f\"Error processing repository: {e}\")\n        raise\n\n    if not processed_data:\n        return {\n            \"report\": \"In diesem Repository wurden keine Jupyter Notebooks (.ipynb) gefunden.\",\n            \"metrics\": {\"total_time\": round(time.time() - t_start, 2), \"main_model\": model}\n        }\n\n    # Initialisiere LLM\n    prompt_file_notebook_llm = \"SystemPrompts/SystemPromptNotebookLLM.txt\"\n    notebook_llm = MainLLM(\n        api_key=input_api_key, \n        prompt_file_path=prompt_file_notebook_llm,\n        model_name=model,\n        base_url=base_url,\n    )\n\n    notebook_reports = []\n    total_notebooks = len(processed_data)\n    \n    # Hilfsfunktion f\u00fcr Gemini-Payload (bleibt gleich)\n    def gemini_payload(info, path, xml, imgs):\n        intro_json = json.dumps({\"basic_info\": info, \"current_notebook_path\": path}, indent=2)\n        payload = [{\"type\": \"text\", \"text\": f\"Context Information:\\n{intro_json}\\n\\nNotebook XML Structure:\\n\"}]\n        pattern = r'(<IMAGE_PLACEHOLDER index=\"(\\d+)\" mime=\"([^\"]+)\"/>)'\n        last_pos = 0\n        for m in re.finditer(pattern, xml):\n            payload.append({\"type\": \"text\", \"text\": xml[last_pos:m.start()]})\n            idx = int(m.group(2))\n            mime = m.group(3)\n            if idx < len(imgs):\n                payload.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:{mime};base64,{imgs[idx]['data']}\"}})\n            last_pos = m.end()\n        payload.append({\"type\": \"text\", \"text\": xml[last_pos:]})\n        return payload\n\n    # Sequentielle Verarbeitung\n    for index, (nb_path, nb_data) in enumerate(processed_data.items(), 1):\n        update_status(f\"\ud83e\udde0 Generiere Report ({index}/{total_notebooks}): {os.path.basename(nb_path)}\")\n        \n        llm_payload = gemini_payload(basic_project_info, nb_path, nb_data['xml'], nb_data['images'])\n\n        try:\n            single_report = notebook_llm.call_llm(llm_payload)\n            if single_report:\n                notebook_reports.append(single_report)\n            else:\n                notebook_reports.append(f\"## Analyse f\u00fcr {os.path.basename(nb_path)}\\nFehler: Keine Antwort vom Modell.\")\n        except Exception as e:\n            logging.error(f\"Fehler bei {nb_path}: {e}\")\n            notebook_reports.append(f\"## Fehler bei {os.path.basename(nb_path)}\\n{str(e)}\")\n\n    final_report = \"\\n\\n---\\n\\n\".join(notebook_reports)\n    \n    # Zeitmessung\n    total_time = time.time() - t_start\n    \n    return {\n        \"report\": final_report,\n        \"metrics\": {\n            \"helper_time\": 0,\n            \"main_time\": round(total_time, 2),\n            \"total_time\": round(total_time, 2),\n            \"helper_model\": \"None\",\n            \"main_model\": model,\n            \"json_tokens\": 0,\n            \"toon_tokens\": 0,\n            \"savings_percent\": 0\n        }\n    }",
      "imports": [
        "json",
        "math",
        "logging",
        "os",
        "re",
        "time",
        "datetime.datetime",
        "matplotlib.pyplot",
        "sys",
        "pathlib.Path",
        "dotenv.load_dotenv",
        "diagram_generation.generator",
        "getRepo.GitRepository",
        "AST_Schema.ASTAnalyzer",
        "MainLLM.MainLLM",
        "basic_info.ProjektInfoExtractor",
        "HelperLLM.LLMHelper",
        "relationship_analyzer.ProjectAnalyzer",
        "schemas.types.FunctionContextInput",
        "schemas.types.FunctionAnalysisInput",
        "schemas.types.ClassContextInput",
        "schemas.types.ClassAnalysisInput",
        "schemas.types.MethodContextInput",
        "ptoon.encode",
        "ptoon.estimate_savings",
        "converter.process_repo_notebooks"
      ],
      "context": {
        "calls": [
          "backend.MainLLM.MainLLM",
          "backend.MainLLM.MainLLM.call_llm",
          "backend.basic_info.ProjektInfoExtractor",
          "backend.basic_info.ProjektInfoExtractor.extrahiere_info",
          "backend.converter.process_repo_notebooks",
          "backend.getRepo.GitRepository",
          "backend.main.gemini_payload",
          "backend.main.update_status"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.main.update_status",
      "source_code": "def update_status(msg):\n        # --- NEU: Pr\u00fcfen ob abgebrochen wurde ---\n        if check_stop and check_stop():\n            logging.info(\"Interrupt empfangen. Breche notebook_workflow ab.\")\n            raise InterruptedError(\"Analyse wurde vom User gestoppt.\")\n            \n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)",
      "imports": [
        "json",
        "math",
        "logging",
        "os",
        "re",
        "time",
        "datetime.datetime",
        "matplotlib.pyplot",
        "sys",
        "pathlib.Path",
        "dotenv.load_dotenv",
        "diagram_generation.generator",
        "getRepo.GitRepository",
        "AST_Schema.ASTAnalyzer",
        "MainLLM.MainLLM",
        "basic_info.ProjektInfoExtractor",
        "HelperLLM.LLMHelper",
        "relationship_analyzer.ProjectAnalyzer",
        "schemas.types.FunctionContextInput",
        "schemas.types.FunctionAnalysisInput",
        "schemas.types.ClassContextInput",
        "schemas.types.ClassAnalysisInput",
        "schemas.types.MethodContextInput",
        "ptoon.encode",
        "ptoon.estimate_savings",
        "converter.process_repo_notebooks"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.main.gemini_payload",
      "source_code": "def gemini_payload(info, path, xml, imgs):\n        intro_json = json.dumps({\"basic_info\": info, \"current_notebook_path\": path}, indent=2)\n        payload = [{\"type\": \"text\", \"text\": f\"Context Information:\\n{intro_json}\\n\\nNotebook XML Structure:\\n\"}]\n        pattern = r'(<IMAGE_PLACEHOLDER index=\"(\\d+)\" mime=\"([^\"]+)\"/>)'\n        last_pos = 0\n        for m in re.finditer(pattern, xml):\n            payload.append({\"type\": \"text\", \"text\": xml[last_pos:m.start()]})\n            idx = int(m.group(2))\n            mime = m.group(3)\n            if idx < len(imgs):\n                payload.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:{mime};base64,{imgs[idx]['data']}\"}})\n            last_pos = m.end()\n        payload.append({\"type\": \"text\", \"text\": xml[last_pos:]})\n        return payload",
      "imports": [
        "json",
        "math",
        "logging",
        "os",
        "re",
        "time",
        "datetime.datetime",
        "matplotlib.pyplot",
        "sys",
        "pathlib.Path",
        "dotenv.load_dotenv",
        "diagram_generation.generator",
        "getRepo.GitRepository",
        "AST_Schema.ASTAnalyzer",
        "MainLLM.MainLLM",
        "basic_info.ProjektInfoExtractor",
        "HelperLLM.LLMHelper",
        "relationship_analyzer.ProjectAnalyzer",
        "schemas.types.FunctionContextInput",
        "schemas.types.FunctionAnalysisInput",
        "schemas.types.ClassContextInput",
        "schemas.types.ClassAnalysisInput",
        "schemas.types.MethodContextInput",
        "ptoon.encode",
        "ptoon.estimate_savings",
        "converter.process_repo_notebooks"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "backend.relationship_analyzer.path_to_module",
      "source_code": "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    module_path = rel_path.replace(os.path.sep, '.')\n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n    return module_path",
      "imports": [
        "ast",
        "os",
        "logging",
        "collections.defaultdict"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.encrypt_text",
      "source_code": "def encrypt_text(text: str) -> str:\n    if not text or not cipher_suite: \n        return text\n    return cipher_suite.encrypt(text.strip().encode()).decode()",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.decrypt_text",
      "source_code": "def decrypt_text(text: str) -> str:\n    if not text or not cipher_suite: \n        return text\n    try:\n        return cipher_suite.decrypt(text.strip().encode()).decode()\n    except Exception:\n        return text",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.insert_user",
      "source_code": "def insert_user(username: str, name: str, password: str):\n    user = {\n        \"_id\": username,\n        \"name\": name, \n        \"hashed_password\": stauth.Hasher.hash(password),\n        \"gemini_api_key\": \"\",\n        \"ollama_base_url\": \"\",\n        \"gpt_api_key\": \"\"\n    }\n    result = dbusers.insert_one(user)\n    return result.inserted_id",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.fetch_all_users",
      "source_code": "def fetch_all_users():\n    return list(dbusers.find())",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.fetch_user",
      "source_code": "def fetch_user(username: str):\n    return dbusers.find_one({\"_id\": username})",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.update_user_name",
      "source_code": "def update_user_name(username: str, new_name: str):\n    # Achtung: _id kann in Mongo nicht einfach so ge\u00e4ndert werden. \n    # Hier wird nur das Name-Feld geupdated.\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"name\": new_name}})\n    return result.modified_count",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.update_gemini_key",
      "source_code": "def update_gemini_key(username: str, gemini_api_key: str):\n    encrypted_key = encrypt_text(gemini_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"gemini_api_key\": encrypted_key}})\n    return result.modified_count",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [
          "database.db.encrypt_text"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.update_gpt_key",
      "source_code": "def update_gpt_key(username: str, gpt_api_key: str):\n    encrypted_key = encrypt_text(gpt_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"gpt_api_key\": encrypted_key}})\n    return result.modified_count",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [
          "database.db.encrypt_text"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.update_ollama_url",
      "source_code": "def update_ollama_url(username: str, ollama_base_url: str):\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"ollama_base_url\": ollama_base_url.strip()}})\n    return result.modified_count",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.update_opensrc_key",
      "source_code": "def update_opensrc_key(username: str, opensrc_api_key: str):\n    encrypted_key = encrypt_text(opensrc_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"opensrc_api_key\": encrypted_key}})\n    return result.modified_count",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [
          "database.db.encrypt_text"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.update_opensrc_url",
      "source_code": "def update_opensrc_url(username: str, opensrc_base_url: str):\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"opensrc_base_url\": opensrc_base_url.strip()}})\n    return result.modified_count",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.fetch_gemini_key",
      "source_code": "def fetch_gemini_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"gemini_api_key\": 1, \"_id\": 0})\n    return user.get(\"gemini_api_key\") if user else None",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.fetch_ollama_url",
      "source_code": "def fetch_ollama_url(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"ollama_base_url\": 1, \"_id\": 0})\n    return user.get(\"ollama_base_url\") if user else None",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.fetch_gpt_key",
      "source_code": "def fetch_gpt_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"gpt_api_key\": 1, \"_id\": 0})\n    return user.get(\"gpt_api_key\") if user else None",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.fetch_opensrc_key",
      "source_code": "def fetch_opensrc_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"opensrc_api_key\": 1, \"_id\": 0})\n    return user.get(\"opensrc_api_key\") if user else None",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.fetch_opensrc_url",
      "source_code": "def fetch_opensrc_url(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"opensrc_base_url\": 1, \"_id\": 0})\n    return user.get(\"opensrc_base_url\") if user else None",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.delete_user",
      "source_code": "def delete_user(username: str):\n    return dbusers.delete_one({\"_id\": username}).deleted_count",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.get_decrypted_api_keys",
      "source_code": "def get_decrypted_api_keys(username: str):\n    user = dbusers.find_one({\"_id\": username})\n    if not user: \n        return None, None\n    gemini_plain = decrypt_text(user.get(\"gemini_api_key\", \"\"))\n    ollama_plain = user.get(\"ollama_base_url\", \"\")\n    gpt_plain = decrypt_text(user.get(\"gpt_api_key\", \"\"))\n    opensrc_plain = decrypt_text(user.get(\"opensrc_api_key\", \"\"))\n    opensrc_url = user.get(\"opensrc_base_url\", \"\")\n    return gemini_plain, ollama_plain, gpt_plain, opensrc_plain, opensrc_url",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [
          "database.db.decrypt_text"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.insert_chat",
      "source_code": "def insert_chat(username: str, chat_name: str):\n    \"\"\"Erstellt einen neuen Chat-Eintrag.\"\"\"\n    chat = {\n        \"_id\": str(uuid.uuid4()),\n        \"username\": username,\n        \"chat_name\": chat_name,\n        \"created_at\": datetime.now()\n    }\n    result = dbchats.insert_one(chat)\n    return result.inserted_id",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.fetch_chats_by_user",
      "source_code": "def fetch_chats_by_user(username: str):\n    \"\"\"Holt alle definierten Chats eines Users.\"\"\"\n    # Sortieren nach Erstellung macht Sinn\n    chats = list(dbchats.find({\"username\": username}).sort(\"created_at\", 1))\n    return chats",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.check_chat_exists",
      "source_code": "def check_chat_exists(username: str, chat_name: str):\n    return dbchats.find_one({\"username\": username, \"chat_name\": chat_name}) is not None",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.rename_chat_fully",
      "source_code": "def rename_chat_fully(username: str, old_name: str, new_name: str):\n    \"\"\"\n    Benennt einen Chat und alle zugeh\u00f6rigen Exchanges um.\n    \"\"\"\n    # 1. Chat-Eintrag umbenennen\n    result = dbchats.update_one(\n        {\"username\": username, \"chat_name\": old_name}, \n        {\"$set\": {\"chat_name\": new_name}}\n    )\n    \n    # 2. Alle Messages (Exchanges) umh\u00e4ngen\n    dbexchanges.update_many(\n        {\"username\": username, \"chat_name\": old_name},\n        {\"$set\": {\"chat_name\": new_name}}\n    )\n    return result.modified_count",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.insert_exchange",
      "source_code": "def insert_exchange(question: str, answer: str, feedback: str, username: str, chat_name: str, \n                    helper_used: str=\"\", main_used: str=\"\", total_time: str=\"\", helper_time: str=\"\", main_time: str=\"\",\n                    json_tokens=0, toon_tokens=0, savings_percent=0.0):\n    new_id = str(uuid.uuid4())\n    exchange = {\n        \"_id\": new_id,\n        \"question\": question,\n        \"answer\": answer,\n        \"feedback\": feedback,\n        \"feedback_message\": \"\",\n        \"chat_name\": chat_name,\n        \"username\": username,\n        \"helper_used\": helper_used,\n        \"main_used\": main_used,\n        \"total_time\": total_time,\n        \"helper_time\": helper_time,\n        \"main_time\": main_time,\n        \"json_tokens\": json_tokens,\n        \"toon_tokens\": toon_tokens,\n        \"savings_percent\": savings_percent,\n        \"created_at\": datetime.now()\n    }\n    try:\n        dbexchanges.insert_one(exchange)\n        return new_id\n    except Exception as e:\n        print(f\"DB Error: {e}\")\n        return None",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.fetch_exchanges_by_user",
      "source_code": "def fetch_exchanges_by_user(username: str):\n    # Sortieren nach Zeitstempel wichtig f\u00fcr die Anzeige\n    exchanges = list(dbexchanges.find({\"username\": username}).sort(\"created_at\", 1))\n    return exchanges",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.fetch_exchanges_by_chat",
      "source_code": "def fetch_exchanges_by_chat(username: str, chat_name: str):\n    exchanges = list(dbexchanges.find({\"username\": username, \"chat_name\": chat_name}).sort(\"created_at\", 1))\n    return exchanges",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.update_exchange_feedback",
      "source_code": "def update_exchange_feedback(exchange_id, feedback: int):\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback\": feedback}})\n    return result.modified_count",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.update_exchange_feedback_message",
      "source_code": "def update_exchange_feedback_message(exchange_id, feedback_message: str):\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback_message\": feedback_message}})\n    return result.modified_count",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.delete_exchange_by_id",
      "source_code": "def delete_exchange_by_id(exchange_id: str):\n    result = dbexchanges.delete_one({\"_id\": exchange_id})\n    return result.deleted_count",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "database.db.delete_full_chat",
      "source_code": "def delete_full_chat(username: str, chat_name: str):\n    \"\"\"\n    L\u00f6scht den Chat UND alle zugeh\u00f6rigen Exchanges.\n    Das sorgt f\u00fcr Konsistenz zwischen Frontend und Backend.\n    \"\"\"\n    # 1. Alle Nachrichten in diesem Chat l\u00f6schen\n    del_exchanges = dbexchanges.delete_many({\"username\": username, \"chat_name\": chat_name})\n    \n    # 2. Den Chat selbst aus der Chat-Liste l\u00f6schen\n    del_chat = dbchats.delete_one({\"username\": username, \"chat_name\": chat_name})\n    \n    return del_chat.deleted_count",
      "imports": [
        "datetime.datetime",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "streamlit_authenticator",
        "cryptography.fernet.Fernet",
        "uuid",
        "os"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "frontend.frontend.clean_names",
      "source_code": "def clean_names(model_list):\n    return [m.split(\"/\")[-1] for m in model_list]",
      "imports": [
        "uuid",
        "numpy",
        "datetime.datetime",
        "time",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "os",
        "sys",
        "urllib.parse.urlparse",
        "logging",
        "traceback",
        "re",
        "streamlit_mermaid.st_mermaid",
        "backend.main",
        "database.db",
        "streamlit",
        "streamlit_authenticator"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "frontend.frontend.get_filtered_models",
      "source_code": "def get_filtered_models(source_list, category_name):\n    \"\"\"Filtert eine Liste basierend auf der gew\u00e4hlten Kategorie.\"\"\"\n    keywords = CATEGORY_KEYWORDS.get(category_name, [\"\"])\n    \n    if \"STANDARD\" in keywords:\n        return [m for m in source_list if m in STANDARD_MODELS]\n    \n    filtered = []\n    for model in source_list:\n        if any(k in model.lower() for k in keywords):\n            filtered.append(model)\n            \n    return filtered if filtered else source_list",
      "imports": [
        "uuid",
        "numpy",
        "datetime.datetime",
        "time",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "os",
        "sys",
        "urllib.parse.urlparse",
        "logging",
        "traceback",
        "re",
        "streamlit_mermaid.st_mermaid",
        "backend.main",
        "database.db",
        "streamlit",
        "streamlit_authenticator"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "frontend.frontend.handle_abort",
      "source_code": "def handle_abort():\n    st.session_state.abort_requested = True\n    st.toast(\"\ud83d\uded1 Abbruch wird verarbeitet...\")",
      "imports": [
        "uuid",
        "numpy",
        "datetime.datetime",
        "time",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "os",
        "sys",
        "urllib.parse.urlparse",
        "logging",
        "traceback",
        "re",
        "streamlit_mermaid.st_mermaid",
        "backend.main",
        "database.db",
        "streamlit",
        "streamlit_authenticator"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "frontend.frontend.save_gemini_cb",
      "source_code": "def save_gemini_cb():\n    new_key = st.session_state.get(\"in_gemini_key\", \"\")\n    if new_key:\n        db.update_gemini_key(st.session_state[\"username\"], new_key)\n        st.session_state[\"in_gemini_key\"] = \"\"\n        st.toast(\"Gemini Key erfolgreich gespeichert! \u2705\")",
      "imports": [
        "uuid",
        "numpy",
        "datetime.datetime",
        "time",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "os",
        "sys",
        "urllib.parse.urlparse",
        "logging",
        "traceback",
        "re",
        "streamlit_mermaid.st_mermaid",
        "backend.main",
        "database.db",
        "streamlit",
        "streamlit_authenticator"
      ],
      "context": {
        "calls": [
          "database.db.update_gemini_key"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "frontend.frontend.save_ollama_cb",
      "source_code": "def save_ollama_cb():\n    new_url = st.session_state.get(\"in_ollama_url\", \"\")\n    if new_url:\n        db.update_ollama_url(st.session_state[\"username\"], new_url)\n        st.toast(\"Ollama URL gespeichert! \u2705\")",
      "imports": [
        "uuid",
        "numpy",
        "datetime.datetime",
        "time",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "os",
        "sys",
        "urllib.parse.urlparse",
        "logging",
        "traceback",
        "re",
        "streamlit_mermaid.st_mermaid",
        "backend.main",
        "database.db",
        "streamlit",
        "streamlit_authenticator"
      ],
      "context": {
        "calls": [
          "database.db.update_ollama_url"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "frontend.frontend.get_last_activity",
      "source_code": "def get_last_activity(chat_name):\n    \"\"\"Ermittelt den Zeitstempel der letzten Nachricht in einem Chat.\"\"\"\n    chat_data = st.session_state.chats.get(chat_name, {})\n    exchanges = chat_data.get(\"exchanges\", [])\n    \n    creation_dt = chat_data.get(\"created_at\", datetime.min)\n\n    if not exchanges:\n        return creation_dt\n    last_ex = exchanges[-1]\n    dt = last_ex.get(\"datetime\", datetime.min)\n    if isinstance(dt, str):\n        try: \n            dt = datetime.fromisoformat(dt)\n        except: \n            dt = datetime.min\n    return dt",
      "imports": [
        "uuid",
        "numpy",
        "datetime.datetime",
        "time",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "os",
        "sys",
        "urllib.parse.urlparse",
        "logging",
        "traceback",
        "re",
        "streamlit_mermaid.st_mermaid",
        "backend.main",
        "database.db",
        "streamlit",
        "streamlit_authenticator"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "frontend.frontend.load_data_from_db",
      "source_code": "def load_data_from_db(username: str):\n    if \"loaded_user\" not in st.session_state or st.session_state.loaded_user != username:\n        st.session_state.chats = {}\n        db_chats = db.fetch_chats_by_user(username)\n        for c in db_chats:\n            c_name = c.get(\"chat_name\")\n            if c_name:\n                st.session_state.chats[c_name] = {\n                    \"exchanges\": [],\n                    \"created_at\": c.get(\"created_at\", datetime.min)\n                }\n\n        db_exchanges = db.fetch_exchanges_by_user(username)\n        for ex in db_exchanges:\n            c_name = ex.get(\"chat_name\", \"Unbenannt\")\n            if c_name not in st.session_state.chats:\n                st.session_state.chats[c_name] = {\"exchanges\": []}\n            if \"feedback\" not in ex or ex[\"feedback\"] is None:\n                ex[\"feedback\"] = np.nan\n            st.session_state.chats[c_name][\"exchanges\"].append(ex)\n\n        if not st.session_state.chats:\n            initial_name = \"Chat 1\"\n            db.insert_chat(username, initial_name)\n            st.session_state.chats[initial_name] = {\"exchanges\": []}\n            st.session_state.active_chat = initial_name\n        else:\n            sorted_names = sorted(st.session_state.chats.keys(), key=get_last_activity, reverse=True)\n            if \"active_chat\" not in st.session_state or st.session_state.active_chat not in st.session_state.chats:\n                st.session_state.active_chat = sorted_names[0]\n        \n        st.session_state.loaded_user = username",
      "imports": [
        "uuid",
        "numpy",
        "datetime.datetime",
        "time",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "os",
        "sys",
        "urllib.parse.urlparse",
        "logging",
        "traceback",
        "re",
        "streamlit_mermaid.st_mermaid",
        "backend.main",
        "database.db",
        "streamlit",
        "streamlit_authenticator"
      ],
      "context": {
        "calls": [
          "database.db.fetch_chats_by_user",
          "database.db.fetch_exchanges_by_user",
          "database.db.insert_chat"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "frontend.frontend.handle_feedback_change",
      "source_code": "def handle_feedback_change(ex, val):\n    ex[\"feedback\"] = val\n    db.update_exchange_feedback(ex[\"_id\"], val)\n    st.rerun()",
      "imports": [
        "uuid",
        "numpy",
        "datetime.datetime",
        "time",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "os",
        "sys",
        "urllib.parse.urlparse",
        "logging",
        "traceback",
        "re",
        "streamlit_mermaid.st_mermaid",
        "backend.main",
        "database.db",
        "streamlit",
        "streamlit_authenticator"
      ],
      "context": {
        "calls": [
          "database.db.update_exchange_feedback"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "frontend.frontend.handle_delete_exchange",
      "source_code": "def handle_delete_exchange(chat_name, ex):\n    db.delete_exchange_by_id(ex[\"_id\"])\n    if chat_name in st.session_state.chats:\n        if ex in st.session_state.chats[chat_name][\"exchanges\"]:\n            st.session_state.chats[chat_name][\"exchanges\"].remove(ex)\n    st.rerun()",
      "imports": [
        "uuid",
        "numpy",
        "datetime.datetime",
        "time",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "os",
        "sys",
        "urllib.parse.urlparse",
        "logging",
        "traceback",
        "re",
        "streamlit_mermaid.st_mermaid",
        "backend.main",
        "database.db",
        "streamlit",
        "streamlit_authenticator"
      ],
      "context": {
        "calls": [
          "database.db.delete_exchange_by_id"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "frontend.frontend.handle_delete_chat",
      "source_code": "def handle_delete_chat(username, chat_name):\n    db.delete_full_chat(username, chat_name)\n    \n    if chat_name in st.session_state.chats:\n        del st.session_state.chats[chat_name]\n    \n    if len(st.session_state.chats) > 0:\n        st.session_state.active_chat = list(st.session_state.chats.keys())[0]\n    else:\n        new_name = \"Chat 1\"\n        db.insert_chat(username, new_name)\n        st.session_state.chats[new_name] = {\"exchanges\": []}\n        st.session_state.active_chat = new_name\n        \n    st.rerun()",
      "imports": [
        "uuid",
        "numpy",
        "datetime.datetime",
        "time",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "os",
        "sys",
        "urllib.parse.urlparse",
        "logging",
        "traceback",
        "re",
        "streamlit_mermaid.st_mermaid",
        "backend.main",
        "database.db",
        "streamlit",
        "streamlit_authenticator"
      ],
      "context": {
        "calls": [
          "database.db.delete_full_chat",
          "database.db.insert_chat"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "frontend.frontend.extract_repo_name",
      "source_code": "def extract_repo_name(text):\n    url_match = re.search(r'(https?://[^\\s]+)', text)\n    if url_match:\n        url = url_match.group(0)\n        parsed = urlparse(url)\n        path = parsed.path.strip(\"/\")\n        if path:\n            repo_name = path.split(\"/\")[-1]\n            if repo_name.endswith(\".git\"):\n                repo_name = repo_name[:-4]\n            return repo_name\n    return None",
      "imports": [
        "uuid",
        "numpy",
        "datetime.datetime",
        "time",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "os",
        "sys",
        "urllib.parse.urlparse",
        "logging",
        "traceback",
        "re",
        "streamlit_mermaid.st_mermaid",
        "backend.main",
        "database.db",
        "streamlit",
        "streamlit_authenticator"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "frontend.frontend.stream_text_generator",
      "source_code": "def stream_text_generator(text):\n    for word in text.split(\" \"):\n        yield word + \" \"\n        time.sleep(0.01)",
      "imports": [
        "uuid",
        "numpy",
        "datetime.datetime",
        "time",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "os",
        "sys",
        "urllib.parse.urlparse",
        "logging",
        "traceback",
        "re",
        "streamlit_mermaid.st_mermaid",
        "backend.main",
        "database.db",
        "streamlit",
        "streamlit_authenticator"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "frontend.frontend.render_text_with_mermaid",
      "source_code": "def render_text_with_mermaid(markdown_text, should_stream=False):\n    if not markdown_text:\n        return\n\n    parts = re.split(r\"```mermaid\\s+(.*?)\\s+```\", markdown_text, flags=re.DOTALL)\n\n    for i, part in enumerate(parts):\n        if i % 2 == 0:\n            if part.strip():\n                if should_stream:\n                    st.write_stream(stream_text_generator(part))\n                else:\n                    st.markdown(part)\n        else:\n            try:\n                st_mermaid(part, key=f\"mermaid_{hash(part)}_{i}\")\n            except Exception:\n                st.code(part, language=\"mermaid\")",
      "imports": [
        "uuid",
        "numpy",
        "datetime.datetime",
        "time",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "os",
        "sys",
        "urllib.parse.urlparse",
        "logging",
        "traceback",
        "re",
        "streamlit_mermaid.st_mermaid",
        "backend.main",
        "database.db",
        "streamlit",
        "streamlit_authenticator"
      ],
      "context": {
        "calls": [
          "frontend.frontend.stream_text_generator"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "frontend.frontend.render_exchange",
      "source_code": "def render_exchange(ex, current_chat_name):\n    st.chat_message(\"user\").write(ex[\"question\"])\n    \n    answer_text = ex.get(\"answer\", \"\")\n    is_error = answer_text.startswith(\"Fehler\") or answer_text.startswith(\"Error\")\n\n    if is_error:\n        st.error(answer_text)\n        if st.button(\"\ud83d\uddd1\ufe0f Fehler-Nachricht l\u00f6schen\", key=f\"del_err_hist_{ex['_id']}\", type=\"secondary\"):\n            handle_delete_exchange(current_chat_name, ex)\n    else:\n        with st.chat_message(\"assistant\"):\n            with st.container(horizontal=True, horizontal_alignment=\"right\"):\n                st.write(\"hier die Antwort:\")\n                if ex.get(\"feedback\") == 1:\n                    st.caption(\"\u2705 Hilfreich\")\n                elif ex.get(\"feedback\") == 0:\n                    st.caption(\"\u274c Nicht hilfreich\")\n                \n                if st.button(\"\ud83d\udc4d\", key=f\"up_{ex['_id']}\", type=\"primary\" if ex.get(\"feedback\") == 1 else \"secondary\"):\n                    handle_feedback_change(ex, 1)\n                if st.button(\"\ud83d\udc4e\", key=f\"down_{ex['_id']}\", type=\"primary\" if ex.get(\"feedback\") == 0 else \"secondary\"):\n                    handle_feedback_change(ex, 0)\n                \n                with st.popover(\"\ud83d\udcac\"):\n                    msg = st.text_area(\"Notiz:\", value=ex.get(\"feedback_message\", \"\"), key=f\"txt_{ex['_id']}\")\n                    if st.button(\"Speichern\", key=f\"save_{ex['_id']}\"):\n                        db.update_exchange_feedback_message(ex[\"_id\"], msg)\n                        st.rerun()\n\n                st.download_button(\"\ud83d\udce5\", data=ex[\"answer\"], file_name=\"response.md\", key=f\"dl_{ex['_id']}\")\n                if st.button(\"\ud83d\uddd1\ufe0f\", key=f\"del_{ex['_id']}\"):\n                    handle_delete_exchange(current_chat_name, ex)\n\n            with st.container(height=500, border=True):\n                 render_text_with_mermaid(ex[\"answer\"], should_stream=False)",
      "imports": [
        "uuid",
        "numpy",
        "datetime.datetime",
        "time",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "os",
        "sys",
        "urllib.parse.urlparse",
        "logging",
        "traceback",
        "re",
        "streamlit_mermaid.st_mermaid",
        "backend.main",
        "database.db",
        "streamlit",
        "streamlit_authenticator"
      ],
      "context": {
        "calls": [
          "database.db.update_exchange_feedback_message",
          "frontend.frontend.handle_delete_exchange",
          "frontend.frontend.handle_feedback_change",
          "frontend.frontend.render_text_with_mermaid"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "frontend.frontend.get_provider",
      "source_code": "def get_provider(model_name):\n                    if not model_name or model_name == \"None\": \n                        return None\n                    if model_name.startswith(\"gemini\"): \n                        return \"Google Gemini\"\n                    if model_name == \"llama3\": \n                        return \"ollama\"\n                    if model_name.startswith(\"gpt-5\"): \n                        return \"gpt\"\n                    return \"Open Source LLM\"",
      "imports": [
        "uuid",
        "numpy",
        "datetime.datetime",
        "time",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "os",
        "sys",
        "urllib.parse.urlparse",
        "logging",
        "traceback",
        "re",
        "streamlit_mermaid.st_mermaid",
        "backend.main",
        "database.db",
        "streamlit",
        "streamlit_authenticator"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "frontend.frontend.render_config_form",
      "source_code": "def render_config_form(model_name):\n                    provider = get_provider(model_name)\n                    if provider == \"Google Gemini\":\n                        status_icon = \"\u2705\" if has_gemini else \"\u274c\"\n                        st.markdown(f\"**Gemini Key**: {status_icon} {'Gesetzt' if has_gemini else 'Fehlt'}\")\n                        with st.form(f\"form_gemini_{model_name}\"):\n                            new_gemini = st.text_input(\"Gemini Key \u00e4ndern\", type=\"password\")\n                            if st.form_submit_button(\"Speichern\") and new_gemini:\n                                db.update_gemini_key(current_user, new_gemini)\n                                st.success(\"Gespeichert!\")\n                                time.sleep(0.5)\n                                st.rerun()\n                            if st.form_submit_button(\"L\u00f6schen\"):\n                                db.update_gemini_key(current_user, \"\")\n                                st.rerun()\n\n                    elif provider == \"ollama\":\n                        status_icon = \"\u2705\" if has_ollama else \"\u274c\"\n                        st.markdown(f\"**Llama URL**: {status_icon} {'Gesetzt' if has_ollama else 'Fehlt'}\")\n                        with st.form(f\"form_ollama_{model_name}\"):\n                            new_ollama = st.text_input(\"Llama Base URL \u00e4ndern\", value=ollama_url if ollama_url else \"\")\n                            if st.form_submit_button(\"Speichern\"):\n                                db.update_ollama_url(current_user, new_ollama)\n                                st.success(\"Gespeichert!\")\n                                time.sleep(0.5)\n                                st.rerun()\n                            if st.form_submit_button(\"L\u00f6schen\"):\n                                db.update_ollama_url(current_user, \"\")\n                                st.rerun()\n\n                    elif provider == \"gpt\":\n                        status_icon = \"\u2705\" if has_gpt else \"\u274c\"\n                        st.markdown(f\"**GPT Key**: {status_icon} {'Gesetzt' if has_gpt else 'Fehlt'}\")\n                        with st.form(f\"form_gpt_{model_name}\"):\n                            new_gpt = st.text_input(\"GPT Key \u00e4ndern\", type=\"password\")\n                            \n                            if st.form_submit_button(\"Speichern\") and new_gpt:\n                                db.update_gpt_key(current_user, new_gpt)\n                                st.success(\"Gespeichert!\")\n                                time.sleep(0.5)\n                                st.rerun()\n                            if st.form_submit_button(\"L\u00f6schen\"):\n                                db.update_gpt_key(current_user, \"\")\n                                st.rerun()\n                    \n                    elif provider == \"Open Source LLM\":\n                        status_icon = \"\u2705\" if has_opensrc_key else \"\u274c\"\n                        st.markdown(f\"**OS Key**: {status_icon} | **URL**: {'\u2705' if has_opensrc_url else '\u274c'}\")\n                        with st.form(f\"form_os_{model_name}\"):\n                            n_key = st.text_input(\"Open Source Key\", type=\"password\")\n                            n_url = st.text_input(\"Open Source URL\", value=opensrc_url if opensrc_url else \"\")\n                            if st.form_submit_button(\"Speichern\"):\n                                if n_key: \n                                    db.update_opensrc_key(current_user, n_key)\n                                db.update_opensrc_url(current_user, n_url)\n                                st.success(\"Gespeichert!\")\n                                time.sleep(0.5)\n                                st.rerun()\n                            if st.form_submit_button(\"L\u00f6schen\"):\n                                db.update_opensrc_key(current_user, \"\")\n                                db.update_opensrc_url(current_user, \"\")\n                                st.rerun()",
      "imports": [
        "uuid",
        "numpy",
        "datetime.datetime",
        "time",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "os",
        "sys",
        "urllib.parse.urlparse",
        "logging",
        "traceback",
        "re",
        "streamlit_mermaid.st_mermaid",
        "backend.main",
        "database.db",
        "streamlit",
        "streamlit_authenticator"
      ],
      "context": {
        "calls": [
          "database.db.update_gemini_key",
          "database.db.update_gpt_key",
          "database.db.update_ollama_url",
          "database.db.update_opensrc_key",
          "database.db.update_opensrc_url",
          "frontend.frontend.get_provider"
        ],
        "called_by": []
      }
    },
    {
      "mode": "function_analysis",
      "identifier": "frontend.frontend.check_stop_callback",
      "source_code": "def check_stop_callback():\n            \"\"\"Diese Funktion wird vom Backend aufgerufen, um zu pr\u00fcfen ob der User Stop gedr\u00fcckt hat.\"\"\"\n            return st.session_state.get(\"abort_requested\", False)",
      "imports": [
        "uuid",
        "numpy",
        "datetime.datetime",
        "time",
        "pymongo.MongoClient",
        "dotenv.load_dotenv",
        "os",
        "sys",
        "urllib.parse.urlparse",
        "logging",
        "traceback",
        "re",
        "streamlit_mermaid.st_mermaid",
        "backend.main",
        "database.db",
        "streamlit",
        "streamlit_authenticator"
      ],
      "context": {
        "calls": [],
        "called_by": []
      }
    }
  ],
  "classes": [
    {
      "mode": "class_analysis",
      "identifier": "backend.AST_Schema.ASTVisitor",
      "source_code": "class ASTVisitor(ast.NodeVisitor):\n    def __init__(self, source_code: str, file_path: str, project_root: str):\n        self.source_code = source_code\n        self.file_path = file_path\n        self.project_root = project_root\n        self.module_path = path_to_module(self.file_path, self.project_root)\n        self.schema = {\"imports\": [], \"functions\": [], \"classes\": []}\n        self._current_class = None\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(f\"{node.module}.{alias.name}\")\n        self.generic_visit(node)\n\n    def visit_ClassDef(self, node):\n        class_identifier = f\"{self.module_path}.{node.name}\"\n        class_info = {\n            \"mode\": \"class_analysis\",\n            \"identifier\": class_identifier,\n            \"name\": node.name,\n            \"docstring\": ast.get_docstring(node),\n            \"source_code\": ast.get_source_segment(self.source_code, node),\n            \"start_line\": node.lineno,\n            \"end_line\": node.end_lineno,\n            \"context\": {\n                \"dependencies\": [],\n                \"instantiated_by\": [],\n                \"method_context\": []\n            },   \n        }\n        self.schema[\"classes\"].append(class_info)\n        self._current_class = class_info \n        self.generic_visit(node)\n        self._current_class = None\n\n    def visit_FunctionDef(self, node):\n        if self._current_class:\n            method_identifier = f\"{self._current_class['identifier']}.{node.name}\"\n            method_context_info = {\n                \"identifier\": method_identifier,\n                \"name\": node.name,\n                \"calls\": [],\n                \"called_by\": [],\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n            }\n            self._current_class[\"context\"][\"method_context\"].append(method_context_info)\n        else:\n            func_identifier = f\"{self.module_path}.{node.name}\"\n            func_info = {\n                \"mode\": \"function_analysis\",\n                \"identifier\": func_identifier,\n                \"name\": node.name,\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"source_code\": ast.get_source_segment(self.source_code, node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n                \"context\": {\n                    \"calls\": [],\n                    \"called_by\": []\n                }\n            }\n            self.schema[\"functions\"].append(func_info)\n        self.generic_visit(node)\n    \n    def visit_AsyncFunctionDef(self, node):\n        self.visit_FunctionDef(node)",
      "imports": [
        "ast",
        "os",
        "getRepo.GitRepository"
      ],
      "context": {
        "dependencies": [
          "backend.AST_Schema.path_to_module"
        ],
        "instantiated_by": [],
        "method_context": [
          {
            "identifier": "backend.AST_Schema.ASTVisitor.__init__",
            "calls": [
              "backend.AST_Schema.path_to_module"
            ],
            "called_by": [],
            "args": [
              "self",
              "source_code",
              "file_path",
              "project_root"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.AST_Schema.ASTVisitor.visit_Import",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.AST_Schema.ASTVisitor.visit_ImportFrom",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.AST_Schema.ASTVisitor.visit_ClassDef",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.AST_Schema.ASTVisitor.visit_FunctionDef",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.AST_Schema.ASTVisitor.visit_AsyncFunctionDef",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          }
        ]
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.AST_Schema.ASTAnalyzer",
      "source_code": "class ASTAnalyzer:\n\n    def __init__(self):\n        pass\n\n    def merge_relationship_data(self, full_schema: dict, raw_relationships: dict) -> dict:\n\n        outgoing_calls = raw_relationships.get(\"outgoing\", {})\n        incoming_calls = raw_relationships.get(\"incoming\", {})\n\n        for file_path, file_data in full_schema.get(\"files\", {}).items():\n            ast_nodes = file_data.get(\"ast_nodes\", {})\n            \n            for func in ast_nodes.get(\"functions\", []):\n                func_id = func.get(\"identifier\")\n                if func_id:\n                    func[\"context\"][\"calls\"] = outgoing_calls.get(func_id, [])\n                    func[\"context\"][\"called_by\"] = incoming_calls.get(func_id, [])\n\n            for cls in ast_nodes.get(\"classes\", []):\n                cls_id = cls.get(\"identifier\")\n                \n                if cls_id:\n                    cls[\"context\"][\"instantiated_by\"] = incoming_calls.get(cls_id, [])\n\n                class_dependencies = set()\n                \n                for method in cls[\"context\"].get(\"method_context\", []):\n                    method_id = method.get(\"identifier\")\n                    if method_id:\n                        m_calls = outgoing_calls.get(method_id, [])\n                        method[\"calls\"] = m_calls\n                        method[\"called_by\"] = incoming_calls.get(method_id, [])\n                        \n                        for callee in m_calls:\n                            if cls_id and not callee.startswith(f\"{cls_id}.\"):\n                                class_dependencies.add(callee)\n                \n                cls[\"context\"][\"dependencies\"] = sorted(list(class_dependencies))\n        \n        return full_schema\n\n    def analyze_repository(self, files: list, repo: GitRepository) -> dict:\n        full_schema = {\n            \"files\": {}\n        }\n\n        all_paths = [file_obj.path for file_obj in files]\n        if not all_paths:\n            return full_schema\n        \n        project_root = os.path.commonpath(all_paths)\n        if os.path.isfile(project_root):\n            project_root = os.path.dirname(project_root)\n\n        for file_obj in files:\n            if not file_obj.path.endswith('.py'):\n                continue\n            \n            file_content = file_obj.content\n            if not file_content.strip():\n                continue\n\n            try:\n                tree = ast.parse(file_content)\n                visitor = ASTVisitor(\n                    source_code=file_content, \n                    file_path=file_obj.path, \n                    project_root=project_root\n                )\n                visitor.visit(tree)\n                file_schema_nodes = visitor.schema\n\n                if file_schema_nodes[\"imports\"] or file_schema_nodes[\"functions\"] or file_schema_nodes[\"classes\"]:\n                    full_schema[\"files\"][file_obj.path] = {\n                        \"ast_nodes\": file_schema_nodes\n                    }\n\n            except (SyntaxError, ValueError) as e:\n                print(f\"Warnung: Konnte Datei '{file_obj.path}' nicht parsen. Fehler: {e}\")\n\n        return full_schema",
      "imports": [
        "ast",
        "os",
        "getRepo.GitRepository"
      ],
      "context": {
        "dependencies": [
          "backend.AST_Schema.ASTVisitor"
        ],
        "instantiated_by": [],
        "method_context": [
          {
            "identifier": "backend.AST_Schema.ASTAnalyzer.__init__",
            "calls": [],
            "called_by": [],
            "args": [
              "self"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.AST_Schema.ASTAnalyzer.merge_relationship_data",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "full_schema",
              "raw_relationships"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.AST_Schema.ASTAnalyzer.analyze_repository",
            "calls": [
              "backend.AST_Schema.ASTVisitor"
            ],
            "called_by": [],
            "args": [
              "self",
              "files",
              "repo"
            ],
            "docstring": null
          }
        ]
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.File_Dependency.FileDependencyGraph",
      "source_code": "class FileDependencyGraph(NodeVisitor):\n\n    import_dependencies: dict[str, set[str]] = {}\n    def __init__(self, filename: str, repo_root):\n        \"\"\"\n        Initialisiert den File Dependency Graphen\n\n        Args:\n\n        \"\"\"\n        self.filename = filename\n        self.repo_root = repo_root\n\n    def _resolve_module_name(self, node: ImportFrom) -> list[str]:\n        \"\"\"\n        L\u00f6st relative Imports der Form `from .. import name1, name2` auf.\n        Liefert die Liste der tats\u00e4chlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\n        Wirft ImportError, wenn nichts aufgel\u00f6st werden konnte.\n        \"\"\"\n        level_depth = node.level\n        names = [alias.name for alias in node.names]  \n        all_files = get_all_temp_files(self.repo_root)  \n\n        candidates = [p for p in all_files if p.stem == Path(self.filename).stem or p.name == f\"{self.filename}.py\"]\n\n        if not candidates:\n            raise ImportError(f\"Kann aktuelle Datei '{self.filename}' im Repo nicht finden.\")\n\n\n        candidates.sort(key=lambda p: len(p.parts))\n        current_rel_path = candidates[0]\n        base_dir = current_rel_path.parent\n        if level_depth <= 0:\n            raise ImportError(\"Erwarteter relativer Import (level >= 1).\")\n        for _ in range(level_depth - 1):\n            if base_dir == base_dir.parent and len(base_dir.parts) == 0:\n                raise ImportError(f\"Relative Import-Ebene ({level_depth}) zu gro\u00df f\u00fcr Datei '{current_rel_path}'.\")\n            base_dir = base_dir.parent\n\n        repo_root_path = Path(self.repo_root).resolve()\n        resolved: list[str] = []\n\n        def module_file_exists(rel_base: Path, name: str) -> bool:\n            file_path = repo_root_path / rel_base / f\"{name}.py\"\n            pkg_init = repo_root_path / rel_base / name / \"__init__.py\"\n            return file_path.exists() or pkg_init.exists()\n\n        def init_exports_symbol(rel_base: Path, symbol: str) -> bool:\n            \"\"\"\n            Pr\u00fcft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\n            oder als Name (funktion/klasse/assign) definiert ist.\n            \"\"\"\n            init_path = repo_root_path / rel_base / \"__init__.py\"\n            if not init_path.exists():\n                return False\n            try:\n                src = init_path.read_text(encoding=\"utf-8\")\n                mod = parse(src, filename=str(init_path))\n            except Exception:\n                return False\n\n            for node_ in walk(mod):\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == \"__all__\":\n                            try:\n                                value = literal_eval(node_.value)\n                                if isinstance(value, (list, tuple)) and symbol in value:\n                                    return True\n                            except Exception:\n                                pass\n                if isinstance(node_, (FunctionDef, ClassDef)) and node_.name == symbol:\n                    return True\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == symbol:\n                            return True\n            return False\n\n        for name in names:\n            if not name.isidentifier() or iskeyword(name):\n                continue\n\n            if module_file_exists(base_dir, name):\n                resolved.append(name)\n                continue\n            if init_exports_symbol(base_dir, name):\n                resolved.append(name)\n                continue\n\n        resolved = sorted(set(resolved))\n\n        if not resolved:\n            raise ImportError(\n                f\"Kein passendes Modul/Symbol f\u00fcr relative Import-Aufl\u00f6sung gefunden \"\n                f\"(level={level_depth}, names={names}, base_dir={base_dir})\"\n            )\n\n        return resolved\n    \n    def visit_Import(self, node: Import | ImportFrom, base_name: str | None = None):\n        for alias in node.names:\n        \n            if self.filename not in self.import_dependencies:\n                self.import_dependencies[self.filename] = set()\n\n            if base_name:\n                self.import_dependencies[self.filename].add(base_name)\n            else:\n                self.import_dependencies[self.filename].add(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node: ImportFrom):\n        \"\"\"\n        Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\n        also c, und dieser wird als callee f\u00fcr den caller, das File, gesetzt.\n        \"\"\"\n        module_name = node.module\n        if module_name:\n            module_base = module_name.split(\".\")[-1]\n            self.visit_Import(node, module_base)\n        else:\n            try:\n                resolved = self._resolve_module_name(node)\n                for base in resolved:\n                    self.visit_Import(node, base)\n            except ImportError as e:\n                print(f\"Aufl\u00f6sung eines relativen Imports fehlgeschlagen: {e}\")\n\n        self.generic_visit(node)",
      "imports": [
        "networkx",
        "os",
        "ast.Assign",
        "ast.AST",
        "ast.ClassDef",
        "ast.FunctionDef",
        "ast.Import",
        "ast.ImportFrom",
        "ast.Name",
        "ast.NodeVisitor",
        "ast.literal_eval",
        "ast.parse",
        "ast.walk",
        "keyword.iskeyword",
        "pathlib.Path",
        "getRepo.GitRepository",
        "callgraph.make_safe_dot",
        "pathlib.Path"
      ],
      "context": {
        "dependencies": [
          "backend.File_Dependency.get_all_temp_files",
          "backend.File_Dependency.init_exports_symbol",
          "backend.File_Dependency.module_file_exists"
        ],
        "instantiated_by": [],
        "method_context": [
          {
            "identifier": "backend.File_Dependency.FileDependencyGraph.__init__",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "filename",
              "repo_root"
            ],
            "docstring": "Initialisiert den File Dependency Graphen\n\nArgs:"
          },
          {
            "identifier": "backend.File_Dependency.FileDependencyGraph._resolve_module_name",
            "calls": [
              "backend.File_Dependency.get_all_temp_files",
              "backend.File_Dependency.init_exports_symbol",
              "backend.File_Dependency.module_file_exists"
            ],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": "L\u00f6st relative Imports der Form `from .. import name1, name2` auf.\nLiefert die Liste der tats\u00e4chlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\nWirft ImportError, wenn nichts aufgel\u00f6st werden konnte."
          },
          {
            "identifier": "backend.File_Dependency.FileDependencyGraph.module_file_exists",
            "calls": [],
            "called_by": [],
            "args": [
              "rel_base",
              "name"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.File_Dependency.FileDependencyGraph.init_exports_symbol",
            "calls": [],
            "called_by": [],
            "args": [
              "rel_base",
              "symbol"
            ],
            "docstring": "Pr\u00fcft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\noder als Name (funktion/klasse/assign) definiert ist."
          },
          {
            "identifier": "backend.File_Dependency.FileDependencyGraph.visit_Import",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node",
              "base_name"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.File_Dependency.FileDependencyGraph.visit_ImportFrom",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": "Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\nalso c, und dieser wird als callee f\u00fcr den caller, das File, gesetzt."
          }
        ]
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.HelperLLM.LLMHelper",
      "source_code": "class LLMHelper:\n    \"\"\"\n    A class to interact with Google Gemini for generating code snippet documentation.\n    It centralizes API interaction, error handling, and validates I/O using Pydantic.\n    \"\"\"\n    def __init__(self, api_key: str, function_prompt_path: str, class_prompt_path: str, model_name: str = \"gemini-2.0-flash-lite\", base_url: str = None):\n        \n        \n        try:\n            with open(function_prompt_path, 'r', encoding='utf-8') as f:\n                self.function_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Function system prompt file not found at: {function_prompt_path}\")\n            raise\n\n        # Handle the second file\n        try:\n            with open(class_prompt_path, 'r', encoding='utf-8') as f:\n                self.class_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Class system prompt file not found at: {class_prompt_path}\")\n            raise\n        \n        # Batch-Size config\n        self.model_name = model_name\n        self._configure_batch_settings(model_name)\n\n\n        if model_name.startswith(\"gemini-\"):\n            base_llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=0.3, \n            )\n        \n        elif model_name.startswith(\"gpt-\") and \"openGPT\" not in model_name:\n            base_llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=0.3,\n            )\n\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            if not SCADSLLM_URL:\n                raise ValueError(f\"SCADSLLM_URL environment variable is required for model {model_name}\")\n            \n            logging.info(f\"Connecting to Custom API at {SCADSLLM_URL} for model {model_name}\")\n            \n            base_llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                base_url=SCADSLLM_URL,\n                temperature=0.3,\n            )\n\n        else:\n            target_url = base_url if base_url else OLLAMA_BASE_URL\n            logging.info(f\"Using Ollama at {target_url} for model {model_name}\")\n            base_llm = ChatOllama(\n                model=model_name,\n                temperature=0.3,\n                base_url=target_url,\n            )\n\n        self.function_llm = base_llm.with_structured_output(FunctionAnalysis, method=\"json_schema\")\n        self.class_llm = base_llm.with_structured_output(ClassAnalysis, method=\"json_schema\")\n\n        self.raw_llm = base_llm\n\n        logging.info(f\"LLMHelper initialized with model '{model_name}'. Batch Size: {self.batch_size}\")\n\n    def _configure_batch_settings(self, model_name: str):\n\n        if model_name == \"gemini-2.0-flash-lite\":\n            self.batch_size = 30\n        \n        elif model_name == \"gemini-flash-latest\":\n            self.batch_size = 10\n\n        elif model_name == \"gemini-2.5-flash\":\n            self.batch_size = 10\n            \n        elif model_name == \"gemini-2.5-pro\":\n            self.batch_size = 2\n        \n        elif model_name == \"gemini-2.0-flash\":\n            self.batch_size = 15\n\n        elif model_name == \"gemini-2.5-flash-lite\":\n            self.batch_size = 15\n\n        elif model_name == \"llama3\":\n            self.batch_size = 50\n\n        elif model_name == \"gpt-5.1\":\n            self.batch_size = 500\n\n        elif model_name == \"gpt-5-mini\":\n            self.batch_size = 500\n\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            self.batch_size = 500\n            \n        else:\n            logging.warning(f\"Unknown model '{model_name}', using conservative defaults.\")\n            self.batch_size = 2\n\n    def generate_for_functions(self, function_inputs: List[FunctionAnalysisInput]) -> List[Optional[FunctionAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of functions.\"\"\"\n\n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 62\n\n        if not function_inputs:\n            return []\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(function_input.model_dump(), indent=2) \n            for function_input in function_inputs\n        ]\n       \n        conversations = [[SystemMessage(content=self.function_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_functions = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n        \n            try:\n                batch_results = self.function_llm.batch(batch_conversations, config={\"max_concurrency\": self.batch_size})\n                all_validated_functions.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                all_validated_functions.extend([None] * len(batch_conversations))\n            \n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_functions\n        \n            \n    def generate_for_classes(self, class_inputs: List[ClassAnalysisInput]) -> List[Optional[ClassAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of classes.\"\"\"\n        if not class_inputs:\n            return []\n        \n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 62\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(class_input.model_dump(), indent=2)\n            for class_input in class_inputs\n        ]\n\n        conversations = [[SystemMessage(content=self.class_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_classes = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n\n            try:\n                batch_results = self.class_llm.batch(batch_conversations , config={\"max_concurrency\": self.batch_size})\n                all_validated_classes.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                # Falls ein Fehler auftritt, f\u00fcllen wir die Liste mit None auf, um die Reihenfolge zu wahren\n                all_validated_classes.extend([None] * len(batch_conversations))\n\n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_classes",
      "imports": [
        "os",
        "json",
        "logging",
        "time",
        "typing.List",
        "typing.Optional",
        "dotenv.load_dotenv",
        "langchain_google_genai.ChatGoogleGenerativeAI",
        "langchain_ollama.ChatOllama",
        "langchain_openai.ChatOpenAI",
        "langchain_core.messages.HumanMessage",
        "langchain_core.messages.SystemMessage",
        "schemas.types.FunctionAnalysis",
        "schemas.types.ClassAnalysis",
        "schemas.types.FunctionAnalysisInput",
        "schemas.types.ClassAnalysisInput",
        "schemas.types.ClassContextInput"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": [
          {
            "identifier": "backend.HelperLLM.LLMHelper.__init__",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "api_key",
              "function_prompt_path",
              "class_prompt_path",
              "model_name",
              "base_url"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.HelperLLM.LLMHelper._configure_batch_settings",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "model_name"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.HelperLLM.LLMHelper.generate_for_functions",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "function_inputs"
            ],
            "docstring": "Generates and validates documentation for a batch of functions."
          },
          {
            "identifier": "backend.HelperLLM.LLMHelper.generate_for_classes",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "class_inputs"
            ],
            "docstring": "Generates and validates documentation for a batch of classes."
          }
        ]
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.MainLLM.MainLLM",
      "source_code": "class MainLLM:\n    \"\"\"\n    Hauptklasse f\u00fcr die Interaktion mit dem LLM.\n    \"\"\"\n    def __init__(self, api_key: str, prompt_file_path: str, model_name: str = \"gemini-2.5-pro\", base_url: str = None):\n        \n        try:\n            with open(prompt_file_path, 'r', encoding='utf-8') as f:\n                self.system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"System prompt file not found at: {prompt_file_path}\")\n            raise\n        \n        self.model_name = model_name\n\n        if model_name.startswith(\"gemini-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n\n        elif model_name.startswith(\"gpt-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            if not SCADSLLM_URL:\n                raise ValueError(f\"SCADSLLM_URL environment variable is required for model {model_name}\")\n            \n            logging.info(f\"Connecting to Custom API at {SCADSLLM_URL} for model {model_name}\")\n            \n            self.llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                base_url=SCADSLLM_URL,\n                temperature=1.0,\n            )\n\n        else:\n            target_url = base_url if base_url else OLLAMA_BASE_URL\n            self.llm = ChatOllama(\n                model=model_name,\n                temperature=1.0,\n                base_url=target_url,\n            )\n\n        logging.info(f\"Main LLM initialized with model '{model_name}'.\")\n    \n    def call_llm(self, user_input: str):\n\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with HelperLLM input...\")\n\n        try:\n            response = self.llm.invoke(messages)\n            logging.info(\"LLM call successful.\")\n            return response.content\n        except Exception as e:\n            logging.error(f\"Error during LLM call: {e}\")\n            return None\n\n    def stream_llm(self, user_input: str):\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with 'stream'...\")\n\n        try:\n            stream_iterator = self.llm.stream(messages)\n            \n            for chunk in stream_iterator:\n                yield chunk.content\n        except Exception as e:\n            error_message = f\"\\n--- Error during LLM stream call: {e} ---\"\n            logging.error(error_message)\n            yield error_message",
      "imports": [
        "os",
        "logging",
        "sys",
        "dotenv.load_dotenv",
        "langchain_google_genai.ChatGoogleGenerativeAI",
        "langchain_ollama.ChatOllama",
        "langchain_openai.ChatOpenAI",
        "langchain_core.messages.HumanMessage",
        "langchain_core.messages.SystemMessage"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": [
          {
            "identifier": "backend.MainLLM.MainLLM.__init__",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "api_key",
              "prompt_file_path",
              "model_name",
              "base_url"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.MainLLM.MainLLM.call_llm",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "user_input"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.MainLLM.MainLLM.stream_llm",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "user_input"
            ],
            "docstring": null
          }
        ]
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.basic_info.ProjektInfoExtractor",
      "source_code": "class ProjektInfoExtractor:\n    \"\"\"\n    Extrahiert grundlegende Projektinformationen aus g\u00e4ngigen Projektdateien\n    wie README, pyproject.toml und requirements.txt.\n    \"\"\"\n    def __init__(self):\n        self.INFO_NICHT_GEFUNDEN = \"Information not found\"\n        # Initialisiert die Struktur mit Platzhaltern\n        self.info = {\n            \"projekt_uebersicht\": {\n                \"titel\": self.INFO_NICHT_GEFUNDEN,\n                \"beschreibung\": self.INFO_NICHT_GEFUNDEN,\n                \"aktueller_status\": self.INFO_NICHT_GEFUNDEN,\n                \"key_features\": self.INFO_NICHT_GEFUNDEN,\n                \"tech_stack\": self.INFO_NICHT_GEFUNDEN,\n            },\n            \"installation\": {\n                \"dependencies\": self.INFO_NICHT_GEFUNDEN,\n                \"setup_anleitung\": self.INFO_NICHT_GEFUNDEN,\n                \"quick_start_guide\": self.INFO_NICHT_GEFUNDEN,\n            }\n        }\n\n    def _clean_content(self, content: str) -> str:\n        \"\"\"Entfernt Null-Bytes, die durch Encoding-Fehler (UTF-16 als UTF-8 gelesen) entstehen.\"\"\"\n        if not content:\n            return \"\"\n        return content.replace('\\x00', '')\n\n    def _finde_datei(self, patterns: List[str], dateien: List[Any]) -> Optional[Any]:\n        \"\"\"Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht.\"\"\"\n        for datei in dateien:\n            for pattern in patterns:\n                if datei.path.lower().endswith(pattern.lower()):\n                    return datei\n        return None\n\n    def _extrahiere_sektion_aus_markdown(self, inhalt: str, keywords: List[str]) -> Optional[str]:\n        \"\"\"\n        Extrahiert den Text unter einer Markdown-\u00dcberschrift (##).\n        \"\"\"\n        # Erstellt ein Regex-Pattern, das auf jedes der Schl\u00fcsselw\u00f6rter reagiert\n        keyword_pattern = \"|\".join(re.escape(k) for k in keywords)\n        \n        # Sucht nach \"## Schl\u00fcsselwort\" und erfasst alles bis zur n\u00e4chsten \"##\" oder dem Dateiende\n        pattern = re.compile(\n            rf\"##\\s*({keyword_pattern})\\s*\\n(.*?)(?=\\n##|\\Z)\",\n            re.IGNORECASE | re.DOTALL\n        )\n        match = pattern.search(inhalt)\n        if match:\n            return match.group(2).strip()\n        return None\n\n    def _parse_readme(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer README-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        if self.info[\"projekt_uebersicht\"][\"titel\"] == self.INFO_NICHT_GEFUNDEN:\n            title_match = re.search(r\"^\\s*#\\s*(.*)\", inhalt)\n            if title_match:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = title_match.group(1).strip()\n\n        # Beschreibung (Fallback)\n        if self.info[\"projekt_uebersicht\"][\"beschreibung\"] == self.INFO_NICHT_GEFUNDEN:\n            desc_match = re.search(r\"^\\s*#\\s*.*\\n+([^#\\n].*)\", inhalt, re.DOTALL)\n            if desc_match:\n                 self.info[\"projekt_uebersicht\"][\"beschreibung\"] = desc_match.group(1).strip().split('\\n\\n')[0]\n\n        # Key Features\n        features = self._extrahiere_sektion_aus_markdown(inhalt, [\"Features\", \"Key Features\", \"Merkmale\"])\n        if features:\n            self.info[\"projekt_uebersicht\"][\"key_features\"] = features\n            \n        # Tech Stack\n        tech_stack = self._extrahiere_sektion_aus_markdown(inhalt, [\"Tech Stack\", \"Technology\", \"Technologien\"])\n        if tech_stack:\n            self.info[\"projekt_uebersicht\"][\"tech_stack\"] = tech_stack\n            \n        # Status\n        status = self._extrahiere_sektion_aus_markdown(inhalt, [\"Status\", \"Current Status\"])\n        if status:\n            self.info[\"projekt_uebersicht\"][\"aktueller_status\"] = status\n\n        # Setup-Anleitung\n        setup = self._extrahiere_sektion_aus_markdown(inhalt, [\"Installation\", \"Setup\", \"Getting Started\"])\n        if setup:\n            self.info[\"installation\"][\"setup_anleitung\"] = setup\n            \n        # Quick Start\n        quick_start = self._extrahiere_sektion_aus_markdown(inhalt, [\"Quick Start\", \"Schnellstart\"])\n        if quick_start:\n            self.info[\"installation\"][\"quick_start_guide\"] = quick_start\n\n    def _parse_toml(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer pyproject.toml-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        if not tomllib:\n            print(\"Warnung: 'tomli' ist nicht installiert. pyproject.toml kann nicht analysiert werden.\")\n            return\n            \n        try:\n            data = tomllib.loads(inhalt)\n            project_data = data.get(\"project\", {})\n            \n            if \"name\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = project_data[\"name\"]\n            if \"description\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"beschreibung\"] = project_data[\"description\"]\n            if \"dependencies\" in project_data:\n                self.info[\"installation\"][\"dependencies\"] = project_data[\"dependencies\"]\n        except tomllib.TOMLDecodeError as e:\n            print(f\"Warnung: Fehler beim Parsen der pyproject.toml: {e}\")\n\n    def _parse_requirements(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer requirements.txt-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        # Nur f\u00fcllen, wenn noch keine Dependencies aus toml gefunden wurden\n        if self.info[\"installation\"][\"dependencies\"] == self.INFO_NICHT_GEFUNDEN:\n            lines = inhalt.splitlines()\n            dependencies = [\n                line.strip() for line in lines \n                if line.strip() and not line.strip().startswith('#')\n            ]\n            if dependencies:\n                self.info[\"installation\"][\"dependencies\"] = dependencies\n\n    def extrahiere_info(self, dateien: List[Any], repo_url: str) -> Dict[str, Any]:\n        \"\"\"\n        Orchestriert die Extraktion von Informationen.\n        \"\"\"\n        # Relevante Dateien finden\n        readme_datei = self._finde_datei([\"readme.md\", \"readme.rst\", \"readme.txt\", \"readme\"], dateien)\n        toml_datei = self._finde_datei([\"pyproject.toml\"], dateien)\n        req_datei = self._finde_datei([\"requirements.txt\"], dateien)\n\n        # Dateien parsen (mit Priorisierung)\n        if toml_datei:\n            self._parse_toml(toml_datei.content)\n\n        if req_datei:\n            self._parse_requirements(req_datei.content)\n            \n        if readme_datei:\n            self._parse_readme(readme_datei.content)\n            \n        # Finale Formatierung der Dependencies\n        deps = self.info[\"installation\"][\"dependencies\"]\n        if isinstance(deps, list):\n            if not deps:\n                self.info[\"installation\"][\"dependencies\"] = self.INFO_NICHT_GEFUNDEN\n            else:\n                self.info[\"installation\"][\"dependencies\"] = \"\\n\".join(f\"- {dep}\" for dep in deps)\n        \n        # Titel aus URL ableiten, falls nichts gefunden wurde (oder als Zusatzinfo)\n        if repo_url:\n            repo_name = os.path.basename(repo_url.removesuffix('.git'))\n            # Wenn noch kein Titel da ist oder er generisch war\n            if self.info[\"projekt_uebersicht\"][\"titel\"] == self.INFO_NICHT_GEFUNDEN:\n                 self.info[\"projekt_uebersicht\"][\"titel\"] = f\"{repo_name} documentation\"\n\n        return self.info",
      "imports": [
        "re",
        "os",
        "tomllib",
        "typing.List",
        "typing.Dict",
        "typing.Any",
        "typing.Optional"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": [
          {
            "identifier": "backend.basic_info.ProjektInfoExtractor.__init__",
            "calls": [],
            "called_by": [],
            "args": [
              "self"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.basic_info.ProjektInfoExtractor._clean_content",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "content"
            ],
            "docstring": "Entfernt Null-Bytes, die durch Encoding-Fehler (UTF-16 als UTF-8 gelesen) entstehen."
          },
          {
            "identifier": "backend.basic_info.ProjektInfoExtractor._finde_datei",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "patterns",
              "dateien"
            ],
            "docstring": "Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht."
          },
          {
            "identifier": "backend.basic_info.ProjektInfoExtractor._extrahiere_sektion_aus_markdown",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "inhalt",
              "keywords"
            ],
            "docstring": "Extrahiert den Text unter einer Markdown-\u00dcberschrift (##)."
          },
          {
            "identifier": "backend.basic_info.ProjektInfoExtractor._parse_readme",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "inhalt"
            ],
            "docstring": "Parst den Inhalt einer README-Datei."
          },
          {
            "identifier": "backend.basic_info.ProjektInfoExtractor._parse_toml",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "inhalt"
            ],
            "docstring": "Parst den Inhalt einer pyproject.toml-Datei."
          },
          {
            "identifier": "backend.basic_info.ProjektInfoExtractor._parse_requirements",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "inhalt"
            ],
            "docstring": "Parst den Inhalt einer requirements.txt-Datei."
          },
          {
            "identifier": "backend.basic_info.ProjektInfoExtractor.extrahiere_info",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "dateien",
              "repo_url"
            ],
            "docstring": "Orchestriert die Extraktion von Informationen."
          }
        ]
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.callgraph.CallGraph",
      "source_code": "class CallGraph(ast.NodeVisitor):\n    def __init__(self, filename: str):\n        self.filename = filename\n        self.current_function = None\n        self.current_class = None\n        \n        self.local_defs: dict[str, str] = {}\n        self.graph = nx.DiGraph()\n        self.import_mapping: dict[str, str] = {}\n        self.function_set: set[str] = set()\n        self.edges: dict[str, set[str]] = {}\n\n    def _recursive_call(self, node):\n        \"\"\"\n        Liefert eine Liste der Namenskomponenten als dotted string z.B. ['pkg', 'mod', 'Class', 'method']\n        \"\"\"\n        if isinstance(node, ast.Call):\n            return self._recursive_call(node.func)\n        if isinstance(node, ast.Name):\n            return [node.id]\n        if isinstance(node, ast.Attribute):\n            parts = self._recursive_call(node.value)\n            parts.append(node.attr)\n            return parts\n        return []\n\n    def _resolve_all_callee_names(self, callee_nodes: list[list[str]]) -> list[str]:\n        \"\"\"\n        callee_nodes ist jetzt eine Liste von Listen (Name-Steps).\n        Wir pr\u00fcfen zuerst lokale Definitionen, dann import_mapping.\n        \"\"\"\n        resolved = []\n        for parts in callee_nodes:\n            if not parts:\n                continue\n            simple = parts[-1]\n            dotted = \".\".join(parts[-2:]) if len(parts) >= 2 else simple\n\n            if simple in self.local_defs:\n                resolved.append(self.local_defs[simple])\n                continue\n            if dotted in self.local_defs:\n                resolved.append(self.local_defs[dotted])\n                continue\n\n            first = parts[0]\n            if first in self.import_mapping:\n                mod = self.import_mapping[first]\n                rest = \"::\".join(parts[1:]) if len(parts) > 1 else simple\n                resolved.append(f\"{mod}::{rest}\")\n                continue\n\n            if self.current_class:\n                resolved.append(f\"{self.filename}::{parts[0]}::{simple}\" if len(parts)==1 else f\"{self.filename}::{'::'.join(parts)}\")\n            else:\n                resolved.append(f\"{self.filename}::{ '::'.join(parts)}\")\n        return resolved\n\n    def _make_full_name(self, basename: str, class_name: str | None = None) -> str:\n        if class_name:\n            return f\"{self.filename}::{class_name}::{basename}\"\n        return f\"{self.filename}::{basename}\"\n    \n    def _current_caller(self) -> str:\n        if self.current_function:\n            return self.current_function\n        return f\"<{self.filename}>\" if self.filename else \"<global-scope>\"\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            module_name = alias.name\n            module_asname = alias.asname if alias.asname else module_name\n            self.import_mapping[module_asname] = module_name\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        module_name = node.module.split(\".\")[-1] if node.module else \"\"\n        for alias in node.names:\n            self.import_mapping[alias.asname or alias.name] = f\"{module_name}\" if module_name else alias.name\n\n    def visit_ClassDef(self, node: ast.ClassDef):\n        prev_class = self.current_class\n        self.current_class = node.name\n        self.generic_visit(node)\n        self.current_class = prev_class\n        \n    def visit_FunctionDef(self, node):\n        prev_function = self.current_function\n        full_name = self._make_full_name(node.name, self.current_class)\n        self.local_defs[node.name] = full_name\n        if self.current_class:\n            self.local_defs[f\"{self.current_class}.{node.name}\"] = full_name\n\n        self.current_function = full_name\n        self.graph.add_node(self.current_function)\n        self.generic_visit(node)\n        self.function_set.add(self.current_function)\n        self.current_function = prev_function\n\n    def visit_AsyncFunctionDef(self, node):\n        self.visit_FunctionDef(node)\n\n    def visit_Call(self, node):\n        caller = self._current_caller()\n        parts = self._recursive_call(node)\n        resolved_callees = self._resolve_all_callee_names([parts])\n\n        if caller not in self.edges:\n            self.edges[caller] = set()\n\n        for callee in resolved_callees:\n            if callee:\n                self.edges[caller].add(callee)\n        self.generic_visit(node)\n\n    def visit_If(self, node):\n        if (isinstance(node.test, ast.Compare) and\n            isinstance(node.test.left, ast.Name) and \n            node.test.left.id == \"__name__\"):\n            caller_backup = self.current_function\n            self.current_function = \"<main_block>\"\n            self.generic_visit(node)\n            self.current_function = caller_backup\n        else:\n            self.generic_visit(node)",
      "imports": [
        "ast",
        "networkx",
        "pathlib.Path",
        "getRepo.GitRepository",
        "getRepo.GitRepository"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": [
          {
            "identifier": "backend.callgraph.CallGraph.__init__",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "filename"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.callgraph.CallGraph._recursive_call",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": "Liefert eine Liste der Namenskomponenten als dotted string z.B. ['pkg', 'mod', 'Class', 'method']"
          },
          {
            "identifier": "backend.callgraph.CallGraph._resolve_all_callee_names",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "callee_nodes"
            ],
            "docstring": "callee_nodes ist jetzt eine Liste von Listen (Name-Steps).\nWir pr\u00fcfen zuerst lokale Definitionen, dann import_mapping."
          },
          {
            "identifier": "backend.callgraph.CallGraph._make_full_name",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "basename",
              "class_name"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.callgraph.CallGraph._current_caller",
            "calls": [],
            "called_by": [],
            "args": [
              "self"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.callgraph.CallGraph.visit_Import",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.callgraph.CallGraph.visit_ImportFrom",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.callgraph.CallGraph.visit_ClassDef",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.callgraph.CallGraph.visit_FunctionDef",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.callgraph.CallGraph.visit_AsyncFunctionDef",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.callgraph.CallGraph.visit_Call",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.callgraph.CallGraph.visit_If",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          }
        ]
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.diagram_generation.call_resolver.CallResolver",
      "source_code": "class CallResolver:\n\n    def __init__(self, project: ProjectIndex):\n        self.project = project\n\n    \n    def resolve_all(self, calls: dict[str, list[RawCall]]) -> dict[str, list[ResolvedCall]]:\n        resolved: dict[str, list[ResolvedCall]] = {}\n\n        for mod, call in calls.items():\n            if mod not in resolved:\n                resolved[mod] = []\n            for c in call:\n                resolved[mod].extend(self.resolved(c))\n        return resolved\n    \n\n    def resolved(self, call: RawCall) -> list[ResolvedCall]:\n        node = call.func_node\n\n        if isinstance(node, ast.Name):\n            return self._resolve_name(call, node)\n        \n        if isinstance(node, ast.Attribute):\n            return self._resolve_attribute(call, node)\n        \n        return [\n            ResolvedCall(\n                caller=call.caller,\n                callee=None,\n                call_type=CallType.UNKNOWN,\n                lineno=call.lineno\n            )\n        ]\n\n    def _resolve_name(self, call: RawCall, node: ast.Name) -> list[ResolvedCall]:\n        name = node.id\n        module = self.project.modules[call.context.module.name]\n\n        if name in module.functions:\n            return [\n                ResolvedCall(\n                    caller=call.caller,\n                    callee = module.functions[name],\n                    call_type = CallType.DIRECT,\n                    lineno = call.lineno\n                )\n            ]\n        \n        if name in module.imports:\n            target = module.imports[name]\n            mod_name, _, attr = target.rpartition(\".\")\n            target_module = self.project.modules.get(mod_name)\n            if target_module in target_module.functions:\n                return [\n                    ResolvedCall(\n                        caller = call.caller,\n                        callee = module.functions[attr],\n                        call_type = CallType.IMPORTED,\n                        lineno = call.lineno\n                    )\n                ]\n            return [\n                ResolvedCall(\n                    caller=call.caller, \n                    callee=None,\n                    call_type=CallType.UNKNOWN,\n                    lineno=call.lineno\n                )\n            ]\n    \n\n    def _resolve_attribute(self, call: RawCall, node: ast.Attribute) -> list[ResolvedCall]:\n        attr = node.attr\n\n        if isinstance(node.value, ast.Name) and node.value.id == \"self\":\n            cls = call.context.classes\n            if cls and attr in cls.methods:\n                return [\n                    ResolvedCall(\n                        caller=call.caller,\n                        callee=cls.methods[attr],\n                        call_type=CallType.METHOD,\n                        lineno=call.lineno\n                    )\n                ]\n        \n        if isinstance(node.value, ast.Name):\n            mod_alias = node.value.id\n            module = self.project.modules[call.context.module.name]\n\n            if mod_alias in module.imports:\n                target_module = self.project.modules.get(module.imports[mod_alias])\n            \n                if target_module and attr in target_module.functions:\n                    return [\n                        ResolvedCall(\n                            caller=call.caller,\n                            callee=target_module.functions[attr],\n                            call_type=CallType.IMPORTED,\n                            lineno=call.lineno\n                        )\n                    ]\n        \n\n        candidates = []\n        for cls in self.project.all_classes():\n            if attr in cls.methods:\n                candidates.append(\n                    ResolvedCall(\n                        caller=call.caller,\n                        callee=cls.methods[attr],\n                        call_type=CallType.DYNAMIC,\n                        lineno=call.lineno,\n                    )\n                )\n\n        if candidates:\n            return candidates\n\n        return [\n            ResolvedCall(\n                caller=call.caller, \n                callee=None, \n                call_type=CallType.UNKNOWN, \n                lineno=call.lineno\n            )\n        ]",
      "imports": [
        "ast",
        "sys",
        "pathlib.Path",
        "diagram_generation.data_types.CallType",
        "diagram_generation.data_types.RawCall",
        "diagram_generation.data_types.ResolvedCall",
        "diagram_generation.data_types.ProjectIndex"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": [
          {
            "identifier": "backend.diagram_generation.call_resolver.CallResolver.__init__",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "project"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.diagram_generation.call_resolver.CallResolver.resolve_all",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "calls"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.diagram_generation.call_resolver.CallResolver.resolved",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "call"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.diagram_generation.call_resolver.CallResolver._resolve_name",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "call",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.diagram_generation.call_resolver.CallResolver._resolve_attribute",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "call",
              "node"
            ],
            "docstring": null
          }
        ]
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.diagram_generation.callgraph.TreeVisitor",
      "source_code": "class TreeVisitor(NodeVisitor):\n\n    def __init__(self, module: ModuleSymbol, project: ProjectIndex):\n        self.module = module\n        self.project = project\n        self.current_function: Optional[FunctionSymbol] = None\n        self.current_class: Optional[ClassSymbol] = None\n        self.calls: list[RawCall] = []\n\n    \n    def visit_ClassDef(self, node: ClassDef) -> None:\n        prev_class = self.current_class\n        self.current_class = self.module.classes.get(node.name)\n        self.generic_visit(node)\n        self.current_class = prev_class\n\n    \n    def visit_FunctionDef(self, node: FunctionDef) -> None:\n        prev_function = self.current_function\n        if self.current_class:\n            self.current_function = self.current_class.methods.get(node.name)\n        else:\n            self.current_function = self.module.functions.get(node.name)\n\n        self.generic_visit(node)\n        self.current_function = prev_function\n\n\n    def visit_Call(self, node) -> None:\n        if not self.current_function:\n            return\n        \n        if isinstance(node.func, Name):\n            if node.func.id not in self.module.functions:\n                return\n        elif isinstance(node.func, Attribute):\n            # package = node.func\n            # while isinstance(package, Attribute):\n            #         package = package.value\n            \n            # if not isinstance(package, Name):\n            #     return\n            # all_functions: dict[str, list[str]]\n            # for mod in self.project.modules.values():\n\n            # all_functions = [\n            #     x.name for mod in self.project.modules.values()\n            #     for x in mod.functions.values()\n            #     for c in mod.classes.values()\n            #     for x in c.methods.values()\n            # ]\n            \n            if node.func not in self.project.modules.values():\n                return\n        self.calls.append(\n            RawCall(\n                caller=self.current_function,\n                func_node=node.func,\n                lineno=node.lineno,\n                context=CallContext(\n                    module=self.module,\n                    function=self.current_function,\n                    classes=self.current_class\n                )\n            )\n        )\n        self.generic_visit(node)",
      "imports": [
        "ast.Attribute",
        "ast.ClassDef",
        "ast.FunctionDef",
        "ast.Name",
        "ast.NodeVisitor",
        "typing.Optional",
        "diagram_generation.data_types.CallContext",
        "diagram_generation.data_types.ClassSymbol",
        "diagram_generation.data_types.FunctionSymbol",
        "diagram_generation.data_types.ModuleSymbol",
        "diagram_generation.data_types.RawCall",
        "diagram_generation.data_types.ProjectIndex"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": [
          {
            "identifier": "backend.diagram_generation.callgraph.TreeVisitor.__init__",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "module",
              "project"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.diagram_generation.callgraph.TreeVisitor.visit_ClassDef",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.diagram_generation.callgraph.TreeVisitor.visit_FunctionDef",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.diagram_generation.callgraph.TreeVisitor.visit_Call",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          }
        ]
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.diagram_generation.data_types.FunctionSymbol",
      "source_code": "class FunctionSymbol:\n    name: str\n    module: str\n    qualname: str\n    asynchron: False\n    input_params: list[str]\n    return_symb: bool\n    lineno: int",
      "imports": [
        "ast.expr",
        "dataclasses.dataclass",
        "enum.Enum",
        "typing.Optional"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.diagram_generation.data_types.ClassSymbol",
      "source_code": "class ClassSymbol:\n    name: str\n    module: str\n    methods: dict[str, FunctionSymbol]",
      "imports": [
        "ast.expr",
        "dataclasses.dataclass",
        "enum.Enum",
        "typing.Optional"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.diagram_generation.data_types.ModuleSymbol",
      "source_code": "class ModuleSymbol:\n    name: str\n    overlying_packages: list[str]\n    functions: dict[str, FunctionSymbol]\n    classes: dict[str, ClassSymbol]\n    imports: dict[str, str]",
      "imports": [
        "ast.expr",
        "dataclasses.dataclass",
        "enum.Enum",
        "typing.Optional"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.diagram_generation.data_types.CallContext",
      "source_code": "class CallContext:\n    module: ModuleSymbol\n    function: FunctionSymbol\n    classes: Optional[ClassSymbol]",
      "imports": [
        "ast.expr",
        "dataclasses.dataclass",
        "enum.Enum",
        "typing.Optional"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.diagram_generation.data_types.RawCall",
      "source_code": "class RawCall:\n    caller: FunctionSymbol\n    func_node: expr\n    lineno: int\n    context: CallContext",
      "imports": [
        "ast.expr",
        "dataclasses.dataclass",
        "enum.Enum",
        "typing.Optional"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.diagram_generation.data_types.CallType",
      "source_code": "class CallType(Enum):\n    DIRECT = \"direct\"\n    METHOD = \"method\"\n    IMPORTED = \"imported\"\n    DYNAMIC = \"dynamic\"\n    UNKNOWN = \"unknown\"",
      "imports": [
        "ast.expr",
        "dataclasses.dataclass",
        "enum.Enum",
        "typing.Optional"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.diagram_generation.data_types.ResolvedCall",
      "source_code": "class ResolvedCall:\n    caller: FunctionSymbol\n    callee: Optional[FunctionSymbol]\n    call_type: CallType\n    lineno: int",
      "imports": [
        "ast.expr",
        "dataclasses.dataclass",
        "enum.Enum",
        "typing.Optional"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.diagram_generation.data_types.ProjectIndex",
      "source_code": "class ProjectIndex:\n    modules: dict[str, ModuleSymbol]\n\n    def all_classes(self) -> list[ClassSymbol]:\n        return [\n            clss \n            for module in self.modules.values()\n            for clss in module.classes.values()\n        ]",
      "imports": [
        "ast.expr",
        "dataclasses.dataclass",
        "enum.Enum",
        "typing.Optional"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": [
          {
            "identifier": "backend.diagram_generation.data_types.ProjectIndex.all_classes",
            "calls": [],
            "called_by": [],
            "args": [
              "self"
            ],
            "docstring": null
          }
        ]
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.diagram_generation.emitter.MermaidSequenceEmitter",
      "source_code": "class MermaidSequenceEmitter:\n    def emit(self, calls: list[ResolvedCall]) -> str:\n        lines: list[str] = [\"```mermaid\"]\n        lines.append(\"sequenceDiagram\")\n\n        participants = self._collect_participants(calls)\n        for p in participants:\n            lines.append(f\"    participant {p}\")\n\n        for call in sorted(calls, key=lambda c: c.lineno):\n            lines.append(self._emit_call(call))\n            if call.callee.return_symb:\n                lines.append(self._emit_response(call))\n        lines.append(\"```\")\n        return \"\\n\".join(lines)\n\n\n    def _collect_participants(self, calls: list[ResolvedCall]) -> list[str]:\n        participants: list[str] = []\n        for call in calls:\n            participants.append(mermaid_id(call.caller.qualname))\n            if call.callee and call.callee not in participants:\n                participants.append(\n                    mermaid_id(call.callee.qualname)\n                )\n            else:\n                participants.append(\"?\")\n        return participants\n\n\n    def _emit_response(self, call:ResolvedCall) -> str:\n        resolved_callee = mermaid_id(call.callee.qualname)\n        resolved_caller = mermaid_id(call.caller.qualname)\n        \n        return f\"    {resolved_callee} ->> {resolved_caller}: return\"\n    \n    \n    def _emit_call(self, call: ResolvedCall) -> str:\n        caller = mermaid_id(call.caller.qualname)\n        if call.callee:\n            callee = mermaid_id(call.callee.qualname)\n            label= call.callee.input_params\n        else:\n            callee = \"?\"\n            label = \"unknown\"\n        return f\"    {caller} ->> {callee}: {label}\"",
      "imports": [
        "diagram_generation.data_types.ModuleSymbol",
        "diagram_generation.data_types.ResolvedCall"
      ],
      "context": {
        "dependencies": [
          "backend.diagram_generation.emitter.mermaid_id"
        ],
        "instantiated_by": [],
        "method_context": [
          {
            "identifier": "backend.diagram_generation.emitter.MermaidSequenceEmitter.emit",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "calls"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.diagram_generation.emitter.MermaidSequenceEmitter._collect_participants",
            "calls": [
              "backend.diagram_generation.emitter.mermaid_id"
            ],
            "called_by": [],
            "args": [
              "self",
              "calls"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.diagram_generation.emitter.MermaidSequenceEmitter._emit_response",
            "calls": [
              "backend.diagram_generation.emitter.mermaid_id"
            ],
            "called_by": [],
            "args": [
              "self",
              "call"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.diagram_generation.emitter.MermaidSequenceEmitter._emit_call",
            "calls": [
              "backend.diagram_generation.emitter.mermaid_id"
            ],
            "called_by": [],
            "args": [
              "self",
              "call"
            ],
            "docstring": null
          }
        ]
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.diagram_generation.emitter.MermaidOverviewArchitectureEmitter",
      "source_code": "class MermaidOverviewArchitectureEmitter:\n    def emit(self, modules: dict[str, ModuleSymbol]) -> str:\n        lines = [\"```mermaid\"]\n        lines.append(\"graph LR\")\n        import_list: list[str] = [ key for module in modules.values() for key in module.imports.keys()]\n        filtered_list: list[str] = []\n\n        for import_name in import_list:\n            if import_name in filtered_list:\n                continue\n            filtered_list.append(import_name)\n\n\n        for module in modules.values():\n            src = module.name.split(\".\")[-1]\n            for target in filtered_list:\n                if target in modules:\n                    dst = target.split(\".\")[-1]\n                    lines.append(f\"    {src} --> {dst}\")\n        lines.append(\"```\")\n        return \"\\n\".join(lines)",
      "imports": [
        "diagram_generation.data_types.ModuleSymbol",
        "diagram_generation.data_types.ResolvedCall"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": [
          {
            "identifier": "backend.diagram_generation.emitter.MermaidOverviewArchitectureEmitter.emit",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "modules"
            ],
            "docstring": null
          }
        ]
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.diagram_generation.emitter.MermaidOverviewEmitter",
      "source_code": "class MermaidOverviewEmitter:\n    def emit(self, modules: dict[str, ModuleSymbol]) -> str:\n        lines = [\"```mermaid\"]\n        lines.append(\"graph TD\")  # Use top-down layout for better visualization of hierarchy\n\n        # Collect all unique package relationships\n        relationships = set()\n\n        for module in modules.values():\n            packages = module.overlying_packages\n            # Create edges between consecutive packages in the hierarchy\n            for i in range(len(packages) - 1):\n                relationships.add((packages[i], packages[i + 1]))\n\n        # Add relationships to the diagram\n        for src, target in relationships:\n            lines.append(f\"    {src} --> {target}\")\n\n        lines.append(\"```\")\n        return \"\\n\".join(lines)",
      "imports": [
        "diagram_generation.data_types.ModuleSymbol",
        "diagram_generation.data_types.ResolvedCall"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": [
          {
            "identifier": "backend.diagram_generation.emitter.MermaidOverviewEmitter.emit",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "modules"
            ],
            "docstring": null
          }
        ]
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.diagram_generation.emitter.MermaidClassDiagramEmitter",
      "source_code": "class MermaidClassDiagramEmitter:\n    def emit(self, modules: dict[str, ModuleSymbol]) -> dict[str, str]:\n\n        class_diagrams: dict[str, str] = {}\n        for module in modules.values():\n            for name, cls in module.classes.items():\n                lines: list[str] = [\"```mermaid\"]\n                lines.append(\"classDiagram\")\n                class_id = mermaid_id(cls.name)\n                lines.append(f\"    class {class_id} {{\")\n\n                for method in cls.methods.values():\n                    if method.name.startswith(\"_\") and method.name != \"__init__\":\n                        lines.append(f\"        -{method.name}()\")\n                        continue\n                    lines.append(f\"        +{method.name}()\")\n\n                lines.append(\"    }\")\n                lines.append(\"```\")\n                class_diagrams[name] = \"\\n\".join(lines)\n                lines = []\n\n\n        # for module in modules.values():\n        #     src = mermaid_id(module.name)\n        #     for imported in module.imports.values():\n        #         dst = mermaid_id(imported)\n        #         lines.append(f\"    {src} ..> {dst} : imports\")\n        # lines.append(\"```\")\n        return class_diagrams",
      "imports": [
        "diagram_generation.data_types.ModuleSymbol",
        "diagram_generation.data_types.ResolvedCall"
      ],
      "context": {
        "dependencies": [
          "backend.diagram_generation.emitter.mermaid_id"
        ],
        "instantiated_by": [],
        "method_context": [
          {
            "identifier": "backend.diagram_generation.emitter.MermaidClassDiagramEmitter.emit",
            "calls": [
              "backend.diagram_generation.emitter.mermaid_id"
            ],
            "called_by": [],
            "args": [
              "self",
              "modules"
            ],
            "docstring": null
          }
        ]
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.diagram_generation.symbol_collector.SymbolCollector",
      "source_code": "class SymbolCollector(NodeVisitor):\n\n    def __init__(self, module_name: str, packages: list[str]):\n        self.module = ModuleSymbol(\n            name=module_name,\n            overlying_packages=packages,\n            functions={},\n            classes={},\n            imports={}\n        )\n\n\n    def _has_return(self, node: FunctionDef) -> bool:\n        \n        has_return = False\n        for stmt in node.body:\n            if isinstance(stmt, Return):\n                has_return = True\n        \n        return has_return\n    \n\n    def _declare_input_parameters(self, node: FunctionDef) -> list[str]:\n        input_parameters: list[str] = []\n        for argument in node.args.args:\n            input_param = argument.arg\n            input_parameters.append(input_param)\n\n        return input_parameters\n            \n\n    def visit_Import(self, node: Import) -> None:\n        for alias in node.names:\n            self.module.imports[alias.asname or alias.name] = alias.name\n        self.generic_visit(node)\n\n\n    def visit_ImportFrom(self, node: ImportFrom) -> None:\n        if node.module is None:\n            # TODO: resolve relative Imports\n            return\n        \n        module_base = node.module.split(\".\")[-1]\n        for alias in node.names:\n            self.module.imports[alias.asname or alias.name] = f\"{module_base}.{alias.name}\"\n        self.generic_visit(node)\n\n\n    def visit_ClassDef(self, node) -> None:\n        cls = ClassSymbol(\n            name=node.name,\n            module=self.module.name,\n            methods={}\n        )\n\n        self.module.classes[node.name] = cls\n\n        for stmt in node.body:\n            if isinstance(stmt, FunctionDef):\n                has_return = self._has_return(node)\n                input_parameters = self._declare_input_parameters(stmt)\n                curr_meth = FunctionSymbol(\n                    name = stmt.name,\n                    module = self.module.name,\n                    qualname=f\"{self.module.name}.{node.name}.{stmt.name}\",\n                    asynchron=True if isinstance(stmt, AsyncFunctionDef) else False,\n                    input_params=input_parameters,\n                    return_symb=has_return,\n                    lineno=stmt.lineno\n                )\n                cls.methods[stmt.name] = curr_meth\n        self.generic_visit(node)\n\n\n    def visit_AsyncFunctionDef(self, node) -> None:\n        if isinstance(getattr(node, \"parent\", None), ClassDef):\n            return\n        \n        has_return = self._has_return(node)\n        input_paramters = self._declare_input_parameters(node)\n        curr_func = FunctionSymbol(\n            name=node.name,\n            module=self.module.name,\n            qualname= f\"{self.module.name}.{node.name}\",\n            asynchron=True if isinstance(node, AsyncFunctionDef) else False,\n            input_params=input_paramters,\n            return_symb=has_return,\n            lineno=node.lineno\n\n        )\n        self.module.functions[node.name] = curr_func\n        self.generic_visit(node)\n\n    \n    def visit_FunctionDef(self, node: FunctionDef) -> None:\n        self.visit_AsyncFunctionDef(node)",
      "imports": [
        "ast.AST",
        "ast.AsyncFunctionDef",
        "ast.ClassDef",
        "ast.FunctionDef",
        "ast.Import",
        "ast.ImportFrom",
        "ast.NodeVisitor",
        "ast.Return",
        "ast.iter_child_nodes",
        "ast.parse",
        "ast.walk",
        "diagram_generation.data_types.ModuleSymbol",
        "diagram_generation.data_types.FunctionSymbol",
        "diagram_generation.data_types.ClassSymbol",
        "diagram_generation.callgraph.TreeVisitor",
        "sys",
        "pathlib.Path",
        "getRepo.GitRepository"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": [
          {
            "identifier": "backend.diagram_generation.symbol_collector.SymbolCollector.__init__",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "module_name",
              "packages"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.diagram_generation.symbol_collector.SymbolCollector._has_return",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.diagram_generation.symbol_collector.SymbolCollector._declare_input_parameters",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.diagram_generation.symbol_collector.SymbolCollector.visit_Import",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.diagram_generation.symbol_collector.SymbolCollector.visit_ImportFrom",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.diagram_generation.symbol_collector.SymbolCollector.visit_ClassDef",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.diagram_generation.symbol_collector.SymbolCollector.visit_AsyncFunctionDef",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.diagram_generation.symbol_collector.SymbolCollector.visit_FunctionDef",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          }
        ]
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.getRepo.RepoFile",
      "source_code": "class RepoFile:\n    \"\"\"\n    Repr\u00e4sentiert eine einzelne Datei in einem Git-Repository.\n    \n    Der Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tats\u00e4chlichem Zugriff.\n    \"\"\"\n    def __init__(self, file_path, commit_tree):\n        \"\"\"\n        Initialisiert das RepoFile-Objekt.\n\n        Args:\n            file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n            commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt.\n        \"\"\"\n        self.path = file_path\n        self._tree = commit_tree\n        \n\n        self._blob = None\n        self._content = None\n        self._size = None\n\n    @property\n    def blob(self):\n        \"\"\"Lazy-l\u00e4dt das Git-Blob-Objekt.\"\"\"\n        if self._blob is None:\n            try:\n                self._blob = self._tree[self.path]\n            except KeyError:\n                raise FileNotFoundError(f\"Datei '{self.path}' konnte im Commit-Tree nicht gefunden werden.\")\n        return self._blob\n\n    @property\n    def content(self):\n        \"\"\"Lazy-l\u00e4dt und gibt den dekodierten Inhalt der Datei zur\u00fcck.\"\"\"\n        if self._content is None:\n            self._content = self.blob.data_stream.read().decode('utf-8', errors='ignore')\n        return self._content\n\n    @property\n    def size(self):\n        \"\"\"Lazy-l\u00e4dt und gibt die Gr\u00f6\u00dfe der Datei in Bytes zur\u00fcck.\"\"\"\n        if self._size is None:\n            self._size = self.blob.size\n        return self._size\n        \n    def analyze_word_count(self):\n        \"\"\"\n        Eine Beispiel-Analyse-Methode. Z\u00e4hlt die W\u00f6rter im Dateiinhalt.\n        \"\"\"\n        return len(self.content.split())\n\n    def __repr__(self):\n        \"\"\"Gibt eine n\u00fctzliche String-Repr\u00e4sentation des Objekts zur\u00fcck.\"\"\"\n        return f\"<RepoFile(path='{self.path}')>\"\n    \n    def to_dict(self, include_content=False):\n        data = {\n            \"path\": self.path,\n            \"name\": os.path.basename(self.path),\n            \"size\": self.size,\n            \"type\": \"file\"\n        }\n        if include_content:\n            data[\"content\"] = self.content\n        return data",
      "imports": [
        "tempfile",
        "git.Repo",
        "git.GitCommandError",
        "logging",
        "os"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": [
          {
            "identifier": "backend.getRepo.RepoFile.__init__",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "file_path",
              "commit_tree"
            ],
            "docstring": "Initialisiert das RepoFile-Objekt.\n\nArgs:\n    file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n    commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt."
          },
          {
            "identifier": "backend.getRepo.RepoFile.blob",
            "calls": [],
            "called_by": [],
            "args": [
              "self"
            ],
            "docstring": "Lazy-l\u00e4dt das Git-Blob-Objekt."
          },
          {
            "identifier": "backend.getRepo.RepoFile.content",
            "calls": [],
            "called_by": [],
            "args": [
              "self"
            ],
            "docstring": "Lazy-l\u00e4dt und gibt den dekodierten Inhalt der Datei zur\u00fcck."
          },
          {
            "identifier": "backend.getRepo.RepoFile.size",
            "calls": [],
            "called_by": [],
            "args": [
              "self"
            ],
            "docstring": "Lazy-l\u00e4dt und gibt die Gr\u00f6\u00dfe der Datei in Bytes zur\u00fcck."
          },
          {
            "identifier": "backend.getRepo.RepoFile.analyze_word_count",
            "calls": [],
            "called_by": [],
            "args": [
              "self"
            ],
            "docstring": "Eine Beispiel-Analyse-Methode. Z\u00e4hlt die W\u00f6rter im Dateiinhalt."
          },
          {
            "identifier": "backend.getRepo.RepoFile.__repr__",
            "calls": [],
            "called_by": [],
            "args": [
              "self"
            ],
            "docstring": "Gibt eine n\u00fctzliche String-Repr\u00e4sentation des Objekts zur\u00fcck."
          },
          {
            "identifier": "backend.getRepo.RepoFile.to_dict",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "include_content"
            ],
            "docstring": null
          }
        ]
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.getRepo.GitRepository",
      "source_code": "class GitRepository:\n    \"\"\"\n    Verwaltet ein Git-Repository, einschlie\u00dflich Klonen in ein tempor\u00e4res\n    Verzeichnis und Bereitstellung von RepoFile-Objekten.\n    \"\"\"\n    def __init__(self, repo_url):\n        self.repo_url = repo_url\n        self.temp_dir = tempfile.mkdtemp()\n        self.repo = None\n\n        self.files = []\n        \n        try:\n            logging.info(f\"Clone Repository {self.repo_url}...\")\n            self.repo = Repo.clone_from(self.repo_url, self.temp_dir)\n            self.latest_commit = self.repo.head.commit\n            self.commit_tree = self.latest_commit.tree\n            logging.info(\"Cloning successful.\")\n        except GitCommandError as e:\n            self.close()\n            raise RuntimeError(f\"Error cloning repository: {e}\") from e\n\n    def get_all_files(self):\n        \"\"\"\n        Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zur\u00fcck.\n\n        Returns:\n            list[RepoFile]: Eine Liste von RepoFile-Instanzen.\n        \"\"\"\n        file_paths = self.repo.git.ls_files().split('\\n')\n        self.files = [RepoFile(path, self.commit_tree) for path in file_paths if path]\n        return self.files\n\n    def close(self):\n        \"\"\"L\u00f6scht das tempor\u00e4re Verzeichnis und dessen Inhalt.\"\"\"\n        if self.temp_dir:\n            print(f\"\\nL\u00f6sche tempor\u00e4res Verzeichnis: {self.temp_dir}\")\n            self.temp_dir = None\n            \n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def get_file_tree(self, include_content=False):\n\n        if not self.files:\n            self.get_all_files()\n\n        tree = {\"name\": \"root\", \"type\": \"directory\", \"children\": []}\n\n        for file_obj in self.files:\n            parts = file_obj.path.split('/')\n            current_level = tree[\"children\"]\n            \n            # Iteriere durch die Ordnerstruktur\n            for part in parts[:-1]:\n                # Suche, ob Ordner schon existiert\n                found = next((item for item in current_level if item[\"name\"] == part and item[\"type\"] == \"directory\"), None)\n                if not found:\n                    new_dir = {\"name\": part, \"type\": \"directory\", \"children\": []}\n                    current_level.append(new_dir)\n                    current_level = new_dir[\"children\"]\n                else:\n                    current_level = found[\"children\"]\n            \n            # Datei am Ende hinzuf\u00fcgen\n            current_level.append(file_obj.to_dict(include_content=include_content))\n\n        return tree",
      "imports": [
        "tempfile",
        "git.Repo",
        "git.GitCommandError",
        "logging",
        "os"
      ],
      "context": {
        "dependencies": [
          "backend.getRepo.RepoFile"
        ],
        "instantiated_by": [],
        "method_context": [
          {
            "identifier": "backend.getRepo.GitRepository.__init__",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "repo_url"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.getRepo.GitRepository.get_all_files",
            "calls": [
              "backend.getRepo.RepoFile"
            ],
            "called_by": [],
            "args": [
              "self"
            ],
            "docstring": "Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zur\u00fcck.\n\nReturns:\n    list[RepoFile]: Eine Liste von RepoFile-Instanzen."
          },
          {
            "identifier": "backend.getRepo.GitRepository.close",
            "calls": [],
            "called_by": [],
            "args": [
              "self"
            ],
            "docstring": "L\u00f6scht das tempor\u00e4re Verzeichnis und dessen Inhalt."
          },
          {
            "identifier": "backend.getRepo.GitRepository.__enter__",
            "calls": [],
            "called_by": [],
            "args": [
              "self"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.getRepo.GitRepository.__exit__",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "exc_type",
              "exc_val",
              "exc_tb"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.getRepo.GitRepository.get_file_tree",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "include_content"
            ],
            "docstring": null
          }
        ]
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.relationship_analyzer.ProjectAnalyzer",
      "source_code": "class ProjectAnalyzer:\n    \n    def __init__(self, project_root):\n        self.project_root = os.path.abspath(project_root)\n        self.definitions = {}\n        self.call_graph = defaultdict(list)\n        self.file_asts = {} \n        self.ignore_dirs = {'.git', '.venv', 'venv', '__pycache__', 'node_modules', 'dist', 'build', 'docs'}\n\n    def analyze(self):\n        py_files = self._find_py_files()\n        \n        for filepath in py_files:\n            self._collect_definitions(filepath)\n            \n        for filepath in py_files:\n            self._resolve_calls(filepath)\n            \n        self.file_asts.clear()\n        \n        return self.call_graph\n\n    def get_raw_relationships(self):\n\n        outgoing = defaultdict(set)\n        incoming = defaultdict(set)\n\n        for callee_id, callers_info_list in self.call_graph.items():\n            for info in callers_info_list:\n                caller_id = info['caller']\n                \n                if caller_id and callee_id:\n                    outgoing[caller_id].add(callee_id)\n                    incoming[callee_id].add(caller_id)\n        \n        return {\n            \"outgoing\": {k: sorted(list(v)) for k, v in outgoing.items()},\n            \"incoming\": {k: sorted(list(v)) for k, v in incoming.items()}\n        }\n\n    def _find_py_files(self):\n        py_files = []\n        for root, dirs, files in os.walk(self.project_root):\n            dirs[:] = [d for d in dirs if d not in self.ignore_dirs]\n            for file in files:\n                if file.endswith(\".py\"):\n                    py_files.append(os.path.join(root, file))\n        return py_files\n\n    def _collect_definitions(self, filepath):\n        try:\n            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n                source = f.read()\n                tree = ast.parse(source, filename=filepath)\n                \n            self.file_asts[filepath] = tree\n            module_path = path_to_module(filepath, self.project_root)\n            \n            for node in ast.walk(tree):\n                if isinstance(node, ast.FunctionDef):\n                    parent = self._get_parent(tree, node)\n                    if isinstance(parent, ast.ClassDef):\n                        path_name = f\"{module_path}.{parent.name}.{node.name}\"\n                        def_type = 'method'\n                    else:\n                        path_name = f\"{module_path}.{node.name}\"\n                        def_type = 'function'\n                    self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': def_type}\n                elif isinstance(node, ast.ClassDef):\n                    path_name = f\"{module_path}.{node.name}\"\n                    self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': 'class'}\n        except Exception as e:\n            logging.error(f\"Error collecting definitions in {filepath}: {e}\")\n            self.file_asts[filepath] = None\n\n    def _get_parent(self, tree, node):\n        for parent in ast.walk(tree):\n            for child in ast.iter_child_nodes(parent):\n                if child is node:\n                    return parent\n        return None\n\n    def _resolve_calls(self, filepath):\n        tree = self.file_asts.get(filepath)\n        if not tree:\n            return\n\n        try:\n            resolver = CallResolverVisitor(filepath, self.project_root, self.definitions)\n            resolver.visit(tree)\n            \n            for callee_pathname, caller_info_list in resolver.calls.items():\n                self.call_graph[callee_pathname].extend(caller_info_list)\n        except Exception as e:\n            logging.error(f\"Error resolving calls in {filepath}: {e}\")",
      "imports": [
        "ast",
        "os",
        "logging",
        "collections.defaultdict"
      ],
      "context": {
        "dependencies": [
          "backend.relationship_analyzer.CallResolverVisitor",
          "backend.relationship_analyzer.path_to_module"
        ],
        "instantiated_by": [],
        "method_context": [
          {
            "identifier": "backend.relationship_analyzer.ProjectAnalyzer.__init__",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "project_root"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.relationship_analyzer.ProjectAnalyzer.analyze",
            "calls": [],
            "called_by": [],
            "args": [
              "self"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.relationship_analyzer.ProjectAnalyzer.get_raw_relationships",
            "calls": [],
            "called_by": [],
            "args": [
              "self"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.relationship_analyzer.ProjectAnalyzer._find_py_files",
            "calls": [],
            "called_by": [],
            "args": [
              "self"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.relationship_analyzer.ProjectAnalyzer._collect_definitions",
            "calls": [
              "backend.relationship_analyzer.path_to_module"
            ],
            "called_by": [],
            "args": [
              "self",
              "filepath"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.relationship_analyzer.ProjectAnalyzer._get_parent",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "tree",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.relationship_analyzer.ProjectAnalyzer._resolve_calls",
            "calls": [
              "backend.relationship_analyzer.CallResolverVisitor"
            ],
            "called_by": [],
            "args": [
              "self",
              "filepath"
            ],
            "docstring": null
          }
        ]
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "backend.relationship_analyzer.CallResolverVisitor",
      "source_code": "class CallResolverVisitor(ast.NodeVisitor):\n    def __init__(self, filepath, project_root, definitions):\n        self.filepath = filepath\n        self.module_path = path_to_module(filepath, project_root)\n        self.definitions = definitions\n        self.scope = {}\n        self.instance_types = {}\n        self.current_caller_name = self.module_path\n        self.current_class_name = None\n        self.calls = defaultdict(list)\n\n    def visit_ClassDef(self, node):\n        old_class_name = self.current_class_name\n        self.current_class_name = node.name\n        self.generic_visit(node)\n        self.current_class_name = old_class_name\n\n    def visit_FunctionDef(self, node):\n        old_caller = self.current_caller_name\n        \n        if self.current_class_name:\n            full_identifier = f\"{self.module_path}.{self.current_class_name}.{node.name}\"\n        else:\n            full_identifier = f\"{self.module_path}.{node.name}\"\n            \n        self.current_caller_name = full_identifier\n        self.generic_visit(node)\n        self.current_caller_name = old_caller\n\n    def visit_Call(self, node):\n        callee_pathname = self._resolve_call_qname(node.func)\n        if callee_pathname and callee_pathname in self.definitions:\n            \n            if self.current_caller_name == self.module_path:\n                caller_type = 'module'\n            elif '.<locals>.' in self.current_caller_name:\n                caller_type = 'local_function'\n            elif self.current_class_name:\n                caller_type = 'method'\n            else:\n                caller_type = 'function'\n            \n            caller_info = {\n                'file': os.path.basename(self.filepath),\n                'line': node.lineno,\n                'caller': self.current_caller_name,\n                'caller_type': caller_type \n            }\n            self.calls[callee_pathname].append(caller_info)\n        self.generic_visit(node)\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.scope[alias.asname or alias.name] = alias.name\n        self.generic_visit(node)\n        \n    def visit_ImportFrom(self, node):\n        module = node.module or ''\n        for alias in node.names:\n            name = alias.asname or alias.name\n            if node.level > 0:\n                base = self.module_path.split('.')\n                prefix = '.'.join(base[:-node.level])\n                full_module_path = f\"{prefix}.{module}\" if module else prefix\n            else:\n                full_module_path = module\n            self.scope[name] = f\"{full_module_path}.{alias.name}\"\n        self.generic_visit(node)\n\n    def visit_Assign(self, node):\n        if isinstance(node.value, ast.Call) and isinstance(node.value.func, ast.Name):\n            class_name = node.value.func.id\n            if class_name in self.scope:\n                qualified_class_name = self.scope[class_name]\n                if qualified_class_name in self.definitions:\n                    for target in node.targets:\n                        if isinstance(target, ast.Name):\n                            self.instance_types[target.id] = qualified_class_name\n        self.generic_visit(node)\n\n    def _resolve_call_qname(self, func_node):\n        if isinstance(func_node, ast.Name):\n            name = func_node.id\n            if name in self.scope:\n                return self.scope[name]\n            local_pathname = f\"{self.module_path}.{name}\"\n            if local_pathname in self.definitions:\n                return local_pathname\n        elif isinstance(func_node, ast.Attribute) and isinstance(func_node.value, ast.Name):\n            var_name = func_node.value.id\n            method_name = func_node.attr\n            if var_name in self.instance_types:\n                class_pathname = self.instance_types[var_name]\n                return f\"{class_pathname}.{method_name}\"\n            if var_name in self.scope:\n                module_pathname = self.scope[var_name]\n                return f\"{module_pathname}.{method_name}\"\n        return None",
      "imports": [
        "ast",
        "os",
        "logging",
        "collections.defaultdict"
      ],
      "context": {
        "dependencies": [
          "backend.relationship_analyzer.path_to_module"
        ],
        "instantiated_by": [],
        "method_context": [
          {
            "identifier": "backend.relationship_analyzer.CallResolverVisitor.__init__",
            "calls": [
              "backend.relationship_analyzer.path_to_module"
            ],
            "called_by": [],
            "args": [
              "self",
              "filepath",
              "project_root",
              "definitions"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.relationship_analyzer.CallResolverVisitor.visit_ClassDef",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.relationship_analyzer.CallResolverVisitor.visit_FunctionDef",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.relationship_analyzer.CallResolverVisitor.visit_Call",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.relationship_analyzer.CallResolverVisitor.visit_Import",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.relationship_analyzer.CallResolverVisitor.visit_ImportFrom",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.relationship_analyzer.CallResolverVisitor.visit_Assign",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "node"
            ],
            "docstring": null
          },
          {
            "identifier": "backend.relationship_analyzer.CallResolverVisitor._resolve_call_qname",
            "calls": [],
            "called_by": [],
            "args": [
              "self",
              "func_node"
            ],
            "docstring": null
          }
        ]
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "schemas.enums.AnalysisMode",
      "source_code": "class AnalysisMode(str, Enum):\n    \"Detailgrad der Analyse\"\n    OVERVIEW = \"overview\"\n    STANDARD = \"standard\"\n    DETAILED = \"detailed\"\n    DEEP_DIVE = \"deep_dive\"",
      "imports": [
        "enum.Enum"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "schemas.enums.DiagramFocus",
      "source_code": "class DiagramFocus(str, Enum):\n    \"\"\"Schwerpunkt der Visualisierung im Diagramm.\"\"\"\n    ARCHITECTURE = \"architecture\"\n    DATA_FLOW = \"data_flow\"\n    DEPENDENCIES = \"dependencies\"\n    CALL_GRAPH = \"call_graph\"",
      "imports": [
        "enum.Enum"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "schemas.enums.DiagramType",
      "source_code": "class DiagramType(str, Enum):\n    \"\"\"Mermaid Diagramm-Typen\"\"\"\n    CLASS_DIAGRAM = \"classDiagram\"\n    SEQUENCE_DIAGRAM = \"sequenceDiagram\"\n    FLOWCHART = \"flowchart\"\n    STATE_DIAGRAM = \"stateDiagram\"\n    GRAPH = \"graph\"\n    CALL_GRAPH = \"callgraph\"\n    FILE_DEPENCY = \"fileDependencyGraph\"",
      "imports": [
        "enum.Enum"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "schemas.enums.GraphType",
      "source_code": "class GraphType(str, Enum):\n    \"\"\"Typ des Input-Graphen\"\"\"\n    CALLGRAPH = \"callgraph\"\n    DEPENDENCY = \"dependency\"\n    COMBINED = \"combined\"",
      "imports": [
        "enum.Enum"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "schemas.types.ParameterDescription",
      "source_code": "class ParameterDescription(BaseModel):\n    \"\"\"Describes a single parameter of a function.\"\"\"\n    name: str\n    type: str\n    description: str",
      "imports": [
        "typing.List",
        "typing.Optional",
        "typing.Literal",
        "pydantic.BaseModel",
        "pydantic.ValidationError",
        "schemas.enums.AnalysisMode",
        "schemas.enums.DiagramFocus",
        "schemas.enums.DiagramType"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "schemas.types.ReturnDescription",
      "source_code": "class ReturnDescription(BaseModel):\n    \"\"\"Describes the return value of a function.\"\"\"\n    name: str\n    type: str\n    description: str",
      "imports": [
        "typing.List",
        "typing.Optional",
        "typing.Literal",
        "pydantic.BaseModel",
        "pydantic.ValidationError",
        "schemas.enums.AnalysisMode",
        "schemas.enums.DiagramFocus",
        "schemas.enums.DiagramType"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "schemas.types.UsageContext",
      "source_code": "class UsageContext(BaseModel):\n    \"\"\"Describes the calling context of a function.\"\"\"\n    calls: str\n    called_by: str",
      "imports": [
        "typing.List",
        "typing.Optional",
        "typing.Literal",
        "pydantic.BaseModel",
        "pydantic.ValidationError",
        "schemas.enums.AnalysisMode",
        "schemas.enums.DiagramFocus",
        "schemas.enums.DiagramType"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "schemas.types.FunctionDescription",
      "source_code": "class FunctionDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a function's purpose and signature.\"\"\"\n    overall: str\n    parameters: List[ParameterDescription]\n    returns: List[ReturnDescription]\n    usage_context: UsageContext",
      "imports": [
        "typing.List",
        "typing.Optional",
        "typing.Literal",
        "pydantic.BaseModel",
        "pydantic.ValidationError",
        "schemas.enums.AnalysisMode",
        "schemas.enums.DiagramFocus",
        "schemas.enums.DiagramType"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "schemas.types.FunctionAnalysis",
      "source_code": "class FunctionAnalysis(BaseModel):\n    \"\"\"The main model representing the entire JSON schema for a function.\"\"\"\n    identifier: str\n    description: FunctionDescription\n    error: Optional[str] = None",
      "imports": [
        "typing.List",
        "typing.Optional",
        "typing.Literal",
        "pydantic.BaseModel",
        "pydantic.ValidationError",
        "schemas.enums.AnalysisMode",
        "schemas.enums.DiagramFocus",
        "schemas.enums.DiagramType"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "schemas.types.ConstructorDescription",
      "source_code": "class ConstructorDescription(BaseModel):\n    \"\"\"Describes the __init__ method of a class.\"\"\"\n    description: str\n    parameters: List[ParameterDescription]",
      "imports": [
        "typing.List",
        "typing.Optional",
        "typing.Literal",
        "pydantic.BaseModel",
        "pydantic.ValidationError",
        "schemas.enums.AnalysisMode",
        "schemas.enums.DiagramFocus",
        "schemas.enums.DiagramType"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "schemas.types.ClassContext",
      "source_code": "class ClassContext(BaseModel):\n    \"\"\"Describes the class's external dependencies and primary points of instantiation.\"\"\"\n    dependencies: str\n    instantiated_by: str",
      "imports": [
        "typing.List",
        "typing.Optional",
        "typing.Literal",
        "pydantic.BaseModel",
        "pydantic.ValidationError",
        "schemas.enums.AnalysisMode",
        "schemas.enums.DiagramFocus",
        "schemas.enums.DiagramType"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "schemas.types.ClassDescription",
      "source_code": "class ClassDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a class's purpose, constructor, and methods.\"\"\"\n    overall: str\n    init_method: ConstructorDescription\n    methods: List[FunctionAnalysis]\n    usage_context: ClassContext",
      "imports": [
        "typing.List",
        "typing.Optional",
        "typing.Literal",
        "pydantic.BaseModel",
        "pydantic.ValidationError",
        "schemas.enums.AnalysisMode",
        "schemas.enums.DiagramFocus",
        "schemas.enums.DiagramType"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "schemas.types.ClassAnalysis",
      "source_code": "class ClassAnalysis(BaseModel):\n    \"\"\"The main model for the entire JSON schema for a class.\"\"\"\n    identifier: str\n    description: ClassDescription\n    error: Optional[str] = None",
      "imports": [
        "typing.List",
        "typing.Optional",
        "typing.Literal",
        "pydantic.BaseModel",
        "pydantic.ValidationError",
        "schemas.enums.AnalysisMode",
        "schemas.enums.DiagramFocus",
        "schemas.enums.DiagramType"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "schemas.types.CallInfo",
      "source_code": "class CallInfo(BaseModel):\n    \"\"\"\n    Represents a specific call event from the relationship analyzer.\n    Used in 'called_by' and 'instantiated_by' lists.\n    \"\"\"\n    file: str\n    function: str  # Name des Aufrufers\n    mode: str      # z.B. 'method', 'function', 'module'\n    line: int",
      "imports": [
        "typing.List",
        "typing.Optional",
        "typing.Literal",
        "pydantic.BaseModel",
        "pydantic.ValidationError",
        "schemas.enums.AnalysisMode",
        "schemas.enums.DiagramFocus",
        "schemas.enums.DiagramType"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "schemas.types.FunctionContextInput",
      "source_code": "class FunctionContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a function.\"\"\"\n    calls: List[str]\n    called_by: List[CallInfo]",
      "imports": [
        "typing.List",
        "typing.Optional",
        "typing.Literal",
        "pydantic.BaseModel",
        "pydantic.ValidationError",
        "schemas.enums.AnalysisMode",
        "schemas.enums.DiagramFocus",
        "schemas.enums.DiagramType"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "schemas.types.FunctionAnalysisInput",
      "source_code": "class FunctionAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a FunctionAnalysis object.\"\"\"\n    mode: Literal[\"function_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: FunctionContextInput",
      "imports": [
        "typing.List",
        "typing.Optional",
        "typing.Literal",
        "pydantic.BaseModel",
        "pydantic.ValidationError",
        "schemas.enums.AnalysisMode",
        "schemas.enums.DiagramFocus",
        "schemas.enums.DiagramType"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "schemas.types.MethodContextInput",
      "source_code": "class MethodContextInput(BaseModel):\n    \"\"\"Structured context for a classes methods\"\"\"\n    identifier: str\n    calls: List[str]\n    called_by: List[CallInfo]\n    args: List[str]\n    docstring: Optional[str]",
      "imports": [
        "typing.List",
        "typing.Optional",
        "typing.Literal",
        "pydantic.BaseModel",
        "pydantic.ValidationError",
        "schemas.enums.AnalysisMode",
        "schemas.enums.DiagramFocus",
        "schemas.enums.DiagramType"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "schemas.types.ClassContextInput",
      "source_code": "class ClassContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a class.\"\"\"\n    dependencies: List[str]\n    instantiated_by: List[CallInfo]\n    method_context: List[MethodContextInput]",
      "imports": [
        "typing.List",
        "typing.Optional",
        "typing.Literal",
        "pydantic.BaseModel",
        "pydantic.ValidationError",
        "schemas.enums.AnalysisMode",
        "schemas.enums.DiagramFocus",
        "schemas.enums.DiagramType"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "schemas.types.ClassAnalysisInput",
      "source_code": "class ClassAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a ClassAnalysis object.\"\"\"\n    mode: Literal[\"class_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: ClassContextInput",
      "imports": [
        "typing.List",
        "typing.Optional",
        "typing.Literal",
        "pydantic.BaseModel",
        "pydantic.ValidationError",
        "schemas.enums.AnalysisMode",
        "schemas.enums.DiagramFocus",
        "schemas.enums.DiagramType"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "schemas.types.DiagramRequest",
      "source_code": "class DiagramRequest(BaseModel):\n    \"\"\"Input schema for requesting a diagram generation.\"\"\"\n    nodes: list[dict]\n    edges: list[tuple]\n\n    mode: AnalysisMode = AnalysisMode.STANDARD\n    focus: DiagramFocus = DiagramFocus.ARCHITECTURE",
      "imports": [
        "typing.List",
        "typing.Optional",
        "typing.Literal",
        "pydantic.BaseModel",
        "pydantic.ValidationError",
        "schemas.enums.AnalysisMode",
        "schemas.enums.DiagramFocus",
        "schemas.enums.DiagramType"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "schemas.types.GraphInput",
      "source_code": "class GraphInput(BaseModel):\n    \"\"\"Repr\u00e4sentiert deinen CallGraph in serialisierbarer Form\"\"\"\n    \n    nodes: list[dict]\n    edges: list[tuple]\n    graph_type: str\n    metadata: dict = {}",
      "imports": [
        "typing.List",
        "typing.Optional",
        "typing.Literal",
        "pydantic.BaseModel",
        "pydantic.ValidationError",
        "schemas.enums.AnalysisMode",
        "schemas.enums.DiagramFocus",
        "schemas.enums.DiagramType"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    },
    {
      "mode": "class_analysis",
      "identifier": "schemas.types.DiagramOutput",
      "source_code": "class DiagramOutput(BaseModel):\n    \"\"\"Output schema for generated diagram information.\"\"\"\n    diagramtype: DiagramType\n    mermaid_code: str\n    explanation: str",
      "imports": [
        "typing.List",
        "typing.Optional",
        "typing.Literal",
        "pydantic.BaseModel",
        "pydantic.ValidationError",
        "schemas.enums.AnalysisMode",
        "schemas.enums.DiagramFocus",
        "schemas.enums.DiagramType"
      ],
      "context": {
        "dependencies": [],
        "instantiated_by": [],
        "method_context": []
      }
    }
  ]
}