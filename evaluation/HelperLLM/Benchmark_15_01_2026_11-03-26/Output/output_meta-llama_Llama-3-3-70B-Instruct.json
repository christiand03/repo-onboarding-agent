{
  "functions": {
    "app_pages.page1.show_page": {
      "identifier": "app_pages.page1.show_page",
      "description": {
        "overall": "The `show_page` function is designed to display key performance indicators (KPIs) and charts related to data quality and error rates in two datasets, `df` and `df2`. It utilizes the Streamlit library to create an interactive dashboard. The function takes in several parameters, including the datasets and their corresponding metrics. It calculates various KPIs such as row counts, null row ratios, and unique identifier checks. The function then displays these KPIs in a series of metrics and charts, including bar charts for null values and heatmaps for error frequencies. The goal of this function is to provide a visual representation of data quality issues and trends, allowing users to quickly identify areas for improvement.",
        "parameters": [
          {
            "name": "df",
            "type": "pandas.DataFrame",
            "description": "The first dataset, used for calculating KPIs and displaying charts."
          },
          {
            "name": "df2",
            "type": "pandas.DataFrame",
            "description": "The second dataset, used for calculating KPIs and displaying charts."
          },
          {
            "name": "metrics_df1",
            "type": "dict",
            "description": "A dictionary containing metrics for the first dataset, such as row counts and null row ratios."
          },
          {
            "name": "metrics_df2",
            "type": "dict",
            "description": "A dictionary containing metrics for the second dataset, such as row counts and null row ratios."
          },
          {
            "name": "metrics_combined",
            "type": "dict",
            "description": "A dictionary containing combined metrics for both datasets, such as unique identifier checks."
          }
        ],
        "returns": [],
        "usage_context": {
          "calls": "This function calls the `metrics.ratio_null_values_rows` function.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "app_pages.page2.show_page": {
      "identifier": "app_pages.page2.show_page",
      "description": {
        "overall": "The `show_page` function is designed to display various metrics and dataframes related to Zeitwerte and Auftr\u00e4ge. It takes in five dataframes as parameters: `df`, `df2`, `metrics_df1`, `metrics_df2`, and `metrics_combined`. The function calculates Zeitwerte error series and count, as well as the number of Auftr\u00e4ge above 50,000\u20ac. It then displays these metrics using Streamlit's `st.metric` function. Additionally, the function shows dataframes for incorrect Zeitwerte, Auftr\u00e4ge above 50,000\u20ac, and a comparison of Positionssummen with Auftragssummen.",
        "parameters": [
          {
            "name": "df",
            "type": "pandas.DataFrame",
            "description": "The primary dataframe containing Zeitwerte and Auftr\u00e4ge data."
          },
          {
            "name": "df2",
            "type": "pandas.DataFrame",
            "description": "A secondary dataframe, purpose not explicitly defined in the provided code."
          },
          {
            "name": "metrics_df1",
            "type": "pandas.DataFrame",
            "description": "A dataframe containing metrics related to Zeitwerte and Auftr\u00e4ge."
          },
          {
            "name": "metrics_df2",
            "type": "pandas.DataFrame",
            "description": "A secondary metrics dataframe, purpose not explicitly defined in the provided code."
          },
          {
            "name": "metrics_combined",
            "type": "pandas.DataFrame",
            "description": "A combined dataframe containing various metrics, including Auftr\u00e4ge abgleich."
          }
        ],
        "returns": [],
        "usage_context": {
          "calls": "This function calls metrics.above_50k and metrics.check_zeitwert.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "app_pages.page3.show_page": {
      "identifier": "app_pages.page3.show_page",
      "description": {
        "overall": "The `show_page` function is designed to display specific metrics and dataframes related to handwerker/gewerke assignments. It takes in five dataframes as parameters: `df`, `df2`, `metrics_df1`, `metrics_df2`, and `metrics_combined`. The function uses the `streamlit` library to create a user interface, displaying metrics such as the number of test datasets in a kundengruppe and dataframes related to handwerker/gewerke assignments. The function does not return any values, instead, it displays the results directly in the streamlit app.",
        "parameters": [
          {
            "name": "df",
            "type": "pandas.DataFrame",
            "description": "The first input dataframe."
          },
          {
            "name": "df2",
            "type": "pandas.DataFrame",
            "description": "The second input dataframe."
          },
          {
            "name": "metrics_df1",
            "type": "dict",
            "description": "A dictionary containing metrics, including 'handwerker_gewerke_outlier' and 'test_kundengruppen_anzahl'."
          },
          {
            "name": "metrics_df2",
            "type": "pandas.DataFrame",
            "description": "The second metrics dataframe."
          },
          {
            "name": "metrics_combined",
            "type": "pandas.DataFrame",
            "description": "The combined metrics dataframe."
          }
        ],
        "returns": [],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "app_pages.page4.show_page": {
      "identifier": "app_pages.page4.show_page",
      "description": {
        "overall": "The `show_page` function is designed to display various metrics and key performance indicators (KPIs) related to order and position data. It takes in five parameters: `df`, `df2`, `metrics_df1`, `metrics_df2`, and `metrics_combined`. The function retrieves specific metrics from `metrics_df1` and `metrics_df2`, such as plausibility differences, counts, and averages, and then displays these metrics using Streamlit's `st.metric` function. Additionally, it calculates and displays KPIs related to potential errors in discount checks and incorrect negative amounts.",
        "parameters": [
          {
            "name": "df",
            "type": "pandas DataFrame",
            "description": "The first input DataFrame, likely containing order data."
          },
          {
            "name": "df2",
            "type": "pandas DataFrame",
            "description": "The second input DataFrame, likely containing position data."
          },
          {
            "name": "metrics_df1",
            "type": "dictionary",
            "description": "A dictionary containing metrics related to order data."
          },
          {
            "name": "metrics_df2",
            "type": "dictionary",
            "description": "A dictionary containing metrics related to position data."
          },
          {
            "name": "metrics_combined",
            "type": "unknown",
            "description": "The purpose of this parameter is unclear, as it is not used within the function."
          }
        ],
        "returns": [],
        "usage_context": {
          "calls": "This function calls metrics.discount_check, metrics.false_negative_df, and metrics.false_negative_df2.",
          "called_by": "This function is not called by any other functions, according to the provided context."
        }
      },
      "error": null
    },
    "app_pages.page5.show_page": {
      "identifier": "app_pages.page5.show_page",
      "description": {
        "overall": "The function `show_page` is designed to display a title on a webpage. It utilizes the Streamlit library to set the title of the page to 'Page 5'. This function does not take any parameters and does not return any values. It appears to be part of a larger application that uses Streamlit for creating web pages. The function's purpose is straightforward, focusing solely on setting the page title. There are no conditional statements or loops within this function, indicating its simplicity.",
        "parameters": [],
        "returns": [],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "dashboard.load": {
      "identifier": "dashboard.load",
      "description": {
        "overall": "The `dashboard.load` function loads data from two parquet files, `Auftragsdaten_konvertiert` and `Positionsdaten_konvertiert`, located in the `resources` directory. It utilizes the `pd.read_parquet` function from the pandas library to read these files into two separate dataframes, `df` and `df2`. The function then returns these two dataframes. This function appears to be used for data loading purposes, possibly as part of a larger data analysis or visualization pipeline. The function does not perform any data manipulation or analysis; it solely focuses on loading the data.",
        "parameters": [],
        "returns": [
          {
            "name": "df",
            "type": "pandas.DataFrame",
            "description": "The first dataframe loaded from `Auftragsdaten_konvertiert` parquet file."
          },
          {
            "name": "df2",
            "type": "pandas.DataFrame",
            "description": "The second dataframe loaded from `Positionsdaten_konvertiert` parquet file."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "dashboard.compute_metrics_df1": {
      "identifier": "dashboard.compute_metrics_df1",
      "description": {
        "overall": "The `compute_metrics_df1` function calculates various metrics for a dataset `df1` (Auftragsdaten). It loads the dataset, performs plausibility checks, calculates Zeitwert errors, generates proforma belege, and computes data cleanliness metrics. The function also calculates error frequencies by weekday and hour, and identifies handwerker gewerke outliers. Finally, it returns a dictionary containing all the calculated metrics.",
        "parameters": [],
        "returns": [
          {
            "name": "metrics_df1",
            "type": "dict",
            "description": "A dictionary containing various metrics calculated for the dataset `df1`."
          }
        ],
        "usage_context": {
          "calls": "This function calls dashboard.load, metrics.Kundengruppe_containing_test, metrics.above_50k, metrics.allgemeine_statistiken_num, metrics.check_keywords_vectorized, metrics.check_zeitwert, metrics.count_rows, metrics.data_cleanliness, metrics.error_frequency_by_weekday_hour, metrics.false_negative_df, metrics.handwerker_gewerke_outlier, metrics.plausibilitaetscheck_forderung_einigung, metrics.proformabelege, metrics.ratio_null_values_column, and metrics.ratio_null_values_rows.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "dashboard.compute_metrics_df2": {
      "identifier": "dashboard.compute_metrics_df2",
      "description": {
        "overall": "The `compute_metrics_df2` function calculates various metrics for the `df2` dataset, which appears to contain position data. It loads the data, performs plausibility checks, and computes statistical metrics such as row count, null value ratios, and general statistics. The function returns a dictionary containing these metrics. The calculations are timed and printed to the console for debugging purposes.",
        "parameters": [],
        "returns": [
          {
            "name": "metrics_df2",
            "type": "dict",
            "description": "A dictionary containing various metrics for the `df2` dataset, including row count, null value ratios, statistical metrics, and plausibility check results."
          }
        ],
        "usage_context": {
          "calls": "This function calls dashboard.load, metrics.allgemeine_statistiken_num, metrics.count_rows, metrics.discount_check, metrics.false_negative_df2, metrics.plausibilitaetscheck_forderung_einigung, metrics.position_count, metrics.ratio_null_values_column, and metrics.ratio_null_values_rows.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "dashboard.compute_metrics_combined": {
      "identifier": "dashboard.compute_metrics_combined",
      "description": {
        "overall": "The `compute_metrics_combined` function calculates and returns combined metrics that require both DataFrames, utilizing caching for efficiency. It loads the necessary DataFrames using the `load` function, then computes uniqueness checks and order comparisons between the two DataFrames using functions from the `metrics` module. The function prints status updates and timing information, returning a dictionary containing the calculated metrics. This function appears to be part of a larger dashboard application, leveraging various modules for data manipulation and visualization.",
        "parameters": [],
        "returns": [
          {
            "name": "metrics_combined",
            "type": "dict",
            "description": "A dictionary containing the calculated combined metrics, including uniqueness checks and order comparisons."
          }
        ],
        "usage_context": {
          "calls": "This function calls dashboard.load, metrics.abgleich_auftraege, and metrics.uniqueness_check.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "dashboard.compute_positions_over_time": {
      "identifier": "dashboard.compute_positions_over_time",
      "description": {
        "overall": "The `compute_positions_over_time` function calculates the positions per order over time. It loads data using the `load` function, then uses the `positions_per_order_over_time` function from the `metrics` module to perform the calculation. The function prints a message indicating the start and completion of the calculation, along with the time taken. The result is returned as a DataFrame.",
        "parameters": [],
        "returns": [
          {
            "name": "positions_over_time_df",
            "type": "DataFrame",
            "description": "A DataFrame containing the positions per order over time."
          }
        ],
        "usage_context": {
          "calls": "This function calls dashboard.load and metrics.positions_per_order_over_time.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "data_drift.load_data": {
      "identifier": "data_drift.load_data",
      "description": {
        "overall": "The `load_data` function generates a synthetic dataset for demonstrating data drift. It creates a date range from January 1, 2022, to December 31, 2023, and then constructs a pandas DataFrame with three columns: 'datum' (date), 'umsatz' (sales), and 'kunden' (customers). The 'umsatz' column is designed to show a gradual increase over time, with added random noise to simulate real-world variability. The 'kunden' column consists of random integers between 10 and 50. The function returns this DataFrame.",
        "parameters": [],
        "returns": [
          {
            "name": "df",
            "type": "pandas.DataFrame",
            "description": "A pandas DataFrame containing the synthetic dataset."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "data_drift.filterby_timeframe": {
      "identifier": "data_drift.filterby_timeframe",
      "description": {
        "overall": "The `data_drift.filterby_timeframe` function filters a given input DataFrame based on a specified timeframe. It takes in an input DataFrame, a start date, and an end date. The function converts the start and end dates to datetime objects and creates a mask to filter the DataFrame. The filtered DataFrame is then returned, containing only the rows where the 'datum' column falls within the specified timeframe. This function is useful for narrowing down data to a specific period. It relies on the pandas library for datetime conversion and DataFrame manipulation.",
        "parameters": [
          {
            "name": "input_df",
            "type": "pandas.DataFrame",
            "description": "The input DataFrame to be filtered."
          },
          {
            "name": "start_date",
            "type": "str or datetime-like",
            "description": "The start date of the timeframe."
          },
          {
            "name": "end_date",
            "type": "str or datetime-like",
            "description": "The end date of the timeframe."
          }
        ],
        "returns": [
          {
            "name": "filtered_df",
            "type": "pandas.DataFrame",
            "description": "The filtered DataFrame containing only the rows within the specified timeframe."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "data_drift.get_drift_stats": {
      "identifier": "data_drift.get_drift_stats",
      "description": {
        "overall": "The `get_drift_stats` function calculates statistical metrics (mean, median, standard deviation, minimum, and maximum) for the 'umsatz' and 'kunden' columns in the input DataFrame, grouped by a specified frequency. It utilizes the pandas library for data manipulation and grouping. The function returns a new DataFrame with the calculated statistics, where the column names are flattened for easier plotting. The input DataFrame is expected to have a 'datum' column, which is used for grouping by time. The function does not handle any errors explicitly, but it may raise exceptions if the input DataFrame is missing required columns or if the frequency parameter is invalid.",
        "parameters": [
          {
            "name": "input_df",
            "type": "pandas.DataFrame",
            "description": "The input DataFrame containing the data to be analyzed."
          },
          {
            "name": "frequency",
            "type": "string",
            "description": "The frequency parameter for grouping the data by time."
          }
        ],
        "returns": [
          {
            "name": "stats_df",
            "type": "pandas.DataFrame",
            "description": "A new DataFrame containing the calculated statistical metrics."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "data_drift.slicing": {
      "identifier": "data_drift.slicing",
      "description": {
        "overall": "The `slicing` function takes an input DataFrame and a frequency as parameters. It groups the input data by the specified frequency using the 'datum' column and creates slices for each group. The function returns a dictionary where each key is a date label and the corresponding value is the slice of the input data for that date. If an error occurs during the slicing process, the function catches the exception, displays an error message, and returns an empty dictionary.",
        "parameters": [
          {
            "name": "input_df",
            "type": "pandas.DataFrame",
            "description": "The input DataFrame to be sliced."
          },
          {
            "name": "frequency",
            "type": "string",
            "description": "The frequency used for grouping the input data."
          }
        ],
        "returns": [
          {
            "name": "slices",
            "type": "dict",
            "description": "A dictionary where each key is a date label and the corresponding value is the slice of the input data for that date."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "data_drift_metrics.load": {
      "identifier": "data_drift_metrics.load",
      "description": {
        "overall": "The `data_drift_metrics.load` function loads data from two Parquet files, sorts the data by the 'CRMEingangszeit' column, merges the two datasets based on the 'KvaRechnung_ID' column, and returns the sorted and merged datasets. The function utilizes the pandas library for data manipulation. It reads data from 'resources/Auftragsdaten_konvertiert' and 'resources/Positionsdaten_konvertiert' files. The function does not take any parameters and returns two DataFrames.",
        "parameters": [],
        "returns": [
          {
            "name": "df",
            "type": "pandas.DataFrame",
            "description": "The sorted DataFrame from 'resources/Auftragsdaten_konvertiert' file."
          },
          {
            "name": "df2",
            "type": "pandas.DataFrame",
            "description": "The merged and sorted DataFrame from 'resources/Positionsdaten_konvertiert' file."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "data_drift_metrics.check_start_end_date": {
      "identifier": "data_drift_metrics.check_start_end_date",
      "description": {
        "overall": "The `check_start_end_date` function is a helper function designed to ensure that a given start date precedes an end date chronologically. It takes two datetime parameters, `start` and `end`, representing the beginning and end of an interval, respectively. If the start date is later than the end date, the function swaps these two dates to maintain chronological order. The function returns a pair of datetime values, which are the chronologically sorted start and end dates of the interval. This function is crucial for maintaining data integrity in applications where date ranges are critical. It operates independently, relying solely on the input dates for its logic.",
        "parameters": [
          {
            "name": "start",
            "type": "datetime",
            "description": "The assumed beginning of the interval."
          },
          {
            "name": "end",
            "type": "datetime",
            "description": "The assumed end of the interval."
          }
        ],
        "returns": [
          {
            "name": "start",
            "type": "datetime",
            "description": "The chronologically sorted start date of the interval."
          },
          {
            "name": "end",
            "type": "datetime",
            "description": "The chronologically sorted end date of the interval."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "data_drift_metrics.datetime_slice_mask": {
      "identifier": "data_drift_metrics.datetime_slice_mask",
      "description": {
        "overall": "The `datetime_slice_mask` function is a helper function that returns a chronologically sliced dataset according to the passed datetime. It takes in a pandas DataFrame `df`, a `start_date`, and an `end_date` as parameters. The function creates a mask to filter the DataFrame based on the datetime range and then converts the sliced DataFrame to an Evidently Dataset. The function also checks for specific columns in the DataFrame to determine which schema to use for the Dataset conversion.",
        "parameters": [
          {
            "name": "df",
            "type": "pandas.DataFrame",
            "description": "The input DataFrame to be sliced."
          },
          {
            "name": "start_date",
            "type": "datetime",
            "description": "The start date of the datetime range."
          },
          {
            "name": "end_date",
            "type": "datetime",
            "description": "The end date of the datetime range."
          }
        ],
        "returns": [
          {
            "name": "sliced_ds",
            "type": "evidently.Dataset",
            "description": "The sliced DataFrame converted to an Evidently Dataset."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "data_drift_metrics.data_drift_evaluation": {
      "identifier": "data_drift_metrics.data_drift_evaluation",
      "description": {
        "overall": "The data_drift_evaluation function evaluates data drift between two samples from a passed DataFrame using the standard preset in the Evidently AI framework. It takes in a DataFrame and four datetime parameters: start_date_reference, end_date_reference, start_date_eval, and end_date_eval. The function first checks if the start and end dates are in chronological order and switches them if needed. Then, it creates sliced datasets for analysis using the datetime_slice_mask function. Depending on the presence of specific columns in the DataFrame, it generates a report using the DataDriftPreset and saves the resulting Snapshot object as HTML for easy embedding. The function handles two different types of DataFrames: Auftragsdaten and Positionsdaten.",
        "parameters": [
          {
            "name": "df",
            "type": "pandas.DataFrame",
            "description": "The DataFrame to sample from."
          },
          {
            "name": "start_date_reference",
            "type": "datetime",
            "description": "The starting datetime of the reference, baseline dataset."
          },
          {
            "name": "end_date_reference",
            "type": "datetime",
            "description": "The ending datetime of the reference, baseline dataset."
          },
          {
            "name": "start_date_eval",
            "type": "datetime",
            "description": "The starting datetime of the evaluated dataset."
          },
          {
            "name": "end_date_eval",
            "type": "datetime",
            "description": "The ending datetime of the evaluated dataset."
          }
        ],
        "returns": [],
        "usage_context": {
          "calls": "This function calls data_drift_metrics.check_start_end_date and data_drift_metrics.datetime_slice_mask.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "data_exploration.load": {
      "identifier": "data_exploration.load",
      "description": {
        "overall": "The `data_exploration.load` function loads data from two Parquet files, `Auftragsdaten_konvertiert` and `Positionsdaten_konvertiert`, located in the `resources` directory. It utilizes the `pd.read_parquet` function from the pandas library to read these files into DataFrames. The function then returns these two DataFrames. The purpose of this function appears to be data exploration or preparation for further analysis. The function does not perform any data manipulation or filtering; it simply loads the data into memory.",
        "parameters": [],
        "returns": [
          {
            "name": "df",
            "type": "pandas.DataFrame",
            "description": "The first DataFrame loaded from `Auftragsdaten_konvertiert`"
          },
          {
            "name": "df2",
            "type": "pandas.DataFrame",
            "description": "The second DataFrame loaded from `Positionsdaten_konvertiert`"
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "db_dashboard.get_db_connection": {
      "identifier": "db_dashboard.get_db_connection",
      "description": {
        "overall": "The `get_db_connection` function establishes a read-only connection to the DuckDB database. It utilizes the `duckdb.connect` method, passing in the `DB_PATH` and setting `read_only` to `True`. This function is designed to provide a connection to the database without allowing modifications. The connection is returned by the function, allowing it to be used in subsequent database operations. The function does not take any parameters, making it a straightforward and reusable way to connect to the database.",
        "parameters": [],
        "returns": [
          {
            "name": "connection",
            "type": "duckdb.Connection",
            "description": "A read-only connection to the DuckDB database."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "db_dashboard.load": {
      "identifier": "db_dashboard.load",
      "description": {
        "overall": "The `db_dashboard.load` function loads raw cleaned dataframes from DuckDB. It establishes a database connection using `get_db_connection`, executes two SQL queries to retrieve data from the `auftragsdaten` and `positionsdaten` tables, and returns the resulting dataframes. The function ensures the database connection is closed after use, regardless of whether an exception occurs. The purpose of this function is to fetch specific data from the database for further processing or analysis. The function does not handle any exceptions that may occur during query execution, relying on the caller to handle such errors.",
        "parameters": [],
        "returns": [
          {
            "name": "df",
            "type": "pandas.DataFrame",
            "description": "The dataframe containing data from the `auftragsdaten` table."
          },
          {
            "name": "df2",
            "type": "pandas.DataFrame",
            "description": "The dataframe containing data from the `positionsdaten` table."
          }
        ],
        "usage_context": {
          "calls": "This function calls db_dashboard.get_db_connection.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "db_dashboard.get_scalar_metrics": {
      "identifier": "db_dashboard.get_scalar_metrics",
      "description": {
        "overall": "The `get_scalar_metrics` function serves as a helper to load data from a single-row scalar table. It establishes a database connection using the `get_db_connection` function, executes a SQL query to select all columns from the `scalar_metrics` table, and returns the first row of the result as a pandas DataFrame. The database connection is closed regardless of the query's outcome. This function appears to be designed for use within a data dashboard application, potentially leveraging libraries such as Streamlit for visualization and DuckDB for database operations.",
        "parameters": [],
        "returns": [
          {
            "name": "scalar_metrics_row",
            "type": "pandas.Series",
            "description": "The first row of the scalar metrics table, returned as a pandas Series."
          }
        ],
        "usage_context": {
          "calls": "This function calls db_dashboard.get_db_connection.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "db_dashboard.compute_metrics_df1": {
      "identifier": "db_dashboard.compute_metrics_df1",
      "description": {
        "overall": "The `compute_metrics_df1` function loads various metrics from a database for the 'df1' dataset, which appears to be related to order data ('Auftragsdaten'). It retrieves data from multiple tables, including null ratios, test data entries, numeric statistics, plausibility differences, cleanliness metrics, proforma data, and error metrics. The function processes this data, calculates additional metrics, and returns a dictionary containing the compiled metrics. The function also measures and prints the time taken to load these metrics.",
        "parameters": [],
        "returns": [
          {
            "name": "metrics_df1",
            "type": "dict",
            "description": "A dictionary containing various metrics for the 'df1' dataset, including row count, null ratios, test data counts, numeric statistics, plausibility differences, cleanliness metrics, proforma data, and error metrics."
          }
        ],
        "usage_context": {
          "calls": "This function calls db_dashboard.get_db_connection and db_dashboard.get_scalar_metrics.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "db_dashboard.compute_metrics_df2": {
      "identifier": "db_dashboard.compute_metrics_df2",
      "description": {
        "overall": "The `compute_metrics_df2` function computes and returns various metrics for the 'df2' dataset, which appears to be related to 'Positionsdaten'. It connects to a database, retrieves several types of metrics, including numeric statistics, plausibility differences, and position counts, and then computes additional metrics such as null ratios and average differences. The function returns a dictionary containing these metrics. The computation involves executing SQL queries, processing the results, and performing calculations. The function also handles database connections and ensures they are closed after use.",
        "parameters": [],
        "returns": [
          {
            "name": "metrics_df2",
            "type": "dict",
            "description": "A dictionary containing various metrics for the 'df2' dataset, including row count, null ratios, numeric statistics, plausibility differences, and position counts."
          }
        ],
        "usage_context": {
          "calls": "This function calls db_dashboard.get_db_connection, db_dashboard.get_scalar_metrics, and db_dashboard.load.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "db_dashboard.compute_metrics_combined": {
      "identifier": "db_dashboard.compute_metrics_combined",
      "description": {
        "overall": "The `compute_metrics_combined` function loads combined metrics from a database, computes various metrics, and returns them as a dictionary. It starts by establishing a database connection and retrieving scalar metrics. It then executes a SQL query to fetch data from the `metric_order_pos_mismatch` table and stores the result in a Pandas DataFrame. The function computes three metrics: `kvarechnung_id_is_unique`, `position_id_is_unique`, and `auftraege_abgleich`. Finally, it closes the database connection, prints the time taken to load the metrics, and returns the computed metrics. The function appears to be part of a data analysis or dashboard application, given the use of Streamlit and DuckDB. The metrics computed by this function can be used to analyze the uniqueness of IDs and the mismatch between orders and positions.",
        "parameters": [],
        "returns": [
          {
            "name": "metrics_combined",
            "type": "dict",
            "description": "A dictionary containing the computed metrics, including `kvarechnung_id_is_unique`, `position_id_is_unique`, and `auftraege_abgleich`."
          }
        ],
        "usage_context": {
          "calls": "This function calls db_dashboard.get_db_connection and db_dashboard.get_scalar_metrics.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "db_dashboard.compute_positions_over_time": {
      "identifier": "db_dashboard.compute_positions_over_time",
      "description": {
        "overall": "The `compute_positions_over_time` function loads positions per order over time from a database. It establishes a database connection, executes a SQL query to retrieve the required data, and returns the result as a pandas DataFrame. The function also measures and prints the time taken to load the data. It uses the `get_db_connection` function to establish a connection to the database. The function does not take any parameters and returns a single value, the DataFrame containing the positions over time.",
        "parameters": [],
        "returns": [
          {
            "name": "df_pos_time",
            "type": "pandas.DataFrame",
            "description": "A pandas DataFrame containing the positions per order over time."
          }
        ],
        "usage_context": {
          "calls": "This function calls db_dashboard.get_db_connection.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "metrics.load_data": {
      "identifier": "metrics.load_data",
      "description": {
        "overall": "The `load_data` function loads data from two Parquet files, `Auftragsdaten_konvertiert` and `Positionsdaten_konvertiert`, located in the `resources` directory. It utilizes the `pd.read_parquet` function from the pandas library to read these files into two separate DataFrames, `df` and `df2`. The function then returns these two DataFrames. The purpose of this function appears to be data ingestion for further analysis or processing. The function does not perform any data manipulation or cleaning beyond loading the data.",
        "parameters": [],
        "returns": [
          {
            "name": "df",
            "type": "pandas.DataFrame",
            "description": "The first DataFrame loaded from `Auftragsdaten_konvertiert`"
          },
          {
            "name": "df2",
            "type": "pandas.DataFrame",
            "description": "The second DataFrame loaded from `Positionsdaten_konvertiert`"
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "metrics.ratio_null_values_column": {
      "identifier": "metrics.ratio_null_values_column",
      "description": {
        "overall": "The function `ratio_null_values_column` calculates the null-value-ratios for each column of the supplied DataFrame. It takes a pandas DataFrame as input and returns a new DataFrame with the null ratio for each column. The null ratio is calculated as the percentage of null entries in each column. The function utilizes the pandas library to efficiently compute the null ratios. The result is a DataFrame with two columns: 'column_name' and 'null_ratio', where 'null_ratio' represents the percentage of null values in each column.",
        "parameters": [
          {
            "name": "input_df",
            "type": "pandas.DataFrame",
            "description": "The input DataFrame to be evaluated."
          }
        ],
        "returns": [
          {
            "name": "null_ratio_df",
            "type": "pd.DataFrame",
            "description": "A DataFrame containing the null ratio for each column in the input DataFrame."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "metrics.ratio_null_values_rows": {
      "identifier": "metrics.ratio_null_values_rows",
      "description": {
        "overall": "The function `ratio_null_values_rows` calculates the ratio of rows containing null values in a given DataFrame to the total number of rows. It accepts an input DataFrame and optionally a list of relevant columns to evaluate. If no columns are specified, it evaluates all columns. The function returns the percentage of rows with at least one null value in the given columns.",
        "parameters": [
          {
            "name": "input_df",
            "type": "pandas.DataFrame",
            "description": "The input DataFrame to be evaluated."
          },
          {
            "name": "relevant_columns",
            "type": "list",
            "description": "An optional list of column identifiers to evaluate. If not provided, all columns are evaluated."
          }
        ],
        "returns": [
          {
            "name": "row_ratio",
            "type": "float",
            "description": "The percentage of rows with at least one null value in the given columns."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "metrics.Kundengruppe_containing_test": {
      "identifier": "metrics.Kundengruppe_containing_test",
      "description": {
        "overall": "The Kundengruppe_containing_test function determines the number of rows in a given 'Auftragsdaten' data set that are part of a test data set. It filters the data based on the presence of 'test' in the 'Kundengruppe' column. The function optionally returns a data frame with all relevant instances if the return_frame parameter is set to True. The function takes a pandas DataFrame and a boolean return_frame as input and returns the total number of test data rows and optionally a DataFrame with the test data.",
        "parameters": [
          {
            "name": "df",
            "type": "pandas.DataFrame",
            "description": "The 'Auftragsdaten'-DataFrame that is to be evaluated."
          },
          {
            "name": "return_frame",
            "type": "bool",
            "description": "If True, this function returns a DataFrame with all found test data."
          }
        ],
        "returns": [
          {
            "name": "anzahl_test",
            "type": "int",
            "description": "The total number of test data rows."
          },
          {
            "name": "test_Kundengruppen",
            "type": "pandas.DataFrame or None",
            "description": "A DataFrame containing all found test data, returned only if return_frame = True."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "metrics.allgemeine_statistiken_num": {
      "identifier": "metrics.allgemeine_statistiken_num",
      "description": {
        "overall": "The `allgemeine_statistiken_num` function calculates simple statistical values for all columns containing number data in a given pandas DataFrame. It iterates over each numerical column, computing the mean, median, standard deviation, minimum, and maximum values. These statistics are then organized into a nested dictionary, where each column's statistics are stored in a separate dictionary. The function returns this dictionary, providing a comprehensive overview of the numerical data in the input DataFrame. The function utilizes the pandas library for data manipulation and statistical calculations. It does not handle any exceptions that may occur during the computation of statistical values.",
        "parameters": [
          {
            "name": "input_df",
            "type": "pandas.DataFrame",
            "description": "The input DataFrame to be evaluated, which should contain numerical columns for statistical analysis."
          }
        ],
        "returns": [
          {
            "name": "statistiken",
            "type": "dict",
            "description": "A nested dictionary containing statistical values (mean, median, standard deviation, minimum, and maximum) for each numerical column in the input DataFrame."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "metrics.plausibilitaetscheck_forderung_einigung": {
      "identifier": "metrics.plausibilitaetscheck_forderung_einigung",
      "description": {
        "overall": "The function plausibilitaetscheck_forderung_einigung checks for differences between 'Einigung_Netto' and 'Forderung_Netto' columns in a given pandas DataFrame. It identifies rows where 'Einigung_Netto' is greater than 'Forderung_Netto', which is considered a significant error. The function returns a pandas Series of differences, the total count of such instances, and the average difference. The purpose of this function is to evaluate the consistency of financial data, specifically the relationship between agreement and claim amounts. The function uses pandas for data manipulation and calculation. It rounds the values to two decimal places for comparison and calculation.",
        "parameters": [
          {
            "name": "input_df",
            "type": "pandas.DataFrame",
            "description": "The input DataFrame to be evaluated, which should contain 'Einigung_Netto' and 'Forderung_Netto' columns."
          }
        ],
        "returns": [
          {
            "name": "statistik",
            "type": "pandas.Series",
            "description": "A Series containing the differences between 'Einigung_Netto' and 'Forderung_Netto' for rows where 'Einigung_Netto' is greater than 'Forderung_Netto', rounded to two decimal places."
          },
          {
            "name": "count",
            "type": "int",
            "description": "The total number of rows where 'Einigung_Netto' is greater than 'Forderung_Netto'."
          },
          {
            "name": "avg",
            "type": "float",
            "description": "The average difference between 'Einigung_Netto' and 'Forderung_Netto' for the identified rows."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "metrics.uniqueness_check": {
      "identifier": "metrics.uniqueness_check",
      "description": {
        "overall": "The `uniqueness_check` function checks whether the assumed unique ID columns in two provided data sets are truly unique. It takes two pandas DataFrames as input, `df` and `df2`, which contain the 'Auftragsdaten' and 'Positionsdaten' data sets, respectively. The function returns two boolean values indicating whether the 'KvaRechnung_ID' column in `df` and the 'Position_ID' column in `df2` are unique. This function is useful for data validation and integrity checks. The function uses the `is_unique` method provided by pandas to check for uniqueness. The function does not perform any error handling or data cleaning, it simply checks for uniqueness and returns the results.",
        "parameters": [
          {
            "name": "df",
            "type": "pandas.DataFrame",
            "description": "DataFrame that contains the 'Auftragsdaten' data set"
          },
          {
            "name": "df2",
            "type": "pandas.DataFrame",
            "description": "DataFrame that contains the 'Positionsdaten' data set"
          }
        ],
        "returns": [
          {
            "name": "kvarechnung_id_is_unique",
            "type": "bool",
            "description": "True if the 'KvaRechnung_ID' column is unique"
          },
          {
            "name": "position_id_is_unique",
            "type": "bool",
            "description": "True if the 'Position_ID' column is unique"
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "metrics.count_rows": {
      "identifier": "metrics.count_rows",
      "description": {
        "overall": "The `metrics.count_rows` function calculates the number of rows in a given pandas DataFrame. It takes one parameter, `input_df`, which is the DataFrame to be evaluated. The function returns the row count as an integer. This function appears to be a helper function for data analysis tasks, providing a simple way to determine the number of rows in a DataFrame after filtering. The function does not perform any filtering itself but rather relies on the input DataFrame being pre-filtered. The function's implementation is straightforward, using the built-in `len` function to count the rows in the DataFrame.",
        "parameters": [
          {
            "name": "input_df",
            "type": "pandas.DataFrame",
            "description": "The DataFrame to be evaluated."
          }
        ],
        "returns": [
          {
            "name": "count",
            "type": "int",
            "description": "The number of rows in the input DataFrame."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "metrics.split_dataframe": {
      "identifier": "metrics.split_dataframe",
      "description": {
        "overall": "The `metrics.split_dataframe` function splits a given data frame into chunks to simulate time series data. It takes two parameters: `input_df` (the data frame to be split) and `chunks` (the number of chunks to split the data frame into, defaulting to 5). The function utilizes the `np.array_split` method from the NumPy library to achieve this. Note that this function is deprecated, as it has been made obsolete by the addition of datetime columns. The function returns a list of data frame chunks.",
        "parameters": [
          {
            "name": "input_df",
            "type": "pandas.DataFrame",
            "description": "The input data frame to be split."
          },
          {
            "name": "chunks",
            "type": "int",
            "description": "The number of chunks to split the data frame into (default is 5)."
          }
        ],
        "returns": [
          {
            "name": "split_dataframes",
            "type": "list of pandas.DataFrame",
            "description": "A list of data frame chunks resulting from the split operation."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "metrics.data_cleanliness": {
      "identifier": "metrics.data_cleanliness",
      "description": {
        "overall": "The `data_cleanliness` function calculates the ratio of null values in a given DataFrame, both by column and by row, with optional grouping by a specified column. It returns the percentage of rows with at least one null value, the percentage of null entries in each column, and the row and column null ratios for each group if grouping is applied. The function utilizes pandas for data manipulation and takes an input DataFrame, an optional column for grouping, and an optional specific group to filter results by.",
        "parameters": [
          {
            "name": "input_df",
            "type": "pandas.DataFrame",
            "description": "The DataFrame to be evaluated for null values."
          },
          {
            "name": "group_by_col",
            "type": "string",
            "description": "Optional column identifier for grouping the DataFrame."
          },
          {
            "name": "specific_group",
            "type": "string",
            "description": "Optional specific group entry to filter the results by."
          }
        ],
        "returns": [
          {
            "name": "null_ratio_rows",
            "type": "float or None",
            "description": "The percentage of rows with at least one null value in the given columns."
          },
          {
            "name": "null_ratio_cols",
            "type": "DataFrame or None",
            "description": "A DataFrame containing the percentage of null entries in each column."
          },
          {
            "name": "grouped_row_ratios",
            "type": "pandas.Series or None",
            "description": "A Series containing the row ratios of all groups as float."
          },
          {
            "name": "grouped_col_ratios",
            "type": "pandas.DataFrame or None",
            "description": "A DataFrame containing groups and null-value-ratios per column for each."
          }
        ],
        "usage_context": {
          "calls": "This function calls metrics.ratio_null_values_column and metrics.ratio_null_values_rows.",
          "called_by": "This function is called by no other functions."
        }
      },
      "error": null
    },
    "metrics.groupby_col": {
      "identifier": "metrics.groupby_col",
      "description": {
        "overall": "The `metrics.groupby_col` function is a helper function designed to group a pandas DataFrame by a specified column. It takes two parameters: `input_df`, which is the DataFrame to be grouped, and `col`, which is the identifier of the column to group by. The function utilizes the pandas library's `groupby` method to achieve this grouping. The `observed=True` parameter ensures that the grouping is performed based on observed values. The function returns a grouped DataFrame, denoted as `input_df_grouped`. This function is straightforward in its purpose and does not handle any exceptions explicitly within its body.",
        "parameters": [
          {
            "name": "input_df",
            "type": "pandas.DataFrame",
            "description": "The DataFrame that is to be evaluated and grouped."
          },
          {
            "name": "col",
            "type": "string",
            "description": "The identifier of the column by which the DataFrame is to be grouped."
          }
        ],
        "returns": [
          {
            "name": "input_df_grouped",
            "type": "pandas.DataFrame",
            "description": "A DataFrame that has been grouped by the specified column."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "metrics.discount_check": {
      "identifier": "metrics.discount_check",
      "description": {
        "overall": "The `discount_check` function checks if rows in the 'Positionsdaten' data set accurately reflect discounts or similar information. It relies on the 'Plausibel' column, which is populated by logic in `data_cleaning.py`. The function takes a pandas DataFrame `df2` as input and returns the number of potentially faulty rows. The check is performed by summing the rows where 'Plausibel' is False. This function is designed to identify inconsistencies in the data set.",
        "parameters": [
          {
            "name": "df2",
            "type": "pandas.DataFrame",
            "description": "DataFrame containing the 'Positionsdaten' data set"
          }
        ],
        "returns": [
          {
            "name": "potential_errors",
            "type": "int",
            "description": "The number of potentially faulty rows"
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "metrics.proformabelege": {
      "identifier": "metrics.proformabelege",
      "description": {
        "overall": "The `proformabelege` function evaluates a given DataFrame to identify pro forma receipts within the 'Auftragsdaten' data set. It filters the DataFrame based on the 'Einigung_Netto' column, selecting rows where the value falls between 0.01 and 1. The function returns two values: a DataFrame containing the identified pro forma receipt rows and the total count of these receipts. This function appears to be designed for data analysis and filtering tasks, specifically targeting pro forma receipts. The function's logic is straightforward, relying on basic DataFrame operations. The purpose of this function is to provide a clear and concise way to extract and count pro forma receipts from a larger dataset.",
        "parameters": [
          {
            "name": "df",
            "type": "pandas.DataFrame",
            "description": "The DataFrame to be evaluated for pro forma receipts."
          }
        ],
        "returns": [
          {
            "name": "proforma",
            "type": "pandas.DataFrame",
            "description": "A DataFrame containing all the identified pro forma receipt rows."
          },
          {
            "name": "proforma_count",
            "type": "int",
            "description": "The total count of pro forma receipts found in the DataFrame."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "metrics.position_count": {
      "identifier": "metrics.position_count",
      "description": {
        "overall": "The `metrics.position_count` function takes a pandas DataFrame as input and returns a new DataFrame with the count of positions for each unique 'KvaRechnung_ID'. It utilizes the pandas library to group the input data by 'KvaRechnung_ID' and count the number of 'Position_ID' for each group. The resulting DataFrame contains two columns: 'KvaRechnung_ID' and 'PositionsAnzahl', where 'PositionsAnzahl' represents the count of positions. This function is designed to provide a summary of the number of positions associated with each 'KvaRechnung_ID' in the input data. The function does not perform any error checking on the input data, assuming that it is a valid pandas DataFrame with the required columns. The output is a pandas DataFrame, making it easy to further manipulate or analyze the results.",
        "parameters": [
          {
            "name": "input_df",
            "type": "pandas.DataFrame",
            "description": "The input DataFrame to be evaluated. It is expected to contain the columns 'KvaRechnung_ID' and 'Position_ID'."
          }
        ],
        "returns": [
          {
            "name": "position_count",
            "type": "pandas.DataFrame",
            "description": "A DataFrame with two columns: 'KvaRechnung_ID' and 'PositionsAnzahl', where 'PositionsAnzahl' represents the count of positions for each unique 'KvaRechnung_ID'."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "metrics.false_negative_df": {
      "identifier": "metrics.false_negative_df",
      "description": {
        "overall": "The function `false_negative_df` evaluates a pandas DataFrame containing 'Auftragsdaten' data set. It checks for instances where at least two values in the Tuple (Forderung, Empfehlung, Einigung) are negative, but the last remaining value is not negative. The function returns the count of such instances. The function takes a pandas DataFrame `df` as input and returns an integer `error_count` representing the number of entries failing the check. The function utilizes pandas for data manipulation and does not rely on any external function calls. It is designed to identify and count specific patterns in the data.",
        "parameters": [
          {
            "name": "df",
            "type": "pandas.DataFrame",
            "description": "DataFrame with 'Auftragsdaten' data set that is to be evaluated."
          }
        ],
        "returns": [
          {
            "name": "error_count",
            "type": "int",
            "description": "Number of entries in any of the three columns Forderung, Empfehlung or Einigung failing the check."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "metrics.false_negative_df2": {
      "identifier": "metrics.false_negative_df2",
      "description": {
        "overall": "The function `false_negative_df2` checks a pandas DataFrame `df2` for entries in specific columns that are out of a sensible value range. It returns the total error count over all columns. The function focuses on columns 'Menge', 'Menge_Einigung', 'EP', 'Ep_Einigung', 'Forderung_Netto', and 'Einigung_Netto', identifying negative values or inconsistent pairs. The total error count is calculated by summing the counts of invalid entries across these columns.",
        "parameters": [
          {
            "name": "df2",
            "type": "pandas.DataFrame",
            "description": "A pandas DataFrame containing the 'Positionsdaten' data set to be evaluated."
          }
        ],
        "returns": [
          {
            "name": "total_errors",
            "type": "int",
            "description": "The total amount of non-valid entries aggregated over all relevant columns."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "metrics.above_50k": {
      "identifier": "metrics.above_50k",
      "description": {
        "overall": "The `above_50k` function checks a pandas DataFrame for receipts or positions that exceed a limit of \u20ac50,000 in the 'Einigung_Netto' column, indicating potential suspicion and the need for manual vetting. It filters the input DataFrame based on this condition and returns a new DataFrame containing the suspicious data. The function takes a pandas DataFrame as input and returns a pandas DataFrame. It does not modify the original DataFrame. The function's purpose is to identify potentially suspicious transactions that require further review.",
        "parameters": [
          {
            "name": "df",
            "type": "pandas.DataFrame",
            "description": "The input DataFrame to be evaluated for suspicious transactions."
          }
        ],
        "returns": [
          {
            "name": "suspicious_data",
            "type": "pandas.DataFrame",
            "description": "A DataFrame containing the suspiciously high positions, i.e., rows where 'Einigung_Netto' is greater than or equal to \u20ac50,000."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "metrics.outliers_by_damage": {
      "identifier": "metrics.outliers_by_damage",
      "description": {
        "overall": "The `outliers_by_damage` function calculates the upper and lower outliers outside a specified quantile range for each type of damage in a given DataFrame. It filters the data by a specific damage type if provided and uses a specified column to detect outliers. The function returns a new DataFrame containing all suspicious rows. The quantile range is symmetric over the mean, and the function adjusts the quantile if it's less than 0.5. It uses the `pandas` library for data manipulation and grouping.",
        "parameters": [
          {
            "name": "df",
            "type": "pandas.DataFrame",
            "description": "The DataFrame to be evaluated."
          },
          {
            "name": "schadenart",
            "type": "string",
            "description": "A specific damage type label to filter for. Optional, defaults to None."
          },
          {
            "name": "set_quantile",
            "type": "float",
            "description": "The desired quantile range, symmetric upper/lower bound is inferred. Optional, defaults to 0.99."
          },
          {
            "name": "column_choice",
            "type": "str",
            "description": "The numeric column containing outliers. Optional, defaults to 'Forderung_Netto'."
          }
        ],
        "returns": [
          {
            "name": "df_outlier",
            "type": "pandas.DataFrame",
            "description": "A DataFrame containing all suspicious rows."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "metrics.check_zeitwert": {
      "identifier": "metrics.check_zeitwert",
      "description": {
        "overall": "The `check_zeitwert` function evaluates a pandas DataFrame containing 'Auftragsdaten' data set to check if the value in the column 'Differenz_vor_Zeitwert_Netto' satisfies the condition [Zeitwert = Forderung-Einigung] and calculates the relative error. It takes a DataFrame as input, performs calculations to determine the difference between the expected and actual Zeitwert values, and returns a pandas Series of error values. The function is designed to work specifically with the 'Auftragsdaten' data set. It first calculates the difference between the 'Forderung_Netto' and 'Einigung_Netto' columns, then subtracts the 'Differenz_vor_Zeitwert_Netto' column from this difference. The resulting error values are filtered to exclude zeros, and the non-zero error values are returned as a pandas Series.",
        "parameters": [
          {
            "name": "df",
            "type": "pandas.DataFrame",
            "description": "DataFrame containing 'Auftragsdaten' data set that is to be evaluated."
          }
        ],
        "returns": [
          {
            "name": "zeitwert_error",
            "type": "pandas.Series",
            "description": "Series of all error values (float) found in the data frame."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "metrics.positions_per_order_over_time": {
      "identifier": "metrics.positions_per_order_over_time",
      "description": {
        "overall": "This function calculates the average number of positions per order over time. It takes in two dataframes, one for order data and one for position data, and a time column name. The function returns a dataframe with the average number of positions per order, total positions, number of orders, and growth rate for each time period. The time period is determined by the month of the time column. The function first counts the number of positions per order, then prepares the time column by converting it to datetime format and dropping any rows with missing values. It then merges the position counts with the order data and aggregates the results by time period. Finally, it calculates the growth rate of the average number of positions per order and returns the result.",
        "parameters": [
          {
            "name": "df",
            "type": "DataFrame",
            "description": "Order data with columns 'KvaRechnung_ID' and a time column."
          },
          {
            "name": "df2",
            "type": "DataFrame",
            "description": "Position data with columns 'KvaRechnung_ID' and 'Position_ID'."
          },
          {
            "name": "time_col",
            "type": "str",
            "description": "Name of the time column in the order dataframe (e.g. 'CRMEingangszeit')."
          }
        ],
        "returns": [
          {
            "name": "result",
            "type": "DataFrame",
            "description": "A dataframe with columns 'Zeitperiode', 'Avg_Positionen_pro_Auftrag', 'Total_Positionen', 'Anzahl_Auftraege', and 'Growth_rate_%'."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "metrics.error_frequency_by_weekday_hour": {
      "identifier": "metrics.error_frequency_by_weekday_hour",
      "description": {
        "overall": "The function error_frequency_by_weekday_hour aggregates error frequency (NaN values) by weekday and hour in a given DataFrame. It considers an entry as faulty if at least one of the relevant columns contains a NaN value. The function takes a DataFrame, a time column name, and a list of relevant columns as input, and returns a new DataFrame with the error frequency and rate for each weekday and hour.",
        "parameters": [
          {
            "name": "df",
            "type": "pandas.DataFrame",
            "description": "The input DataFrame containing order data, which must include 'KvaRechnung_ID' and the time column."
          },
          {
            "name": "time_col",
            "type": "string",
            "description": "The name of the time column in the DataFrame, e.g., 'CRMEingangszeit'."
          },
          {
            "name": "relevant_columns",
            "type": "list",
            "description": "A list of columns to check for NaN values. If None, all columns except 'KvaRechnung_ID' and the time column are considered."
          }
        ],
        "returns": [
          {
            "name": "result",
            "type": "pandas.DataFrame",
            "description": "A DataFrame with columns 'weekday', 'hour', 'total_rows', 'error_rows', and 'error_rate', representing the error frequency and rate for each weekday and hour."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "metrics.get_mismatched_entries": {
      "identifier": "metrics.get_mismatched_entries",
      "description": {
        "overall": "The `get_mismatched_entries` function calculates the similarity between 'Gewerk_Name' and 'Handwerker_Name' columns in a given DataFrame using SentenceTransformer embeddings. It then identifies and returns the entries with a similarity score below a specified threshold. The function utilizes the 'paraphrase-multilingual-MiniLM-L12-v2' model and leverages GPU acceleration if available. The embeddings are computed in batches to improve efficiency. The function returns a new DataFrame containing the mismatched entries, sorted by their similarity scores in ascending order.",
        "parameters": [
          {
            "name": "df",
            "type": "pandas.DataFrame",
            "description": "The input DataFrame containing 'Gewerk_Name' and 'Handwerker_Name' columns."
          },
          {
            "name": "threshold",
            "type": "float",
            "description": "The minimum similarity score required for an entry to be considered a match (default: 0.2)."
          }
        ],
        "returns": [
          {
            "name": "mismatches",
            "type": "pandas.DataFrame",
            "description": "A new DataFrame containing the entries with a similarity score below the specified threshold, sorted by their similarity scores in ascending order."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "metrics.handwerker_gewerke_outlier": {
      "identifier": "metrics.handwerker_gewerke_outlier",
      "description": {
        "overall": "The handwerker_gewerke_outlier function takes a pandas DataFrame as input, filters it to include only the 'Handwerker_Name' and 'Gewerk_Name' columns, and removes any rows with missing values. It then calculates the count of each combination of 'Handwerker_Name' and 'Gewerk_Name', as well as the total count of each 'Handwerker_Name'. The function calculates the ratio of the count to the total count and identifies outliers based on this ratio and the number of 'Gewerk_Name' values for each 'Handwerker_Name'. The function returns a DataFrame containing the results, including the 'Handwerker_Name', 'Gewerk_Name', 'count', 'total_count', 'ratio', 'anzahl_gewerke', and 'is_outlier' columns.",
        "parameters": [
          {
            "name": "df",
            "type": "pandas DataFrame",
            "description": "The input DataFrame containing the 'Handwerker_Name' and 'Gewerk_Name' columns."
          }
        ],
        "returns": [
          {
            "name": "stats",
            "type": "pandas DataFrame",
            "description": "A DataFrame containing the results of the outlier detection, including the 'Handwerker_Name', 'Gewerk_Name', 'count', 'total_count', 'ratio', 'anzahl_gewerke', and 'is_outlier' columns."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "metrics.check_keywords_vectorized": {
      "identifier": "metrics.check_keywords_vectorized",
      "description": {
        "overall": "The function `check_keywords_vectorized` takes a pandas DataFrame `df` as input and checks if the 'Handwerker_Name' column contains specific keywords related to different trades. It uses a dictionary `keywords_mapping` to map trades to their corresponding keywords. The function returns a numpy array where each element is either 'CONFIRMED_BY_NAME', 'CONFLICT_WITH_<trade>', or 'NO_KEYWORD_INFO', indicating whether the name matches the trade, conflicts with another trade, or has no keyword information, respectively.",
        "parameters": [
          {
            "name": "df",
            "type": "pandas DataFrame",
            "description": "The input DataFrame containing the 'Handwerker_Name' and 'Gewerk_Name' columns."
          }
        ],
        "returns": [
          {
            "name": "final_result",
            "type": "numpy array",
            "description": "A numpy array where each element is either 'CONFIRMED_BY_NAME', 'CONFLICT_WITH_<trade>', or 'NO_KEYWORD_INFO'."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    },
    "metrics.abgleich_auftraege": {
      "identifier": "metrics.abgleich_auftraege",
      "description": {
        "overall": "The function abgleich_auftraege compares the header data of orders (df1) with the sum of their positions (df2). It groups the position data (df2) by 'Kva_RechnungID', calculates the sums for 'Forderung_Netto' and 'Einigung_Netto', and compares these with the values stored in df1. The function takes into account floating-point inaccuracies. It returns a DataFrame containing the discrepancies, including the IDs where the values do not match, along with the difference amounts for 'Forderung_Netto' and 'Einigung_Netto'.",
        "parameters": [
          {
            "name": "df1",
            "type": "pd.DataFrame",
            "description": "DataFrame containing order data (target values), which must include the columns 'Kva_RechnungID', 'Forderung_Netto', and 'Einigung_Netto'."
          },
          {
            "name": "df2",
            "type": "pd.DataFrame",
            "description": "DataFrame containing position data (actual values), which must include the columns 'Kva_RechnungID', 'Forderung_Netto', and 'Einigung_Netto'."
          }
        ],
        "returns": [
          {
            "name": "result_df",
            "type": "pd.DataFrame",
            "description": "A DataFrame containing the discrepancies, with columns 'Kva_RechnungID', 'Diff_Forderung', and 'Diff_Einigung'."
          }
        ],
        "usage_context": {
          "calls": "This function calls no other functions.",
          "called_by": "This function is not called by any other functions."
        }
      },
      "error": null
    }
  },
  "classes": {}
}