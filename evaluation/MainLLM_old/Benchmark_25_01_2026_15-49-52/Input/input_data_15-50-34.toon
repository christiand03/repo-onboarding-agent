basic_info:
  projekt_uebersicht:
    titel: Repo Onboarding Agent üöÄ
    beschreibung: ```
    aktueller_status: Information not found
    key_features: Information not found
    tech_stack: Information not found
  installation:
    dependencies: "- altair==4.2.2\n- annotated-types==0.7.0\n- anyio==4.11.0\n- attrs==25.4.0\n- bcrypt==5.0.0\n- blinker==1.9.0\n- cachetools==6.2.2\n- captcha==0.7.1\n- certifi==2025.11.12\n- cffi==2.0.0\n- charset-normalizer==3.4.4\n- click==8.3.1\n- colorama==0.4.6\n- contourpy==1.3.3\n- cryptography==46.0.3\n- cycler==0.12.1\n- distro==1.9.0\n- dnspython==2.8.0\n- dotenv==0.9.9\n- entrypoints==0.4\n- extra-streamlit-components==0.1.81\n- filetype==1.2.0\n- fonttools==4.61.0\n- gitdb==4.0.12\n- GitPython==3.1.45\n- google-ai-generativelanguage==0.9.0\n- google-api-core==2.28.1\n- google-auth==2.43.0\n- googleapis-common-protos==1.72.0\n- grpcio==1.76.0\n- grpcio-status==1.76.0\n- h11==0.16.0\n- httpcore==1.0.9\n- httpx==0.28.1\n- idna==3.11\n- Jinja2==3.1.6\n- jiter==0.12.0\n- jsonpatch==1.33\n- jsonpointer==3.0.0\n- jsonschema==4.25.1\n- jsonschema-specifications==2025.9.1\n- kiwisolver==1.4.9\n- langchain==1.0.8\n- langchain-core==1.1.0\n- langchain-google-genai==3.1.0\n- langchain-ollama==1.0.0\n- langchain-openai==1.1.0\n- langgraph==1.0.3\n- langgraph-checkpoint==3.0.1\n- langgraph-prebuilt==1.0.5\n- langgraph-sdk==0.2.9\n- langsmith==0.4.46\n- MarkupSafe==3.0.3\n- matplotlib==3.10.7\n- narwhals==2.12.0\n- networkx==3.6\n- numpy==2.3.5\n- ollama==0.6.1\n- openai==2.8.1\n- orjson==3.11.4\n- ormsgpack==1.12.0\n- packaging==25.0\n- pandas==2.3.3\n- pillow==12.0.0\n- proto-plus==1.26.1\n- protobuf==6.33.1\n- pyarrow==21.0.0\n- pyasn1==0.6.1\n- pyasn1_modules==0.4.2\n- pycparser==2.23\n- pydantic==2.12.4\n- pydantic_core==2.41.5\n- pydeck==0.9.1\n- PyJWT==2.10.1\n- pymongo==4.15.4\n- pyparsing==3.2.5\n- python-dateutil==2.9.0.post0\n- python-dotenv==1.2.1\n- pytz==2025.2\n- PyYAML==6.0.3\n- referencing==0.37.0\n- regex==2025.11.3\n- requests==2.32.5\n- requests-toolbelt==1.0.0\n- rpds-py==0.29.0\n- rsa==4.9.1\n- setuptools==75.9.1\n- six==1.17.0\n- smmap==5.0.2\n- sniffio==1.3.1\n- streamlit==1.51.0\n- streamlit-authenticator==0.4.2\n- streamlit-mermaid==0.3.0\n- tenacity==9.1.2\n- tiktoken==0.12.0\n- toml==0.10.2\n- toolz==1.1.0\n- toon_format @ git+https://github.com/toon-format/toon-python.git@9c4f0c0c24f2a0b0b376315f4b8707f8c9006de6\n- tornado==6.5.2\n- tqdm==4.67.1\n- typing-inspection==0.4.2\n- typing_extensions==4.15.0\n- tzdata==2025.2\n- urllib3==2.5.0\n- watchdog==6.0.0\n- xxhash==3.6.0\n- zstandard==0.25.0\n- nbformat"
    setup_anleitung: Information not found
    quick_start_guide: Information not found
file_tree:
  name: root
  type: directory
  children[16]:
    - path: .env.example
      name: .env.example
      size: 48
      type: file
    - path: .gitignore
      name: .gitignore
      size: 184
      type: file
    - name: SystemPrompts
      type: directory
      children[6]{path,name,size,type}:
        SystemPrompts/SystemPromptClassHelperLLM.txt,SystemPromptClassHelperLLM.txt,6645,file
        SystemPrompts/SystemPromptFunctionHelperLLM.txt,SystemPromptFunctionHelperLLM.txt,5546,file
        SystemPrompts/SystemPromptHelperLLM.txt,SystemPromptHelperLLM.txt,9350,file
        SystemPrompts/SystemPromptMainLLM.txt,SystemPromptMainLLM.txt,8380,file
        SystemPrompts/SystemPromptMainLLMToon.txt,SystemPromptMainLLMToon.txt,8456,file
        SystemPrompts/SystemPromptNotebookLLM.txt,SystemPromptNotebookLLM.txt,6921,file
    - path: analysis_output.json
      name: analysis_output.json
      size: 569396
      type: file
    - name: backend
      type: directory
      children[12]{path,name,size,type}:
        backend/AST_Schema.py,AST_Schema.py,6622,file
        backend/File_Dependency.py,File_Dependency.py,7384,file
        backend/HelperLLM.py,HelperLLM.py,17280,file
        backend/MainLLM.py,MainLLM.py,4281,file
        backend/__init__.py,__init__.py,0,file
        backend/basic_info.py,basic_info.py,7284,file
        backend/callgraph.py,callgraph.py,9020,file
        backend/converter.py,converter.py,3594,file
        backend/getRepo.py,getRepo.py,4739,file
        backend/main.py,main.py,25591,file
        backend/relationship_analyzer.py,relationship_analyzer.py,8279,file
        backend/scads_key_test.py,scads_key_test.py,663,file
    - name: database
      type: directory
      children[1]{path,name,size,type}:
        database/db.py,db.py,8222,file
    - name: frontend
      type: directory
      children[4]:
        - name: .streamlit
          type: directory
          children[1]{path,name,size,type}:
            frontend/.streamlit/config.toml,config.toml,165,file
        - path: frontend/__init__.py
          name: __init__.py
          size: 0
          type: file
        - path: frontend/frontend.py
          name: frontend.py
          size: 31176
          type: file
        - name: gifs
          type: directory
          children[1]{path,name,size,type}:
            frontend/gifs/4j.gif,4j.gif,3306723,file
    - name: notizen
      type: directory
      children[8]:
        - path: notizen/Report Agenda.txt
          name: Report Agenda.txt
          size: 3492
          type: file
        - path: notizen/Zwischenpraesentation Agenda.txt
          name: Zwischenpraesentation Agenda.txt
          size: 809
          type: file
        - path: notizen/doc_bestandteile.md
          name: doc_bestandteile.md
          size: 847
          type: file
        - name: grafiken
          type: directory
          children[4]:
            - name: "1"
              type: directory
              children[5]{path,name,size,type}:
                notizen/grafiken/1/File_Dependency_Graph_Repo.dot,File_Dependency_Graph_Repo.dot,1227,file
                notizen/grafiken/1/global_callgraph.png,global_callgraph.png,940354,file
                notizen/grafiken/1/global_graph.png,global_graph.png,4756519,file
                notizen/grafiken/1/global_graph_2.png,global_graph_2.png,242251,file
                notizen/grafiken/1/repo.dot,repo.dot,13870,file
            - name: "2"
              type: directory
              children[12]{path,name,size,type}:
                notizen/grafiken/2/FDG_repo.dot,FDG_repo.dot,9150,file
                notizen/grafiken/2/fdg_graph.png,fdg_graph.png,425374,file
                notizen/grafiken/2/fdg_graph_2.png,fdg_graph_2.png,4744718,file
                notizen/grafiken/2/filtered_callgraph_flask.png,filtered_callgraph_flask.png,614631,file
                notizen/grafiken/2/filtered_callgraph_repo-agent.png,filtered_callgraph_repo-agent.png,280428,file
                notizen/grafiken/2/filtered_callgraph_repo-agent_3.png,filtered_callgraph_repo-agent_3.png,1685838,file
                notizen/grafiken/2/filtered_repo_callgraph_flask.dot,filtered_repo_callgraph_flask.dot,19984,file
                notizen/grafiken/2/filtered_repo_callgraph_repo-agent-3.dot,filtered_repo_callgraph_repo-agent-3.dot,20120,file
                notizen/grafiken/2/filtered_repo_callgraph_repo-agent.dot,filtered_repo_callgraph_repo-agent.dot,3118,file
                notizen/grafiken/2/global_callgraph.png,global_callgraph.png,608807,file
                notizen/grafiken/2/graph_flask.md,graph_flask.md,17398,file
                notizen/grafiken/2/repo.dot,repo.dot,19984,file
            - name: Flask-Repo
              type: directory
              children[65]{path,name,size,type}:
                notizen/grafiken/Flask-Repo/__init__.dot,__init__.dot,89,file
                notizen/grafiken/Flask-Repo/__main__.dot,__main__.dot,87,file
                notizen/grafiken/Flask-Repo/app.dot,app.dot,78,file
                notizen/grafiken/Flask-Repo/auth.dot,auth.dot,1126,file
                notizen/grafiken/Flask-Repo/blog.dot,blog.dot,939,file
                notizen/grafiken/Flask-Repo/blueprints.dot,blueprints.dot,4601,file
                notizen/grafiken/Flask-Repo/cli.dot,cli.dot,8269,file
                notizen/grafiken/Flask-Repo/conf.dot,conf.dot,489,file
                notizen/grafiken/Flask-Repo/config.dot,config.dot,2130,file
                notizen/grafiken/Flask-Repo/conftest.dot,conftest.dot,1756,file
                notizen/grafiken/Flask-Repo/ctx.dot,ctx.dot,3009,file
                notizen/grafiken/Flask-Repo/db.dot,db.dot,782,file
                notizen/grafiken/Flask-Repo/debughelpers.dot,debughelpers.dot,1903,file
                notizen/grafiken/Flask-Repo/factory.dot,factory.dot,220,file
                notizen/grafiken/Flask-Repo/flask.dot,flask.dot,82,file
                notizen/grafiken/Flask-Repo/globals.dot,globals.dot,370,file
                notizen/grafiken/Flask-Repo/hello.dot,hello.dot,152,file
                notizen/grafiken/Flask-Repo/helpers.dot,helpers.dot,2510,file
                notizen/grafiken/Flask-Repo/importerrorapp.dot,importerrorapp.dot,155,file
                notizen/grafiken/Flask-Repo/logging.dot,logging.dot,564,file
                notizen/grafiken/Flask-Repo/make_celery.dot,make_celery.dot,99,file
                notizen/grafiken/Flask-Repo/multiapp.dot,multiapp.dot,88,file
                notizen/grafiken/Flask-Repo/provider.dot,provider.dot,1769,file
                notizen/grafiken/Flask-Repo/scaffold.dot,scaffold.dot,3700,file
                notizen/grafiken/Flask-Repo/sessions.dot,sessions.dot,4178,file
                notizen/grafiken/Flask-Repo/signals.dot,signals.dot,133,file
                notizen/grafiken/Flask-Repo/tag.dot,tag.dot,3750,file
                notizen/grafiken/Flask-Repo/tasks.dot,tasks.dot,312,file
                notizen/grafiken/Flask-Repo/templating.dot,templating.dot,2277,file
                notizen/grafiken/Flask-Repo/test_appctx.dot,test_appctx.dot,2470,file
                notizen/grafiken/Flask-Repo/test_async.dot,test_async.dot,1919,file
                notizen/grafiken/Flask-Repo/test_auth.dot,test_auth.dot,725,file
                notizen/grafiken/Flask-Repo/test_basic.dot,test_basic.dot,17383,file
                notizen/grafiken/Flask-Repo/test_blog.dot,test_blog.dot,1105,file
                notizen/grafiken/Flask-Repo/test_blueprints.dot,test_blueprints.dot,11515,file
                notizen/grafiken/Flask-Repo/test_cli.dot,test_cli.dot,6435,file
                notizen/grafiken/Flask-Repo/test_config.dot,test_config.dot,2854,file
                notizen/grafiken/Flask-Repo/test_config.png,test_config.png,288693,file
                notizen/grafiken/Flask-Repo/test_converters.dot,test_converters.dot,937,file
                notizen/grafiken/Flask-Repo/test_db.dot,test_db.dot,481,file
                notizen/grafiken/Flask-Repo/test_factory.dot,test_factory.dot,201,file
                notizen/grafiken/Flask-Repo/test_helpers.dot,test_helpers.dot,5857,file
                notizen/grafiken/Flask-Repo/test_instance_config.dot,test_instance_config.dot,1249,file
                notizen/grafiken/Flask-Repo/test_js_example.dot,test_js_example.dot,453,file
                notizen/grafiken/Flask-Repo/test_json.dot,test_json.dot,4021,file
                notizen/grafiken/Flask-Repo/test_json_tag.dot,test_json_tag.dot,1452,file
                notizen/grafiken/Flask-Repo/test_logging.dot,test_logging.dot,1348,file
                notizen/grafiken/Flask-Repo/test_regression.dot,test_regression.dot,763,file
                notizen/grafiken/Flask-Repo/test_reqctx.dot,test_reqctx.dot,3809,file
                notizen/grafiken/Flask-Repo/test_request.dot,test_request.dot,668,file
                notizen/grafiken/Flask-Repo/test_session_interface.dot,test_session_interface.dot,667,file
                notizen/grafiken/Flask-Repo/test_signals.dot,test_signals.dot,1671,file
                notizen/grafiken/Flask-Repo/test_subclassing.dot,test_subclassing.dot,612,file
                notizen/grafiken/Flask-Repo/test_templating.dot,test_templating.dot,5129,file
                notizen/grafiken/Flask-Repo/test_testing.dot,test_testing.dot,4081,file
                notizen/grafiken/Flask-Repo/test_user_error_handler.dot,test_user_error_handler.dot,5984,file
                notizen/grafiken/Flask-Repo/test_views.dot,test_views.dot,2641,file
                notizen/grafiken/Flask-Repo/testing.dot,testing.dot,2785,file
                notizen/grafiken/Flask-Repo/typing.dot,typing.dot,86,file
                notizen/grafiken/Flask-Repo/typing_app_decorators.dot,typing_app_decorators.dot,500,file
                notizen/grafiken/Flask-Repo/typing_error_handler.dot,typing_error_handler.dot,420,file
                notizen/grafiken/Flask-Repo/typing_route.dot,typing_route.dot,1717,file
                notizen/grafiken/Flask-Repo/views.dot,views.dot,1125,file
                notizen/grafiken/Flask-Repo/wrappers.dot,wrappers.dot,911,file
                notizen/grafiken/Flask-Repo/wsgi.dot,wsgi.dot,19,file
            - name: Repo-onboarding
              type: directory
              children[15]{path,name,size,type}:
                notizen/grafiken/Repo-onboarding/AST.dot,AST.dot,1361,file
                notizen/grafiken/Repo-onboarding/Frontend.dot,Frontend.dot,538,file
                notizen/grafiken/Repo-onboarding/HelperLLM.dot,HelperLLM.dot,1999,file
                notizen/grafiken/Repo-onboarding/HelperLLM.png,HelperLLM.png,234861,file
                notizen/grafiken/Repo-onboarding/MainLLM.dot,MainLLM.dot,2547,file
                notizen/grafiken/Repo-onboarding/agent.dot,agent.dot,2107,file
                notizen/grafiken/Repo-onboarding/basic_info.dot,basic_info.dot,1793,file
                notizen/grafiken/Repo-onboarding/callgraph.dot,callgraph.dot,1478,file
                notizen/grafiken/Repo-onboarding/getRepo.dot,getRepo.dot,1431,file
                notizen/grafiken/Repo-onboarding/graph_AST.png,graph_AST.png,132743,file
                notizen/grafiken/Repo-onboarding/graph_AST2.png,graph_AST2.png,12826,file
                notizen/grafiken/Repo-onboarding/graph_AST3.png,graph_AST3.png,136016,file
                notizen/grafiken/Repo-onboarding/main.dot,main.dot,1091,file
                notizen/grafiken/Repo-onboarding/tools.dot,tools.dot,1328,file
                notizen/grafiken/Repo-onboarding/types.dot,types.dot,133,file
        - path: notizen/notizen.md
          name: notizen.md
          size: 4937
          type: file
        - path: notizen/paul_notizen.md
          name: paul_notizen.md
          size: 2033
          type: file
        - path: notizen/praesentation_notizen.md
          name: praesentation_notizen.md
          size: 2738
          type: file
        - path: notizen/technische_notizen.md
          name: technische_notizen.md
          size: 3616
          type: file
    - path: output.json
      name: output.json
      size: 454008
      type: file
    - path: output.toon
      name: output.toon
      size: 341854
      type: file
    - path: readme.md
      name: readme.md
      size: 1905
      type: file
    - path: requirements.txt
      name: requirements.txt
      size: 4286
      type: file
    - name: result
      type: directory
      children[28]{path,name,size,type}:
        result/ast_schema_01_12_2025_11-49-24.json,ast_schema_01_12_2025_11-49-24.json,201215,file
        result/notebook_report_23_12_2025_12-56-24_NotebookLLM_gemini-2.5-flash.md,notebook_report_23_12_2025_12-56-24_NotebookLLM_gemini-2.5-flash.md,15776,file
        result/notebook_report_27_12_2025_15-06-09_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-06-09_NotebookLLM_gemini-2.5-flash.md,14804,file
        result/notebook_report_27_12_2025_15-09-29_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-09-29_NotebookLLM_gemini-2.5-flash.md,17598,file
        result/notebook_report_27_12_2025_15-26-34_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-26-34_NotebookLLM_gemini-2.5-flash.md,20458,file
        result/notebook_report_27_12_2025_15-33-06_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-33-06_NotebookLLM_gemini-2.5-flash.md,15268,file
        result/notebook_report_29_12_2025_15-03-21_NotebookLLM_gemini-2.5-flash.md,notebook_report_29_12_2025_15-03-21_NotebookLLM_gemini-2.5-flash.md,16111,file
        result/report_01_12_2025_12-26-46_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_12-26-46_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,79523,file
        result/report_01_12_2025_12-55-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_12-55-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,143883,file
        result/report_01_12_2025_13-37-30_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_13-37-30_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,141924,file
        result/report_01_12_2025_14-15-04_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_14-15-04_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,129176,file
        result/report_01_12_2025_14-42-38_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_14-42-38_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,38933,file
        result/report_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,138015,file
        result/report_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.md,report_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.md,127747,file
        result/report_03_12_2025_22-46-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_03_12_2025_22-46-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,80305,file
        result/report_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,163921,file
        result/report_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.md,report_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.md,128520,file
        result/report_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.md,report_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.md,7245,file
        result/report_14_11_2025_14-52-36.md,report_14_11_2025_14-52-36.md,16393,file
        result/report_14_11_2025_15-21-53.md,report_14_11_2025_15-21-53.md,108585,file
        result/report_14_11_2025_15-26-24.md,report_14_11_2025_15-26-24.md,11659,file
        result/report_21_11_2025_15-43-30.md,report_21_11_2025_15-43-30.md,57940,file
        result/report_21_11_2025_16-06-12.md,report_21_11_2025_16-06-12.md,76423,file
        result/report_22_11_2025_14-01-50_Helper_llama3_Main_geminipro.md,report_22_11_2025_14-01-50_Helper_llama3_Main_geminipro.md,39595,file
        result/report_22_11_2025_14-39-55_Helper_llama3_MainLLM_llama3.md,report_22_11_2025_14-39-55_Helper_llama3_MainLLM_llama3.md,2013,file
        result/result_2025-11-11_12-30-53.md,result_2025-11-11_12-30-53.md,1232,file
        result/result_2025-11-11_12-43-51.md,result_2025-11-11_12-43-51.md,33683,file
        result/result_2025-11-11_12-45-37.md,result_2025-11-11_12-45-37.md,1193,file
    - name: schemas
      type: directory
      children[1]{path,name,size,type}:
        schemas/types.py,types.py,7559,file
    - name: statistics
      type: directory
      children[5]{path,name,size,type}:
        statistics/savings_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.png,savings_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.png,23279,file
        statistics/savings_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.png,savings_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.png,23650,file
        statistics/savings_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.png,savings_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.png,23805,file
        statistics/savings_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.png,savings_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.png,22698,file
        statistics/savings_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.png,savings_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.png,22539,file
    - path: test.json
      name: test.json
      size: 241669
      type: file
ast_schema:
  files:
    "backend/AST_Schema.py":
      ast_nodes:
        imports[3]: ast,os,getRepo.GitRepository
        functions[1]:
          - mode: function_analysis
            identifier: backend.AST_Schema.path_to_module
            name: path_to_module
            args[2]: filepath,project_root
            docstring: Wandelt einen Dateipfad in einen Python-Modulpfad um.
            source_code: "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    \n    module_path = rel_path.replace(os.path.sep, '.')\n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n    return module_path"
            start_line: 5
            end_line: 18
            context:
              calls[0]:
              called_by[0]:
        classes[2]:
          - mode: class_analysis
            identifier: backend.AST_Schema.ASTVisitor
            name: ASTVisitor
            docstring: null
            source_code: "class ASTVisitor(ast.NodeVisitor):\n    def __init__(self, source_code: str, file_path: str, project_root: str):\n        self.source_code = source_code\n        self.file_path = file_path\n        self.project_root = project_root\n        self.module_path = path_to_module(self.file_path, self.project_root)\n        self.schema = {\"imports\": [], \"functions\": [], \"classes\": []}\n        self._current_class = None\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(f\"{node.module}.{alias.name}\")\n        self.generic_visit(node)\n\n    def visit_ClassDef(self, node):\n        class_identifier = f\"{self.module_path}.{node.name}\"\n        class_info = {\n            \"mode\": \"class_analysis\",\n            \"identifier\": class_identifier,\n            \"name\": node.name,\n            \"docstring\": ast.get_docstring(node),\n            \"source_code\": ast.get_source_segment(self.source_code, node),\n            \"start_line\": node.lineno,\n            \"end_line\": node.end_lineno,\n            \"context\": {\n                \"dependencies\": [],\n                \"instantiated_by\": [],\n                \"method_context\": []\n            },   \n        }\n        self.schema[\"classes\"].append(class_info)\n        self._current_class = class_info \n        self.generic_visit(node)\n        self._current_class = None\n\n    def visit_FunctionDef(self, node):\n        if self._current_class:\n            method_identifier = f\"{self._current_class['identifier']}.{node.name}\"\n            method_context_info = {\n                \"identifier\": method_identifier,\n                \"name\": node.name,\n                \"calls\": [],\n                \"called_by\": [],\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n            }\n            self._current_class[\"context\"][\"method_context\"].append(method_context_info)\n        else:\n            func_identifier = f\"{self.module_path}.{node.name}\"\n            func_info = {\n                \"mode\": \"function_analysis\",\n                \"identifier\": func_identifier,\n                \"name\": node.name,\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"source_code\": ast.get_source_segment(self.source_code, node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n                \"context\": {\n                    \"calls\": [],\n                    \"called_by\": []\n                }\n            }\n            self.schema[\"functions\"].append(func_info)\n        self.generic_visit(node)\n    \n    def visit_AsyncFunctionDef(self, node):\n        self.visit_FunctionDef(node)"
            start_line: 20
            end_line: 94
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[6]:
                - identifier: backend.AST_Schema.ASTVisitor.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[4]: self,source_code,file_path,project_root
                  docstring: null
                  start_line: 21
                  end_line: 27
                - identifier: backend.AST_Schema.ASTVisitor.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 29
                  end_line: 32
                - identifier: backend.AST_Schema.ASTVisitor.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 34
                  end_line: 37
                - identifier: backend.AST_Schema.ASTVisitor.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 39
                  end_line: 58
                - identifier: backend.AST_Schema.ASTVisitor.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 60
                  end_line: 91
                - identifier: backend.AST_Schema.ASTVisitor.visit_AsyncFunctionDef
                  name: visit_AsyncFunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 93
                  end_line: 94
          - mode: class_analysis
            identifier: backend.AST_Schema.ASTAnalyzer
            name: ASTAnalyzer
            docstring: null
            source_code: "class ASTAnalyzer:\n\n    def __init__(self):\n        pass\n\n    def merge_relationship_data(self, full_schema: dict, raw_relationships: dict) -> dict:\n\n        outgoing_calls = raw_relationships.get(\"outgoing\", {})\n        incoming_calls = raw_relationships.get(\"incoming\", {})\n\n        for file_path, file_data in full_schema.get(\"files\", {}).items():\n            ast_nodes = file_data.get(\"ast_nodes\", {})\n            \n            for func in ast_nodes.get(\"functions\", []):\n                func_id = func.get(\"identifier\")\n                if func_id:\n                    func[\"context\"][\"calls\"] = outgoing_calls.get(func_id, [])\n                    func[\"context\"][\"called_by\"] = incoming_calls.get(func_id, [])\n\n            for cls in ast_nodes.get(\"classes\", []):\n                cls_id = cls.get(\"identifier\")\n                \n                if cls_id:\n                    cls[\"context\"][\"instantiated_by\"] = incoming_calls.get(cls_id, [])\n\n                class_dependencies = set()\n                \n                for method in cls[\"context\"].get(\"method_context\", []):\n                    method_id = method.get(\"identifier\")\n                    if method_id:\n                        m_calls = outgoing_calls.get(method_id, [])\n                        method[\"calls\"] = m_calls\n                        method[\"called_by\"] = incoming_calls.get(method_id, [])\n                        \n                        for callee in m_calls:\n                            if cls_id and not callee.startswith(f\"{cls_id}.\"):\n                                class_dependencies.add(callee)\n                \n                cls[\"context\"][\"dependencies\"] = sorted(list(class_dependencies))\n        \n        return full_schema\n\n    def analyze_repository(self, files: list, repo: GitRepository) -> dict:\n        full_schema = {\n            \"files\": {}\n        }\n\n        all_paths = [file_obj.path for file_obj in files]\n        if not all_paths:\n            return full_schema\n        \n        project_root = os.path.commonpath(all_paths)\n        if os.path.isfile(project_root):\n            project_root = os.path.dirname(project_root)\n\n        for file_obj in files:\n            if not file_obj.path.endswith('.py'):\n                continue\n            \n            file_content = file_obj.content\n            if not file_content.strip():\n                continue\n\n            try:\n                tree = ast.parse(file_content)\n                visitor = ASTVisitor(\n                    source_code=file_content, \n                    file_path=file_obj.path, \n                    project_root=project_root\n                )\n                visitor.visit(tree)\n                file_schema_nodes = visitor.schema\n\n                if file_schema_nodes[\"imports\"] or file_schema_nodes[\"functions\"] or file_schema_nodes[\"classes\"]:\n                    full_schema[\"files\"][file_obj.path] = {\n                        \"ast_nodes\": file_schema_nodes\n                    }\n\n            except (SyntaxError, ValueError) as e:\n                print(f\"Warnung: Konnte Datei '{file_obj.path}' nicht parsen. Fehler: {e}\")\n\n        return full_schema"
            start_line: 97
            end_line: 178
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[3]:
                - identifier: backend.AST_Schema.ASTAnalyzer.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 99
                  end_line: 100
                - identifier: backend.AST_Schema.ASTAnalyzer.merge_relationship_data
                  name: merge_relationship_data
                  calls[0]:
                  called_by[0]:
                  args[3]: self,full_schema,raw_relationships
                  docstring: null
                  start_line: 102
                  end_line: 137
                - identifier: backend.AST_Schema.ASTAnalyzer.analyze_repository
                  name: analyze_repository
                  calls[0]:
                  called_by[0]:
                  args[3]: self,files,repo
                  docstring: null
                  start_line: 139
                  end_line: 178
    "backend/File_Dependency.py":
      ast_nodes:
        imports[17]: networkx,os,ast.Assign,ast.AST,ast.ClassDef,ast.FunctionDef,ast.Import,ast.ImportFrom,ast.Name,ast.NodeVisitor,ast.literal_eval,ast.parse,ast.walk,keyword.iskeyword,pathlib.Path,getRepo.GitRepository,callgraph.make_safe_dot
        functions[3]:
          - mode: function_analysis
            identifier: backend.File_Dependency.build_file_dependency_graph
            name: build_file_dependency_graph
            args[3]: filename,tree,repo_root
            docstring: null
            source_code: "def build_file_dependency_graph(filename: str, tree: AST, repo_root: str) -> nx.DiGraph:\n    graph = nx.DiGraph()\n\n    tree_visitor = FileDependencyGraph(filename, repo_root)\n    tree_visitor.visit(tree)\n\n    for caller, callees in tree_visitor.import_dependencies.items():\n        graph.add_node(caller)\n        graph.add_nodes_from(callees)\n        for callee in callees:\n            graph.add_edge(caller, callee)\n    \n    return graph"
            start_line: 153
            end_line: 165
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.File_Dependency.build_repository_graph
            name: build_repository_graph
            args[1]: repository
            docstring: null
            source_code: "def build_repository_graph(repository: GitRepository) -> nx.DiGraph:\n    all_files = repository.get_all_files()\n    repo_root = repository.temp_dir\n    global_graph = nx.DiGraph()\n\n    for file in all_files: \n        if not file.path.endswith(\".py\"):\n            continue\n        filename = str(os.path.basename(file.path)).removesuffix(\".py\")\n        tree = parse(file.content)\n        graph = build_file_dependency_graph(filename, tree, repo_root)\n        \n        for node in graph.nodes:\n            global_graph.add_node(node)\n\n        for caller, callee in graph.edges:\n            if callee:\n                global_graph.add_node(callee)\n                global_graph.add_edge(caller, callee)\n    \n    return global_graph"
            start_line: 167
            end_line: 187
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.File_Dependency.get_all_temp_files
            name: get_all_temp_files
            args[1]: directory
            docstring: null
            source_code: "def get_all_temp_files(directory: str) -> list[Path]:\n    root_path = Path(directory).resolve()\n    all_files = [file.relative_to(root_path) for file in root_path.rglob(\"*.py\")]\n\n    return all_files"
            start_line: 189
            end_line: 193
            context:
              calls[0]:
              called_by[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.File_Dependency.FileDependencyGraph
            name: FileDependencyGraph
            docstring: null
            source_code: "class FileDependencyGraph(NodeVisitor):\n\n    import_dependencies: dict[str, set[str]] = {}\n    def __init__(self, filename: str, repo_root):\n        \"\"\"\n        Initialisiert den File Dependency Graphen\n\n        Args:\n\n        \"\"\"\n        self.filename = filename\n        self.repo_root = repo_root\n\n    def _resolve_module_name(self, node: ImportFrom) -> list[str]:\n        \"\"\"\n        L√∂st relative Imports der Form `from .. import name1, name2` auf.\n        Liefert die Liste der tats√§chlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\n        Wirft ImportError, wenn nichts aufgel√∂st werden konnte.\n        \"\"\"\n        level_depth = node.level\n        names = [alias.name for alias in node.names]  \n        all_files = get_all_temp_files(self.repo_root)  \n\n        candidates = [p for p in all_files if p.stem == Path(self.filename).stem or p.name == f\"{self.filename}.py\"]\n\n        if not candidates:\n            raise ImportError(f\"Kann aktuelle Datei '{self.filename}' im Repo nicht finden.\")\n\n\n        candidates.sort(key=lambda p: len(p.parts))\n        current_rel_path = candidates[0]\n        base_dir = current_rel_path.parent\n        if level_depth <= 0:\n            raise ImportError(\"Erwarteter relativer Import (level >= 1).\")\n        for _ in range(level_depth - 1):\n            if base_dir == base_dir.parent and len(base_dir.parts) == 0:\n                raise ImportError(f\"Relative Import-Ebene ({level_depth}) zu gro√ü f√ºr Datei '{current_rel_path}'.\")\n            base_dir = base_dir.parent\n\n        repo_root_path = Path(self.repo_root).resolve()\n        resolved: list[str] = []\n\n        def module_file_exists(rel_base: Path, name: str) -> bool:\n            file_path = repo_root_path / rel_base / f\"{name}.py\"\n            pkg_init = repo_root_path / rel_base / name / \"__init__.py\"\n            return file_path.exists() or pkg_init.exists()\n\n        def init_exports_symbol(rel_base: Path, symbol: str) -> bool:\n            \"\"\"\n            Pr√ºft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\n            oder als Name (funktion/klasse/assign) definiert ist.\n            \"\"\"\n            init_path = repo_root_path / rel_base / \"__init__.py\"\n            if not init_path.exists():\n                return False\n            try:\n                src = init_path.read_text(encoding=\"utf-8\")\n                mod = parse(src, filename=str(init_path))\n            except Exception:\n                return False\n\n            for node_ in walk(mod):\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == \"__all__\":\n                            try:\n                                value = literal_eval(node_.value)\n                                if isinstance(value, (list, tuple)) and symbol in value:\n                                    return True\n                            except Exception:\n                                pass\n                if isinstance(node_, (FunctionDef, ClassDef)) and node_.name == symbol:\n                    return True\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == symbol:\n                            return True\n            return False\n\n        for name in names:\n            if not name.isidentifier() or iskeyword(name):\n                continue\n\n            if module_file_exists(base_dir, name):\n                resolved.append(name)\n                continue\n            if init_exports_symbol(base_dir, name):\n                resolved.append(name)\n                continue\n\n        resolved = sorted(set(resolved))\n\n        if not resolved:\n            raise ImportError(\n                f\"Kein passendes Modul/Symbol f√ºr relative Import-Aufl√∂sung gefunden \"\n                f\"(level={level_depth}, names={names}, base_dir={base_dir})\"\n            )\n\n        return resolved\n    \n    def visit_Import(self, node: Import | ImportFrom, base_name: str | None = None):\n        for alias in node.names:\n        \n            if self.filename not in self.import_dependencies:\n                self.import_dependencies[self.filename] = set()\n\n            if base_name:\n                self.import_dependencies[self.filename].add(base_name)\n            else:\n                self.import_dependencies[self.filename].add(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node: ImportFrom):\n        \"\"\"\n        Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\n        also c, und dieser wird als callee f√ºr den caller, das File, gesetzt.\n        \"\"\"\n        module_name = node.module\n        if module_name:\n            module_base = module_name.split(\".\")[-1]\n            self.visit_Import(node, module_base)\n        else:\n            try:\n                resolved = self._resolve_module_name(node)\n                for base in resolved:\n                    self.visit_Import(node, base)\n            except ImportError as e:\n                print(f\"Aufl√∂sung eines relativen Imports fehlgeschlagen: {e}\")\n\n        self.generic_visit(node)"
            start_line: 22
            end_line: 151
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[6]:
                - identifier: backend.File_Dependency.FileDependencyGraph.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[3]: self,filename,repo_root
                  docstring: "Initialisiert den File Dependency Graphen\n\nArgs:"
                  start_line: 25
                  end_line: 33
                - identifier: backend.File_Dependency.FileDependencyGraph._resolve_module_name
                  name: _resolve_module_name
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: "L√∂st relative Imports der Form `from .. import name1, name2` auf.\nLiefert die Liste der tats√§chlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\nWirft ImportError, wenn nichts aufgel√∂st werden konnte."
                  start_line: 35
                  end_line: 120
                - identifier: backend.File_Dependency.FileDependencyGraph.module_file_exists
                  name: module_file_exists
                  calls[0]:
                  called_by[0]:
                  args[2]: rel_base,name
                  docstring: null
                  start_line: 64
                  end_line: 67
                - identifier: backend.File_Dependency.FileDependencyGraph.init_exports_symbol
                  name: init_exports_symbol
                  calls[0]:
                  called_by[0]:
                  args[2]: rel_base,symbol
                  docstring: "Pr√ºft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\noder als Name (funktion/klasse/assign) definiert ist."
                  start_line: 69
                  end_line: 99
                - identifier: backend.File_Dependency.FileDependencyGraph.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[3]: self,node,base_name
                  docstring: null
                  start_line: 122
                  end_line: 132
                - identifier: backend.File_Dependency.FileDependencyGraph.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: "Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\nalso c, und dieser wird als callee f√ºr den caller, das File, gesetzt."
                  start_line: 134
                  end_line: 151
    "backend/HelperLLM.py":
      ast_nodes:
        imports[23]: os,json,logging,time,typing.List,typing.Dict,typing.Any,typing.Optional,typing.Union,dotenv.load_dotenv,langchain_google_genai.ChatGoogleGenerativeAI,langchain_ollama.ChatOllama,langchain_openai.ChatOpenAI,langchain.messages.HumanMessage,langchain.messages.SystemMessage,langchain.messages.AIMessage,pydantic.ValidationError,schemas.types.FunctionAnalysis,schemas.types.ClassAnalysis,schemas.types.FunctionAnalysisInput,schemas.types.FunctionContextInput,schemas.types.ClassAnalysisInput,schemas.types.ClassContextInput
        functions[1]:
          - mode: function_analysis
            identifier: backend.HelperLLM.main_orchestrator
            name: main_orchestrator
            args[0]:
            docstring: "Dummy Data and processing loop for testing the LLMHelper class.\nThis version is syntactically correct and logically matches the Pydantic models."
            source_code: "def main_orchestrator():\n    \"\"\"\n    Dummy Data and processing loop for testing the LLMHelper class.\n    This version is syntactically correct and logically matches the Pydantic models.\n    \"\"\"\n    \n    # --- Step 1: Define the pre-computed analysis for each method ---\n    \n    # Example Input 1: For the 'add_item' function\n    add_item_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"source_code\": \"\"\"def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\\\"\\\"\\\"\n        if not isinstance(quantity, int) or quantity <= 0:\n            raise ValueError(\"Quantity must be a positive integer.\")\n    \n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n        print(f\"Added {quantity} of {item_name}. New total: {self.inventory[item_name]}\")\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"process_shipment\", \"restock_api_endpoint\"]\n        }\n    })\n\n    # Example Input 2: For the 'check_stock' function\n    check_stock_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"source_code\": \"\"\"def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Retrieves the current stock quantity for a given item.\\\"\\\"\\\"\n        stock_level = self.inventory.get(item_name, 0)\n        return stock_level\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"fulfill_order\", \"ui_display_handler\"]\n        }\n    })\n\n    # Example Input 3: For the 'generate_report' function\n    generate_report_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"source_code\": \"\"\"def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a timestamped string summary of the current inventory state.\\\"\\\"\\\"\n        timestamp = datetime.now()\n        report_header = \"Inventory Report as of: {ts}\\\\n\".format(ts=timestamp)\n        report_lines = [report_header]\n        \n        for item, quantity in self.inventory.items():\n            line = \"- {name}: {qty}\".format(name=item, qty=quantity)\n            report_lines.append(line)\n            \n        return \"\\\\n\".join(report_lines)\"\"\",\n        \"imports\": [\"from datetime import datetime\"],\n        \"context\": {\n            \"calls\": [\"datetime.now\", \"str.format\"],\n            \"called_by\": [\"daily_cron_job\", \"admin_dashboard_export\"]\n        }\n    })\n\n    add_item_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"description\": {\n            \"overall\": \"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to add.\"},\n                {\"name\": \"quantity\", \"type\": \"int\", \"description\": \"The number of units to add. Must be a positive integer.\"}\n            ],\n            \"returns\": [],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"process_shipment and restock_api_endpoint\" }\n        },\n        \"error\": None\n    })\n\n    check_stock_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"description\": {\n            \"overall\": \"Retrieves the current stock quantity for a given item.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to check.\"}\n            ],\n            \"returns\": [\n                {\"name\": \"stock_level\", \"type\": \"int\", \"description\": \"The quantity of the item in stock. Returns 0 if the item is not found.\"}\n            ],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"fulfill_order and ui_display_handler\" }\n        },\n        \"error\": None\n    })\n\n    generate_report_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"description\": {\n            \"overall\": \"Generates a timestamped string summary of the current inventory state.\",\n            \"parameters\": [ {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"} ],\n            \"returns\": [\n                {\"name\": \"report\", \"type\": \"str\", \"description\": \"A formatted string detailing all items and their quantities.\"}\n            ],\n            \"usage_context\": { \"calls\": \"datetime.now and str.format\", \"called_by\": \"daily_cron_job and admin_dashboard_export\" }\n        },\n        \"error\": None\n    })\n\n    class_input = ClassAnalysisInput(\n        mode=\"class_analysis\",\n        identifier=\"InventoryManager\",\n        # NOTE: The indentation of this string is now fixed. It starts at column 0.\n        source_code=\"\"\"class InventoryManager:\n    \\\"\\\"\\\"Manages stock levels for products in a warehouse.\\\"\\\"\\\"\n    def __init__(self, warehouse_id: str):\n        self.warehouse_id = warehouse_id\n        self.inventory = {}  # item_name: quantity\n\n    def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds an item to the inventory.\\\"\\\"\\\"\n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n\n    def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Checks the stock of a specific item.\\\"\\\"\\\"\n        return self.inventory.get(item_name, 0)\n\n    def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a summary report of the inventory.\\\"\\\"\\\"\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        report_lines = [f\"Inventory Report for {self.warehouse_id} at {timestamp}\"]\n        if not self.inventory:\n            report_lines.append(\"Inventory is empty.\")\n        else:\n            for item, quantity in self.inventory.items():\n                report_lines.append(f\"- {item}: {quantity}\")\n        return \"\\\\n\".join(report_lines)\n\"\"\",\n        imports=[\"from datetime import datetime\"],\n        context=ClassContextInput(\n            dependencies=[\"datetime\"],\n            instantiated_by=[\"main_app_startup\", \"warehouse_worker_script\"],\n            methods_analysis=[\n                add_item_analysis,\n                check_stock_analysis,\n                generate_report_analysis\n            ]\n        )\n    )\n\n    \n    # The helper methods expect a LIST of inputs, so we wrap our single object in a list.\n    input = [add_item_input, check_stock_input, generate_report_input]\n    analysis = [add_item_analysis, check_stock_analysis, generate_report_analysis] \n\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    llm_helper = LLMHelper(api_key=GEMINI_API_KEY, function_prompt_path=function_prompt_file, class_prompt_path=class_prompt_file)\n\n    # This will be the final JSON containing all documentation\n    final_documentation = {}\n\n    # The analysis of the methods is already provided in the context.\n    logging.info(\"\\n--- Generating documentation for classes ---\")\n    \n    # The `generate_for_classes` method returns a list of results.\n    analysis_results = llm_helper.generate_for_functions(input)\n\n    # --- Step 4: Process the results ---\n    \n    # We loop through the list of results (even though there's only one in this case).\n    for doc in analysis_results:\n        if doc:\n            logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n            if \"classes\" not in final_documentation:\n                final_documentation[\"classes\"] = {}\n            # Use .model_dump() to convert the Pydantic object back to a dict for JSON serialization\n            final_documentation[\"classes\"][doc.identifier] = doc.model_dump() \n        else:\n            logging.warning(f\"Failed to generate doc for a class\")\n\n    # --- Step 5: Display the final aggregated result ---\n    logging.info(\"\\n--- Final Generated Documentation ---\")\n    print(json.dumps(final_documentation, indent=2))"
            start_line: 227
            end_line: 413
            context:
              calls[0]:
              called_by[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.HelperLLM.LLMHelper
            name: LLMHelper
            docstring: "A class to interact with Google Gemini for generating code snippet documentation.\nIt centralizes API interaction, error handling, and validates I/O using Pydantic."
            source_code: "class LLMHelper:\n    \"\"\"\n    A class to interact with Google Gemini for generating code snippet documentation.\n    It centralizes API interaction, error handling, and validates I/O using Pydantic.\n    \"\"\"\n    def __init__(self, api_key: str, function_prompt_path: str, class_prompt_path: str, model_name: str = \"gemini-2.0-flash-lite\", base_url: str = None):\n        if not api_key:\n            raise ValueError(\"Gemini API Key must be set.\")\n        \n        try:\n            with open(function_prompt_path, 'r', encoding='utf-8') as f:\n                self.function_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Function system prompt file not found at: {function_prompt_path}\")\n            raise\n\n        # Handle the second file\n        try:\n            with open(class_prompt_path, 'r', encoding='utf-8') as f:\n                self.class_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Class system prompt file not found at: {class_prompt_path}\")\n            raise\n        \n        # Batch-Size config\n        self.model_name = model_name\n        self._configure_batch_settings(model_name)\n\n\n        if model_name.startswith(\"gemini-\"):\n            base_llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=0.3, \n            )\n        \n        elif model_name.startswith(\"gpt-\") and \"openGPT\" not in model_name:\n            base_llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=0.3,\n            )\n\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            if not SCADSLLM_URL:\n                raise ValueError(f\"SCADSLLM_URL environment variable is required for model {model_name}\")\n            \n            logging.info(f\"Connecting to Custom API at {SCADSLLM_URL} for model {model_name}\")\n            \n            base_llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                base_url=SCADSLLM_URL,\n                temperature=0.3,\n            )\n\n        else:\n            target_url = base_url if base_url else OLLAMA_BASE_URL\n            logging.info(f\"Using Ollama at {target_url} for model {model_name}\")\n            base_llm = ChatOllama(\n                model=model_name,\n                temperature=0.3,\n                base_url=target_url,\n            )\n\n        self.function_llm = base_llm.with_structured_output(FunctionAnalysis, method=\"json_schema\")\n        self.class_llm = base_llm.with_structured_output(ClassAnalysis, method=\"json_schema\")\n\n        self.raw_llm = base_llm\n\n        logging.info(f\"LLMHelper initialized with model '{model_name}'. Batch Size: {self.batch_size}\")\n\n    def _configure_batch_settings(self, model_name: str):\n\n        if model_name == \"gemini-2.0-flash-lite\":\n            self.batch_size = 30\n        \n        elif model_name == \"gemini-flash-latest\":\n            self.batch_size = 10\n\n        elif model_name == \"gemini-2.5-flash\":\n            self.batch_size = 10\n            \n        elif model_name == \"gemini-2.5-pro\":\n            self.batch_size = 2\n        \n        elif model_name == \"gemini-2.0-flash\":\n            self.batch_size = 15\n\n        elif model_name == \"gemini-2.5-flash-lite\":\n            self.batch_size = 15\n\n        elif model_name == \"llama3\":\n            self.batch_size = 50\n\n        elif model_name == \"gpt-5.1\":\n            self.batch_size = 500\n\n        elif model_name == \"gpt-5-mini\":\n            self.batch_size = 500\n\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            self.batch_size = 500\n            \n        else:\n            logging.warning(f\"Unknown model '{model_name}', using conservative defaults.\")\n            self.batch_size = 2\n\n    def generate_for_functions(self, function_inputs: List[FunctionAnalysisInput]) -> List[Optional[FunctionAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of functions.\"\"\"\n\n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 62\n\n        if not function_inputs:\n            return []\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(function_input.model_dump(), indent=2) \n            for function_input in function_inputs\n        ]\n       \n        conversations = [[SystemMessage(content=self.function_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_functions = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n        \n            try:\n                batch_results = self.function_llm.batch(batch_conversations, config={\"max_concurrency\": self.batch_size})\n                all_validated_functions.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                all_validated_functions.extend([None] * len(batch_conversations))\n            \n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_functions\n        \n            \n    \n\n    def generate_for_classes(self, class_inputs: List[ClassAnalysisInput]) -> List[Optional[ClassAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of classes.\"\"\"\n        if not class_inputs:\n            return []\n        \n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 62\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(class_input.model_dump(), indent=2)\n            for class_input in class_inputs\n        ]\n\n        conversations = [[SystemMessage(content=self.class_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_classes = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n\n            try:\n                batch_results = self.class_llm.batch(batch_conversations , config={\"max_concurrency\": self.batch_size})\n                all_validated_classes.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                # Falls ein Fehler auftritt, f√ºllen wir die Liste mit None auf, um die Reihenfolge zu wahren\n                all_validated_classes.extend([None] * len(batch_conversations))\n\n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_classes"
            start_line: 31
            end_line: 221
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[4]:
                - identifier: backend.HelperLLM.LLMHelper.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[6]: self,api_key,function_prompt_path,class_prompt_path,model_name,base_url
                  docstring: null
                  start_line: 36
                  end_line: 101
                - identifier: backend.HelperLLM.LLMHelper._configure_batch_settings
                  name: _configure_batch_settings
                  calls[0]:
                  called_by[0]:
                  args[2]: self,model_name
                  docstring: null
                  start_line: 103
                  end_line: 137
                - identifier: backend.HelperLLM.LLMHelper.generate_for_functions
                  name: generate_for_functions
                  calls[0]:
                  called_by[0]:
                  args[2]: self,function_inputs
                  docstring: Generates and validates documentation for a batch of functions.
                  start_line: 139
                  end_line: 178
                - identifier: backend.HelperLLM.LLMHelper.generate_for_classes
                  name: generate_for_classes
                  calls[0]:
                  called_by[0]:
                  args[2]: self,class_inputs
                  docstring: Generates and validates documentation for a batch of classes.
                  start_line: 183
                  end_line: 221
    "backend/MainLLM.py":
      ast_nodes:
        imports[9]: os,logging,sys,dotenv.load_dotenv,langchain_google_genai.ChatGoogleGenerativeAI,langchain_ollama.ChatOllama,langchain_openai.ChatOpenAI,langchain.messages.HumanMessage,langchain.messages.SystemMessage
        functions[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.MainLLM.MainLLM
            name: MainLLM
            docstring: Hauptklasse f√ºr die Interaktion mit dem LLM.
            source_code: "class MainLLM:\n    \"\"\"\n    Hauptklasse f√ºr die Interaktion mit dem LLM.\n    \"\"\"\n    def __init__(self, api_key: str, prompt_file_path: str, model_name: str = \"gemini-2.5-pro\", base_url: str = None):\n        if not api_key:\n            raise ValueError(\"Gemini API Key must be set.\")\n        \n        try:\n            with open(prompt_file_path, 'r', encoding='utf-8') as f:\n                self.system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"System prompt file not found at: {prompt_file_path}\")\n            raise\n        \n        self.model_name = model_name\n\n        if model_name.startswith(\"gemini-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n\n        elif model_name.startswith(\"gpt-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            if not SCADSLLM_URL:\n                raise ValueError(f\"SCADSLLM_URL environment variable is required for model {model_name}\")\n            \n            logging.info(f\"Connecting to Custom API at {SCADSLLM_URL} for model {model_name}\")\n            \n            self.llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                base_url=SCADSLLM_URL,\n                temperature=1.0,\n            )\n\n        else:\n            target_url = base_url if base_url else OLLAMA_BASE_URL\n            self.llm = ChatOllama(\n                model=model_name,\n                temperature=1.0,\n                base_url=target_url,\n            )\n\n        logging.info(f\"Main LLM initialized with model '{model_name}'.\")\n    \n    def call_llm(self, user_input: str):\n\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with HelperLLM input...\")\n\n        try:\n            response = self.llm.invoke(messages)\n            logging.info(\"LLM call successful.\")\n            return response.content\n        except Exception as e:\n            logging.error(f\"Error during LLM call: {e}\")\n            return None\n\n    def stream_llm(self, user_input: str):\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with 'stream'...\")\n\n        try:\n            stream_iterator = self.llm.stream(messages)\n            \n            for chunk in stream_iterator:\n                yield chunk.content\n        except Exception as e:\n            error_message = f\"\\n--- Error during LLM stream call: {e} ---\"\n            logging.error(error_message)\n            yield error_message"
            start_line: 22
            end_line: 106
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[3]:
                - identifier: backend.MainLLM.MainLLM.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[5]: self,api_key,prompt_file_path,model_name,base_url
                  docstring: null
                  start_line: 26
                  end_line: 73
                - identifier: backend.MainLLM.MainLLM.call_llm
                  name: call_llm
                  calls[0]:
                  called_by[0]:
                  args[2]: self,user_input
                  docstring: null
                  start_line: 75
                  end_line: 89
                - identifier: backend.MainLLM.MainLLM.stream_llm
                  name: stream_llm
                  calls[0]:
                  called_by[0]:
                  args[2]: self,user_input
                  docstring: null
                  start_line: 91
                  end_line: 106
    "backend/basic_info.py":
      ast_nodes:
        imports[7]: re,os,tomllib,typing.List,typing.Dict,typing.Any,typing.Optional
        functions[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.basic_info.ProjektInfoExtractor
            name: ProjektInfoExtractor
            docstring: "Extrahiert grundlegende Projektinformationen aus g√§ngigen Projektdateien\nwie README, pyproject.toml und requirements.txt."
            source_code: "class ProjektInfoExtractor:\n    \"\"\"\n    Extrahiert grundlegende Projektinformationen aus g√§ngigen Projektdateien\n    wie README, pyproject.toml und requirements.txt.\n    \"\"\"\n    def __init__(self):\n        self.INFO_NICHT_GEFUNDEN = \"Information not found\"\n        # Initialisiert die Struktur mit Platzhaltern\n        self.info = {\n            \"projekt_uebersicht\": {\n                \"titel\": self.INFO_NICHT_GEFUNDEN,\n                \"beschreibung\": self.INFO_NICHT_GEFUNDEN,\n                \"aktueller_status\": self.INFO_NICHT_GEFUNDEN,\n                \"key_features\": self.INFO_NICHT_GEFUNDEN,\n                \"tech_stack\": self.INFO_NICHT_GEFUNDEN,\n            },\n            \"installation\": {\n                \"dependencies\": self.INFO_NICHT_GEFUNDEN,\n                \"setup_anleitung\": self.INFO_NICHT_GEFUNDEN,\n                \"quick_start_guide\": self.INFO_NICHT_GEFUNDEN,\n            }\n        }\n\n    def _clean_content(self, content: str) -> str:\n        \"\"\"Entfernt Null-Bytes, die durch Encoding-Fehler (UTF-16 als UTF-8 gelesen) entstehen.\"\"\"\n        if not content:\n            return \"\"\n        return content.replace('\\x00', '')\n\n    def _finde_datei(self, patterns: List[str], dateien: List[Any]) -> Optional[Any]:\n        \"\"\"Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht.\"\"\"\n        for datei in dateien:\n            for pattern in patterns:\n                if datei.path.lower().endswith(pattern.lower()):\n                    return datei\n        return None\n\n    def _extrahiere_sektion_aus_markdown(self, inhalt: str, keywords: List[str]) -> Optional[str]:\n        \"\"\"\n        Extrahiert den Text unter einer Markdown-√úberschrift (##).\n        \"\"\"\n        # Erstellt ein Regex-Pattern, das auf jedes der Schl√ºsselw√∂rter reagiert\n        keyword_pattern = \"|\".join(re.escape(k) for k in keywords)\n        \n        # Sucht nach \"## Schl√ºsselwort\" und erfasst alles bis zur n√§chsten \"##\" oder dem Dateiende\n        pattern = re.compile(\n            rf\"##\\s*({keyword_pattern})\\s*\\n(.*?)(?=\\n##|\\Z)\",\n            re.IGNORECASE | re.DOTALL\n        )\n        match = pattern.search(inhalt)\n        if match:\n            return match.group(2).strip()\n        return None\n\n    def _parse_readme(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer README-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        if self.info[\"projekt_uebersicht\"][\"titel\"] == self.INFO_NICHT_GEFUNDEN:\n            title_match = re.search(r\"^\\s*#\\s*(.*)\", inhalt)\n            if title_match:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = title_match.group(1).strip()\n\n        # Beschreibung (Fallback)\n        if self.info[\"projekt_uebersicht\"][\"beschreibung\"] == self.INFO_NICHT_GEFUNDEN:\n            desc_match = re.search(r\"^\\s*#\\s*.*\\n+([^#\\n].*)\", inhalt, re.DOTALL)\n            if desc_match:\n                 self.info[\"projekt_uebersicht\"][\"beschreibung\"] = desc_match.group(1).strip().split('\\n\\n')[0]\n\n        # Key Features\n        features = self._extrahiere_sektion_aus_markdown(inhalt, [\"Features\", \"Key Features\", \"Merkmale\"])\n        if features:\n            self.info[\"projekt_uebersicht\"][\"key_features\"] = features\n            \n        # Tech Stack\n        tech_stack = self._extrahiere_sektion_aus_markdown(inhalt, [\"Tech Stack\", \"Technology\", \"Technologien\"])\n        if tech_stack:\n            self.info[\"projekt_uebersicht\"][\"tech_stack\"] = tech_stack\n            \n        # Status\n        status = self._extrahiere_sektion_aus_markdown(inhalt, [\"Status\", \"Current Status\"])\n        if status:\n            self.info[\"projekt_uebersicht\"][\"aktueller_status\"] = status\n\n        # Setup-Anleitung\n        setup = self._extrahiere_sektion_aus_markdown(inhalt, [\"Installation\", \"Setup\", \"Getting Started\"])\n        if setup:\n            self.info[\"installation\"][\"setup_anleitung\"] = setup\n            \n        # Quick Start\n        quick_start = self._extrahiere_sektion_aus_markdown(inhalt, [\"Quick Start\", \"Schnellstart\"])\n        if quick_start:\n            self.info[\"installation\"][\"quick_start_guide\"] = quick_start\n\n    def _parse_toml(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer pyproject.toml-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        if not tomllib:\n            print(\"Warnung: 'tomli' ist nicht installiert. pyproject.toml kann nicht analysiert werden.\")\n            return\n            \n        try:\n            data = tomllib.loads(inhalt)\n            project_data = data.get(\"project\", {})\n            \n            if \"name\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = project_data[\"name\"]\n            if \"description\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"beschreibung\"] = project_data[\"description\"]\n            if \"dependencies\" in project_data:\n                self.info[\"installation\"][\"dependencies\"] = project_data[\"dependencies\"]\n        except tomllib.TOMLDecodeError as e:\n            print(f\"Warnung: Fehler beim Parsen der pyproject.toml: {e}\")\n\n    def _parse_requirements(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer requirements.txt-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        # Nur f√ºllen, wenn noch keine Dependencies aus toml gefunden wurden\n        if self.info[\"installation\"][\"dependencies\"] == self.INFO_NICHT_GEFUNDEN:\n            lines = inhalt.splitlines()\n            dependencies = [\n                line.strip() for line in lines \n                if line.strip() and not line.strip().startswith('#')\n            ]\n            if dependencies:\n                self.info[\"installation\"][\"dependencies\"] = dependencies\n\n    def extrahiere_info(self, dateien: List[Any], repo_url: str) -> Dict[str, Any]:\n        \"\"\"\n        Orchestriert die Extraktion von Informationen.\n        \"\"\"\n        # Relevante Dateien finden\n        readme_datei = self._finde_datei([\"readme.md\", \"readme.rst\", \"readme.txt\", \"readme\"], dateien)\n        toml_datei = self._finde_datei([\"pyproject.toml\"], dateien)\n        req_datei = self._finde_datei([\"requirements.txt\"], dateien)\n\n        # Dateien parsen (mit Priorisierung)\n        if toml_datei:\n            self._parse_toml(toml_datei.content)\n\n        if req_datei:\n            self._parse_requirements(req_datei.content)\n            \n        if readme_datei:\n            self._parse_readme(readme_datei.content)\n            \n        # Finale Formatierung der Dependencies\n        deps = self.info[\"installation\"][\"dependencies\"]\n        if isinstance(deps, list):\n            if not deps:\n                self.info[\"installation\"][\"dependencies\"] = self.INFO_NICHT_GEFUNDEN\n            else:\n                self.info[\"installation\"][\"dependencies\"] = \"\\n\".join(f\"- {dep}\" for dep in deps)\n        \n        # Titel aus URL ableiten, falls nichts gefunden wurde (oder als Zusatzinfo)\n        if repo_url:\n            repo_name = os.path.basename(repo_url.removesuffix('.git'))\n            # Wenn noch kein Titel da ist oder er generisch war\n            if self.info[\"projekt_uebersicht\"][\"titel\"] == self.INFO_NICHT_GEFUNDEN:\n                 self.info[\"projekt_uebersicht\"][\"titel\"] = f\"{repo_name} documentation\"\n\n        return self.info"
            start_line: 9
            end_line: 172
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[8]:
                - identifier: backend.basic_info.ProjektInfoExtractor.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 14
                  end_line: 30
                - identifier: backend.basic_info.ProjektInfoExtractor._clean_content
                  name: _clean_content
                  calls[0]:
                  called_by[0]:
                  args[2]: self,content
                  docstring: "Entfernt Null-Bytes, die durch Encoding-Fehler (UTF-16 als UTF-8 gelesen) entstehen."
                  start_line: 32
                  end_line: 36
                - identifier: backend.basic_info.ProjektInfoExtractor._finde_datei
                  name: _finde_datei
                  calls[0]:
                  called_by[0]:
                  args[3]: self,patterns,dateien
                  docstring: "Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht."
                  start_line: 38
                  end_line: 44
                - identifier: backend.basic_info.ProjektInfoExtractor._extrahiere_sektion_aus_markdown
                  name: _extrahiere_sektion_aus_markdown
                  calls[0]:
                  called_by[0]:
                  args[3]: self,inhalt,keywords
                  docstring: Extrahiert den Text unter einer Markdown-√úberschrift (##).
                  start_line: 46
                  end_line: 61
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_readme
                  name: _parse_readme
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer README-Datei.
                  start_line: 63
                  end_line: 101
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_toml
                  name: _parse_toml
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer pyproject.toml-Datei.
                  start_line: 103
                  end_line: 122
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_requirements
                  name: _parse_requirements
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer requirements.txt-Datei.
                  start_line: 124
                  end_line: 136
                - identifier: backend.basic_info.ProjektInfoExtractor.extrahiere_info
                  name: extrahiere_info
                  calls[0]:
                  called_by[0]:
                  args[3]: self,dateien,repo_url
                  docstring: Orchestriert die Extraktion von Informationen.
                  start_line: 138
                  end_line: 172
    "backend/callgraph.py":
      ast_nodes:
        imports[9]: ast,networkx,os,pathlib.Path,typing.Dict,getRepo.GitRepository,getRepo.GitRepository,basic_info.ProjektInfoExtractor,os
        functions[2]:
          - mode: function_analysis
            identifier: backend.callgraph.make_safe_dot
            name: make_safe_dot
            args[2]: graph,out_path
            docstring: null
            source_code: "def make_safe_dot(graph: nx.DiGraph, out_path: str):\n    mapping = {}\n    H = graph.copy()\n    for i, n in enumerate(list(graph.nodes())):\n        safe = f\"n{i}\"           \n        mapping[n] = safe\n    H = nx.relabel_nodes(H, mapping)\n    for orig, safe in mapping.items():\n        H.nodes[safe][\"label\"] = orig\n    nx.drawing.nx_pydot.write_dot(H, out_path)"
            start_line: 173
            end_line: 182
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.callgraph.build_filtered_callgraph
            name: build_filtered_callgraph
            args[1]: repo
            docstring: Baut den globalen Call-Graphen und filtert ihn auf selbst geschriebene Funktionen.
            source_code: "def build_filtered_callgraph(repo: GitRepository) -> nx.DiGraph:\n    \"\"\"\n    Baut den globalen Call-Graphen und filtert ihn auf selbst geschriebene Funktionen.\n    \"\"\"\n    all_file_objects = repo.get_all_files()\n    own_functions = set()\n    file_trees = {}\n\n    for file in all_file_objects:\n        if not file.path.endswith(\".py\"):\n            continue\n        filename = Path(file.path).stem\n        tree = ast.parse(file.content)\n        file_trees[filename] = tree\n        visitor = CallGraph(filename)\n        visitor.visit(tree)\n        own_functions.update(visitor.function_set)\n\n    # Globalen Call-Graph bauen\n    global_graph = nx.DiGraph()\n    for filename, tree in file_trees.items():\n        visitor = CallGraph(filename)\n        visitor.visit(tree)\n        for caller, callees in visitor.edges.items():\n            if caller not in own_functions:\n                continue \n            for callee in callees:\n                if callee in own_functions:\n                    global_graph.add_edge(caller, callee)\n                    global_graph.add_node(caller)\n                    global_graph.add_node(callee)\n    return global_graph"
            start_line: 185
            end_line: 216
            context:
              calls[0]:
              called_by[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.callgraph.CallGraph
            name: CallGraph
            docstring: null
            source_code: "class CallGraph(ast.NodeVisitor):\n    def __init__(self, filename: str):\n        self.filename = filename\n        self.current_function = None\n        self.current_class = None\n        \n        self.local_defs: dict[str, str] = {}\n        self.graph = nx.DiGraph()\n        self.import_mapping: dict[str, str] = {}\n        self.function_set: set[str] = set()\n        self.edges: Dict[str, set[str]] = {}\n\n    def _recursive_call(self, node):\n        \"\"\"\n        Liefert eine Liste der Namenskomponenten als dotted string z.B. ['pkg', 'mod', 'Class', 'method']\n        \"\"\"\n        if isinstance(node, ast.Call):\n            return self._recursive_call(node.func)\n        if isinstance(node, ast.Name):\n            return [node.id]\n        if isinstance(node, ast.Attribute):\n            parts = self._recursive_call(node.value)\n            parts.append(node.attr)\n            return parts\n        return []\n\n    def _resolve_all_callee_names(self, callee_nodes: list[list[str]]) -> list[str]:\n        \"\"\"\n        callee_nodes ist jetzt eine Liste von Listen (Name-Steps).\n        Wir pr√ºfen zuerst lokale Definitionen, dann import_mapping.\n        \"\"\"\n        resolved = []\n        for parts in callee_nodes:\n            if not parts:\n                continue\n            simple = parts[-1]\n            dotted = \".\".join(parts[-2:]) if len(parts) >= 2 else simple\n\n            if simple in self.local_defs:\n                resolved.append(self.local_defs[simple])\n                continue\n            if dotted in self.local_defs:\n                resolved.append(self.local_defs[dotted])\n                continue\n\n            first = parts[0]\n            if first in self.import_mapping:\n                mod = self.import_mapping[first]\n                rest = \"::\".join(parts[1:]) if len(parts) > 1 else simple\n                resolved.append(f\"{mod}::{rest}\")\n                continue\n\n            if self.current_class:\n                resolved.append(f\"{self.filename}::{parts[0]}::{simple}\" if len(parts)==1 else f\"{self.filename}::{'::'.join(parts)}\")\n            else:\n                resolved.append(f\"{self.filename}::{ '::'.join(parts)}\")\n        return resolved\n\n    def _make_full_name(self, basename: str, class_name: str | None = None) -> str:\n        if class_name:\n            return f\"{self.filename}::{class_name}::{basename}\"\n        return f\"{self.filename}::{basename}\"\n    \n    def _current_caller(self) -> str:\n        if self.current_function:\n            return self.current_function\n        return f\"<{self.filename}>\" if self.filename else \"<global-scope>\"\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            module_name = alias.name\n            module_asname = alias.asname if alias.asname else module_name\n            self.import_mapping[module_asname] = module_name\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        module_name = node.module.split(\".\")[-1] if node.module else \"\"\n        for alias in node.names:\n            self.import_mapping[alias.asname or alias.name] = f\"{module_name}\" if module_name else alias.name\n\n    def visit_ClassDef(self, node: ast.ClassDef):\n        prev_class = self.current_class\n        self.current_class = node.name\n        self.generic_visit(node)\n        self.current_class = prev_class\n        \n    def visit_FunctionDef(self, node):\n        prev_function = self.current_function\n        full_name = self._make_full_name(node.name, self.current_class)\n        self.local_defs[node.name] = full_name\n        if self.current_class:\n            self.local_defs[f\"{self.current_class}.{node.name}\"] = full_name\n\n        self.current_function = full_name\n        self.graph.add_node(self.current_function)\n        self.generic_visit(node)\n        self.function_set.add(self.current_function)\n        self.current_function = prev_function\n\n    def visit_AsyncFunctionDef(self, node):\n        self.visit_FunctionDef(node)\n\n    def visit_Call(self, node):\n        caller = self._current_caller()\n        parts = self._recursive_call(node)\n        resolved_callees = self._resolve_all_callee_names([parts])\n\n        if caller not in self.edges:\n            self.edges[caller] = set()\n\n        for callee in resolved_callees:\n            if callee:\n                self.edges[caller].add(callee)\n        self.generic_visit(node)\n\n    def visit_If(self, node):\n        if (isinstance(node.test, ast.Compare) and\n            isinstance(node.test.left, ast.Name) and \n            node.test.left.id == \"__name__\"):\n            caller_backup = self.current_function\n            self.current_function = \"<main_block>\"\n            self.generic_visit(node)\n            self.current_function = caller_backup\n        else:\n            self.generic_visit(node)"
            start_line: 10
            end_line: 134
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[12]:
                - identifier: backend.callgraph.CallGraph.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,filename
                  docstring: null
                  start_line: 11
                  end_line: 20
                - identifier: backend.callgraph.CallGraph._recursive_call
                  name: _recursive_call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: "Liefert eine Liste der Namenskomponenten als dotted string z.B. ['pkg', 'mod', 'Class', 'method']"
                  start_line: 22
                  end_line: 34
                - identifier: backend.callgraph.CallGraph._resolve_all_callee_names
                  name: _resolve_all_callee_names
                  calls[0]:
                  called_by[0]:
                  args[2]: self,callee_nodes
                  docstring: "callee_nodes ist jetzt eine Liste von Listen (Name-Steps).\nWir pr√ºfen zuerst lokale Definitionen, dann import_mapping."
                  start_line: 36
                  end_line: 66
                - identifier: backend.callgraph.CallGraph._make_full_name
                  name: _make_full_name
                  calls[0]:
                  called_by[0]:
                  args[3]: self,basename,class_name
                  docstring: null
                  start_line: 68
                  end_line: 71
                - identifier: backend.callgraph.CallGraph._current_caller
                  name: _current_caller
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 73
                  end_line: 76
                - identifier: backend.callgraph.CallGraph.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 78
                  end_line: 83
                - identifier: backend.callgraph.CallGraph.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 85
                  end_line: 88
                - identifier: backend.callgraph.CallGraph.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 90
                  end_line: 94
                - identifier: backend.callgraph.CallGraph.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 96
                  end_line: 107
                - identifier: backend.callgraph.CallGraph.visit_AsyncFunctionDef
                  name: visit_AsyncFunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 109
                  end_line: 110
                - identifier: backend.callgraph.CallGraph.visit_Call
                  name: visit_Call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 112
                  end_line: 123
                - identifier: backend.callgraph.CallGraph.visit_If
                  name: visit_If
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 125
                  end_line: 134
    "backend/converter.py":
      ast_nodes:
        imports[4]: logging,nbformat,base64,nbformat.reader.NotJSONError
        functions[5]:
          - mode: function_analysis
            identifier: backend.converter.wrap_cdata
            name: wrap_cdata
            args[1]: content
            docstring: Wraps content in CDATA tags.
            source_code: "def wrap_cdata(content):\n    \"\"\"Wraps content in CDATA tags.\"\"\"\n    return f\"<![CDATA[\\n{content}\\n]]>\""
            start_line: 8
            end_line: 10
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.converter.extract_output_content
            name: extract_output_content
            args[2]: outputs,image_list
            docstring: "Extracts text and handles images by decoding Base64 to bytes.\nReturns: A list of text strings or placeholders."
            source_code: "def extract_output_content(outputs, image_list):\n    \"\"\"\n    Extracts text and handles images by decoding Base64 to bytes.\n    Returns: A list of text strings or placeholders.\n    \"\"\"\n    extracted_xml_snippets = []\n    \n    for output in outputs:\n\n        if output.output_type in ('display_data', 'execute_result') and hasattr(output, 'data'):\n            data = output.data\n            \n            # Helper to process image\n            def process_image(mime_type):\n                if mime_type in data:\n                    try:\n                        b64_str = data[mime_type]\n                        b64_str = b64_str.replace('\\n', '')\n\n                        image_index = len(image_list)\n                        image_list.append({\n                            \"mime_type\": mime_type,\n                            \"data\": b64_str\n                        })\n\n                        return f'\\n<IMAGE_PLACEHOLDER index=\"{image_index}\" mime=\"{mime_type}\"/>\\n'\n                    except Exception as e:\n                        return f\"<ERROR>Could not decode image: {e}</ERROR>\"\n                return None\n\n            # Ignore jpeg if png is found\n            img_xml = process_image('image/png')\n            if not img_xml:\n                img_xml = process_image('image/jpeg')\n            \n            if img_xml:\n                extracted_xml_snippets.append(img_xml)\n            elif 'text/plain' in data:\n                extracted_xml_snippets.append(data['text/plain'])\n\n        elif output.output_type == 'stream':\n            extracted_xml_snippets.append(output.text)\n            \n        elif output.output_type == 'error':\n            extracted_xml_snippets.append(f\"{output.ename}: {output.evalue}\")\n\n    return extracted_xml_snippets"
            start_line: 12
            end_line: 58
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.converter.process_image
            name: process_image
            args[1]: mime_type
            docstring: null
            source_code: "def process_image(mime_type):\n                if mime_type in data:\n                    try:\n                        b64_str = data[mime_type]\n                        b64_str = b64_str.replace('\\n', '')\n\n                        image_index = len(image_list)\n                        image_list.append({\n                            \"mime_type\": mime_type,\n                            \"data\": b64_str\n                        })\n\n                        return f'\\n<IMAGE_PLACEHOLDER index=\"{image_index}\" mime=\"{mime_type}\"/>\\n'\n                    except Exception as e:\n                        return f\"<ERROR>Could not decode image: {e}</ERROR>\"\n                return None"
            start_line: 25
            end_line: 40
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.converter.convert_notebook_to_xml
            name: convert_notebook_to_xml
            args[1]: file_content
            docstring: null
            source_code: "def convert_notebook_to_xml(file_content):\n    try:\n        nb = nbformat.reads(file_content, as_version=4)\n    except NotJSONError:\n        return \"<ERROR>Could not parse file as JSON/Notebook</ERROR>\", []\n\n    xml_parts = []\n    extracted_images = [] \n\n    for cell in nb.cells:\n        if cell.cell_type == 'markdown':\n            cell_xml = f'<CELL type=\"markdown\">\\n{cell.source}\\n</CELL>'\n            xml_parts.append(cell_xml)\n\n        elif cell.cell_type == 'code':\n            source_code = wrap_cdata(cell.source)\n            xml_parts.append(f'<CELL type=\"code\">\\n{source_code}\\n</CELL>')\n\n            if hasattr(cell, 'outputs') and cell.outputs:\n                snippets = extract_output_content(cell.outputs, extracted_images)\n                \n                content = \"\\n\".join(snippets)\n                \n                if content.strip():\n                    wrapped_output = wrap_cdata(content)\n                    xml_parts.append(f'<CELL type=\"output\">\\n{wrapped_output}\\n</CELL>')\n\n    return \"\\n\\n\".join(xml_parts), extracted_images"
            start_line: 60
            end_line: 87
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.converter.process_repo_notebooks
            name: process_repo_notebooks
            args[1]: repo_files
            docstring: null
            source_code: "def process_repo_notebooks(repo_files):\n    notebook_files = [f for f in repo_files if f.path.endswith('.ipynb')]\n    logging.info(f\"Found {len(notebook_files)} notebooks.\")\n        \n    results = {}\n\n    for nb_file in notebook_files:\n        logging.info(f\"Processing: {nb_file.path}\")\n\n        xml_output, images = convert_notebook_to_xml(nb_file.content)\n        results[nb_file.path] = {\n            \"xml\": xml_output,\n            \"images\": images\n        }\n            \n    return results"
            start_line: 90
            end_line: 105
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "backend/getRepo.py":
      ast_nodes:
        imports[5]: tempfile,git.Repo,git.GitCommandError,logging,os
        functions[0]:
        classes[2]:
          - mode: class_analysis
            identifier: backend.getRepo.RepoFile
            name: RepoFile
            docstring: "Repr√§sentiert eine einzelne Datei in einem Git-Repository.\n\nDer Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tats√§chlichem Zugriff."
            source_code: "class RepoFile:\n    \"\"\"\n    Repr√§sentiert eine einzelne Datei in einem Git-Repository.\n    \n    Der Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tats√§chlichem Zugriff.\n    \"\"\"\n    def __init__(self, file_path, commit_tree):\n        \"\"\"\n        Initialisiert das RepoFile-Objekt.\n\n        Args:\n            file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n            commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt.\n        \"\"\"\n        self.path = file_path\n        self._tree = commit_tree\n        \n\n        self._blob = None\n        self._content = None\n        self._size = None\n\n    @property\n    def blob(self):\n        \"\"\"Lazy-l√§dt das Git-Blob-Objekt.\"\"\"\n        if self._blob is None:\n            try:\n                self._blob = self._tree[self.path]\n            except KeyError:\n                raise FileNotFoundError(f\"Datei '{self.path}' konnte im Commit-Tree nicht gefunden werden.\")\n        return self._blob\n\n    @property\n    def content(self):\n        \"\"\"Lazy-l√§dt und gibt den dekodierten Inhalt der Datei zur√ºck.\"\"\"\n        if self._content is None:\n            self._content = self.blob.data_stream.read().decode('utf-8', errors='ignore')\n        return self._content\n\n    @property\n    def size(self):\n        \"\"\"Lazy-l√§dt und gibt die Gr√∂√üe der Datei in Bytes zur√ºck.\"\"\"\n        if self._size is None:\n            self._size = self.blob.size\n        return self._size\n        \n    def analyze_word_count(self):\n        \"\"\"\n        Eine Beispiel-Analyse-Methode. Z√§hlt die W√∂rter im Dateiinhalt.\n        \"\"\"\n        return len(self.content.split())\n\n    def __repr__(self):\n        \"\"\"Gibt eine n√ºtzliche String-Repr√§sentation des Objekts zur√ºck.\"\"\"\n        return f\"<RepoFile(path='{self.path}')>\"\n    \n    def to_dict(self, include_content=False):\n        data = {\n            \"path\": self.path,\n            \"name\": os.path.basename(self.path),\n            \"size\": self.size,\n            \"type\": \"file\"\n        }\n        if include_content:\n            data[\"content\"] = self.content\n        return data"
            start_line: 7
            end_line: 72
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[7]:
                - identifier: backend.getRepo.RepoFile.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[3]: self,file_path,commit_tree
                  docstring: "Initialisiert das RepoFile-Objekt.\n\nArgs:\n    file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n    commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt."
                  start_line: 13
                  end_line: 27
                - identifier: backend.getRepo.RepoFile.blob
                  name: blob
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt das Git-Blob-Objekt.
                  start_line: 30
                  end_line: 37
                - identifier: backend.getRepo.RepoFile.content
                  name: content
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt und gibt den dekodierten Inhalt der Datei zur√ºck.
                  start_line: 40
                  end_line: 44
                - identifier: backend.getRepo.RepoFile.size
                  name: size
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt und gibt die Gr√∂√üe der Datei in Bytes zur√ºck.
                  start_line: 47
                  end_line: 51
                - identifier: backend.getRepo.RepoFile.analyze_word_count
                  name: analyze_word_count
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Eine Beispiel-Analyse-Methode. Z√§hlt die W√∂rter im Dateiinhalt.
                  start_line: 53
                  end_line: 57
                - identifier: backend.getRepo.RepoFile.__repr__
                  name: __repr__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Gibt eine n√ºtzliche String-Repr√§sentation des Objekts zur√ºck.
                  start_line: 59
                  end_line: 61
                - identifier: backend.getRepo.RepoFile.to_dict
                  name: to_dict
                  calls[0]:
                  called_by[0]:
                  args[2]: self,include_content
                  docstring: null
                  start_line: 63
                  end_line: 72
          - mode: class_analysis
            identifier: backend.getRepo.GitRepository
            name: GitRepository
            docstring: "Verwaltet ein Git-Repository, einschlie√ülich Klonen in ein tempor√§res\nVerzeichnis und Bereitstellung von RepoFile-Objekten."
            source_code: "class GitRepository:\n    \"\"\"\n    Verwaltet ein Git-Repository, einschlie√ülich Klonen in ein tempor√§res\n    Verzeichnis und Bereitstellung von RepoFile-Objekten.\n    \"\"\"\n    def __init__(self, repo_url):\n        self.repo_url = repo_url\n        self.temp_dir = tempfile.mkdtemp()\n        self.repo = None\n\n        self.files = []\n        \n        try:\n            logging.info(f\"Clone Repository {self.repo_url}...\")\n            self.repo = Repo.clone_from(self.repo_url, self.temp_dir)\n            self.latest_commit = self.repo.head.commit\n            self.commit_tree = self.latest_commit.tree\n            logging.info(\"Cloning successful.\")\n        except GitCommandError as e:\n            self.close()\n            raise RuntimeError(f\"Error cloning repository: {e}\") from e\n\n    def get_all_files(self):\n        \"\"\"\n        Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zur√ºck.\n\n        Returns:\n            list[RepoFile]: Eine Liste von RepoFile-Instanzen.\n        \"\"\"\n        file_paths = self.repo.git.ls_files().split('\\n')\n        self.files = [RepoFile(path, self.commit_tree) for path in file_paths if path]\n        return self.files\n\n    def close(self):\n        \"\"\"L√∂scht das tempor√§re Verzeichnis und dessen Inhalt.\"\"\"\n        if self.temp_dir:\n            print(f\"\\nL√∂sche tempor√§res Verzeichnis: {self.temp_dir}\")\n            self.temp_dir = None\n            \n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def get_file_tree(self, include_content=False):\n\n        if not self.files:\n            self.get_all_files()\n\n        tree = {\"name\": \"root\", \"type\": \"directory\", \"children\": []}\n\n        for file_obj in self.files:\n            parts = file_obj.path.split('/')\n            current_level = tree[\"children\"]\n            \n            # Iteriere durch die Ordnerstruktur\n            for part in parts[:-1]:\n                # Suche, ob Ordner schon existiert\n                found = next((item for item in current_level if item[\"name\"] == part and item[\"type\"] == \"directory\"), None)\n                if not found:\n                    new_dir = {\"name\": part, \"type\": \"directory\", \"children\": []}\n                    current_level.append(new_dir)\n                    current_level = new_dir[\"children\"]\n                else:\n                    current_level = found[\"children\"]\n            \n            # Datei am Ende hinzuf√ºgen\n            current_level.append(file_obj.to_dict(include_content=include_content))\n\n        return tree"
            start_line: 78
            end_line: 148
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[6]:
                - identifier: backend.getRepo.GitRepository.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,repo_url
                  docstring: null
                  start_line: 83
                  end_line: 98
                - identifier: backend.getRepo.GitRepository.get_all_files
                  name: get_all_files
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: "Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zur√ºck.\n\nReturns:\n    list[RepoFile]: Eine Liste von RepoFile-Instanzen."
                  start_line: 100
                  end_line: 109
                - identifier: backend.getRepo.GitRepository.close
                  name: close
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: L√∂scht das tempor√§re Verzeichnis und dessen Inhalt.
                  start_line: 111
                  end_line: 115
                - identifier: backend.getRepo.GitRepository.__enter__
                  name: __enter__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 117
                  end_line: 118
                - identifier: backend.getRepo.GitRepository.__exit__
                  name: __exit__
                  calls[0]:
                  called_by[0]:
                  args[4]: self,exc_type,exc_val,exc_tb
                  docstring: null
                  start_line: 120
                  end_line: 121
                - identifier: backend.getRepo.GitRepository.get_file_tree
                  name: get_file_tree
                  calls[0]:
                  called_by[0]:
                  args[2]: self,include_content
                  docstring: null
                  start_line: 123
                  end_line: 148
    "backend/main.py":
      ast_nodes:
        imports[28]: json,math,logging,os,re,time,math,datetime.datetime,matplotlib.pyplot,datetime.datetime,pathlib.Path,dotenv.load_dotenv,getRepo.GitRepository,AST_Schema.ASTAnalyzer,MainLLM.MainLLM,basic_info.ProjektInfoExtractor,HelperLLM.LLMHelper,relationship_analyzer.ProjectAnalyzer,schemas.types.FunctionContextInput,schemas.types.FunctionAnalysisInput,schemas.types.ClassContextInput,schemas.types.ClassAnalysisInput,schemas.types.MethodContextInput,toon_format.encode,toon_format.count_tokens,toon_format.estimate_savings,toon_format.compare_formats,converter.process_repo_notebooks
        functions[7]:
          - mode: function_analysis
            identifier: backend.main.create_savings_chart
            name: create_savings_chart
            args[4]: json_tokens,toon_tokens,savings_percent,output_path
            docstring: Erstellt ein Balkendiagramm f√ºr den Token-Vergleich und speichert es.
            source_code: "def create_savings_chart(json_tokens, toon_tokens, savings_percent, output_path):\n    \"\"\"Erstellt ein Balkendiagramm f√ºr den Token-Vergleich und speichert es.\"\"\"\n    labels = ['JSON', 'TOON']\n    values = [json_tokens, toon_tokens]\n    colors = ['#ff9999', '#66b3ff']\n\n    plt.figure(figsize=(8, 6))\n    bars = plt.bar(labels, values, color=colors, width=0.5)\n\n    # Titel und Beschriftungen\n    plt.title(f'Token-Vergleich: {savings_percent:.2f}% Einsparung', fontsize=14)\n    plt.ylabel('Anzahl Token')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n\n    # Werte √ºber den Balken anzeigen\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width() / 2.0, height, \n                 f'{int(height):,}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n\n    # Speichern\n    plt.savefig(output_path)\n    plt.close()"
            start_line: 33
            end_line: 55
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.calculate_net_time
            name: calculate_net_time
            args[5]: start_time,end_time,total_items,batch_size,model_name
            docstring: Berechnet die Dauer abz√ºglich der Sleep-Zeiten f√ºr Rate-Limits.
            source_code: "def calculate_net_time(start_time, end_time, total_items, batch_size, model_name):\n    \"\"\"Berechnet die Dauer abz√ºglich der Sleep-Zeiten f√ºr Rate-Limits.\"\"\"\n    total_duration = end_time - start_time\n    \n    if not model_name.startswith(\"gemini-\"):\n        return total_duration\n\n    if total_items == 0:\n        return 0\n\n    num_batches = math.ceil(total_items / batch_size)\n    sleep_count = max(0, num_batches - 1)\n    total_sleep_time = sleep_count * 61\n    \n    net_time = total_duration - total_sleep_time\n    return max(0, net_time)"
            start_line: 57
            end_line: 72
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.main_workflow
            name: main_workflow
            args[4]: input,api_keys,model_names,status_callback
            docstring: null
            source_code: "def main_workflow(input, api_keys: dict, model_names: dict, status_callback=None):\n    \n    def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)\n\n    update_status(\"üîç Analysiere Input...\")\n    \n    user_input = input\n    \n    # API Key & Ollama Base URL aus Frontend holen\n    gemini_api_key = api_keys.get(\"gemini\")\n    openai_api_key = api_keys.get(\"gpt\")\n    scadsllm_api_key = api_keys.get(\"scadsllm\")\n    scadsllm_base_url = api_keys.get(\"scadsllm_base_url\")\n    ollama_base_url = api_keys.get(\"ollama\")\n    base_url = None\n\n    if model_names[\"helper\"].startswith(\"gpt-\"):\n        helper_api_key = openai_api_key\n    elif model_names[\"helper\"].startswith(\"gemini-\"):\n        helper_api_key = gemini_api_key\n    elif \"/\" in model_names[\"helper\"] or model_names[\"helper\"].startswith(\"alias-\") or any(x in model_names[\"helper\"] for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        helper_api_key = scadsllm_api_key\n        base_url = scadsllm_base_url\n    else:\n        helper_api_key = None\n        base_url = ollama_base_url\n    if model_names[\"main\"].startswith(\"gpt-\"):\n        main_api_key = openai_api_key\n    elif model_names[\"main\"].startswith(\"gemini-\"):\n        main_api_key = gemini_api_key\n    elif \"/\" in model_names[\"main\"] or model_names[\"main\"].startswith(\"alias-\") or any(x in model_names[\"main\"] for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        main_api_key = scadsllm_api_key\n        base_url = scadsllm_base_url\n    else:\n        main_api_key = None\n        base_url = ollama_base_url\n\n    # Standardeinstellungen f√ºr Modelle\n    helper_model = model_names.get(\"helper\", \"gpt-5-mini\")\n    main_model = model_names.get(\"main\", \"gpt-5.1\")\n\n    # Error Handling f√ºr fehlende API Keys\n    if not gemini_api_key and \"gemini\" in helper_model:\n        raise ValueError(\"Gemini API Key was not provided in api_keys dictionary.\")\n\n    # URL Extraktion\n    repo_url = None\n    url_pattern = r\"https?://(?:www\\.)?github\\.com/[^\\s]+\"\n    match = re.search(url_pattern, user_input)\n\n    if match:\n        repo_url = match.group(0)\n        logging.info(f\"Extracted repository URL: {repo_url}\")\n    else:\n        raise ValueError(\"Could not find a valid URL in the provided input.\")\n    \n    # Repo klonen und Dateien extrahieren\n    update_status(f\"‚¨áÔ∏è Klone Repository: {repo_url} ...\")\n    \n    repo_files = []\n    local_repo_path = \"\" \n\n    try: \n\n        with GitRepository(repo_url) as repo:\n            repo_files = repo.get_all_files()\n            local_repo_path = repo.temp_dir\n\n            logging.info(f\"Total files retrieved: {len(repo_files)}\")\n\n    except Exception as e:\n        logging.error(f\"Error cloning repository: {e}\")\n        raise \n\n\n    # Extrahiere Basic Infos\n    update_status(\"‚ÑπÔ∏è Extrahiere Basis-Informationen...\")\n    try:\n        info_extractor = ProjektInfoExtractor()\n        basic_project_info = info_extractor.extrahiere_info(dateien=repo_files, repo_url=repo_url)\n        logging.info(\"Basic project info extracted\")\n    except Exception as e:\n        logging.error(f\"Error extracting basic project info: {e}\")\n        basic_project_info = \"Could not extract basic info.\"\n\n    # Erstelle Repository Dateibaum\n    update_status(\"üå≤ Erstelle Repository Dateibaum...\")\n    try:\n        repo_file_tree = repo.get_file_tree()\n        logging.info(\"Repository file tree constructed\")\n    except Exception as e:\n        logging.error(f\"Error constructing repository file tree: {e}\")\n        repo_file_tree = \"Could not create file tree.\"\n\n    # Relationship Analyse durchf√ºhren\n    update_status(\"üîó Analysiere Beziehungen (Calls & Instanziierungen)...\")\n    try:\n        rel_analyzer = ProjectAnalyzer(project_root=local_repo_path)\n        rel_analyzer.analyze()\n        \n        raw_relationships = rel_analyzer.get_raw_relationships()\n        \n        logging.info(f\"Relationships analyzed.\")\n    except Exception as e:\n        logging.error(f\"Error in relationship analyzer: {e}\")\n        raw_relationships = {\"outgoing\": {}, \"incoming\": {}}\n\n    # Erstelle AST Schema\n    update_status(\"üå≥ Erstelle Abstract Syntax Tree (AST)...\")\n    try:        \n        ast_analyzer = ASTAnalyzer()   \n        ast_schema = ast_analyzer.analyze_repository(files=repo_files, repo=repo)\n        logging.info(\"AST schema created\")\n    except Exception as e:\n        logging.error(f\"Error retrieving repository files: {e}\")\n        raise\n\n    # Anreichern des AST Schemas mit Relationship Daten\n    update_status(\"‚ûï Reiche AST mit Beziehungsdaten an...\")            \n    try:   \n        ast_schema = ast_analyzer.merge_relationship_data(ast_schema, raw_relationships)\n        logging.info(\"AST schema created and enriched\")\n\n    except Exception as e:\n        logging.error(f\"Error processing repository: {e}\")\n        raise\n\n    # Vorbereitung der HelperLLM Eingaben\n    update_status(\"‚öôÔ∏è Bereite Daten f√ºr Helper LLM vor...\")\n    \n    helper_llm_function_input = []\n    helper_llm_class_input = []\n\n    try:\n        for filename, file_data in ast_schema['files'].items():\n            ast_nodes = file_data.get('ast_nodes', {})\n            imports = ast_nodes.get('imports', [])\n            functions = ast_nodes.get('functions', [])\n            classes = ast_nodes.get('classes', [])\n\n            for function in functions:\n                context = function.get('context', {})\n                \n                raw_called_by = context.get('called_by', [])\n                clean_called_by = [cb for cb in raw_called_by if isinstance(cb, dict)]\n\n                function_context = FunctionContextInput(\n                    calls = context.get('calls', []),\n                    called_by = clean_called_by \n                )\n                \n                function_input = FunctionAnalysisInput(\n                    mode = function.get('mode', 'function_analysis'),\n                    identifier = function.get('identifier'),\n                    source_code = function.get('source_code'),\n                    imports = imports,\n                    context = function_context\n                )\n                \n                helper_llm_function_input.append(function_input)\n\n            for _class in classes:\n                context = _class.get('context', {})\n                \n                method_context_inputs = []\n                for method in context.get('method_context', []):\n                    \n                    raw_method_called_by = method.get('called_by', [])\n                    clean_method_called_by = [cb for cb in raw_method_called_by if isinstance(cb, dict)]\n\n                    method_context_inputs.append(\n                        MethodContextInput(\n                            identifier=method.get('identifier'),\n                            calls=method.get('calls', []),\n                            called_by=clean_method_called_by, \n                            args=method.get('args', []),\n                            docstring=method.get('docstring')\n                        )\n                    )\n\n                raw_instantiated_by = context.get('instantiated_by', [])\n                clean_instantiated_by = [ib for ib in raw_instantiated_by if isinstance(ib, dict)]\n\n                class_context = ClassContextInput(\n                    dependencies = context.get('dependencies', []),\n                    instantiated_by = clean_instantiated_by, \n                    method_context = method_context_inputs\n                )\n\n                class_input = ClassAnalysisInput(\n                    mode = _class.get('mode', 'class_analysis'),\n                    identifier =_class.get('identifier'),\n                    source_code = _class.get('source_code'), \n                    imports = imports, \n                    context = class_context\n                )\n                \n                helper_llm_class_input.append(class_input)\n\n    except Exception as e:\n        logging.error(f\"Error preparing inputs for Helper LLM: {e}\")\n        raise\n    \n    # Initialisiere HelperLLM\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    \n    llm_helper = LLMHelper(\n        api_key=helper_api_key, \n        function_prompt_path=function_prompt_file, \n        class_prompt_path=class_prompt_file,\n        model_name=helper_model,\n        base_url=base_url,\n    )\n    \n    if ollama_base_url:\n        os.environ[\"OLLAMA_BASE_URL\"] = ollama_base_url\n\n    # Initialisiere Ergebniscontainer\n    analysis_results = {}\n    function_analysis_results = []\n    class_analysis_results = []\n\n    # Call HelperLLM f√ºr Funktionen\n    update_status(f\"ü§ñ Helper LLM: Analysiere {len(helper_llm_function_input)} Funktionen ({helper_model})...\")\n\n    try:\n        net_time_func = 0\n        if len(helper_llm_function_input) > 0:\n\n            logging.info(\"\\n--- Generating documentation for Functions ---\")\n            t_start_func = time.time()    \n            function_analysis_results = llm_helper.generate_for_functions(helper_llm_function_input)    \n            t_end_func = time.time()\n            net_time_func = calculate_net_time(t_start_func, t_end_func, len(helper_llm_function_input), llm_helper.batch_size, helper_model)\n\n        if len(function_analysis_results) != 0:\n            for doc in function_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"functions\" not in analysis_results:\n                        analysis_results[\"functions\"] = {}\n                    analysis_results[\"functions\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(f\"Failed to generate doc for a function\") \n    except Exception as e:\n        logging.error(f\"Error during Helper LLM function analysis: {e}\")\n        raise\n    \n\n    # Call HelperLLM f√ºr Klassen\n    try:\n        net_time_class = 0\n        if len(helper_llm_class_input) > 0:\n            # Rate Limit Sleep f√ºr Gemini Modelle\n            if llm_helper.model_name.startswith(\"gemini-\") & (len(helper_llm_function_input) > 0):\n                update_status(\"üí§ Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n                time.sleep(65)\n            \n            update_status(f\"ü§ñ Helper LLM: Analysiere {len(helper_llm_class_input)} Klassen ({helper_model})...\")\n            \n            logging.info(\"\\n--- Generating documentation for Classes ---\")\n            t_start_class = time.time()\n            class_analysis_results = llm_helper.generate_for_classes(helper_llm_class_input)\n            t_end_class = time.time()\n            net_time_class = calculate_net_time(t_start_class, t_end_class, len(helper_llm_class_input), llm_helper.batch_size, helper_model)\n\n        if len(class_analysis_results) != 0:\n            for doc in class_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"classes\" not in analysis_results:\n                        analysis_results[\"classes\"] = {}\n                    analysis_results[\"classes\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(f\"Failed to generate doc for a class\")\n    except Exception as e:\n        logging.error(f\"Error during Helper LLM class analysis: {e}\")\n        raise\n\n    total_helper_time = net_time_func + net_time_class\n\n    # MainLLM Input Vorbereitung\n    main_llm_input = {\n        \"basic_info\": basic_project_info,\n        \"file_tree\": repo_file_tree,\n        \"ast_schema\": ast_schema,\n        \"analysis_results\": analysis_results\n    }\n\n    # Speichern als JSON (Optional)\n    main_llm_input_json = json.dumps(main_llm_input, indent=2)\n    # with open(\"output.json\", \"w\", encoding=\"utf-8\") as f:\n    #     f.write(main_llm_input_json)\n    #     logging.info(\"JSON-Datei wurde gespeichert.\")\n\n    # Konvertiere Input in Toon Format\n    main_llm_input_toon = encode(main_llm_input)\n\n    #Speichern in TOON Format (Optional)\n    # with open(\"output.toon\", \"w\", encoding=\"utf-8\") as f:\n    #     f.write(main_llm_input_toon)\n    #     logging.info(\"output.toon erfolgreich gespeichert.\")\n    \n    # Token Evaluation\n    savings_data = None\n    try:\n        logging.info(\"--- Evaluierung der Token-Ersparnis ---\")\n        savings_data = estimate_savings(main_llm_input)\n        logging.info(f\"JSON Tokens: {savings_data['json_tokens']}\")\n        logging.info(f\"TOON Tokens: {savings_data['toon_tokens']}\")\n        logging.info(f\"Ersparnis:   {savings_data['savings_percent']:.2f}%\")\n        \n    except Exception as e:\n        logging.warning(f\"Token evaluation could not be performed: {e}\")    \n\n\n\n    prompt_file_mainllm = \"SystemPrompts/SystemPromptMainLLM.txt\"\n    prompt_file_mainllm_toon = \"SystemPrompts/SystemPromptMainLLMToon.txt\"\n    # MainLLM Ausf√ºhrung\n    main_llm = MainLLM(\n        api_key=main_api_key, \n        prompt_file_path=prompt_file_mainllm_toon,\n        model_name=main_model,\n        base_url=base_url,\n    )\n\n\n    # RPM Limit Sleep f√ºr Gemini Modelle\n    if llm_helper.model_name == main_llm.model_name and main_llm.model_name.startswith(\"gemini-\"):\n        time.sleep(65)\n        update_status(\"üí§ Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n\n    # Call MainLLM f√ºr finalen Report\n    update_status(f\"üß† Main LLM: Generiere finalen Report ({main_model})...\")\n    try:\n        total_main_time = 0\n        logging.info(\"\\n--- Generating Final Report ---\")\n        t_start_main = time.time()\n        final_report = main_llm.call_llm(main_llm_input_toon)\n        t_end_main = time.time()\n        total_main_time = t_end_main - t_start_main\n    except Exception as e:\n        logging.error(f\"Error during Main LLM final report generation: {e}\")\n        raise\n\n    \n    # --- Speichern der Ergebnisse ---\n    output_dir = \"result\"\n    stats_dir = \"Statistics\" # Neuer Ordner\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(stats_dir, exist_ok=True) # Stelle sicher, dass Statistics Ordner existiert\n\n    total_active_time = total_helper_time + total_main_time\n    \n    # Dateiname f√ºr Report\n    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M-%S\")\n    report_filename = f\"report_{timestamp}_Helper_{llm_helper.model_name}_MainLLM_{main_llm.model_name}.md\"\n    report_filepath = os.path.join(output_dir, report_filename)\n    \n    if final_report:\n        with open(report_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(final_report)\n        logging.info(f\"Final report saved to '{report_filepath}'.\")\n        \n        if savings_data:\n            try:\n                savings_filename = report_filename.replace(\"report_\", \"savings_\").replace(\".md\", \".png\")\n                savings_filepath = os.path.join(stats_dir, savings_filename)\n                \n                logging.info(f\"Erstelle Token-Chart: {savings_filepath}\")\n                create_savings_chart(\n                    json_tokens=savings_data['json_tokens'], \n                    toon_tokens=savings_data['toon_tokens'], \n                    savings_percent=savings_data['savings_percent'],\n                    output_path=savings_filepath\n                )\n            except Exception as chart_error:\n                logging.error(f\"Konnte Diagramm nicht erstellen: {chart_error}\")\n\n    else:\n        final_report = \"Error: Report generation failed or returned empty.\"\n\n    metrics = {\n        \"helper_time\": round(total_helper_time, 2),\n        \"main_time\": round(total_main_time, 2),\n        \"total_time\": round(total_active_time, 2),\n        \"helper_model\": helper_model,\n        \"main_model\": main_model,\n        \"json_tokens\": savings_data['json_tokens'] if savings_data else None,\n        \"toon_tokens\": savings_data['toon_tokens'] if savings_data else None,\n        \"savings_percent\": round(savings_data['savings_percent'], 2) if savings_data else None\n    \n    }\n\n    return {\n        \"report\": final_report,\n        \"metrics\": metrics\n    }"
            start_line: 75
            end_line: 477
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.update_status
            name: update_status
            args[1]: msg
            docstring: null
            source_code: "def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)"
            start_line: 77
            end_line: 80
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.notebook_workflow
            name: notebook_workflow
            args[4]: input,api_keys,model,status_callback
            docstring: null
            source_code: "def notebook_workflow(input, api_keys, model, status_callback=None):\n    t_start = time.time()\n    final_report = \"keine Notebooks gefunden\"\n    def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)\n\n    def gemini_payload(basic_info, nb_path, xml_content, images):\n        \n        intro_json = json.dumps({\n            \"basic_info\": basic_info,\n            \"current_notebook_path\": nb_path\n        }, indent=2)\n        \n        payload_content = []\n\n        payload_content.append({\n            \"type\": \"text\",\n            \"text\": f\"Context Information:\\n{intro_json}\\n\\nNotebook XML Structure:\\n\"\n        })\n\n        # Regex to find: <IMAGE_PLACEHOLDER index=\"0\" mime=\"image/png\"/>\n        pattern = r'(<IMAGE_PLACEHOLDER index=\"(\\d+)\" mime=\"([^\"]+)\"/>)'\n        last_pos = 0\n        \n        for match in re.finditer(pattern, xml_content):\n            text_segment = xml_content[last_pos:match.start()]\n            if text_segment.strip():\n                payload_content.append({\n                    \"type\": \"text\",\n                    \"text\": text_segment\n                })\n                \n            image_index = int(match.group(2))\n            mime_type = match.group(3)\n            \n            if image_index < len(images):\n                img_data = images[image_index]\n                b64_string = img_data['data']\n                \n                payload_content.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:{mime_type};base64,{b64_string}\"\n                    }\n                })\n                \n            last_pos = match.end()\n\n        # Add any remaining text after the last image\n        remaining_text = xml_content[last_pos:]\n        if remaining_text.strip():\n            payload_content.append({\n                \"type\": \"text\",\n                \"text\": remaining_text\n            })\n\n        return payload_content\n\n\n    base_url = None\n\n    if model.startswith(\"gpt-\"):\n        input_api_key = api_keys.get(\"gpt\")\n    elif model.startswith(\"gemini-\"):\n        input_api_key = api_keys.get(\"gemini\")\n    elif \"/\" in model or model.startswith(\"alias-\") or any(x in model for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        input_api_key = api_keys.get(\"scadsllm\")\n        base_url = api_keys.get(\"scadsllm_base_url\")\n    else:\n        input_api_key = None\n        base_url = api_keys.get(\"ollama\")\n\n    update_status(\"üîç Analysiere Input...\")\n\n    # URL Extraktion\n    repo_url = None\n    url_pattern = r\"https?://(?:www\\.)?github\\.com/[^\\s]+\"\n    match = re.search(url_pattern, input)\n\n    if match:\n        repo_url = match.group(0)\n        logging.info(f\"Extracted repository URL: {repo_url}\")\n    else:\n        raise ValueError(\"Could not find a valid URL in the provided input.\")\n    \n    update_status(f\"‚¨áÔ∏è Klone Repository: {repo_url} ...\")\n\n    try: \n        with GitRepository(repo_url) as repo:\n            repo_files = repo.get_all_files()\n    except Exception as e:\n        logging.error(f\"Error cloning repository: {e}\")\n        raise\n    \n    update_status(f\"üîó Bereite Input vor...\")\n\n    # convert to XML\n    processed_data = process_repo_notebooks(repo_files)\n\n    \n    # Extrahiere Basic Infos\n    update_status(\"‚ÑπÔ∏è Extrahiere Basis-Informationen...\")\n    try:\n        info_extractor = ProjektInfoExtractor()\n        basic_project_info = info_extractor.extrahiere_info(dateien=repo_files, repo_url=repo_url)\n        logging.info(\"Basic project info extracted\")\n    except Exception as e:\n        logging.error(f\"Error extracting basic project info: {e}\")\n        basic_project_info = \"Could not extract basic info.\"\n\n\n\n    prompt_file_notebook_llm = \"SystemPrompts/SystemPromptNotebookLLM.txt\"\n    notebook_llm = MainLLM(\n        api_key=input_api_key, \n        prompt_file_path=prompt_file_notebook_llm,\n        model_name=model,\n        base_url=base_url,\n    )\n\n    notebook_reports = []\n    total_notebooks = len(processed_data)\n    \n    logging.info(f\"Starting sequential processing of {total_notebooks} notebooks...\")\n\n    # Iterate over each notebook file individually\n    for index, (nb_path, nb_data) in enumerate(processed_data.items(), 1):\n        \n        update_status(f\"üß† Generiere Report ({index}/{total_notebooks}): {os.path.basename(nb_path)}\")\n        \n        nb_xml = nb_data['xml']\n        nb_images = nb_data['images']\n\n        llm_payload = gemini_payload(basic_project_info, nb_path, nb_xml, nb_images)\n\n        try:\n            single_report = notebook_llm.call_llm(llm_payload)\n            if single_report and isinstance(single_report, str):\n                notebook_reports.append(single_report)\n            else:\n                logging.warning(f\"LLM lieferte kein Ergebnis f√ºr {nb_path}\")\n                notebook_reports.append(f\"## Analyse f√ºr {os.path.basename(nb_path)}\\nFehler: Das Modell lieferte keine Antwort.\")\n            \n        except Exception as e:\n            logging.error(f\"Error generating report for {nb_path}: {e}\")\n            notebook_reports.append(f\"# Error processing {nb_path}\\n\\nError: {str(e)}\\n\\n---\\n\")\n\n    # Concatenate all reports\n    final_report = \"\\n\\n<br>\\n<br>\\n<br>\\n\\n\".join([str(r) for r in notebook_reports if r is not None])\n\n    \n    # --- Speichern der Ergebnisse ---\n    output_dir = \"result\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Dateiname f√ºr Report\n    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M-%S\")\n    report_filename = f\"notebook_report_{timestamp}_NotebookLLM_{notebook_llm.model_name}.md\"\n    report_filepath = os.path.join(output_dir, report_filename)\n    \n    if final_report:\n        with open(report_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(final_report)\n        logging.info(f\"Final report saved to '{report_filepath}'.\")\n\n    # Am Ende der Funktion nach dem Speichern:\n    total_time = time.time() - t_start\n    \n    # Metriken f√ºr das Frontend bauen\n    metrics = {\n        \"helper_time\": \"0\", # Notebook-Workflow hat meist keinen separaten Helper\n        \"main_time\": round(total_time, 2),\n        \"total_time\": round(total_time, 2),\n        \"helper_model\": \"None\",\n        \"main_model\": model,\n        \"json_tokens\": 0,\n        \"toon_tokens\": 0,\n        \"savings_percent\": 0\n    }\n\n    return {\n        \"report\": final_report,\n        \"metrics\": metrics\n    }"
            start_line: 483
            end_line: 668
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.update_status
            name: update_status
            args[1]: msg
            docstring: null
            source_code: "def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)"
            start_line: 486
            end_line: 489
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.gemini_payload
            name: gemini_payload
            args[4]: basic_info,nb_path,xml_content,images
            docstring: null
            source_code: "def gemini_payload(basic_info, nb_path, xml_content, images):\n        \n        intro_json = json.dumps({\n            \"basic_info\": basic_info,\n            \"current_notebook_path\": nb_path\n        }, indent=2)\n        \n        payload_content = []\n\n        payload_content.append({\n            \"type\": \"text\",\n            \"text\": f\"Context Information:\\n{intro_json}\\n\\nNotebook XML Structure:\\n\"\n        })\n\n        # Regex to find: <IMAGE_PLACEHOLDER index=\"0\" mime=\"image/png\"/>\n        pattern = r'(<IMAGE_PLACEHOLDER index=\"(\\d+)\" mime=\"([^\"]+)\"/>)'\n        last_pos = 0\n        \n        for match in re.finditer(pattern, xml_content):\n            text_segment = xml_content[last_pos:match.start()]\n            if text_segment.strip():\n                payload_content.append({\n                    \"type\": \"text\",\n                    \"text\": text_segment\n                })\n                \n            image_index = int(match.group(2))\n            mime_type = match.group(3)\n            \n            if image_index < len(images):\n                img_data = images[image_index]\n                b64_string = img_data['data']\n                \n                payload_content.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:{mime_type};base64,{b64_string}\"\n                    }\n                })\n                \n            last_pos = match.end()\n\n        # Add any remaining text after the last image\n        remaining_text = xml_content[last_pos:]\n        if remaining_text.strip():\n            payload_content.append({\n                \"type\": \"text\",\n                \"text\": remaining_text\n            })\n\n        return payload_content"
            start_line: 491
            end_line: 541
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "backend/relationship_analyzer.py":
      ast_nodes:
        imports[4]: ast,os,logging,collections.defaultdict
        functions[1]:
          - mode: function_analysis
            identifier: backend.relationship_analyzer.path_to_module
            name: path_to_module
            args[2]: filepath,project_root
            docstring: Wandelt einen Dateipfad in einen Python-Modulpfad um.
            source_code: "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    module_path = rel_path.replace(os.path.sep, '.')\n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n    return module_path"
            start_line: 6
            end_line: 18
            context:
              calls[0]:
              called_by[0]:
        classes[2]:
          - mode: class_analysis
            identifier: backend.relationship_analyzer.ProjectAnalyzer
            name: ProjectAnalyzer
            docstring: null
            source_code: "class ProjectAnalyzer:\n    \n    def __init__(self, project_root):\n        self.project_root = os.path.abspath(project_root)\n        self.definitions = {}\n        self.call_graph = defaultdict(list)\n        self.file_asts = {} \n        self.ignore_dirs = {'.git', '.venv', 'venv', '__pycache__', 'node_modules', 'dist', 'build', 'docs'}\n\n    def analyze(self):\n        py_files = self._find_py_files()\n        \n        for filepath in py_files:\n            self._collect_definitions(filepath)\n            \n        for filepath in py_files:\n            self._resolve_calls(filepath)\n            \n        self.file_asts.clear()\n        \n        return self.call_graph\n\n    def get_raw_relationships(self):\n\n        outgoing = defaultdict(set)\n        incoming = defaultdict(set)\n\n        for callee_id, callers_info_list in self.call_graph.items():\n            for info in callers_info_list:\n                caller_id = info['caller']\n                \n                if caller_id and callee_id:\n                    outgoing[caller_id].add(callee_id)\n                    incoming[callee_id].add(caller_id)\n        \n        return {\n            \"outgoing\": {k: sorted(list(v)) for k, v in outgoing.items()},\n            \"incoming\": {k: sorted(list(v)) for k, v in incoming.items()}\n        }\n\n    def _find_py_files(self):\n        py_files = []\n        for root, dirs, files in os.walk(self.project_root):\n            dirs[:] = [d for d in dirs if d not in self.ignore_dirs]\n            for file in files:\n                if file.endswith(\".py\"):\n                    py_files.append(os.path.join(root, file))\n        return py_files\n\n    def _collect_definitions(self, filepath):\n        try:\n            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n                source = f.read()\n                tree = ast.parse(source, filename=filepath)\n                \n            self.file_asts[filepath] = tree\n            module_path = path_to_module(filepath, self.project_root)\n            \n            for node in ast.walk(tree):\n                if isinstance(node, ast.FunctionDef):\n                    parent = self._get_parent(tree, node)\n                    if isinstance(parent, ast.ClassDef):\n                        path_name = f\"{module_path}.{parent.name}.{node.name}\"\n                        def_type = 'method'\n                    else:\n                        path_name = f\"{module_path}.{node.name}\"\n                        def_type = 'function'\n                    self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': def_type}\n                elif isinstance(node, ast.ClassDef):\n                    path_name = f\"{module_path}.{node.name}\"\n                    self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': 'class'}\n        except Exception as e:\n            logging.error(f\"Error collecting definitions in {filepath}: {e}\")\n            self.file_asts[filepath] = None\n\n    def _get_parent(self, tree, node):\n        for parent in ast.walk(tree):\n            for child in ast.iter_child_nodes(parent):\n                if child is node:\n                    return parent\n        return None\n\n    def _resolve_calls(self, filepath):\n        tree = self.file_asts.get(filepath)\n        if not tree:\n            return\n\n        try:\n            resolver = CallResolverVisitor(filepath, self.project_root, self.definitions)\n            resolver.visit(tree)\n            \n            for callee_pathname, caller_info_list in resolver.calls.items():\n                self.call_graph[callee_pathname].extend(caller_info_list)\n        except Exception as e:\n            logging.error(f\"Error resolving calls in {filepath}: {e}\")"
            start_line: 20
            end_line: 114
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[7]:
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,project_root
                  docstring: null
                  start_line: 22
                  end_line: 27
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.analyze
                  name: analyze
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 29
                  end_line: 40
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.get_raw_relationships
                  name: get_raw_relationships
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 42
                  end_line: 58
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._find_py_files
                  name: _find_py_files
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 60
                  end_line: 67
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._collect_definitions
                  name: _collect_definitions
                  calls[0]:
                  called_by[0]:
                  args[2]: self,filepath
                  docstring: null
                  start_line: 69
                  end_line: 93
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._get_parent
                  name: _get_parent
                  calls[0]:
                  called_by[0]:
                  args[3]: self,tree,node
                  docstring: null
                  start_line: 95
                  end_line: 100
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._resolve_calls
                  name: _resolve_calls
                  calls[0]:
                  called_by[0]:
                  args[2]: self,filepath
                  docstring: null
                  start_line: 102
                  end_line: 114
          - mode: class_analysis
            identifier: backend.relationship_analyzer.CallResolverVisitor
            name: CallResolverVisitor
            docstring: null
            source_code: "class CallResolverVisitor(ast.NodeVisitor):\n    def __init__(self, filepath, project_root, definitions):\n        self.filepath = filepath\n        self.module_path = path_to_module(filepath, project_root)\n        self.definitions = definitions\n        self.scope = {}\n        self.instance_types = {}\n        self.current_caller_name = self.module_path\n        self.current_class_name = None\n        self.calls = defaultdict(list)\n\n    def visit_ClassDef(self, node):\n        old_class_name = self.current_class_name\n        self.current_class_name = node.name\n        self.generic_visit(node)\n        self.current_class_name = old_class_name\n\n    def visit_FunctionDef(self, node):\n        old_caller = self.current_caller_name\n        \n        if self.current_class_name:\n            full_identifier = f\"{self.module_path}.{self.current_class_name}.{node.name}\"\n        else:\n            full_identifier = f\"{self.module_path}.{node.name}\"\n            \n        self.current_caller_name = full_identifier\n        self.generic_visit(node)\n        self.current_caller_name = old_caller\n\n    def visit_Call(self, node):\n        callee_pathname = self._resolve_call_qname(node.func)\n        if callee_pathname and callee_pathname in self.definitions:\n            \n            if self.current_caller_name == self.module_path:\n                caller_type = 'module'\n            elif '.<locals>.' in self.current_caller_name:\n                caller_type = 'local_function'\n            elif self.current_class_name:\n                caller_type = 'method'\n            else:\n                caller_type = 'function'\n            \n            caller_info = {\n                'file': os.path.basename(self.filepath),\n                'line': node.lineno,\n                'caller': self.current_caller_name,\n                'caller_type': caller_type \n            }\n            self.calls[callee_pathname].append(caller_info)\n        self.generic_visit(node)\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.scope[alias.asname or alias.name] = alias.name\n        self.generic_visit(node)\n        \n    def visit_ImportFrom(self, node):\n        module = node.module or ''\n        for alias in node.names:\n            name = alias.asname or alias.name\n            if node.level > 0:\n                base = self.module_path.split('.')\n                prefix = '.'.join(base[:-node.level])\n                full_module_path = f\"{prefix}.{module}\" if module else prefix\n            else:\n                full_module_path = module\n            self.scope[name] = f\"{full_module_path}.{alias.name}\"\n        self.generic_visit(node)\n\n    def visit_Assign(self, node):\n        if isinstance(node.value, ast.Call) and isinstance(node.value.func, ast.Name):\n            class_name = node.value.func.id\n            if class_name in self.scope:\n                qualified_class_name = self.scope[class_name]\n                if qualified_class_name in self.definitions:\n                    for target in node.targets:\n                        if isinstance(target, ast.Name):\n                            self.instance_types[target.id] = qualified_class_name\n        self.generic_visit(node)\n\n    def _resolve_call_qname(self, func_node):\n        if isinstance(func_node, ast.Name):\n            name = func_node.id\n            if name in self.scope:\n                return self.scope[name]\n            local_pathname = f\"{self.module_path}.{name}\"\n            if local_pathname in self.definitions:\n                return local_pathname\n        elif isinstance(func_node, ast.Attribute) and isinstance(func_node.value, ast.Name):\n            var_name = func_node.value.id\n            method_name = func_node.attr\n            if var_name in self.instance_types:\n                class_pathname = self.instance_types[var_name]\n                return f\"{class_pathname}.{method_name}\"\n            if var_name in self.scope:\n                module_pathname = self.scope[var_name]\n                return f\"{module_pathname}.{method_name}\"\n        return None"
            start_line: 117
            end_line: 214
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[8]:
                - identifier: backend.relationship_analyzer.CallResolverVisitor.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[4]: self,filepath,project_root,definitions
                  docstring: null
                  start_line: 118
                  end_line: 126
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 128
                  end_line: 132
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 134
                  end_line: 144
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Call
                  name: visit_Call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 146
                  end_line: 166
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 168
                  end_line: 171
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 173
                  end_line: 184
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Assign
                  name: visit_Assign
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 186
                  end_line: 195
                - identifier: backend.relationship_analyzer.CallResolverVisitor._resolve_call_qname
                  name: _resolve_call_qname
                  calls[0]:
                  called_by[0]:
                  args[2]: self,func_node
                  docstring: null
                  start_line: 197
                  end_line: 214
    "backend/scads_key_test.py":
      ast_nodes:
        imports[3]: os,dotenv.load_dotenv,openai.OpenAI
        functions[0]:
        classes[0]:
    "database/db.py":
      ast_nodes:
        imports[7]: datetime.datetime,pymongo.MongoClient,dotenv.load_dotenv,streamlit_authenticator,cryptography.fernet.Fernet,uuid,os
        functions[29]:
          - mode: function_analysis
            identifier: database.db.encrypt_text
            name: encrypt_text
            args[1]: text
            docstring: null
            source_code: "def encrypt_text(text: str) -> str:\n    if not text or not cipher_suite: return text\n    return cipher_suite.encrypt(text.strip().encode()).decode()"
            start_line: 28
            end_line: 30
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.decrypt_text
            name: decrypt_text
            args[1]: text
            docstring: null
            source_code: "def decrypt_text(text: str) -> str:\n    if not text or not cipher_suite: return text\n    try:\n        return cipher_suite.decrypt(text.strip().encode()).decode()\n    except Exception:\n        return text"
            start_line: 32
            end_line: 37
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.insert_user
            name: insert_user
            args[3]: username,name,password
            docstring: null
            source_code: "def insert_user(username: str, name: str, password: str):\n    user = {\n        \"_id\": username,\n        \"name\": name, \n        \"hashed_password\": stauth.Hasher.hash(password),\n        \"gemini_api_key\": \"\",\n        \"ollama_base_url\": \"\",\n        \"gpt_api_key\": \"\"\n    }\n    result = dbusers.insert_one(user)\n    return result.inserted_id"
            start_line: 43
            end_line: 53
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_all_users
            name: fetch_all_users
            args[0]:
            docstring: null
            source_code: "def fetch_all_users():\n    return list(dbusers.find())"
            start_line: 55
            end_line: 56
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_user
            name: fetch_user
            args[1]: username
            docstring: null
            source_code: "def fetch_user(username: str):\n    return dbusers.find_one({\"_id\": username})"
            start_line: 58
            end_line: 59
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_user_name
            name: update_user_name
            args[2]: username,new_name
            docstring: null
            source_code: "def update_user_name(username: str, new_name: str):\n    # Achtung: _id kann in Mongo nicht einfach so ge√§ndert werden. \n    # Hier wird nur das Name-Feld geupdated.\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"name\": new_name}})\n    return result.modified_count"
            start_line: 61
            end_line: 65
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_gemini_key
            name: update_gemini_key
            args[2]: username,gemini_api_key
            docstring: null
            source_code: "def update_gemini_key(username: str, gemini_api_key: str):\n    encrypted_key = encrypt_text(gemini_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"gemini_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 67
            end_line: 70
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_gpt_key
            name: update_gpt_key
            args[2]: username,gpt_api_key
            docstring: null
            source_code: "def update_gpt_key(username: str, gpt_api_key: str):\n    encrypted_key = encrypt_text(gpt_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"gpt_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 72
            end_line: 75
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_ollama_url
            name: update_ollama_url
            args[2]: username,ollama_base_url
            docstring: null
            source_code: "def update_ollama_url(username: str, ollama_base_url: str):\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"ollama_base_url\": ollama_base_url.strip()}})\n    return result.modified_count"
            start_line: 77
            end_line: 79
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_opensrc_key
            name: update_opensrc_key
            args[2]: username,opensrc_api_key
            docstring: null
            source_code: "def update_opensrc_key(username: str, opensrc_api_key: str):\n    encrypted_key = encrypt_text(opensrc_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"opensrc_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 81
            end_line: 84
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_opensrc_url
            name: update_opensrc_url
            args[2]: username,opensrc_base_url
            docstring: null
            source_code: "def update_opensrc_url(username: str, opensrc_base_url: str):\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"opensrc_base_url\": opensrc_base_url.strip()}})\n    return result.modified_count"
            start_line: 86
            end_line: 88
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_gemini_key
            name: fetch_gemini_key
            args[1]: username
            docstring: null
            source_code: "def fetch_gemini_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"gemini_api_key\": 1, \"_id\": 0})\n    return user.get(\"gemini_api_key\") if user else None"
            start_line: 90
            end_line: 92
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_ollama_url
            name: fetch_ollama_url
            args[1]: username
            docstring: null
            source_code: "def fetch_ollama_url(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"ollama_base_url\": 1, \"_id\": 0})\n    return user.get(\"ollama_base_url\") if user else None"
            start_line: 94
            end_line: 96
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_gpt_key
            name: fetch_gpt_key
            args[1]: username
            docstring: null
            source_code: "def fetch_gpt_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"gpt_api_key\": 1, \"_id\": 0})\n    return user.get(\"gpt_api_key\") if user else None"
            start_line: 98
            end_line: 100
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_opensrc_key
            name: fetch_opensrc_key
            args[1]: username
            docstring: null
            source_code: "def fetch_opensrc_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"opensrc_api_key\": 1, \"_id\": 0})\n    return user.get(\"opensrc_api_key\") if user else None"
            start_line: 102
            end_line: 104
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_opensrc_url
            name: fetch_opensrc_url
            args[1]: username
            docstring: null
            source_code: "def fetch_opensrc_url(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"opensrc_base_url\": 1, \"_id\": 0})\n    return user.get(\"opensrc_base_url\") if user else None"
            start_line: 106
            end_line: 108
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.delete_user
            name: delete_user
            args[1]: username
            docstring: null
            source_code: "def delete_user(username: str):\n    return dbusers.delete_one({\"_id\": username}).deleted_count"
            start_line: 110
            end_line: 111
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.get_decrypted_api_keys
            name: get_decrypted_api_keys
            args[1]: username
            docstring: null
            source_code: "def get_decrypted_api_keys(username: str):\n    user = dbusers.find_one({\"_id\": username})\n    if not user: return None, None\n    gemini_plain = decrypt_text(user.get(\"gemini_api_key\", \"\"))\n    ollama_plain = user.get(\"ollama_base_url\", \"\")\n    gpt_plain = decrypt_text(user.get(\"gpt_api_key\", \"\"))\n    opensrc_plain = decrypt_text(user.get(\"opensrc_api_key\", \"\"))\n    opensrc_url = user.get(\"opensrc_base_url\", \"\")\n    return gemini_plain, ollama_plain, gpt_plain, opensrc_plain, opensrc_url"
            start_line: 113
            end_line: 121
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.insert_chat
            name: insert_chat
            args[2]: username,chat_name
            docstring: Erstellt einen neuen Chat-Eintrag.
            source_code: "def insert_chat(username: str, chat_name: str):\n    \"\"\"Erstellt einen neuen Chat-Eintrag.\"\"\"\n    chat = {\n        \"_id\": str(uuid.uuid4()),\n        \"username\": username,\n        \"chat_name\": chat_name,\n        \"created_at\": datetime.now()\n    }\n    result = dbchats.insert_one(chat)\n    return result.inserted_id"
            start_line: 127
            end_line: 136
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_chats_by_user
            name: fetch_chats_by_user
            args[1]: username
            docstring: Holt alle definierten Chats eines Users.
            source_code: "def fetch_chats_by_user(username: str):\n    \"\"\"Holt alle definierten Chats eines Users.\"\"\"\n    # Sortieren nach Erstellung macht Sinn\n    chats = list(dbchats.find({\"username\": username}).sort(\"created_at\", 1))\n    return chats"
            start_line: 138
            end_line: 142
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.check_chat_exists
            name: check_chat_exists
            args[2]: username,chat_name
            docstring: null
            source_code: "def check_chat_exists(username: str, chat_name: str):\n    return dbchats.find_one({\"username\": username, \"chat_name\": chat_name}) is not None"
            start_line: 144
            end_line: 145
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.rename_chat_fully
            name: rename_chat_fully
            args[3]: username,old_name,new_name
            docstring: Benennt einen Chat und alle zugeh√∂rigen Exchanges um.
            source_code: "def rename_chat_fully(username: str, old_name: str, new_name: str):\n    \"\"\"\n    Benennt einen Chat und alle zugeh√∂rigen Exchanges um.\n    \"\"\"\n    # 1. Chat-Eintrag umbenennen\n    result = dbchats.update_one(\n        {\"username\": username, \"chat_name\": old_name}, \n        {\"$set\": {\"chat_name\": new_name}}\n    )\n    \n    # 2. Alle Messages (Exchanges) umh√§ngen\n    dbexchanges.update_many(\n        {\"username\": username, \"chat_name\": old_name},\n        {\"$set\": {\"chat_name\": new_name}}\n    )\n    return result.modified_count"
            start_line: 148
            end_line: 163
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.insert_exchange
            name: insert_exchange
            args[13]: question,answer,feedback,username,chat_name,helper_used,main_used,total_time,helper_time,main_time,json_tokens,toon_tokens,savings_percent
            docstring: null
            source_code: "def insert_exchange(question: str, answer: str, feedback: str, username: str, chat_name: str, \n                    helper_used: str=\"\", main_used: str=\"\", total_time: str=\"\", helper_time: str=\"\", main_time: str=\"\",\n                    json_tokens=0, toon_tokens=0, savings_percent=0.0):\n    new_id = str(uuid.uuid4())\n    exchange = {\n        \"_id\": new_id,\n        \"question\": question,\n        \"answer\": answer,\n        \"feedback\": feedback,\n        \"feedback_message\": \"\",\n        \"chat_name\": chat_name,\n        \"username\": username,\n        \"helper_used\": helper_used,\n        \"main_used\": main_used,\n        \"total_time\": total_time,\n        \"helper_time\": helper_time,\n        \"main_time\": main_time,\n        \"json_tokens\": json_tokens,\n        \"toon_tokens\": toon_tokens,\n        \"savings_percent\": savings_percent,\n        \"created_at\": datetime.now()\n    }\n    try:\n        dbexchanges.insert_one(exchange)\n        return new_id\n    except Exception as e:\n        print(f\"DB Error: {e}\")\n        return None"
            start_line: 169
            end_line: 196
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_exchanges_by_user
            name: fetch_exchanges_by_user
            args[1]: username
            docstring: null
            source_code: "def fetch_exchanges_by_user(username: str):\n    # Sortieren nach Zeitstempel wichtig f√ºr die Anzeige\n    exchanges = list(dbexchanges.find({\"username\": username}).sort(\"created_at\", 1))\n    return exchanges"
            start_line: 198
            end_line: 201
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_exchanges_by_chat
            name: fetch_exchanges_by_chat
            args[2]: username,chat_name
            docstring: null
            source_code: "def fetch_exchanges_by_chat(username: str, chat_name: str):\n    exchanges = list(dbexchanges.find({\"username\": username, \"chat_name\": chat_name}).sort(\"created_at\", 1))\n    return exchanges"
            start_line: 203
            end_line: 205
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_exchange_feedback
            name: update_exchange_feedback
            args[2]: exchange_id,feedback
            docstring: null
            source_code: "def update_exchange_feedback(exchange_id, feedback: int):\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback\": feedback}})\n    return result.modified_count"
            start_line: 207
            end_line: 209
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_exchange_feedback_message
            name: update_exchange_feedback_message
            args[2]: exchange_id,feedback_message
            docstring: null
            source_code: "def update_exchange_feedback_message(exchange_id, feedback_message: str):\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback_message\": feedback_message}})\n    return result.modified_count"
            start_line: 211
            end_line: 213
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.delete_exchange_by_id
            name: delete_exchange_by_id
            args[1]: exchange_id
            docstring: null
            source_code: "def delete_exchange_by_id(exchange_id: str):\n    result = dbexchanges.delete_one({\"_id\": exchange_id})\n    return result.deleted_count"
            start_line: 215
            end_line: 217
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.delete_full_chat
            name: delete_full_chat
            args[2]: username,chat_name
            docstring: "L√∂scht den Chat UND alle zugeh√∂rigen Exchanges.\nDas sorgt f√ºr Konsistenz zwischen Frontend und Backend."
            source_code: "def delete_full_chat(username: str, chat_name: str):\n    \"\"\"\n    L√∂scht den Chat UND alle zugeh√∂rigen Exchanges.\n    Das sorgt f√ºr Konsistenz zwischen Frontend und Backend.\n    \"\"\"\n    # 1. Alle Nachrichten in diesem Chat l√∂schen\n    del_exchanges = dbexchanges.delete_many({\"username\": username, \"chat_name\": chat_name})\n    \n    # 2. Den Chat selbst aus der Chat-Liste l√∂schen\n    del_chat = dbchats.delete_one({\"username\": username, \"chat_name\": chat_name})\n    \n    return del_chat.deleted_count"
            start_line: 221
            end_line: 232
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "frontend/frontend.py":
      ast_nodes:
        imports[16]: numpy,datetime.datetime,time,pymongo.MongoClient,dotenv.load_dotenv,os,sys,urllib.parse.urlparse,logging,traceback,re,streamlit_mermaid.st_mermaid,backend.main,database.db,streamlit,streamlit_authenticator
        functions[12]:
          - mode: function_analysis
            identifier: frontend.frontend.clean_names
            name: clean_names
            args[1]: model_list
            docstring: null
            source_code: "def clean_names(model_list):\n    return [m.split(\"/\")[-1] for m in model_list]"
            start_line: 54
            end_line: 55
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.get_filtered_models
            name: get_filtered_models
            args[2]: source_list,category_name
            docstring: Filtert eine Liste basierend auf der gew√§hlten Kategorie.
            source_code: "def get_filtered_models(source_list, category_name):\n    \"\"\"Filtert eine Liste basierend auf der gew√§hlten Kategorie.\"\"\"\n    keywords = CATEGORY_KEYWORDS.get(category_name, [\"\"])\n    \n    if \"STANDARD\" in keywords:\n        # Nur Modelle zur√ºckgeben, die auch in der Standard-Liste sind\n        return [m for m in source_list if m in STANDARD_MODELS]\n    \n    filtered = []\n    for model in source_list:\n        # Pr√ºfen ob ein Keyword im Namen steckt\n        if any(k in model.lower() for k in keywords):\n            filtered.append(model)\n            \n    return filtered if filtered else source_list"
            start_line: 86
            end_line: 100
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.save_gemini_cb
            name: save_gemini_cb
            args[0]:
            docstring: null
            source_code: "def save_gemini_cb():\n    new_key = st.session_state.get(\"in_gemini_key\", \"\")\n    if new_key:\n        db.update_gemini_key(st.session_state[\"username\"], new_key)\n        st.session_state[\"in_gemini_key\"] = \"\"\n        st.toast(\"Gemini Key erfolgreich gespeichert! ‚úÖ\")"
            start_line: 143
            end_line: 148
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.save_ollama_cb
            name: save_ollama_cb
            args[0]:
            docstring: null
            source_code: "def save_ollama_cb():\n    new_url = st.session_state.get(\"in_ollama_url\", \"\")\n    if new_url:\n        db.update_ollama_url(st.session_state[\"username\"], new_url)\n        st.toast(\"Ollama URL gespeichert! ‚úÖ\")"
            start_line: 150
            end_line: 154
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.load_data_from_db
            name: load_data_from_db
            args[1]: username
            docstring: L√§dt Chats und Exchanges konsistent aus der DB.
            source_code: "def load_data_from_db(username: str):\n    \"\"\"L√§dt Chats und Exchanges konsistent aus der DB.\"\"\"\n    \n    # Nur laden, wenn neuer User oder noch nicht geladen\n    if \"loaded_user\" not in st.session_state or st.session_state.loaded_user != username:\n        st.session_state.chats = {}\n        \n        # 1. Erst die definierten Chats laden (damit auch leere Chats da sind)\n        db_chats = db.fetch_chats_by_user(username)\n        for c in db_chats:\n            c_name = c.get(\"chat_name\")\n            if c_name:\n                st.session_state.chats[c_name] = {\"exchanges\": []}\n\n        # 2. Dann die Exchanges laden und einsortieren\n        db_exchanges = db.fetch_exchanges_by_user(username)\n        for ex in db_exchanges:\n            c_name = ex.get(\"chat_name\", \"Unbenannt\")\n            \n            # Falls Exchanges existieren f√ºr Chats, die nicht in dbchats sind (Legacy Support)\n            if c_name not in st.session_state.chats:\n                st.session_state.chats[c_name] = {\"exchanges\": []}\n            \n            if \"feedback\" not in ex or ex[\"feedback\"] is None:\n                ex[\"feedback\"] = np.nan\n            \n            st.session_state.chats[c_name][\"exchanges\"].append(ex)\n\n        # 3. Default Chat erstellen, falls gar nichts existiert\n        if not st.session_state.chats:\n            initial_name = \"Chat 1\"\n            # Konsistent in DB anlegen\n            db.insert_chat(username, initial_name)\n            st.session_state.chats[initial_name] = {\"exchanges\": []}\n            st.session_state.active_chat = initial_name\n        else:\n            # Active Chat setzen, falls n√∂tig\n            if \"active_chat\" not in st.session_state or st.session_state.active_chat not in st.session_state.chats:\n                # Nimm den ersten verf√ºgbaren Chat\n                st.session_state.active_chat = list(st.session_state.chats.keys())[0]\n        \n        st.session_state.loaded_user = username"
            start_line: 160
            end_line: 201
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.handle_feedback_change
            name: handle_feedback_change
            args[2]: ex,val
            docstring: null
            source_code: "def handle_feedback_change(ex, val):\n    ex[\"feedback\"] = val\n    db.update_exchange_feedback(ex[\"_id\"], val)\n    st.rerun()"
            start_line: 207
            end_line: 210
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.handle_delete_exchange
            name: handle_delete_exchange
            args[2]: chat_name,ex
            docstring: null
            source_code: "def handle_delete_exchange(chat_name, ex):\n    db.delete_exchange_by_id(ex[\"_id\"])\n    if chat_name in st.session_state.chats:\n        if ex in st.session_state.chats[chat_name][\"exchanges\"]:\n            st.session_state.chats[chat_name][\"exchanges\"].remove(ex)\n    st.rerun()"
            start_line: 212
            end_line: 217
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.handle_delete_chat
            name: handle_delete_chat
            args[2]: username,chat_name
            docstring: null
            source_code: "def handle_delete_chat(username, chat_name):\n    # KONSISTENZ: Ruft jetzt delete_full_chat in DB auf\n    db.delete_full_chat(username, chat_name)\n    \n    # State bereinigen\n    if chat_name in st.session_state.chats:\n        del st.session_state.chats[chat_name]\n    \n    # Active Chat neu setzen\n    if len(st.session_state.chats) > 0:\n        st.session_state.active_chat = list(st.session_state.chats.keys())[0]\n    else:\n        # Wenn alles weg ist, neuen leeren Chat anlegen\n        new_name = \"Chat 1\"\n        db.insert_chat(username, new_name)\n        st.session_state.chats[new_name] = {\"exchanges\": []}\n        st.session_state.active_chat = new_name\n        \n    st.rerun()"
            start_line: 219
            end_line: 237
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.extract_repo_name
            name: extract_repo_name
            args[1]: text
            docstring: null
            source_code: "def extract_repo_name(text):\n    url_match = re.search(r'(https?://[^\\s]+)', text)\n    if url_match:\n        url = url_match.group(0)\n        parsed = urlparse(url)\n        path = parsed.path.strip(\"/\")\n        if path:\n            repo_name = path.split(\"/\")[-1]\n            if repo_name.endswith(\".git\"):\n                repo_name = repo_name[:-4]\n            return repo_name\n    return None"
            start_line: 243
            end_line: 254
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.stream_text_generator
            name: stream_text_generator
            args[1]: text
            docstring: null
            source_code: "def stream_text_generator(text):\n    for word in text.split(\" \"):\n        yield word + \" \"\n        time.sleep(0.01)"
            start_line: 256
            end_line: 259
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.render_text_with_mermaid
            name: render_text_with_mermaid
            args[2]: markdown_text,should_stream
            docstring: null
            source_code: "def render_text_with_mermaid(markdown_text, should_stream=False):\n    if not markdown_text:\n        return\n\n    parts = re.split(r\"```mermaid\\s+(.*?)\\s+```\", markdown_text, flags=re.DOTALL)\n\n    for i, part in enumerate(parts):\n        if i % 2 == 0:\n            if part.strip():\n                if should_stream:\n                    st.write_stream(stream_text_generator(part))\n                else:\n                    st.markdown(part)\n        else:\n            try:\n                st_mermaid(part, key=f\"mermaid_{hash(part)}_{i}\")\n            except Exception:\n                st.code(part, language=\"mermaid\")"
            start_line: 261
            end_line: 278
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.render_exchange
            name: render_exchange
            args[2]: ex,current_chat_name
            docstring: null
            source_code: "def render_exchange(ex, current_chat_name):\n    st.chat_message(\"user\").write(ex[\"question\"])\n\n    with st.chat_message(\"assistant\"):\n        answer_text = ex.get(\"answer\", \"\")\n        is_error = answer_text.startswith(\"Fehler\") or answer_text.startswith(\"Error\")\n\n        # --- 1. TOOLBAR CONTAINER ---\n        # Nutzt die neuen Parameter horizontal=True und alignment\n        # border=True rahmt die Toolbar ein\n        # horizontal_alignment=\"right\" schiebt alles nach rechts\n        with st.container(horizontal=True, horizontal_alignment=\"right\"):\n            \n            if not is_error:\n                # 1. Status Text (wird jetzt links von den Buttons angezeigt, aber rechtsb√ºndig im Container)\n                st.write(\"hier die Antwort:\")\n                if ex.get(\"feedback\") == 1:\n                    st.caption(\"‚úÖ Hilfreich\")\n                elif ex.get(\"feedback\") == 0:\n                    st.caption(\"‚ùå Nicht hilfreich\")\n                \n                # 2. Die Buttons (einfach untereinander im Code, erscheinen nebeneinander im UI)\n                \n                # Like\n                type_primary_up = ex.get(\"feedback\") == 1\n                if st.button(\"üëç\", key=f\"up_{ex['_id']}\", type=\"primary\" if type_primary_up else \"secondary\", help=\"Hilfreich\"):\n                    handle_feedback_change(ex, 1)\n\n                # Dislike\n                type_primary_down = ex.get(\"feedback\") == 0\n                if st.button(\"üëé\", key=f\"down_{ex['_id']}\", type=\"primary\" if type_primary_down else \"secondary\", help=\"Nicht hilfreich\"):\n                    handle_feedback_change(ex, 0)\n\n                # Comment Popover\n                with st.popover(\"üí¨\", help=\"Notiz hinzuf√ºgen\"):\n                    msg = st.text_area(\"Notiz:\", value=ex.get(\"feedback_message\", \"\"), key=f\"txt_{ex['_id']}\")\n                    if st.button(\"Speichern\", key=f\"save_{ex['_id']}\"):\n                        ex[\"feedback_message\"] = msg\n                        db.update_exchange_feedback_message(ex[\"_id\"], msg)\n                        st.success(\"Gespeichert!\")\n                        time.sleep(0.5)\n                        st.rerun()\n\n                # Download\n                st.download_button(\n                    label=\"üì•\",\n                    data=ex[\"answer\"],\n                    file_name=f\"response_{ex['_id']}.md\",\n                    mime=\"text/markdown\",\n                    key=f\"dl_{ex['_id']}\",\n                    help=\"Als Markdown herunterladen\"\n                )\n\n                # Delete\n                if st.button(\"üóëÔ∏è\", key=f\"del_{ex['_id']}\", help=\"Nachricht l√∂schen\", type=\"secondary\"):\n                    handle_delete_exchange(current_chat_name, ex)\n\n            else:\n                # Fehlerfall\n                st.error(\"‚ö†Ô∏è Fehler\")\n                if st.button(\"üóëÔ∏è L√∂schen\", key=f\"del_err_{ex['_id']}\"):\n                    handle_delete_exchange(current_chat_name, ex)\n\n        # --- 2. CONTENT (Antwort) ---\n        with st.container(height=500, border=True):\n             render_text_with_mermaid(ex[\"answer\"], should_stream=False)"
            start_line: 284
            end_line: 349
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "schemas/types.py":
      ast_nodes:
        imports[5]: typing.List,typing.Optional,typing.Literal,pydantic.BaseModel,pydantic.ValidationError
        functions[0]:
        classes[15]:
          - mode: class_analysis
            identifier: schemas.types.ParameterDescription
            name: ParameterDescription
            docstring: Describes a single parameter of a function.
            source_code: "class ParameterDescription(BaseModel):\n    \"\"\"Describes a single parameter of a function.\"\"\"\n    name: str\n    type: str\n    description: str"
            start_line: 6
            end_line: 10
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ReturnDescription
            name: ReturnDescription
            docstring: Describes the return value of a function.
            source_code: "class ReturnDescription(BaseModel):\n    \"\"\"Describes the return value of a function.\"\"\"\n    name: str\n    type: str\n    description: str"
            start_line: 12
            end_line: 16
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.UsageContext
            name: UsageContext
            docstring: Describes the calling context of a function.
            source_code: "class UsageContext(BaseModel):\n    \"\"\"Describes the calling context of a function.\"\"\"\n    calls: str\n    called_by: str"
            start_line: 18
            end_line: 21
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionDescription
            name: FunctionDescription
            docstring: Contains the detailed analysis of a function's purpose and signature.
            source_code: "class FunctionDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a function's purpose and signature.\"\"\"\n    overall: str\n    parameters: List[ParameterDescription]\n    returns: List[ReturnDescription]\n    usage_context: UsageContext"
            start_line: 23
            end_line: 28
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionAnalysis
            name: FunctionAnalysis
            docstring: The main model representing the entire JSON schema for a function.
            source_code: "class FunctionAnalysis(BaseModel):\n    \"\"\"The main model representing the entire JSON schema for a function.\"\"\"\n    identifier: str\n    description: FunctionDescription\n    error: Optional[str] = None"
            start_line: 30
            end_line: 34
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ConstructorDescription
            name: ConstructorDescription
            docstring: Describes the __init__ method of a class.
            source_code: "class ConstructorDescription(BaseModel):\n    \"\"\"Describes the __init__ method of a class.\"\"\"\n    description: str\n    parameters: List[ParameterDescription]"
            start_line: 39
            end_line: 42
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassContext
            name: ClassContext
            docstring: Describes the class's external dependencies and primary points of instantiation.
            source_code: "class ClassContext(BaseModel):\n    \"\"\"Describes the class's external dependencies and primary points of instantiation.\"\"\"\n    dependencies: str\n    instantiated_by: str"
            start_line: 44
            end_line: 47
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassDescription
            name: ClassDescription
            docstring: "Contains the detailed analysis of a class's purpose, constructor, and methods."
            source_code: "class ClassDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a class's purpose, constructor, and methods.\"\"\"\n    overall: str\n    init_method: ConstructorDescription\n    methods: List[FunctionAnalysis]\n    usage_context: ClassContext"
            start_line: 49
            end_line: 54
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassAnalysis
            name: ClassAnalysis
            docstring: The main model for the entire JSON schema for a class.
            source_code: "class ClassAnalysis(BaseModel):\n    \"\"\"The main model for the entire JSON schema for a class.\"\"\"\n    identifier: str\n    description: ClassDescription\n    error: Optional[str] = None"
            start_line: 56
            end_line: 60
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.CallInfo
            name: CallInfo
            docstring: "Represents a specific call event from the relationship analyzer.\nUsed in 'called_by' and 'instantiated_by' lists."
            source_code: "class CallInfo(BaseModel):\n    \"\"\"\n    Represents a specific call event from the relationship analyzer.\n    Used in 'called_by' and 'instantiated_by' lists.\n    \"\"\"\n    file: str\n    function: str  # Name des Aufrufers\n    mode: str      # z.B. 'method', 'function', 'module'\n    line: int"
            start_line: 65
            end_line: 73
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionContextInput
            name: FunctionContextInput
            docstring: Structured context for analyzing a function.
            source_code: "class FunctionContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a function.\"\"\"\n    calls: List[str]\n    called_by: List[CallInfo]"
            start_line: 78
            end_line: 81
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionAnalysisInput
            name: FunctionAnalysisInput
            docstring: The required input to generate a FunctionAnalysis object.
            source_code: "class FunctionAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a FunctionAnalysis object.\"\"\"\n    mode: Literal[\"function_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: FunctionContextInput"
            start_line: 83
            end_line: 89
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.MethodContextInput
            name: MethodContextInput
            docstring: Structured context for a classes methods
            source_code: "class MethodContextInput(BaseModel):\n    \"\"\"Structured context for a classes methods\"\"\"\n    identifier: str\n    calls: List[str]\n    called_by: List[CallInfo]\n    args: List[str]\n    docstring: Optional[str]"
            start_line: 94
            end_line: 100
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassContextInput
            name: ClassContextInput
            docstring: Structured context for analyzing a class.
            source_code: "class ClassContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a class.\"\"\"\n    dependencies: List[str]\n    instantiated_by: List[CallInfo]\n    method_context: List[MethodContextInput]"
            start_line: 102
            end_line: 106
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassAnalysisInput
            name: ClassAnalysisInput
            docstring: The required input to generate a ClassAnalysis object.
            source_code: "class ClassAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a ClassAnalysis object.\"\"\"\n    mode: Literal[\"class_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: ClassContextInput"
            start_line: 108
            end_line: 114
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
analysis_results:
  functions:
    backend.AST_Schema.path_to_module:
      identifier: backend.AST_Schema.path_to_module
      description:
        overall: "This function converts a given file path into a Python module path string. It first attempts to calculate the relative path of the file with respect to a specified project root. If this fails, it falls back to using just the base name of the file. The function then removes the '.py' extension if present and replaces all operating system path separators with dots to form the module path. Finally, it handles '__init__' modules by removing '. __init__' from the end of the module path."
        parameters[2]{name,type,description}:
          filepath,str,The absolute or relative path to the Python file.
          project_root,str,"The root directory of the project, used to calculate the relative path."
        returns[1]{name,type,description}:
          module_path,str,The converted Python module path string.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    backend.File_Dependency.build_file_dependency_graph:
      identifier: backend.File_Dependency.build_file_dependency_graph
      description:
        overall: "This function constructs a directed graph representing file-level import dependencies within a repository. It initializes a NetworkX directed graph and then uses an instance of `FileDependencyGraph` to traverse the Abstract Syntax Tree (AST) of a specified file. The visitor collects all import relationships, which are then used to populate the graph. Each file involved in an import relationship becomes a node, and a directed edge is added from a file to another if it imports that file. The resulting graph illustrates the import structure discovered."
        parameters[3]{name,type,description}:
          filename,str,The path to the file whose dependencies are being analyzed.
          tree,AST,The Abstract Syntax Tree (AST) of the file to be analyzed.
          repo_root,str,"The root directory of the repository, used for resolving relative import paths."
        returns[1]{name,type,description}:
          graph,nx.DiGraph,A NetworkX directed graph where nodes represent files and edges represent import dependencies between them.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.File_Dependency.build_repository_graph:
      identifier: backend.File_Dependency.build_repository_graph
      description:
        overall: "The `build_repository_graph` function constructs a directed graph representing file-level dependencies across a Git repository. It iterates through all Python files, parsing each into an Abstract Syntax Tree. For each file, it generates a local dependency graph using `build_file_dependency_graph`. These individual file graphs are then merged into a single global NetworkX directed graph, which is subsequently returned."
        parameters[1]{name,type,description}:
          repository,GitRepository,The GitRepository object representing the code repository to analyze.
        returns[1]{name,type,description}:
          global_graph,nx.DiGraph,A NetworkX directed graph representing the aggregated file-level dependencies across the entire repository.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.File_Dependency.get_all_temp_files:
      identifier: backend.File_Dependency.get_all_temp_files
      description:
        overall: "This function identifies and returns a list of all Python files within a specified directory and its subdirectories. It first converts the input directory string into an absolute Path object. Then, it recursively searches for all files ending with '.py' within this root path. Finally, it returns these file paths as a list of Path objects, with each path made relative to the initial root directory."
        parameters[1]{name,type,description}:
          directory,str,The string path to the root directory from which to start the recursive search for Python files.
        returns[1]{name,type,description}:
          all_files,"list[Path]","A list of pathlib.Path objects, where each Path represents a Python file found within the specified directory, with paths relative to the input 'directory'."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.HelperLLM.main_orchestrator:
      identifier: backend.HelperLLM.main_orchestrator
      description:
        overall: "This function serves as a test orchestrator for the LLMHelper class, demonstrating its functionality by processing predefined dummy data. It sets up multiple FunctionAnalysisInput and FunctionAnalysis objects, along with a ClassAnalysisInput, to simulate a complete analysis workflow. The function initializes an LLMHelper instance, then calls its `generate_for_functions` method with the prepared inputs. Finally, it logs the results and prints the aggregated documentation in JSON format."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    backend.callgraph.make_safe_dot:
      identifier: backend.callgraph.make_safe_dot
      description:
        overall: "This function takes a NetworkX directed graph and a file path as input. It creates a copy of the input graph and relabels its nodes with simple, safe identifiers (e.g., \"n0\", \"n1\") suitable for DOT file format. The original node names are preserved by assigning them as 'label' attributes to the new nodes. Finally, the modified graph is written to the specified output path as a DOT file."
        parameters[2]{name,type,description}:
          graph,nx.DiGraph,The input NetworkX directed graph to be processed and saved.
          out_path,str,The file path where the DOT representation of the graph will be written.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    backend.callgraph.build_filtered_callgraph:
      identifier: backend.callgraph.build_filtered_callgraph
      description:
        overall: "This function `build_filtered_callgraph` constructs a directed graph representing function calls within a Git repository. It first identifies all Python files and parses their Abstract Syntax Trees (ASTs) to determine a set of \"own functions\" using a `CallGraph` visitor. Subsequently, it iterates through these parsed files again, building a call graph that exclusively includes edges between functions identified as \"own functions\". The final output is a `networkx.DiGraph` where nodes are \"own functions\" and edges represent calls between them."
        parameters[1]{name,type,description}:
          repo,GitRepository,The Git repository object from which Python files and their contents will be extracted to build the call graph.
        returns[1]{name,type,description}:
          global_graph,nx.DiGraph,"A NetworkX directed graph where nodes represent \"self-written\" functions found in the repository and edges represent calls between these functions."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.wrap_cdata:
      identifier: backend.converter.wrap_cdata
      description:
        overall: "The `wrap_cdata` function takes a string `content` as input and encloses it within XML CDATA tags. It constructs an f-string that prepends \"<![CDATA[\\n\" and appends \"\\n]]>\" to the provided content. This utility function is designed to escape arbitrary text for safe inclusion within XML documents, preventing issues with special characters."
        parameters[1]{name,type,description}:
          content,str,The string content to be wrapped within CDATA tags.
        returns[1]{name,type,description}:
          wrapped_content,str,"A new string with the input content enclosed by `<![CDATA[\\n` and `\\n]]>`."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    backend.converter.extract_output_content:
      identifier: backend.converter.extract_output_content
      description:
        overall: "The `extract_output_content` function processes a list of output objects, typically from a notebook execution, to extract text content and handle embedded images. It iterates through each output, categorizing it by type. For display data or execution results, it attempts to extract and store base64-encoded PNG or JPEG images into a provided `image_list`, returning an XML-like placeholder. If no image is present, it extracts plain text. Stream outputs are appended as raw text, and error outputs are formatted into a string containing the error name and value. The function compiles all extracted content into a list of strings."
        parameters[2]{name,type,description}:
          outputs,list,"A list of output objects, likely from a notebook execution, which can contain various types of data such as display data, execution results, streams, or errors."
          image_list,list,A mutable list that will be populated with dictionaries representing extracted images. Each dictionary contains the 'mime_type' and the base64-encoded 'data' string for an image.
        returns[1]{name,type,description}:
          extracted_xml_snippets,"list[str]","A list of strings, where each string is either extracted text content, a formatted error message, or an XML-like placeholder for an image that was stored in the `image_list`."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.process_image:
      identifier: backend.converter.process_image
      description:
        overall: "This function processes image data identified by a given MIME type. It expects to find base64 encoded image data within an external `data` dictionary, using the `mime_type` as a key. Upon retrieval, it cleans the base64 string by removing newline characters and then stores the processed image information (MIME type and data) into an external `image_list`. The function returns a specific placeholder string containing the image's assigned index and its MIME type. If the `mime_type` is not found in `data`, it returns `None`, and any processing errors result in an error message string."
        parameters[1]{name,type,description}:
          mime_type,str,The MIME type of the image to be processed. This value is used as a key to access base64 encoded image data from an external `data` dictionary.
        returns[3]{name,type,description}:
          image_placeholder_string,str,"A formatted string representing an image placeholder, including its index and MIME type, returned upon successful processing and storage of the image data."
          error_message,str,A string containing an error message if an exception occurs during the processing or decoding of the image data.
          no_image_data_found,None,Returns `None` if the specified `mime_type` is not found as a key in the external `data` dictionary.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: "The function relies on external variables `data` and `image_list` which are not defined within the function's scope or passed as parameters, making its execution context ambiguous without further information."
    backend.converter.convert_notebook_to_xml:
      identifier: backend.converter.convert_notebook_to_xml
      description:
        overall: "This function converts the content of a Jupyter notebook, provided as a string, into an XML representation. It parses the input using `nbformat.reads` and iterates through each cell. Markdown cells are wrapped in `<CELL type=\"markdown\">` tags, and code cells are wrapped in `<CELL type=\"code\">` tags, with their source code CDATA-wrapped. If a code cell has outputs, these are also processed, extracted, CDATA-wrapped, and appended within `<CELL type=\"output\">` tags. The function handles potential parsing errors by returning an error message."
        parameters[1]{name,type,description}:
          file_content,str,The raw string content of a Jupyter notebook file to be converted.
        returns[1]{name,type,description}:
          result,"tuple[str, list]","A tuple containing two elements: the first is a string representing the XML conversion of the notebook (or an error message if parsing fails), and the second is a list of extracted images from the notebook outputs."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.process_repo_notebooks:
      identifier: backend.converter.process_repo_notebooks
      description:
        overall: "This function processes a collection of repository files to identify and convert Jupyter notebooks. It filters the input list to find files with a '.ipynb' extension. For each identified notebook, it extracts its content and passes it to an external conversion utility. The function then aggregates the XML output and any extracted images into a structured dictionary, keyed by the notebook's file path."
        parameters[1]{name,type,description}:
          repo_files,"List[FileObject]","An iterable collection of file objects from a repository. Each FileObject is expected to have a 'path' attribute (string) and a 'content' attribute (Any), representing the file's location and raw data, respectively."
        returns[1]{name,type,description}:
          results,"Dict[str, Dict[str, Any]]","A dictionary where keys are the string paths of the processed Jupyter notebooks. Each value is another dictionary containing two keys: 'xml' (string, the XML representation of the notebook) and 'images' (Any, any images extracted during the conversion)."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.create_savings_chart:
      identifier: backend.main.create_savings_chart
      description:
        overall: "This function generates a bar chart to visually compare two token counts: JSON tokens and TOON tokens. It calculates and displays a savings percentage in the chart's title. The chart includes labels, colors, a title, y-axis label, and a grid. It also annotates each bar with its corresponding integer value. Finally, the generated chart is saved to a specified output path and the plot is closed."
        parameters[4]{name,type,description}:
          json_tokens,int,The number of tokens for the JSON format.
          toon_tokens,int,The number of tokens for the TOON format.
          savings_percent,float,The calculated percentage of savings to display in the chart title.
          output_path,str,The file path where the generated bar chart will be saved.
        returns[1]{name,type,description}:
          None,None,This function does not return any value; it saves a chart to a file as a side effect.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.calculate_net_time:
      identifier: backend.main.calculate_net_time
      description:
        overall: "This function calculates the effective processing time by subtracting estimated rate-limiting sleep durations from the total elapsed time. It first determines the raw duration between a start and end time. If the specified model is not a 'gemini-' model, or if the total number of items is zero, the raw duration or zero is returned, respectively. For 'gemini-' models, it calculates the number of batches, estimates the total sleep time based on a fixed sleep duration per batch (61 units), and then subtracts this from the total duration to yield the net processing time. The final result is ensured to be non-negative."
        parameters[5]{name,type,description}:
          start_time,float,The starting timestamp or time object for the duration calculation.
          end_time,float,The ending timestamp or time object for the duration calculation.
          total_items,int,"The total number of items processed, used to determine the number of batches."
          batch_size,int,"The size of each processing batch, used in conjunction with total_items to calculate batches."
          model_name,str,"The name of the model being used, which determines if rate-limiting sleep times should be considered."
        returns[1]{name,type,description}:
          net_time,float,"The calculated net processing time, excluding estimated sleep durations, or the total duration if no specific model is used. Returns 0 if total_items is 0."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.main_workflow:
      identifier: backend.main.main_workflow
      description:
        overall: "The main_workflow function orchestrates a comprehensive analysis pipeline for a given GitHub repository URL. It begins by extracting API keys and model configurations, then clones the specified repository to retrieve its files. The workflow proceeds to extract basic project information, construct a file tree, analyze code relationships, and build an Abstract Syntax Tree (AST) schema, which is subsequently enriched with relationship data. It then prepares inputs for a 'Helper LLM' to analyze individual functions and classes, and finally uses a 'Main LLM' to generate a comprehensive report based on all collected data. The function concludes by saving the final report and associated performance metrics, including token savings, to designated output directories."
        parameters[4]{name,type,description}:
          input,str,"The initial input string, expected to contain a GitHub repository URL for analysis."
          api_keys,dict,"A dictionary containing various API keys (e.g., 'gemini', 'gpt', 'scadsllm') and base URLs required for LLM interactions."
          model_names,dict,A dictionary specifying the names of the models to be used by the helper and main LLMs.
          status_callback,callable | None,An optional callable function used to provide real-time status updates during the workflow execution. Defaults to None.
        returns[1]{name,type,description}:
          result,dict,A dictionary containing the 'report' (the final generated report as a string) and 'metrics' (a dictionary of performance statistics).
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.update_status:
      identifier: backend.main.update_status
      description:
        overall: "The `update_status` function processes a given message, `msg`. It first checks if a `status_callback` function is available and, if so, invokes it with the provided message. Subsequently, it logs the message using the `logging.info` facility. This function serves to standardize status updates and ensure they are consistently logged."
        parameters[1]{name,type,description}:
          msg,str,The message string to be used for updating the status via a callback and for logging.
        returns[1]{name,type,description}:
          None,None,This function does not explicitly return any value; it performs side effects by calling a callback and logging.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.notebook_workflow:
      identifier: backend.main.notebook_workflow
      description:
        overall: "The notebook_workflow function orchestrates the analysis of Jupyter notebooks found within a specified GitHub repository. It begins by extracting a repository URL from the input, cloning the repository, and then processing its contents. The function extracts basic project information and converts any identified notebooks into an XML-like structure, including embedded images. It then iterates through each processed notebook, constructing a payload for an external Large Language Model (LLM) based on the project information, notebook content, and images. The LLM generates a report for each notebook, which are then concatenated into a final comprehensive report. Finally, the function saves this report to a markdown file and returns the report along with execution metrics."
        parameters[4]{name,type,description}:
          input,str,"The input string, expected to contain a GitHub repository URL from which notebooks will be processed."
          api_keys,dict,"A dictionary containing API keys for various Large Language Model (LLM) services, such as 'gpt', 'gemini', 'scadsllm', or 'ollama'."
          model,str,"The name of the LLM model to be used for generating notebook reports (e.g., 'gpt-4', 'gemini-pro')."
          status_callback,callable | None,An optional callback function that receives status messages as strings to provide real-time updates during the workflow execution.
        returns[1]{name,type,description}:
          analysis_result,dict,A dictionary containing the final concatenated report and execution metrics. The dictionary includes 'report' (str) with the LLM-generated analysis and 'metrics' (dict) with performance data.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.gemini_payload:
      identifier: backend.main.gemini_payload
      description:
        overall: "This function constructs a multi-part payload suitable for the Gemini API, integrating contextual information, notebook structure, and embedded images. It serializes basic project details and the notebook path into an initial text part. The function then parses the provided XML content, identifying image placeholders using a regular expression. For each placeholder, it extracts the corresponding image data from the 'images' list, converts it to a base64 URL, and adds it as an image part to the payload. Any text segments within the XML content, before, between, or after image placeholders, are added as separate text parts. Finally, it returns a list of dictionaries, each representing a text or image component of the Gemini payload."
        parameters[4]{name,type,description}:
          basic_info,dict,A dictionary containing basic project information to be included in the payload context.
          nb_path,str,"The file path of the current notebook, included in the payload context."
          xml_content,str,"The XML structure of the notebook, which may contain image placeholders to be replaced with actual image data."
          images,list,"A list of dictionaries, where each dictionary contains image data (e.g., 'data' key with a base64 string) corresponding to the image placeholders."
        returns[1]{name,type,description}:
          payload_content,list,"A list of dictionaries, each formatted as a content part (either 'text' or 'image_url') for the Gemini API."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    backend.relationship_analyzer.path_to_module:
      identifier: backend.relationship_analyzer.path_to_module
      description:
        overall: "This function converts a given file system path into a Python module import path. It first determines the path relative to a specified project root, falling back to the base filename if a relative path cannot be computed. It then removes the '.py' extension if present and replaces system path separators with dots. Finally, it handles '__init__.py' files by removing the '.__init__' suffix to yield the package name."
        parameters[2]{name,type,description}:
          filepath,str,The absolute or relative file system path to be converted.
          project_root,str,"The root directory of the project, used to calculate the relative path."
        returns[1]{name,type,description}:
          module_path,str,The converted Python module path string.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.encrypt_text:
      identifier: database.db.encrypt_text
      description:
        overall: "This function encrypts a given text string using a `cipher_suite`. It first performs a check: if the input text is empty or the `cipher_suite` is not initialized, it returns the original text without modification. Otherwise, it prepares the text by stripping leading/trailing whitespace and encoding it to bytes, then encrypts it using the `cipher_suite` and decodes the result back into a string."
        parameters[1]{name,type,description}:
          text,str,The string value to be encrypted.
        returns[1]{name,type,description}:
          encrypted_text,str,"The encrypted string if encryption is performed, or the original text if encryption is skipped due to empty input or uninitialized cipher_suite."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.decrypt_text:
      identifier: database.db.decrypt_text
      description:
        overall: "This function attempts to decrypt a given string using an external `cipher_suite` object. It first performs a guard check, returning the original text if the input text is empty or if `cipher_suite` is not initialized. If decryption is attempted, the text is stripped of whitespace, encoded to bytes, decrypted, and then decoded back to a string. In case of any exception during the decryption process, the original text is returned."
        parameters[1]{name,type,description}:
          text,str,The string value that needs to be decrypted.
        returns[1]{name,type,description}:
          decrypted_string,str,"The successfully decrypted string, or the original `text` if decryption is skipped or an error occurs."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.insert_user:
      identifier: database.db.insert_user
      description:
        overall: "This function is responsible for creating a new user entry in a database. It takes a username, display name, and a plain-text password as input. The password is first hashed using `stauth.Hasher` for security before being stored. The function constructs a user document, including empty fields for various API keys, and then inserts this document into the `dbusers` collection. Finally, it returns the unique identifier assigned to the newly created user document."
        parameters[3]{name,type,description}:
          username,str,The unique identifier for the user.
          name,str,The display name of the user.
          password,str,"The plain-text password for the user, which will be hashed before storage."
        returns[1]{name,type,description}:
          inserted_id,Any,The unique identifier of the newly inserted user document.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_all_users:
      identifier: database.db.fetch_all_users
      description:
        overall: "This function, `fetch_all_users`, is responsible for retrieving all user records from a database collection named `dbusers`. It executes a `find()` operation on the collection, which typically returns a cursor. The function then converts this cursor into a Python list, effectively collecting all user documents. The primary purpose is to provide a comprehensive list of all users stored in the database."
        parameters[0]:
        returns[1]{name,type,description}:
          users,list,"A list containing all user documents retrieved from the `dbusers` collection. Each item in the list represents a user record, likely as a dictionary or similar document structure."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_user:
      identifier: database.db.fetch_user
      description:
        overall: This function is designed to retrieve a single user record from a database collection named `dbusers`. It takes a username as input and uses it to query the collection for a document where the `_id` field matches the provided username. The function returns the first document found that satisfies this query.
        parameters[1]{name,type,description}:
          username,str,"The unique identifier for the user to be fetched, which is used to match the `_id` field in the database."
        returns[1]{name,type,description}:
          user_document,dict | None,"A dictionary representing the user document if a match is found, otherwise None if no user with the specified username exists."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_user_name:
      identifier: database.db.update_user_name
      description:
        overall: "This function is responsible for updating a user's name in a database. It takes the existing username, which is used as the document's `_id`, and a new name to be assigned. It performs an `update_one` operation on the `dbusers` collection, setting the 'name' field to the provided new name. The function returns the count of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          username,str,"The unique identifier of the user whose name is to be updated, used as the `_id` in the database."
          new_name,str,The new name to be assigned to the specified user.
        returns[1]{name,type,description}:
          modified_count,int,An integer representing the number of documents that were modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.update_gemini_key:
      identifier: database.db.update_gemini_key
      description:
        overall: This function updates a user's Gemini API key in the database. It takes a username and the new Gemini API key as input. The API key is first stripped of whitespace and then encrypted before being stored. The function then updates the 'gemini_api_key' field for the specified user in the 'dbusers' collection. It returns the count of documents that were modified by the update operation.
        parameters[2]{name,type,description}:
          username,str,The username of the user whose Gemini API key is to be updated.
          gemini_api_key,str,"The new Gemini API key to be stored, which will be encrypted before storage."
        returns[1]{name,type,description}:
          modified_count,int,The number of documents modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_gpt_key:
      identifier: database.db.update_gpt_key
      description:
        overall: "This function is responsible for updating a user's GPT API key in the database. It first takes the provided API key, strips any leading or trailing whitespace, and then encrypts it. Finally, it updates the document corresponding to the given username in the `dbusers` collection with the newly encrypted key, returning the count of modified documents."
        parameters[2]{name,type,description}:
          username,str,The username of the user whose GPT API key is to be updated.
          gpt_api_key,str,The new GPT API key to be encrypted and stored for the specified user.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_ollama_url:
      identifier: database.db.update_ollama_url
      description:
        overall: "This function updates the Ollama base URL for a specified user in the database. It takes a username and a new Ollama base URL as input. The provided URL is first stripped of any leading or trailing whitespace before being stored. The function then performs an update operation on the user's document in the 'dbusers' collection, setting the 'ollama_base_url' field. It returns an integer indicating the number of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose Ollama URL is to be updated.
          ollama_base_url,str,"The new base URL for Ollama, which will be stripped of leading/trailing whitespace before storage."
        returns[1]{name,type,description}:
          modified_count,int,The number of documents modified by the update operation (typically 0 or 1).
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_opensrc_key:
      identifier: database.db.update_opensrc_key
      description:
        overall: "This function is responsible for securely updating a user's open-source API key in the database. It takes a username and the new API key as input. The provided API key is first stripped of leading/trailing whitespace and then encrypted before being stored. Finally, it updates the 'opensrc_api_key' field for the specified user in the 'dbusers' collection and returns the count of modified documents."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose API key needs to be updated.
          opensrc_api_key,str,The new open-source API key to be encrypted and stored for the user.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation. A value of 1 indicates success if the user exists.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.update_opensrc_url:
      identifier: database.db.update_opensrc_url
      description:
        overall: This function updates the `opensrc_base_url` field for a specific user in a database collection. It takes a username and a new opensource base URL as input. The function locates the user record using the provided username as the `_id` and sets the `opensrc_base_url` field to the new URL after stripping any leading or trailing whitespace. It then returns the count of documents that were modified by this operation.
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose `opensrc_base_url` is to be updated.
          opensrc_base_url,str,"The new base URL for opensource projects, which will be stripped of leading/trailing whitespace before being stored."
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents modified by the update operation. A value of 1 typically indicates success, while 0 indicates no document was found or changed."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_gemini_key:
      identifier: database.db.fetch_gemini_key
      description:
        overall: "This function is designed to retrieve a user's Gemini API key from a database. It takes a username as input and queries a 'dbusers' collection to find a matching user document. The query specifically projects the 'gemini_api_key' field. If a user document is found, the function extracts and returns the associated Gemini API key. If no user is found for the given username, the function returns None."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose Gemini API key is to be fetched.
        returns[1]{name,type,description}:
          gemini_api_key,str | None,"The Gemini API key associated with the user, or None if the user is not found or the key does not exist in the user document."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_ollama_url:
      identifier: database.db.fetch_ollama_url
      description:
        overall: "This function retrieves the Ollama base URL associated with a specific user from a database. It queries the 'dbusers' collection using the provided username as the document's '_id'. If a user document is found, it extracts and returns the 'ollama_base_url' field. If no user is found, the function returns None."
        parameters[1]{name,type,description}:
          username,str,The unique identifier (username) of the user whose Ollama base URL is to be fetched.
        returns[1]{name,type,description}:
          ollama_base_url,str | None,"The Ollama base URL string if found for the specified user, otherwise None."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_gpt_key:
      identifier: database.db.fetch_gpt_key
      description:
        overall: "This function retrieves the GPT API key associated with a given username from a database. It queries the `dbusers` collection to find a user document matching the provided `username`. The query specifically projects only the `gpt_api_key` field. If a user is found, it returns the `gpt_api_key`; otherwise, it returns `None`."
        parameters[1]{name,type,description}:
          username,str,The username for which to fetch the GPT API key.
        returns[1]{name,type,description}:
          gpt_api_key,str | None,"The GPT API key associated with the username, or `None` if the user is not found or the key does not exist."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_opensrc_key:
      identifier: database.db.fetch_opensrc_key
      description:
        overall: "This function is designed to retrieve an 'opensrc_api_key' associated with a specific username from a database. It queries the 'dbusers' collection, searching for a document where the '_id' field matches the provided username. If a user document is found, the function extracts the 'opensrc_api_key' from it. If no user is found matching the username, the function returns None."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose opensource API key is to be fetched.
        returns[1]{name,type,description}:
          opensrc_api_key,"Optional[str]","The opensource API key as a string if found, otherwise None."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_opensrc_url:
      identifier: database.db.fetch_opensrc_url
      description:
        overall: "The `fetch_opensrc_url` function is designed to retrieve the 'opensrc_base_url' associated with a specific user from a database. It takes a username as input and performs a database query on the `dbusers` collection to locate the corresponding user document. If a user is found, the function extracts and returns the 'opensrc_base_url' field. If no user is found or the field is missing, it returns `None`."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose opensrc base URL is to be fetched.
        returns[1]{name,type,description}:
          opensrc_base_url,str | None,"The opensrc base URL associated with the user, or None if the user is not found or the URL is not set."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.delete_user:
      identifier: database.db.delete_user
      description:
        overall: "This function is designed to remove a specific user record from the `dbusers` collection in a database. It identifies the user to be deleted using their `username`, which is mapped to the `_id` field in the database. The function executes a `delete_one` operation and returns the count of documents successfully deleted."
        parameters[1]{name,type,description}:
          username,str,"The unique identifier of the user to be deleted, corresponding to the `_id` field in the database."
        returns[1]{name,type,description}:
          deleted_count,int,The number of documents deleted by the operation (typically 0 or 1 for `delete_one`).
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.get_decrypted_api_keys:
      identifier: database.db.get_decrypted_api_keys
      description:
        overall: "This function retrieves and decrypts various API keys and base URLs associated with a specific username from a database. It first queries the 'dbusers' collection to find the user. If the user is not found, it returns null values for all keys and URLs. Otherwise, it decrypts the Gemini, GPT, and open-source API keys and returns them along with the Ollama and open-source base URLs."
        parameters[1]{name,type,description}:
          username,str,The username used to identify the user in the database.
        returns[5]{name,type,description}:
          gemini_plain,str or None,"The decrypted Gemini API key, or None if the user is not found."
          ollama_plain,str,"The Ollama base URL, or an empty string if not found in user data, or None if the user is not found."
          gpt_plain,str or None,"The decrypted GPT API key, or None if the user is not found."
          opensrc_plain,str or None,"The decrypted open-source API key, or None if the user is not found."
          opensrc_url,str,"The open-source base URL, or an empty string if not found in user data, or None if the user is not found."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.insert_chat:
      identifier: database.db.insert_chat
      description:
        overall: "This function creates a new chat entry in a database. It constructs a dictionary containing a unique identifier generated using UUID, the provided username, the chat name, and the current timestamp. This chat dictionary is then inserted into the 'dbchats' collection using `insert_one`. The function returns the unique ID of the newly inserted chat document."
        parameters[2]{name,type,description}:
          username,str,The username associated with the new chat entry.
          chat_name,str,The name of the chat to be created.
        returns[1]{name,type,description}:
          inserted_id,str,The unique identifier of the newly inserted chat document.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_chats_by_user:
      identifier: database.db.fetch_chats_by_user
      description:
        overall: "This function is designed to retrieve all chat records associated with a specific user from a database. It queries the 'dbchats' collection, filtering documents by the provided username. The retrieved chat records are then sorted chronologically by their 'created_at' timestamp in ascending order. Finally, the function returns these sorted chat records as a list."
        parameters[1]{name,type,description}:
          username,str,The username for which to fetch chat records.
        returns[1]{name,type,description}:
          chats,"list[dict]","A list of chat records (dictionaries), each representing a chat associated with the specified username, sorted by creation date."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.check_chat_exists:
      identifier: database.db.check_chat_exists
      description:
        overall: "This function queries a database collection, `dbchats`, to determine if a chat entry exists based on a provided username and chat name. It performs a lookup for a document matching both criteria and returns a boolean indicating its presence. The function effectively checks for the uniqueness or existence of a specific chat."
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to be checked.
          chat_name,str,The name of the chat to verify its existence.
        returns[1]{name,type,description}:
          exists,bool,"Returns True if a chat matching the provided username and chat name is found in the database, False otherwise."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.rename_chat_fully:
      identifier: database.db.rename_chat_fully
      description:
        overall: "This function renames a chat and all its associated exchanges (messages) within the database. It first updates the chat entry in the `dbchats` collection, changing its name from `old_name` to `new_name` for the specified `username`. Subsequently, it updates all related exchange entries in the `dbexchanges` collection to reflect this new chat name. The function returns the count of documents modified during the initial chat entry update operation."
        parameters[3]{name,type,description}:
          username,str,The username associated with the chat to be renamed.
          old_name,str,The current name of the chat.
          new_name,str,The desired new name for the chat.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents modified in the `dbchats` collection during the chat entry rename operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.insert_exchange:
      identifier: database.db.insert_exchange
      description:
        overall: "This function inserts a new chat exchange record into a database collection. It generates a unique identifier for the exchange using UUID, then constructs a dictionary containing the question, answer, feedback, user details, chat name, and various optional metrics such as models used, time taken, and token counts. The record also includes a timestamp for creation. The function attempts to insert this structured data into the 'dbexchanges' collection and returns the new ID upon successful insertion, or None if a database error occurs."
        parameters[13]{name,type,description}:
          question,str,The user's question or prompt in the exchange.
          answer,str,The generated answer or response for the question.
          feedback,str,"Feedback provided for the exchange, typically a rating or category."
          username,str,The username associated with this chat exchange.
          chat_name,str,The name of the chat session to which this exchange belongs.
          helper_used,str,"Optional: The name of the helper model utilized for this exchange. Defaults to an empty string."
          main_used,str,"Optional: The name of the main model utilized for this exchange. Defaults to an empty string."
          total_time,str,"Optional: The total time taken for processing the exchange. Defaults to an empty string."
          helper_time,str,"Optional: The time taken specifically by the helper model. Defaults to an empty string."
          main_time,str,"Optional: The time taken specifically by the main model. Defaults to an empty string."
          json_tokens,int,"Optional: The number of JSON tokens consumed during the exchange. Defaults to 0."
          toon_tokens,int,"Optional: The number of 'toon' tokens consumed during the exchange. Defaults to 0."
          savings_percent,float,"Optional: The percentage of savings achieved for this exchange. Defaults to 0.0."
        returns[2]{name,type,description}:
          new_id,str,The unique identifier (UUID) of the newly inserted exchange record upon successful insertion.
          None,NoneType,Indicates that the insertion failed due to a database exception.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.fetch_exchanges_by_user:
      identifier: database.db.fetch_exchanges_by_user
      description:
        overall: "This function retrieves exchange records from a database based on a provided username. It queries the 'dbexchanges' collection, filtering documents by the 'username' field. The results are then sorted in ascending order by the 'created_at' timestamp before being converted into a list and returned. The sorting is explicitly noted as important for display purposes."
        parameters[1]{name,type,description}:
          username,str,The username used to filter the exchange records in the database.
        returns[1]{name,type,description}:
          exchanges,list,"A list of exchange documents (dictionaries) associated with the given username, sorted by their 'created_at' timestamp."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_exchanges_by_chat:
      identifier: database.db.fetch_exchanges_by_chat
      description:
        overall: This function retrieves a list of exchange records from a database collection named `dbexchanges`. It filters these records based on a provided username and chat name. The results are then sorted in ascending order by their `created_at` timestamp before being returned as a list.
        parameters[2]{name,type,description}:
          username,str,The username used to filter the exchange records.
          chat_name,str,The name of the chat used to filter the exchange records.
        returns[1]{name,type,description}:
          exchanges,list,"A list of exchange records (documents) that match the specified username and chat name, sorted by their creation time."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_exchange_feedback:
      identifier: database.db.update_exchange_feedback
      description:
        overall: "This function is designed to update the feedback score for a specific exchange record within a database. It takes an exchange identifier and an integer feedback value as input. The function executes an update operation on the `dbexchanges` collection, targeting a document by its `_id` and setting its `feedback` field to the provided integer value. It then returns the count of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          exchange_id,Any,The unique identifier of the exchange record to be updated.
          feedback,int,The integer feedback value to be set for the specified exchange.
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents that were modified by the update operation, typically 0 or 1."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_exchange_feedback_message:
      identifier: database.db.update_exchange_feedback_message
      description:
        overall: This function updates the feedback message associated with a specific exchange record in the database. It takes an exchange identifier and a new feedback message as input. The function uses `dbexchanges.update_one` to locate the document by its `_id` and set the `feedback_message` field. It then returns the count of documents that were modified by this operation.
        parameters[2]{name,type,description}:
          exchange_id,Any,The unique identifier for the exchange record to be updated.
          feedback_message,str,The new feedback message to be set for the specified exchange.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.delete_exchange_by_id:
      identifier: database.db.delete_exchange_by_id
      description:
        overall: "This function is responsible for deleting a single exchange record from a MongoDB collection. It accepts a unique identifier, `exchange_id`, and uses it to locate and remove the corresponding document. The operation targets the `_id` field within the `dbexchanges` collection. Upon completion, it reports the number of documents that were successfully deleted."
        parameters[1]{name,type,description}:
          exchange_id,str,The unique string identifier of the exchange document to be removed from the database.
        returns[1]{name,type,description}:
          deleted_count,int,An integer indicating the number of documents that were deleted. This will typically be 0 or 1 for a delete_one operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.delete_full_chat:
      identifier: database.db.delete_full_chat
      description:
        overall: This function is responsible for completely deleting a chat and all its associated messages (exchanges) for a given user. It ensures data consistency by first removing all messages linked to the chat and then deleting the chat entry itself. The function returns the count of chats that were deleted.
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to be deleted.
          chat_name,str,The name of the chat to be deleted.
        returns[1]{name,type,description}:
          deleted_count,int,"The number of chat documents deleted from the database. This is typically 1 if the chat was found and deleted, or 0 if not found."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    frontend.frontend.clean_names:
      identifier: frontend.frontend.clean_names
      description:
        overall: "This function processes a list of strings, where each string is expected to represent a path or a URL. It iterates through the provided `model_list` and for each item, it splits the string by the '/' character. The function then extracts the last component of the split string. The primary purpose is to obtain a 'cleaned' or base name from each input string, returning a new list containing these extracted names."
        parameters[1]{name,type,description}:
          model_list,"list[str]","A list of strings, where each string is expected to be a path or URL containing '/' characters that need to be cleaned."
        returns[1]{name,type,description}:
          cleaned_names,"list[str]","A new list containing the last component of each input string after splitting by '/', effectively providing a list of base names."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.get_filtered_models:
      identifier: frontend.frontend.get_filtered_models
      description:
        overall: "The `get_filtered_models` function filters a given list of models (`source_list`) based on a specified `category_name`. It retrieves a set of keywords associated with the category from a global `CATEGORY_KEYWORDS` dictionary. If the category's keywords include \"STANDARD\", the function returns only those models from `source_list` that are also present in `STANDARD_MODELS`. Otherwise, it iterates through the `source_list` and collects models whose names (case-insensitive) contain any of the category's keywords. If any models match the keyword filter, the filtered list is returned; otherwise, the original `source_list` is returned."
        parameters[2]{name,type,description}:
          source_list,list,The initial list of models to be filtered.
          category_name,str,The name of the category used to determine filtering keywords.
        returns[1]{name,type,description}:
          filtered_models,list,"A list of models filtered according to the specified category's keywords. If no models match the keyword filter, the original `source_list` is returned. If the category implies 'STANDARD', only models present in `STANDARD_MODELS` are returned."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.save_gemini_cb:
      identifier: frontend.frontend.save_gemini_cb
      description:
        overall: "This function, `save_gemini_cb`, is designed to save a new Gemini API key. It retrieves the potential new key from the Streamlit session state. If a new key is present, it updates the Gemini key in the database using the current username and the new key. After successfully updating, it clears the key from the session state and displays a success notification to the user."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.save_ollama_cb:
      identifier: frontend.frontend.save_ollama_cb
      description:
        overall: "This function is designed to save a new Ollama URL. It retrieves the potential new URL from the Streamlit session state. If a URL is present, it updates the Ollama URL in the database for the current user, also retrieved from the session state. Finally, it displays a confirmation toast message to the user."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by other functions in the provided context.
      error: null
    frontend.frontend.load_data_from_db:
      identifier: frontend.frontend.load_data_from_db
      description:
        overall: "This function is responsible for loading chat and exchange data from the database into the Streamlit session state. It ensures data consistency by first fetching predefined chats, then associating exchanges with them, and creating a default chat if no chats exist. The function also manages the active chat within the session state and prevents redundant data loading for the same user."
        parameters[1]{name,type,description}:
          username,str,The username for which chat and exchange data should be loaded from the database.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.handle_feedback_change:
      identifier: frontend.frontend.handle_feedback_change
      description:
        overall: "This function is designed to handle changes in feedback for an exchange object. It updates the 'feedback' key within the provided exchange dictionary (`ex`) with the new value (`val`). Subsequently, it calls a database utility function, `db.update_exchange_feedback`, to persist this feedback change using the exchange's ID. Finally, it triggers a full re-execution of the Streamlit application using `st.rerun()` to reflect the changes in the UI."
        parameters[2]{name,type,description}:
          ex,dict,"The exchange object, expected to be a dictionary containing at least a 'feedback' key and an '_id' key."
          val,any,The new feedback value to be assigned to the exchange object.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.handle_delete_exchange:
      identifier: frontend.frontend.handle_delete_exchange
      description:
        overall: "This function is responsible for deleting a specific exchange from both the database and the application's session state. It first calls `db.delete_exchange_by_id` using the exchange's `_id`. Subsequently, it checks if the associated chat exists in `st.session_state.chats` and, if the exchange is found within that chat's exchanges list, it removes it. Finally, it triggers a full Streamlit rerun to refresh the UI."
        parameters[2]{name,type,description}:
          chat_name,str,The name of the chat from which the exchange should be deleted.
          ex,dict,"The exchange object to be deleted, which is expected to contain an '_id' key for database deletion."
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.handle_delete_chat:
      identifier: frontend.frontend.handle_delete_chat
      description:
        overall: "This function is responsible for deleting a specified chat for a given user and managing the application's chat state. It first removes the chat from the database using `db.delete_full_chat`. Subsequently, it updates the Streamlit session state by removing the chat from `st.session_state.chats`. If other chats exist, the active chat is reset to the first available chat; otherwise, a new default chat named 'Chat 1' is created via `db.insert_chat` and set as the active chat. Finally, it triggers a Streamlit rerun to reflect these changes."
        parameters[2]{name,type,description}:
          username,str,The username of the user whose chat is to be deleted.
          chat_name,str,The name of the chat to be deleted.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.extract_repo_name:
      identifier: frontend.frontend.extract_repo_name
      description:
        overall: "This function aims to extract a repository name from a given input text. It first attempts to find a URL within the text using a regular expression. If a URL is identified, it then parses this URL to isolate its path component. The last segment of the URL path is considered the potential repository name, with any '.git' suffix being removed. The function returns the extracted repository name as a string or None if no URL is found or a repository name cannot be successfully derived."
        parameters[1]{name,type,description}:
          text,str,The input string that may contain a URL from which to extract a repository name.
        returns[1]{name,type,description}:
          repo_name,str | None,"The extracted repository name as a string, or None if no valid URL or repository name could be found."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.stream_text_generator:
      identifier: frontend.frontend.stream_text_generator
      description:
        overall: "This function acts as a generator that simulates streaming text by yielding individual words from an input string. It takes a complete string, splits it into words, and then yields each word sequentially, appending a space after it. A small delay is introduced between each word yield to mimic a real-time streaming effect. This is typically used in user interfaces to display text progressively."
        parameters[1]{name,type,description}:
          text,str,The input string that needs to be streamed word by word.
        returns[1]{name,type,description}:
          word_chunk,str,"A single word from the input text, followed by a space, yielded sequentially."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.render_text_with_mermaid:
      identifier: frontend.frontend.render_text_with_mermaid
      description:
        overall: "This function processes a given markdown text, identifying and rendering embedded Mermaid diagrams. It splits the input text into segments of regular markdown and Mermaid code blocks. Regular markdown segments are rendered using `st.markdown` or streamed via `st.write_stream` if `should_stream` is true. Mermaid code blocks are rendered using `st_mermaid`, with a fallback to `st.code` if the Mermaid rendering fails."
        parameters[2]{name,type,description}:
          markdown_text,str,"The input text, which may contain markdown and embedded Mermaid diagram code blocks."
          should_stream,bool,A boolean flag indicating whether non-Mermaid text parts should be streamed using `st.write_stream` or rendered directly with `st.markdown`. Defaults to `False`.
        returns[1]{name,type,description}:
          None,None,This function does not explicitly return a value. It performs side effects by rendering content to a Streamlit application. It returns early if `markdown_text` is empty.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.render_exchange:
      identifier: frontend.frontend.render_exchange
      description:
        overall: "This function `render_exchange` is designed to display a single chat exchange within a Streamlit application. It renders the user's question and the assistant's answer, providing an interactive interface for user feedback and actions. The assistant's response includes a toolbar with buttons for 'like' (helpful), 'dislike' (not helpful), adding a comment via a popover, downloading the response, and deleting the exchange. It also handles error scenarios, displaying an error message and a delete option if the answer indicates an error."
        parameters[2]{name,type,description}:
          ex,dict,"A dictionary-like object representing a single chat exchange. It contains keys such as 'question', 'answer', 'feedback' (integer, 1 for helpful, 0 for not helpful), 'feedback_message' (string), and '_id' (unique identifier)."
          current_chat_name,str,"A string representing the name of the current chat session, used for context in operations like deleting an exchange."
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
  classes:
    backend.AST_Schema.ASTVisitor:
      identifier: backend.AST_Schema.ASTVisitor
      description:
        overall: "The ASTVisitor class extends `ast.NodeVisitor` and is designed to traverse the Abstract Syntax Tree (AST) of Python code. Its primary purpose is to extract structured information about imports, functions, and classes within a given source file. It builds a `schema` dictionary that categorizes these elements, including details like identifiers, docstrings, and source code segments. This class acts as a foundational component for static code analysis, enabling the collection of metadata from Python source files."
        init_method:
          description: "The `__init__` method initializes the ASTVisitor with the raw source code, the file's absolute path, and the project's root directory. It calculates the module path based on these inputs and sets up an empty `schema` dictionary to store discovered imports, functions, and classes. It also initializes `_current_class` to `None` to track the class currently being visited during AST traversal."
          parameters[4]{name,type,description}:
            self,ASTVisitor,The instance of the ASTVisitor class.
            source_code,str,The raw source code string of the file being visited.
            file_path,str,The absolute path to the file being visited.
            project_root,str,The root directory of the project.
        methods[5]:
          - identifier: visit_Import
            description:
              overall: "This method processes `ast.Import` nodes, which represent module import statements (e.g., `import module`). It iterates through each alias defined in the import statement and appends the module's name to the `imports` list within the `self.schema` dictionary. After processing the current node, it calls `self.generic_visit` to continue the AST traversal to child nodes."
              parameters[2]{name,type,description}:
                self,ASTVisitor,The instance of the ASTVisitor class.
                node,ast.Import,The AST node representing an import statement.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other methods or functions based on the provided context.
                called_by: This method is not explicitly called by other methods within the provided context.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method handles `ast.ImportFrom` nodes, which represent 'from ... import ...' statements. It iterates through the imported names (aliases) and constructs a fully qualified import string (e.g., `node.module.alias.name`), which is then appended to the `imports` list in `self.schema`. Following this, it invokes `self.generic_visit` to ensure continued traversal of the AST."
              parameters[2]{name,type,description}:
                self,ASTVisitor,The instance of the ASTVisitor class.
                node,ast.ImportFrom,The AST node representing a 'from ... import ...' statement.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other methods or functions based on the provided context.
                called_by: This method is not explicitly called by other methods within the provided context.
            error: null
          - identifier: visit_ClassDef
            description:
              overall: "This method processes `ast.ClassDef` nodes, which define classes in the source code. It constructs a dictionary containing comprehensive information about the class, including its fully qualified identifier, name, docstring, the exact source code segment, and its start and end line numbers. This class information is then appended to the `classes` list within `self.schema`. It temporarily sets `_current_class` to this new class info for nested method processing and resets it after visiting the class's children."
              parameters[2]{name,type,description}:
                self,ASTVisitor,The instance of the ASTVisitor class.
                node,ast.ClassDef,The AST node representing a class definition.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other methods or functions based on the provided context.
                called_by: This method is not explicitly called by other methods within the provided context.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method processes `ast.FunctionDef` nodes, handling both standalone functions and methods defined within classes. If a class is currently being visited (indicated by `_current_class` being set), it extracts method-specific information and appends it to the `method_context` of the current class. Otherwise, for standalone functions, it extracts function-specific details and adds them to the `functions` list in `self.schema`. After processing, it calls `self.generic_visit` to continue the AST traversal."
              parameters[2]{name,type,description}:
                self,ASTVisitor,The instance of the ASTVisitor class.
                node,ast.FunctionDef,The AST node representing a function definition.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other methods or functions based on the provided context.
                called_by: This method is not explicitly called by other methods within the provided context.
            error: null
          - identifier: visit_AsyncFunctionDef
            description:
              overall: "This method is specifically designed to handle `ast.AsyncFunctionDef` nodes, which represent asynchronous function definitions. Its implementation is straightforward: it delegates the entire processing task to the `visit_FunctionDef` method. This approach ensures that asynchronous functions are analyzed and recorded in the schema in the same manner as regular synchronous functions."
              parameters[2]{name,type,description}:
                self,ASTVisitor,The instance of the ASTVisitor class.
                node,ast.AsyncFunctionDef,The AST node representing an asynchronous function definition.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other methods or functions based on the provided context.
                called_by: This method is not explicitly called by other methods within the provided context.
            error: null
        usage_context:
          dependencies: This class does not have explicit external dependencies based on the provided context.
          instantiated_by: This class is not explicitly instantiated by other components based on the provided context.
      error: null
    backend.AST_Schema.ASTAnalyzer:
      identifier: backend.AST_Schema.ASTAnalyzer
      description:
        overall: "The ASTAnalyzer class is designed to analyze the Abstract Syntax Trees (ASTs) of Python files within a repository and integrate call relationship data. It provides functionality to parse Python source code, extract structural information about functions, classes, and imports, and then enrich this structural data with details about how different code elements interact through calls and instantiations. Its primary role is to build a comprehensive, interconnected schema of a codebase."
        init_method:
          description: "This constructor initializes an instance of the ASTAnalyzer class. It currently performs no specific setup or attribute assignments, serving as a placeholder for future initialization logic."
          parameters[0]:
        methods[2]:
          - identifier: merge_relationship_data
            description:
              overall: "This method integrates call relationship data (incoming and outgoing calls) into a pre-existing full schema of AST nodes. It iterates through functions and classes within the schema, updating their respective 'context' fields with 'calls', 'called_by', and 'instantiated_by' information. For classes, it also identifies and lists external dependencies based on method calls that are not internal to the class, providing a holistic view of inter-component communication."
              parameters[3]{name,type,description}:
                self,ASTAnalyzer,The instance of the ASTAnalyzer class.
                full_schema,dict,"A dictionary representing the complete AST schema, including files, functions, and classes, to be enriched with relationship data."
                raw_relationships,dict,"A dictionary containing raw incoming and outgoing call relationships, typically structured by identifier."
              returns[1]{name,type,description}:
                full_schema,dict,"The updated full schema dictionary with integrated relationship data for functions, classes, and methods."
              usage_context:
                calls: This method primarily uses dictionary access methods like 'get' and string methods like 'startswith' to process and update the schema.
                called_by: This method is not explicitly called by any other functions or methods within the provided context.
            error: null
          - identifier: analyze_repository
            description:
              overall: "This method processes a list of file objects from a Git repository to construct a comprehensive AST schema. It filters for Python files, parses their content using the 'ast' module, and then visits the AST tree with an 'ASTVisitor' to extract structured node information. The method aggregates these file-specific schemas into a single 'full_schema' dictionary, handling potential parsing errors gracefully by printing warnings."
              parameters[3]{name,type,description}:
                self,ASTAnalyzer,The instance of the ASTAnalyzer class.
                files,list,"A list of file objects, where each object is expected to have 'path' and 'content' attributes representing a file in the repository."
                repo,GitRepository,"An object representing the Git repository, used for context but not directly accessed in the provided method body."
              returns[1]{name,type,description}:
                full_schema,dict,"A dictionary containing the AST schema for all processed Python files in the repository, structured by file path."
              usage_context:
                calls: "This method calls 'os.path.commonpath', 'os.path.isfile', 'os.path.dirname' for path manipulation, 'ast.parse' to parse Python code, and instantiates and uses 'ASTVisitor' to traverse the AST."
                called_by: This method is not explicitly called by any other functions or methods within the provided context.
            error: null
        usage_context:
          dependencies: "The class depends on the 'ast' module for parsing Python code, the 'os' module for path manipulation, and 'getRepo.GitRepository' for repository interaction, although 'GitRepository' is only used as a type hint in 'analyze_repository's signature."
          instantiated_by: This class is not explicitly instantiated by any other components within the provided context.
      error: null
    backend.File_Dependency.FileDependencyGraph:
      identifier: backend.File_Dependency.FileDependencyGraph
      description:
        overall: "The FileDependencyGraph class extends NodeVisitor to analyze Python source code and build a graph of file dependencies. It initializes with a specific file and repository root, then traverses the Abstract Syntax Tree (AST) to identify and resolve import statements. The class's primary responsibility is to populate an internal dictionary, `import_dependencies`, which maps each file to the set of modules it imports, handling both absolute and relative import paths."
        init_method:
          description: This constructor initializes the FileDependencyGraph instance by setting the `filename` and `repo_root` attributes. These attributes are crucial for resolving file paths and managing dependencies within the repository during AST traversal.
          parameters[2]{name,type,description}:
            filename,str,The path to the file currently being analyzed for dependencies.
            repo_root,Any,"The root directory of the repository, used for resolving file paths and imports."
        methods[3]:
          - identifier: _resolve_module_name
            description:
              overall: "This method is responsible for resolving relative import statements, such as `from .. import name1, name2`. It calculates the actual module or symbol names based on the import's `level` and the current file's location within the repository. The method checks for the existence of corresponding module files or symbols exported via `__init__.py` files. If no matching module or symbol can be resolved, it raises an ImportError."
              parameters[1]{name,type,description}:
                node,ImportFrom,An AST node representing the 'from ... import ...' statement to be resolved.
              returns[1]{name,type,description}:
                resolved,"list[str]","A list of strings, where each string is a resolved module or symbol name."
              usage_context:
                calls: This method does not explicitly call any other functions or methods according to the provided context.
                called_by: This method is not explicitly called by any other functions or methods according to the provided context.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method processes AST nodes representing `Import` or `ImportFrom` statements. Its purpose is to record the identified import dependencies in the class's `import_dependencies` dictionary. It adds the imported module or symbol name, either directly from the node's alias or from an optionally provided `base_name`, to the set of dependencies for the current `self.filename`. After processing, it calls `self.generic_visit(node)` to continue the AST traversal."
              parameters[2]{name,type,description}:
                node,Import | ImportFrom,The AST node representing an import statement.
                base_name,str | None,"An optional base name for the module, typically used when resolving 'from ... import ...' statements."
              returns[0]:
              usage_context:
                calls: This method does not explicitly call any other functions or methods according to the provided context.
                called_by: This method is not explicitly called by any other functions or methods according to the provided context.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method specifically handles `ImportFrom` AST nodes, which represent 'from module import name' statements. If the import is absolute (i.e., `node.module` is present), it extracts the last component of the module name and passes it to `visit_Import`. For relative imports (where `node.module` is None), it attempts to resolve the actual module names using the `_resolve_module_name` helper method. Any successfully resolved base names are then passed to `visit_Import` to record the dependency. It also includes basic error handling for failed relative import resolutions and continues the AST traversal with `self.generic_visit(node)`."
              parameters[1]{name,type,description}:
                node,ImportFrom,The AST node representing a 'from ... import ...' statement.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call any other functions or methods according to the provided context.
                called_by: This method is not explicitly called by any other functions or methods according to the provided context.
            error: null
        usage_context:
          dependencies: "The class relies on external components such as `ast` module elements (e.g., `NodeVisitor`, `Import`, `ImportFrom`, `parse`, `walk`, `literal_eval`), `keyword.iskeyword`, and `pathlib.Path` for file system interactions."
          instantiated_by: This class is not explicitly instantiated by any other components listed in the provided context.
      error: null
    backend.HelperLLM.LLMHelper:
      identifier: backend.HelperLLM.LLMHelper
      description:
        overall: "The LLMHelper class provides a robust interface for interacting with various Large Language Models (LLMs), including Google Gemini, OpenAI, Ollama, and custom endpoints, to generate structured documentation for Python functions and classes. It centralizes the logic for loading LLM system prompts, dynamically configuring LLM clients, managing API batch requests, and handling rate limits. The class leverages Pydantic for strict input/output validation, ensuring that the generated documentation adheres to predefined schemas for FunctionAnalysis and ClassAnalysis."
        init_method:
          description: "This constructor initializes the LLMHelper, loading system prompts from specified files for function and class analysis. It dynamically configures the appropriate LLM client (Gemini, OpenAI, Ollama, or a custom endpoint) based on the provided model name and API key. It also sets up structured output capabilities for FunctionAnalysis and ClassAnalysis schemas and configures batch processing settings for API calls."
          parameters[5]{name,type,description}:
            api_key,str,The API key required for authenticating with the chosen LLM service.
            function_prompt_path,str,The file path to the system prompt text used for guiding the LLM in function analysis.
            class_prompt_path,str,The file path to the system prompt text used for guiding the LLM in class analysis.
            model_name,str,"The name of the LLM model to be used, defaulting to 'gemini-2.0-flash-lite'."
            base_url,str,"An optional base URL for custom LLM endpoints, such as Ollama or other self-hosted models."
        methods[3]:
          - identifier: _configure_batch_settings
            description:
              overall: "This private method sets the `batch_size` attribute of the LLMHelper instance based on the provided `model_name`. It assigns specific batch sizes for various Gemini, Llama, and GPT models, as well as custom or aliased models, to optimize API calls and respect rate limits. If an unknown model is encountered, it logs a warning and defaults to a conservative batch size of 2."
              parameters[1]{name,type,description}:
                model_name,str,The name of the LLM model for which to configure batch processing settings.
              returns[0]:
              usage_context:
                calls: This method internally calls `logging.warning` if an unknown model is provided.
                called_by: This method is called by the `__init__` constructor of the `LLMHelper` class during initialization.
            error: null
          - identifier: generate_for_functions
            description:
              overall: "This method processes a list of `FunctionAnalysisInput` objects to generate structured function documentation using the configured LLM. It converts each input into a JSON payload, constructs conversations with a system prompt, and then sends these in batches to the LLM. It handles potential errors during batch processing by extending the results with `None` and includes a waiting period between batches to manage API rate limits effectively."
              parameters[1]{name,type,description}:
                function_inputs,"List[FunctionAnalysisInput]","A list of input objects, each containing the necessary data for function analysis."
              returns[1]{name,type,description}:
                None,"List[Optional[FunctionAnalysis]]","A list of `FunctionAnalysis` objects, or `None` for inputs that failed during processing, maintaining the original order."
              usage_context:
                calls: "This method calls `json.dumps` to serialize input, `model_dump` on input objects, `SystemMessage` and `HumanMessage` for conversation formatting, `self.function_llm.batch` to send requests, `logging.info` for progress, `logging.error` for exceptions, and `time.sleep` for rate limiting."
                called_by: This method is not explicitly called by other methods within the provided context.
            error: null
          - identifier: generate_for_classes
            description:
              overall: "This method is responsible for generating structured class documentation by processing a list of `ClassAnalysisInput` objects. It prepares JSON payloads from the inputs, creates conversations with a class-specific system prompt, and dispatches these conversations to the LLM in batches. The method incorporates error handling for batch calls, filling the result list with `None` for failed items, and implements a delay between batches to comply with API rate limits."
              parameters[1]{name,type,description}:
                class_inputs,"List[ClassAnalysisInput]","A list of input objects, each containing the necessary data for class analysis."
              returns[1]{name,type,description}:
                None,"List[Optional[ClassAnalysis]]","A list of `ClassAnalysis` objects, or `None` for inputs that failed during processing, maintaining the original order."
              usage_context:
                calls: "This method calls `json.dumps` to serialize input, `model_dump` on input objects, `SystemMessage` and `HumanMessage` for conversation formatting, `self.class_llm.batch` to send requests, `logging.info` for progress, `logging.error` for exceptions, and `time.sleep` for rate limiting."
                called_by: This method is not explicitly called by other methods within the provided context.
            error: null
        usage_context:
          dependencies: "This class depends on external libraries such as `json`, `logging`, `time`, `langchain_google_genai.ChatGoogleGenerativeAI`, `langchain_ollama.ChatOllama`, `langchain_openai.ChatOpenAI`, `langchain.messages.HumanMessage`, `langchain.messages.SystemMessage`, and custom Pydantic schemas like `FunctionAnalysis`, `ClassAnalysis`, `FunctionAnalysisInput`, and `ClassAnalysisInput`."
          instantiated_by: This class is not explicitly shown to be instantiated by other components within the provided context.
      error: null
    backend.MainLLM.MainLLM:
      identifier: backend.MainLLM.MainLLM
      description:
        overall: "The MainLLM class serves as a central interface for interacting with various large language models (LLMs), abstracting away the specifics of different providers. It handles the initialization of LLM clients (Gemini, OpenAI-compatible, Ollama) based on configuration, loads a system prompt, and provides methods for both single-response and streaming interactions. This class enables flexible integration of LLMs into applications, allowing for different models and deployment environments."
        init_method:
          description: "This constructor initializes the MainLLM class by setting up the API key, loading a system prompt from a specified file, and configuring the appropriate Language Model (LLM) client based on the `model_name`. It supports various LLM providers like Gemini, OpenAI-compatible APIs (e.g., SCADSLLM), and Ollama, ensuring the correct client is instantiated for subsequent interactions."
          parameters[4]{name,type,description}:
            api_key,str,The API key required for authenticating with the chosen LLM service. A ValueError is raised if it's not provided.
            prompt_file_path,str,The file path to a text file containing the system prompt that will guide the LLM's behavior. A FileNotFoundError is raised if the file does not exist.
            model_name,str,"The name of the LLM model to be used, defaulting to \"gemini-2.5-pro\". This parameter determines which LLM client (Gemini, OpenAI, or Ollama) is initialized."
            base_url,str,"An optional base URL for custom LLM endpoints, particularly relevant for Ollama or other OpenAI-compatible services. If not provided for Ollama, it defaults to OLLAMA_BASE_URL."
        methods[2]:
          - identifier: call_llm
            description:
              overall: "This method sends a user input to the configured LLM and retrieves a single, complete response. It constructs a list of messages including the class's system_prompt and the provided user_input, then uses the self.llm.invoke method to get the LLM's reply. The method includes error handling to log any issues during the LLM call and returns None in case of an exception."
              parameters[1]{name,type,description}:
                user_input,str,The user's query or message to be sent to the LLM.
              returns[2]{name,type,description}:
                content,str,The textual content of the LLM's response if the call is successful.
                None,None,Returns None if an error occurs during the LLM invocation.
              usage_context:
                calls: "This method calls SystemMessage and HumanMessage to format the input, logging.info and logging.error for operational messages, and self.llm.invoke to interact with the underlying LLM."
                called_by: This method is not explicitly called by other methods within the provided context.
            error: null
          - identifier: stream_llm
            description:
              overall: "This method provides a streaming interface to the LLM, allowing it to return responses in chunks rather than a single complete message. It constructs the message payload similar to call_llm and then uses self.llm.stream to obtain an iterator. Each chunk's content is yielded, enabling real-time display or processing of the LLM's output. Error handling is included to yield an error message if the streaming process fails."
              parameters[1]{name,type,description}:
                user_input,str,The user's query or message to be sent to the LLM for streaming response.
              returns[2]{name,type,description}:
                content,str,Yields chunks of textual content from the LLM's streaming response.
                error_message,str,Yields an error message string if an exception occurs during the streaming process.
              usage_context:
                calls: "This method calls SystemMessage and HumanMessage to format the input, logging.info and logging.error for operational messages, and self.llm.stream to interact with the underlying LLM in a streaming fashion."
                called_by: This method is not explicitly called by other methods within the provided context.
            error: null
        usage_context:
          dependencies: The class has no explicit functional dependencies listed in the input context.
          instantiated_by: The class is not explicitly instantiated by any other components within the provided context.
      error: null
    backend.basic_info.ProjektInfoExtractor:
      identifier: backend.basic_info.ProjektInfoExtractor
      description:
        overall: "The ProjektInfoExtractor class is designed to extract fundamental project information from common project files such as README, pyproject.toml, and requirements.txt. It initializes an internal data structure to store various project details, including an overview, installation steps, key features, and tech stack. The class orchestrates the parsing of these files, prioritizing structured data sources like pyproject.toml over less structured ones like README, and provides a consolidated view of the project's basic information."
        init_method:
          description: "The constructor initializes the instance by setting a constant string for 'information not found' and creating a nested dictionary `self.info`. This dictionary is pre-populated with placeholder values for project overview and installation details, ensuring a consistent structure for extracted information."
          parameters[0]:
        methods[7]:
          - identifier: _clean_content
            description:
              overall: "This method removes null bytes from a given string, which can occur due to encoding errors, particularly when a file encoded in UTF-16 is incorrectly read as UTF-8. It first checks if the input content is empty, returning an empty string if so, otherwise it replaces all occurrences of the null byte character ('\\x00') with an empty string."
              parameters[1]{name,type,description}:
                content,str,The string content to be cleaned.
              returns[1]{name,type,description}:
                "",str,The cleaned string with null bytes removed.
              usage_context:
                calls: This method does not explicitly call other methods or functions within its source code.
                called_by: "This method is called by _parse_readme, _parse_toml, and _parse_requirements."
            error: null
          - identifier: _finde_datei
            description:
              overall: "This method searches for a specific file within a provided list of file objects based on a list of patterns. It iterates through each file and each pattern, performing a case-insensitive comparison of the file's path against the pattern. The method returns the first file object whose path ends with any of the specified patterns, or None if no matching file is found."
              parameters[2]{name,type,description}:
                patterns,"List[str]","A list of string patterns (e.g., file extensions or names) to match against file paths."
                dateien,"List[Any]","A list of file-like objects, each expected to have a 'path' attribute."
              returns[1]{name,type,description}:
                "","Optional[Any]","The first file object that matches one of the patterns, or None if no match is found."
              usage_context:
                calls: This method does not explicitly call other methods or functions within its source code.
                called_by: This method is called by extrahiere_info.
            error: null
          - identifier: _extrahiere_sektion_aus_markdown
            description:
              overall: "This method extracts text content located directly under a Markdown level 2 heading (##) that matches one of the specified keywords. It dynamically constructs a regular expression pattern using the provided keywords to find the relevant heading. The method then captures all subsequent content until another level 2 heading or the end of the document, returning the stripped text of that section."
              parameters[2]{name,type,description}:
                inhalt,str,The full Markdown content as a string.
                keywords,"List[str]",A list of keywords to match against the Markdown heading (case-insensitive).
              returns[1]{name,type,description}:
                "","Optional[str]","The extracted section content as a string, or None if no matching section is found."
              usage_context:
                calls: "This method calls re.escape, re.compile, re.IGNORECASE, re.DOTALL, pattern.search, match.group, and strip."
                called_by: This method is called by _parse_readme.
            error: null
          - identifier: _parse_readme
            description:
              overall: "This method parses the content of a README file to extract various project details such as the title, description, key features, tech stack, current status, installation instructions, and a quick start guide. It first cleans the content using `_clean_content` and then uses regular expressions and the `_extrahiere_sektion_aus_markdown` helper to find and extract information from different sections. The extracted data is then used to update the `self.info` dictionary."
              parameters[1]{name,type,description}:
                inhalt,str,The raw string content of the README file.
              returns[0]:
              usage_context:
                calls: "This method calls self._clean_content, re.search, and self._extrahiere_sektion_aus_markdown."
                called_by: This method is called by extrahiere_info.
            error: null
          - identifier: _parse_toml
            description:
              overall: "This method parses the content of a `pyproject.toml` file to extract project-level information like the project name, description, and dependencies. It begins by cleaning the input content using `_clean_content`. If the `tomllib` module is available, it attempts to load the TOML content; otherwise, it prints a warning. Any successfully extracted project data is then used to update the `self.info` dictionary, handling potential `TOMLDecodeError` during parsing."
              parameters[1]{name,type,description}:
                inhalt,str,The raw string content of the pyproject.toml file.
              returns[0]:
              usage_context:
                calls: "This method calls self._clean_content, tomllib.loads, data.get, and print."
                called_by: This method is called by extrahiere_info.
            error: null
          - identifier: _parse_requirements
            description:
              overall: "This method parses the content of a `requirements.txt` file to extract project dependencies. It first cleans the input content using `_clean_content`. It then processes each line, filtering out empty lines and comments to identify actual dependency specifications. The extracted dependencies are stored in the `self.info` dictionary, but only if the dependencies have not already been populated from another source, such as a `pyproject.toml` file."
              parameters[1]{name,type,description}:
                inhalt,str,The raw string content of the requirements.txt file.
              returns[0]:
              usage_context:
                calls: This method calls self._clean_content.
                called_by: This method is called by extrahiere_info.
            error: null
          - identifier: extrahiere_info
            description:
              overall: "This method orchestrates the entire process of extracting project information from various files. It first identifies relevant project files (README, pyproject.toml, requirements.txt) using `_finde_datei`. It then parses these files in a specific order of priority: `pyproject.toml` first, then `requirements.txt`, and finally `README.md`. After parsing, it formats the extracted dependencies and, if no project title was found, attempts to derive one from the provided repository URL. The method returns the comprehensive `self.info` dictionary containing all gathered project details."
              parameters[2]{name,type,description}:
                dateien,"List[Any]","A list of file-like objects, each containing 'path' and 'content' attributes, representing the project files."
                repo_url,str,"The URL of the repository, used as a fallback to derive the project title if not found elsewhere."
              returns[1]{name,type,description}:
                "","Dict[str, Any]","A dictionary containing all extracted project information, including overview and installation details."
              usage_context:
                calls: "This method calls self._finde_datei, self._parse_toml, self._parse_requirements, self._parse_readme, os.path.basename, and repo_url.removesuffix."
                called_by: This method is not explicitly called by other methods or functions within the provided context.
            error: null
        usage_context:
          dependencies: "This class depends on the 're' module for regular expressions, the 'os' module for path manipulation, the 'tomllib' module for TOML parsing, and 'typing.List', 'typing.Dict', 'typing.Any', 'typing.Optional' for type hinting."
          instantiated_by: This class is not explicitly instantiated by any known components within the provided context.
      error: null
    backend.callgraph.CallGraph:
      identifier: backend.callgraph.CallGraph
      description:
        overall: "The CallGraph class is an AST NodeVisitor designed to construct a directed call graph for a given Python source file. It traverses the Abstract Syntax Tree (AST) of a file, identifying function definitions, class definitions, import statements, and function calls. The class resolves function and method names, handles import aliases, and tracks the current scope (function or class) to accurately map caller-callee relationships, ultimately building a networkx.DiGraph representing the call structure."
        init_method:
          description: "The constructor initializes the CallGraph instance with the filename of the source code being analyzed. It sets up various internal state variables, including `current_function` and `current_class` for tracking scope, `local_defs` for local name resolution, `graph` as a networkx.DiGraph to store the call graph, `import_mapping` for resolving imported names, `function_set` to keep track of all defined functions, and `edges` to store raw caller-callee relationships."
          parameters[1]{name,type,description}:
            filename,str,The path to the Python source file being analyzed.
        methods[11]:
          - identifier: _recursive_call
            description:
              overall: "This private helper method recursively extracts the name components from an AST node representing a function call, attribute access, or simple name. It processes `ast.Call`, `ast.Name`, and `ast.Attribute` nodes to build a list of strings that represent the hierarchical path of a callable, such as `['pkg', 'mod', 'Class', 'method']`. This list is then used for resolving the full name of the callee."
              parameters[1]{name,type,description}:
                node,ast.AST,"The AST node to be recursively processed, typically an ast.Call, ast.Name, or ast.Attribute."
              returns[1]{name,type,description}:
                parts,"list[str]",A list of string components representing the dotted name of the callable.
              usage_context:
                calls: This method calls itself recursively to traverse the AST node structure.
                called_by: This method is called by the `visit_Call` method to extract callee name components.
            error: null
          - identifier: _resolve_all_callee_names
            description:
              overall: "This private helper method takes a list of potential callee name components and resolves them into fully qualified names. It first checks against local definitions (`self.local_defs`) and then against the import mapping (`self.import_mapping`). If a name cannot be resolved through these mechanisms, it constructs a full name using the current filename and class context. This ensures that each callee is identified with a unique, fully qualified path."
              parameters[1]{name,type,description}:
                callee_nodes,"list[list[str]]",A list where each inner list contains string components of a potential callee's name.
              returns[1]{name,type,description}:
                resolved,"list[str]",A list of fully qualified string names for the resolved callees.
              usage_context:
                calls: This method does not explicitly call other methods within the class.
                called_by: This method is called by the `visit_Call` method to resolve the full names of called functions or methods.
            error: null
          - identifier: _make_full_name
            description:
              overall: "This private helper method constructs a fully qualified name for a given base name, incorporating the filename and an optional class name. If a `class_name` is provided, the format will be `filename::class_name::basename`; otherwise, it will be `filename::basename`. This ensures unique identification of functions and methods within the call graph."
              parameters[2]{name,type,description}:
                basename,str,"The base name of the function or method (e.g., 'my_function')."
                class_name,str | None,"The name of the class if the function is a method, or None if it's a top-level function."
              returns[1]{name,type,description}:
                full_name,str,The fully qualified name of the function or method.
              usage_context:
                calls: This method does not explicitly call other methods within the class.
                called_by: This method is called by the `visit_FunctionDef` method to create a unique identifier for a defined function or method.
            error: null
          - identifier: _current_caller
            description:
              overall: "This private helper method determines the identifier for the current calling context. It returns the `self.current_function` if a function is currently being visited. If no function context is active, it returns a placeholder string indicating either the filename (if available) or a generic '<global-scope>' to represent calls made at the module level."
              parameters[0]:
              returns[1]{name,type,description}:
                caller_identifier,str,A string representing the identifier of the current caller (function name or global scope).
              usage_context:
                calls: This method does not explicitly call other methods within the class.
                called_by: This method is called by the `visit_Call` method to identify the source of a function call.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method is an override from `ast.NodeVisitor` that processes `ast.Import` nodes. It iterates through each alias in the import statement, extracting the original module name and its potential alias. It then populates the `self.import_mapping` dictionary, associating the alias (or original name if no alias) with the module's full name. After processing, it calls `generic_visit` to continue AST traversal."
              parameters[1]{name,type,description}:
                node,ast.Import,"The AST node representing an import statement (e.g., `import module as alias`)."
              returns[0]:
              usage_context:
                calls: This method calls `self.generic_visit(node)` to ensure continued traversal of the AST.
                called_by: This method is implicitly called by the `ast.NodeVisitor`'s traversal mechanism when an `ast.Import` node is encountered.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method is an override from `ast.NodeVisitor` that processes `ast.ImportFrom` nodes. It extracts the module name from which objects are imported and then iterates through the imported names. For each name, it updates `self.import_mapping` to associate the imported name (or its alias) with the module it came from. This helps in resolving fully qualified names for imported functions or classes."
              parameters[1]{name,type,description}:
                node,ast.ImportFrom,The AST node representing an `from ... import ...` statement.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other methods within the class.
                called_by: This method is implicitly called by the `ast.NodeVisitor`'s traversal mechanism when an `ast.ImportFrom` node is encountered.
            error: null
          - identifier: visit_ClassDef
            description:
              overall: "This method is an override from `ast.NodeVisitor` that processes `ast.ClassDef` nodes. It manages the `self.current_class` context by saving the previous class, setting the current class to the name of the visited class, and then recursively visiting the class's body. After the class body has been visited, it restores the `self.current_class` to its previous value, ensuring correct scope tracking."
              parameters[1]{name,type,description}:
                node,ast.ClassDef,The AST node representing a class definition.
              returns[0]:
              usage_context:
                calls: This method calls `self.generic_visit(node)` to continue the AST traversal into the class's body.
                called_by: This method is implicitly called by the `ast.NodeVisitor`'s traversal mechanism when an `ast.ClassDef` node is encountered.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method is an override from `ast.NodeVisitor` that processes `ast.FunctionDef` nodes. It saves the current function context, constructs a fully qualified name for the new function using `_make_full_name`, and updates `self.local_defs` to map the function's simple name (and class-qualified name if applicable) to its full name. It then sets `self.current_function`, adds the function as a node to the `self.graph`, visits its body, adds the function to `self.function_set`, and finally restores the previous function context."
              parameters[1]{name,type,description}:
                node,ast.FunctionDef,The AST node representing a function definition.
              returns[0]:
              usage_context:
                calls: This method calls `self._make_full_name` to generate a unique identifier for the function and `self.generic_visit(node)` to traverse the function's body.
                called_by: This method is implicitly called by the `ast.NodeVisitor`'s traversal mechanism when an `ast.FunctionDef` node is encountered. It is also explicitly called by `visit_AsyncFunctionDef`.
            error: null
          - identifier: visit_AsyncFunctionDef
            description:
              overall: "This method is an override from `ast.NodeVisitor` that processes `ast.AsyncFunctionDef` nodes. It delegates the processing of asynchronous function definitions directly to the `visit_FunctionDef` method. This approach treats async functions similarly to regular functions for the purpose of call graph construction, ensuring they are added to the graph and their calls are tracked."
              parameters[1]{name,type,description}:
                node,ast.AsyncFunctionDef,The AST node representing an asynchronous function definition.
              returns[0]:
              usage_context:
                calls: This method calls `self.visit_FunctionDef(node)` to handle the actual processing of the function definition.
                called_by: This method is implicitly called by the `ast.NodeVisitor`'s traversal mechanism when an `ast.AsyncFunctionDef` node is encountered.
            error: null
          - identifier: visit_Call
            description:
              overall: "This method is an override from `ast.NodeVisitor` that processes `ast.Call` nodes. It first identifies the current caller using `_current_caller` and then extracts the callee's name components using `_recursive_call`. These components are then resolved into fully qualified names using `_resolve_all_callee_names`. Finally, it adds the resolved callees as edges from the current caller in the `self.edges` dictionary, effectively mapping the call. It then continues AST traversal with `generic_visit`."
              parameters[1]{name,type,description}:
                node,ast.Call,The AST node representing a function call.
              returns[0]:
              usage_context:
                calls: "This method calls `self._current_caller()`, `self._recursive_call(node)`, `self._resolve_all_callee_names([parts])`, and `self.generic_visit(node)`."
                called_by: This method is implicitly called by the `ast.NodeVisitor`'s traversal mechanism when an `ast.Call` node is encountered.
            error: null
          - identifier: visit_If
            description:
              overall: "This method is an override from `ast.NodeVisitor` that processes `ast.If` nodes. It specifically checks for the `if __name__ == \"__main__\":` idiom. If this condition is met, it temporarily sets `self.current_function` to \"<main_block>\" to correctly attribute calls within this block to the main execution scope. After visiting the `if` block, it restores the previous `current_function`. For other `if` statements, it simply continues the generic AST traversal."
              parameters[1]{name,type,description}:
                node,ast.If,The AST node representing an if statement.
              returns[0]:
              usage_context:
                calls: This method calls `self.generic_visit(node)` to continue the AST traversal into the `if` or `else` block.
                called_by: This method is implicitly called by the `ast.NodeVisitor`'s traversal mechanism when an `ast.If` node is encountered.
            error: null
        usage_context:
          dependencies: This class depends on the `ast` module for parsing Python code into an Abstract Syntax Tree and the `networkx` library for creating and managing the directed graph structure.
          instantiated_by: The instantiation points for this class are not provided in the input context.
      error: null
    backend.getRepo.RepoFile:
      identifier: backend.getRepo.RepoFile
      description:
        overall: "The RepoFile class represents a single file within a Git repository, providing a structured way to access its metadata and content. It implements lazy loading for the Git blob object, file content, and size, optimizing performance by deferring expensive operations until data is actually needed. The class also offers utility methods for basic analysis and serialization into a dictionary format."
        init_method:
          description: "This constructor initializes a RepoFile object by storing the file path and the Git Tree object from which the file originates. It also sets up internal attributes (`_blob`, `_content`, `_size`) to `None`, indicating that these properties will be loaded lazily upon first access."
          parameters[2]{name,type,description}:
            file_path,str,The path to the file within the repository.
            commit_tree,git.Tree,"The Tree object of the commit, from which the file originates."
        methods[6]:
          - identifier: blob
            description:
              overall: "This property provides lazy loading for the Git blob object associated with the file. It checks if the `_blob` attribute is already set; if not, it attempts to retrieve the blob from the `_tree` using the stored file path. If the file is not found within the commit tree, a `FileNotFoundError` is raised."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",git.Blob,The Git blob object representing the file.
              usage_context:
                calls: This method accesses `self._tree` and `self.path` to retrieve the blob.
                called_by: This method is called by the `content` and `size` properties to access the underlying Git blob.
            error: null
          - identifier: content
            description:
              overall: "This property provides lazy loading for the file's content. It first checks if the `_content` attribute is already populated. If not, it accesses the `blob` property, reads its data stream, and decodes it into a UTF-8 string, ignoring any decoding errors."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",str,The decoded content of the file as a string.
              usage_context:
                calls: "This method calls the `blob` property to get the Git blob object, and then calls `data_stream.read()` and `decode()` on the blob."
                called_by: This method is called by `analyze_word_count` and `to_dict` when `include_content` is true.
            error: null
          - identifier: size
            description:
              overall: "This property provides lazy loading for the file's size in bytes. It checks if the `_size` attribute is already set. If not, it accesses the `blob` property to retrieve the Git blob object and then extracts its `size` attribute."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",int,The size of the file in bytes.
              usage_context:
                calls: This method calls the `blob` property to get the Git blob object and accesses its `size` attribute.
                called_by: This method is called by `to_dict`.
            error: null
          - identifier: analyze_word_count
            description:
              overall: "This method serves as an example analysis function, demonstrating how to process the file's content. It calculates the total number of words in the file by first retrieving the file's content via the `content` property, then splitting the string by whitespace, and finally counting the resulting elements."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",int,The total number of words in the file content.
              usage_context:
                calls: This method calls the `content` property to retrieve the file's content and then calls the `split()` and `len()` functions.
                called_by: "The input context indicates no explicit callers, suggesting it's an internal utility or exposed for external use."
            error: null
          - identifier: __repr__
            description:
              overall: "This special method provides a developer-friendly string representation of the RepoFile object. It returns a string that includes the class name and the file's path, which is useful for debugging and logging purposes."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",str,"A string representation of the RepoFile object, including its path."
              usage_context:
                calls: This method accesses `self.path`.
                called_by: This method is implicitly called by Python's `repr()` function or when the object is printed in an interactive session.
            error: null
          - identifier: to_dict
            description:
              overall: "This method converts the RepoFile object into a dictionary representation, providing a structured data format. It includes the file's path, its base name, size, and type. Optionally, if `include_content` is set to `True`, the full file content is also added to the dictionary."
              parameters[1]{name,type,description}:
                include_content,bool,A flag indicating whether to include the file's content in the dictionary. Defaults to `False`.
              returns[1]{name,type,description}:
                "null",dict,"A dictionary representation of the file, optionally including its content."
              usage_context:
                calls: "This method calls `os.path.basename`, `self.size` (property), and `self.content` (property)."
                called_by: "The input context indicates no explicit callers, suggesting it's an exposed method for serialization or data transfer."
            error: null
        usage_context:
          dependencies: "This class depends on the `git` library for `git.Tree` and `git.Blob` objects, and the `os` module for path manipulation."
          instantiated_by: "The input context indicates no explicit instantiation points, implying it's likely instantiated by other components within the `backend.getRepo` module or related repository handling logic."
      error: null
    backend.getRepo.GitRepository:
      identifier: backend.getRepo.GitRepository
      description:
        overall: "The GitRepository class provides a robust mechanism for interacting with a Git repository. It handles the cloning of a remote repository into a temporary local directory upon instantiation and offers methods to retrieve all files, organize them into a hierarchical tree structure, and ensure proper cleanup of the temporary directory. It also implements the context manager protocol, allowing for safe and automatic resource management."
        init_method:
          description: "The constructor initializes a GitRepository object by cloning the specified remote Git repository into a newly created temporary directory. It sets up instance attributes for the repository URL, the path to the temporary directory, the GitPython Repo object, the latest commit, and its tree, while also handling potential cloning errors."
          parameters[1]{name,type,description}:
            repo_url,str,The URL of the Git repository to be cloned.
        methods[5]:
          - identifier: get_all_files
            description:
              overall: "This method retrieves a list of all file paths present in the cloned Git repository using the underlying Git command `ls-files`. It then iterates through these paths to create and store `RepoFile` objects in the `self.files` attribute, which are then returned. This effectively populates the repository object with a collection of its constituent files."
              parameters[0]:
              returns[1]{name,type,description}:
                files,"list[RepoFile]","A list of RepoFile instances, each representing a file within the repository."
              usage_context:
                calls: This method calls `self.repo.git.ls_files()` to list files and `RepoFile()` to create file objects.
                called_by: This method is called by `get_file_tree` if the `self.files` attribute is empty.
            error: null
          - identifier: close
            description:
              overall: "This method is responsible for cleaning up the resources used by the GitRepository instance. Specifically, it deletes the temporary directory and all its contents where the Git repository was cloned. It checks if `self.temp_dir` is set before attempting to delete it and then nullifies the `self.temp_dir` attribute."
              parameters[0]:
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other methods or functions within the provided context.
                called_by: This method is called by the `__exit__` context manager method.
            error: null
          - identifier: __enter__
            description:
              overall: "This special method enables the GitRepository object to be used as a context manager. When the object is entered in a `with` statement, this method is implicitly called and simply returns the instance itself, allowing it to be bound to a variable in the `as` clause."
              parameters[0]:
              returns[1]{name,type,description}:
                self,GitRepository,The instance of the GitRepository class itself.
              usage_context:
                calls: This method does not call any other methods or functions.
                called_by: This method is implicitly called when the `GitRepository` object is used in a `with` statement.
            error: null
          - identifier: __exit__
            description:
              overall: "This special method is part of the context manager protocol, ensuring proper resource cleanup when exiting a `with` statement. It calls the `close()` method to delete the temporary repository directory, guaranteeing that resources are released regardless of whether an exception occurred within the `with` block."
              parameters[3]{name,type,description}:
                exc_type,type | None,"The type of the exception that caused the context to be exited, or None if no exception occurred."
                exc_val,Exception | None,"The exception instance that caused the context to be exited, or None."
                exc_tb,TracebackType | None,"The traceback object associated with the exception, or None."
              returns[0]:
              usage_context:
                calls: This method calls `self.close()` to perform cleanup.
                called_by: This method is implicitly called when the `GitRepository` object exits a `with` statement.
            error: null
          - identifier: get_file_tree
            description:
              overall: "This method generates a hierarchical dictionary representation of the repository's file structure, mimicking a file system tree. If the `self.files` attribute is not already populated, it first calls `get_all_files()` to retrieve all repository files. It then iterates through these `RepoFile` objects, parsing their paths to build a nested dictionary where directories and files are represented with their names, types, and child elements. File content can optionally be included in the output."
              parameters[1]{name,type,description}:
                include_content,bool,A flag indicating whether the content of the files should be included in the dictionary representation. Defaults to False.
              returns[1]{name,type,description}:
                tree,dict,A dictionary representing the hierarchical file tree of the repository.
              usage_context:
                calls: "This method calls `self.get_all_files()` if `self.files` is empty, and `file_obj.to_dict()` for each `RepoFile`."
                called_by: This method is not explicitly called by other methods within the provided context.
            error: null
        usage_context:
          dependencies: "This class depends on `tempfile` for temporary directory management, `git.Repo` and `git.GitCommandError` from the GitPython library for Git operations, and `logging` for informational messages. It also implicitly depends on a `RepoFile` class for file representation."
          instantiated_by: This class is not explicitly shown to be instantiated by other components in the provided context.
      error: null
    backend.relationship_analyzer.ProjectAnalyzer:
      identifier: backend.relationship_analyzer.ProjectAnalyzer
      description:
        overall: "The ProjectAnalyzer class is designed to perform static analysis on a Python project to build a comprehensive call graph. It initializes with a project root and maintains internal data structures to store definitions of classes, functions, and methods, as well as the relationships between them. The class orchestrates the process of finding Python files, parsing their Abstract Syntax Trees (ASTs), collecting all defined entities, and then resolving the calls made between these entities to construct a detailed call graph."
        init_method:
          description: "Initializes the ProjectAnalyzer instance by setting the project root, initializing data structures for definitions, call graph, and ASTs, and defining a set of directories to ignore during file traversal."
          parameters[1]{name,type,description}:
            project_root,str,The root directory of the project to be analyzed.
        methods[6]:
          - identifier: analyze
            description:
              overall: "This method orchestrates the entire project analysis process. It first identifies all Python files within the project, then iterates through them to collect definitions of functions, methods, and classes. Subsequently, it iterates through the files again to resolve call relationships between these definitions, building a comprehensive call graph. Finally, it clears the cached ASTs and returns the generated call graph."
              parameters[0]:
              returns[1]{name,type,description}:
                call_graph,defaultdict(list),"A dictionary-like object representing the call graph, where keys are callee identifiers and values are lists of caller information."
              usage_context:
                calls: "This method calls _find_py_files to locate Python files, _collect_definitions to gather definitions, and _resolve_calls to establish call relationships."
                called_by: This method is not explicitly called by any other method within the provided context.
            error: null
          - identifier: get_raw_relationships
            description:
              overall: "This method processes the internal call_graph to generate a structured representation of outgoing and incoming relationships. It iterates through the call graph, extracting caller and callee identifiers, and populates two dictionaries: 'outgoing' (showing what each entity calls) and 'incoming' (showing what calls each entity). The results are then sorted and returned as a dictionary."
              parameters[0]:
              returns[1]{name,type,description}:
                relationships,dict,"A dictionary containing two keys, 'outgoing' and 'incoming'. Each value is a dictionary mapping entity identifiers to a sorted list of related entity identifiers."
              usage_context:
                calls: This method does not explicitly call other methods or functions within the provided context.
                called_by: This method is not explicitly called by any other method within the provided context.
            error: null
          - identifier: _find_py_files
            description:
              overall: "This private helper method recursively traverses the project_root directory to locate all Python files, excluding directories specified in self.ignore_dirs. It uses os.walk to navigate the file system and filters directories and files based on the .py extension and the ignore list. It returns a list of absolute paths to all identified Python files."
              parameters[0]:
              returns[1]{name,type,description}:
                py_files,"list[str]","A list of absolute file paths to Python files found within the project root, excluding ignored directories."
              usage_context:
                calls: This method calls os.walk to traverse the file system and os.path.join to construct file paths.
                called_by: This method is called by the analyze method.
            error: null
          - identifier: _collect_definitions
            description:
              overall: "This private method is responsible for parsing a given Python file and collecting all function, method, and class definitions. It reads the file, parses it into an Abstract Syntax Tree (AST), and then walks the AST to identify FunctionDef and ClassDef nodes. For each identified definition, it determines its full qualified path name and type, storing this information in self.definitions. It also caches the AST in self.file_asts. Error handling is included for file reading or AST parsing issues."
              parameters[1]{name,type,description}:
                filepath,str,The absolute path to the Python file being analyzed.
              returns[0]:
              usage_context:
                calls: "This method calls open, f.read, ast.parse, path_to_module (an external function), ast.walk, isinstance, _get_parent, and logging.error."
                called_by: This method is called by the analyze method.
            error: null
          - identifier: _get_parent
            description:
              overall: This private helper method attempts to find the direct parent AST node of a given node within a larger tree. It iterates through all nodes in the AST and checks their children to identify if any child matches the target node. This is primarily used to determine if a FunctionDef is nested within a ClassDef to correctly identify it as a method.
              parameters[2]{name,type,description}:
                tree,ast.AST,The root of the Abstract Syntax Tree to search within.
                node,ast.AST,The child node for which to find the parent.
              returns[1]{name,type,description}:
                parent_node,ast.AST or None,"The parent AST node if found, otherwise None."
              usage_context:
                calls: This method calls ast.walk and ast.iter_child_nodes.
                called_by: This method is called by the _collect_definitions method.
            error: null
          - identifier: _resolve_calls
            description:
              overall: "This private method processes a given Python file's AST to identify and resolve function and method calls. It retrieves the cached AST for the filepath and then instantiates a CallResolverVisitor (an external class) to traverse the AST and detect calls. The resolved calls, along with their caller information, are then extended into the self.call_graph. Error handling is included for issues during call resolution."
              parameters[1]{name,type,description}:
                filepath,str,The absolute path to the Python file whose calls are to be resolved.
              returns[0]:
              usage_context:
                calls: "This method calls self.file_asts.get, CallResolverVisitor (an external class), resolver.visit, resolver.calls.items, self.call_graph.extend, and logging.error."
                called_by: This method is called by the analyze method.
            error: null
        usage_context:
          dependencies: "The class depends on the os module for path manipulation and file system traversal, the ast module for parsing Python source code, the logging module for error reporting, and collections.defaultdict for efficient graph construction. It also implicitly depends on external components like path_to_module and CallResolverVisitor."
          instantiated_by: This class is not explicitly instantiated by any other component within the provided context.
      error: null
    backend.relationship_analyzer.CallResolverVisitor:
      identifier: backend.relationship_analyzer.CallResolverVisitor
      description:
        overall: "The CallResolverVisitor class is an AST NodeVisitor designed to traverse Python source code and identify all function and method calls. It resolves the fully qualified names of these calls and records detailed information about their callers, including the file, line number, and caller type. This class is crucial for building a comprehensive map of call relationships within a project, enabling dependency analysis and understanding code flow."
        init_method:
          description: "The constructor initializes the CallResolverVisitor with essential context for AST traversal and call resolution. It sets up the file path, project root, and a dictionary of known definitions. It also establishes internal state variables such as `scope` for imports, `instance_types` for tracking object types, and `calls` (a defaultdict) to store the identified call relationships."
          parameters[3]{name,type,description}:
            filepath,str,The absolute path to the Python file currently being analyzed.
            project_root,str,"The root directory of the entire project, used for resolving module paths."
            definitions,dict,"A dictionary containing all known qualified definitions (functions, classes) within the project, used for validating resolved call targets."
        methods[7]:
          - identifier: visit_ClassDef
            description:
              overall: "This method is an AST visitor specifically for `ast.ClassDef` nodes. When a class definition is encountered, it updates the visitor's internal state to reflect the current class name. This ensures that any nested function or method definitions are correctly associated with their containing class. After visiting the class's children, it restores the previous class context."
              parameters[1]{name,type,description}:
                node,ast.ClassDef,The AST node representing the class definition being visited.
              returns[0]:
              usage_context:
                calls: This method calls self.generic_visit to continue the AST traversal for child nodes.
                called_by: This method is called by the ast.NodeVisitor traversal mechanism when a ClassDef node is encountered.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method is an AST visitor for `ast.FunctionDef` nodes, handling both top-level functions and methods within classes. It constructs a fully qualified identifier for the function, incorporating the module and class name if applicable. This identifier is then set as the `current_caller_name` for the duration of the function's traversal, ensuring that any calls made within this function are correctly attributed to it. The previous caller name is restored upon exiting the function definition."
              parameters[1]{name,type,description}:
                node,ast.FunctionDef,The AST node representing the function definition being visited.
              returns[0]:
              usage_context:
                calls: This method calls self.generic_visit to continue the AST traversal for child nodes.
                called_by: This method is called by the ast.NodeVisitor traversal mechanism when a FunctionDef node is encountered.
            error: null
          - identifier: visit_Call
            description:
              overall: "This method is an AST visitor for `ast.Call` nodes, which represent function or method invocations. It attempts to resolve the fully qualified name of the called entity using the `_resolve_call_qname` helper. If the callee's qualified name is successfully resolved and exists in the known definitions, the method records detailed information about the caller, including its file, line number, identifier, and type (module, local function, method, or function). This caller information is then appended to the `calls` dictionary, keyed by the callee's qualified name."
              parameters[1]{name,type,description}:
                node,ast.Call,The AST node representing the function or method call being visited.
              returns[0]:
              usage_context:
                calls: This method calls self._resolve_call_qname to determine the qualified name of the called entity and os.path.basename to extract the base filename.
                called_by: This method is called by the ast.NodeVisitor traversal mechanism when a Call node is encountered.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method is an AST visitor for `ast.Import` nodes, which represent `import module` statements. It processes each imported module alias (or its original name if no alias is used) and stores its fully qualified name in the `self.scope` dictionary. This scope is crucial for resolving subsequent calls to imported modules or their attributes. After processing the import, it continues the generic AST traversal."
              parameters[1]{name,type,description}:
                node,ast.Import,The AST node representing the import statement being visited.
              returns[0]:
              usage_context:
                calls: This method calls self.generic_visit to continue the AST traversal for child nodes.
                called_by: This method is called by the ast.NodeVisitor traversal mechanism when an Import node is encountered.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method is an AST visitor for `ast.ImportFrom` nodes, which represent `from module import name` statements. It resolves the full qualified path for each imported name, correctly handling relative imports based on `node.level`. The resolved qualified names are then stored in the `self.scope` dictionary, making them available for subsequent call resolution. It then proceeds with a generic visit of child nodes."
              parameters[1]{name,type,description}:
                node,ast.ImportFrom,The AST node representing the 'from ... import ...' statement being visited.
              returns[0]:
              usage_context:
                calls: This method calls self.generic_visit to continue the AST traversal for child nodes.
                called_by: This method is called by the ast.NodeVisitor traversal mechanism when an ImportFrom node is encountered.
            error: null
          - identifier: visit_Assign
            description:
              overall: "This method is an AST visitor for `ast.Assign` nodes, focusing on assignments that involve class instantiation. It checks if the assigned value is a call expression where the function being called is a simple name (e.g., `x = MyClass()`). If the called name corresponds to a known class in the `self.scope` and `self.definitions`, it records the qualified class name as the type of the assigned variable in `self.instance_types`. This mapping is essential for resolving method calls on instances later."
              parameters[1]{name,type,description}:
                node,ast.Assign,The AST node representing the assignment statement being visited.
              returns[0]:
              usage_context:
                calls: This method calls self.generic_visit to continue the AST traversal for child nodes.
                called_by: This method is called by the ast.NodeVisitor traversal mechanism when an Assign node is encountered.
            error: null
          - identifier: _resolve_call_qname
            description:
              overall: "This private helper method is responsible for resolving the fully qualified name (QName) of a function or method call. It handles two primary scenarios: direct calls to names (e.g., `func()`) by checking `self.scope` and local definitions, and attribute access calls (e.g., `obj.method()`) by looking up the type of the object in `self.instance_types` or checking `self.scope` for module-level attributes. It returns the resolved QName as a string or `None` if the name cannot be resolved."
              parameters[1]{name,type,description}:
                func_node,ast.expr,"The AST node representing the function or method being called (e.g., ast.Name for direct calls, ast.Attribute for method calls)."
              returns[1]{name,type,description}:
                qualified_name,str | None,"The fully qualified name of the callable if resolved, otherwise None."
              usage_context:
                calls: "This method does not make explicit calls to other methods or functions within the provided context, but relies on dictionary lookups and type checks."
                called_by: This method is called by the visit_Call method to resolve the target of a function or method invocation.
            error: null
        usage_context:
          dependencies: "This class depends on the 'ast' module for Abstract Syntax Tree traversal, the 'os' module for path manipulation, and 'collections.defaultdict' for its internal data structures."
          instantiated_by: This class is not explicitly instantiated by any known entities within the provided context.
      error: null
    schemas.types.ParameterDescription:
      identifier: schemas.types.ParameterDescription
      description:
        overall: "The ParameterDescription class is a Pydantic BaseModel designed to encapsulate the essential metadata for a single parameter within a function. It provides a structured format to consistently represent a parameter's name, its data type, and a descriptive explanation of its role or purpose. This model is fundamental for systems requiring detailed, standardized descriptions of function signatures."
        init_method:
          description: "As a Pydantic BaseModel, the `__init__` method for ParameterDescription is automatically generated. It initializes an instance of the class by accepting values for `name`, `type`, and `description`, which are then validated according to their type hints."
          parameters[3]{name,type,description}:
            name,str,The name of the parameter.
            type,str,The type annotation or inferred type of the parameter.
            description,str,A brief explanation of the parameter's purpose or usage.
        methods[0]:
        usage_context:
          dependencies: "This class relies on `pydantic.BaseModel` for its core functionality, but does not explicitly list other external functional dependencies."
          instantiated_by: The specific locations where this class is instantiated are not provided in the current context.
      error: null
    schemas.types.ReturnDescription:
      identifier: schemas.types.ReturnDescription
      description:
        overall: "The ReturnDescription class is a Pydantic BaseModel designed to provide a structured representation of a function's return value. It encapsulates three key pieces of information: the name of the return value, its Python type, and a textual description. This model is intended to be used as a component within larger data structures for comprehensive function signature documentation, ensuring consistency and validation of return value details."
        init_method:
          description: "The `__init__` method for ReturnDescription is implicitly generated by Pydantic's BaseModel. It initializes an instance of the class by accepting values for `name`, `type`, and `description`, performing automatic data validation and type coercion based on the defined annotations."
          parameters[3]{name,type,description}:
            name,str,"The name or identifier of the return value, if it has one (e.g., for named tuples or specific data fields)."
            type,str,"The Python type hint or a string representation of the return value's type (e.g., 'str', 'int', 'List[str]')."
            description,str,"A detailed textual explanation of what the return value represents, its purpose, or its content."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in its provided context.
          instantiated_by: The provided context does not specify any locations where this class is instantiated.
      error: null
    schemas.types.UsageContext:
      identifier: schemas.types.UsageContext
      description:
        overall: "The UsageContext class is a Pydantic BaseModel designed to encapsulate information about how a function is used within a system. It serves as a structured data container for describing the calling context of a function. This class defines two string attributes, 'calls' and 'called_by', to detail the functions or methods that a given function calls, and the entities that call the given function, respectively."
        init_method:
          description: "The UsageContext class does not define an explicit `__init__` method. It inherits from `pydantic.BaseModel`, and its initialization is handled by Pydantic based on the `calls` and `called_by` fields, which are expected to be provided as keyword arguments during instantiation."
          parameters[0]:
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies.
          instantiated_by: The instantiation points for this class are not explicitly provided in the context.
      error: null
    schemas.types.FunctionDescription:
      identifier: schemas.types.FunctionDescription
      description:
        overall: "The `FunctionDescription` class is a Pydantic BaseModel designed to provide a structured and detailed analysis of a Python function. It serves as a data schema, encapsulating an overall summary of the function's purpose, a list of its parameters, a list of its return values, and its usage context within a larger system. This class is fundamental for representing function metadata in a standardized format."
        init_method:
          description: "This class is a Pydantic BaseModel, meaning its `__init__` method is automatically generated to validate and assign values to its defined fields. It initializes an instance by taking an overall description string, a list of `ParameterDescription` objects, a list of `ReturnDescription` objects, and a `UsageContext` object, ensuring type correctness and data integrity upon instantiation."
          parameters[4]{name,type,description}:
            overall,str,A high-level summary describing the function's purpose and its implementation details.
            parameters,"List[ParameterDescription]","A list of objects, each detailing a parameter of the function, including its name, type, and description."
            returns,"List[ReturnDescription]","A list of objects, each describing a possible return value of the function, including its name, type, and description."
            usage_context,UsageContext,An object providing context on what the function calls and where it is called within the system.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in its provided context.
          instantiated_by: The specific locations where this class is instantiated are not provided in the context.
      error: null
    schemas.types.FunctionAnalysis:
      identifier: schemas.types.FunctionAnalysis
      description:
        overall: "This class serves as a Pydantic model designed to structure the comprehensive analysis of a single Python function. It encapsulates the function's unique identifier, a detailed 'FunctionDescription' object containing its purpose, signature, and usage context, and an optional field for any errors encountered during its analysis. This model is fundamental for standardizing the output of function analysis within a larger system, ensuring a consistent data format for further processing."
        init_method:
          description: "This class, being a Pydantic BaseModel, implicitly generates an __init__ method. This constructor is responsible for initializing instances of FunctionAnalysis by validating and assigning the 'identifier', 'description', and 'error' fields based on the provided arguments, ensuring data integrity according to the defined types."
          parameters[3]{name,type,description}:
            identifier,str,The unique name or identifier of the function being analyzed.
            description,FunctionDescription,"A detailed analysis object containing the function's overall purpose, parameters, return values, and usage context."
            error,"Optional[str]",An optional string describing any error encountered during the function's analysis. Defaults to None.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in the provided context.
          instantiated_by: The specific points of instantiation for this class are not provided in the current context.
      error: null
    schemas.types.ConstructorDescription:
      identifier: schemas.types.ConstructorDescription
      description:
        overall: "The ConstructorDescription class is a Pydantic BaseModel designed to encapsulate metadata about a Python class's constructor (`__init__` method). It structures this information into a textual description of the constructor's purpose and a list detailing each of its parameters. This class serves as a data schema for representing constructor details in a standardized, machine-readable format."
        init_method:
          description: "The `__init__` method, implicitly generated by Pydantic's BaseModel, initializes an instance of ConstructorDescription. It validates and assigns the provided 'description' string and a list of 'ParameterDescription' objects to the corresponding instance attributes. This ensures that constructor metadata conforms to the defined schema."
          parameters[2]{name,type,description}:
            description,str,A string providing a high-level summary or explanation of the constructor's functionality.
            parameters,"List[ParameterDescription]","A list of ParameterDescription objects, each detailing a specific parameter accepted by the constructor."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies.
          instantiated_by: The specific points of instantiation for this class are not provided in the context.
      error: null
    schemas.types.ClassContext:
      identifier: schemas.types.ClassContext
      description:
        overall: "The ClassContext class is a Pydantic BaseModel designed to encapsulate and validate contextual information about other classes. It specifically stores two string attributes: 'dependencies', which summarizes external dependencies, and 'instantiated_by', which describes where the class is instantiated. This model provides a structured way to represent a class's integration points and external requirements within a larger system."
        init_method:
          description: "The `__init__` method for ClassContext is implicitly generated by Pydantic's BaseModel. It handles the initialization of instances by accepting `dependencies` and `instantiated_by` as keyword arguments, performing type validation, and assigning these values as instance attributes."
          parameters[2]{name,type,description}:
            dependencies,str,A string summarizing the external dependencies required by the class being described.
            instantiated_by,str,A string summarizing the locations or components that instantiate the class being described.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly declare external functional dependencies within the provided context.
          instantiated_by: The instantiation points for this class are not specified in the provided context.
      error: null
    schemas.types.ClassDescription:
      identifier: schemas.types.ClassDescription
      description:
        overall: "The ClassDescription class is a Pydantic BaseModel designed to structure a comprehensive analysis of a Python class. It serves as a data container, holding an overall textual description of the class, detailed information about its constructor, a list of analyses for each of its methods, and contextual information regarding its dependencies and where it is instantiated. This model ensures a standardized format for class analysis outputs."
        init_method:
          description: "This class inherits from Pydantic's BaseModel, so its constructor is implicitly generated by Pydantic. It initializes the instance attributes `overall`, `init_method`, `methods`, and `usage_context` based on the provided arguments, performing validation according to their type hints."
          parameters[4]{name,type,description}:
            overall,str,A high-level summary of the class's purpose and functionality.
            init_method,ConstructorDescription,An object containing the description and parameters of the class's constructor.
            methods,"List[FunctionAnalysis]",A list of detailed analyses for each method within the class.
            usage_context,ClassContext,An object providing context about the class's dependencies and instantiation points.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies within the provided context.
          instantiated_by: The specific points of instantiation for this class are not provided in the current context.
      error: null
    schemas.types.ClassAnalysis:
      identifier: schemas.types.ClassAnalysis
      description:
        overall: "The ClassAnalysis class serves as the primary data model for representing a comprehensive analysis of a Python class. It encapsulates the class's unique identifier, a detailed ClassDescription object containing its constructor and method analyses, and an optional error field to report issues during the analysis process. This model is designed to provide a structured, machine-readable output for further processing by other AI systems."
        init_method:
          description: "This class, being a Pydantic BaseModel, has its constructor implicitly generated. It initializes instances with an `identifier` (string), a `description` (ClassDescription object), and an optional `error` (string or None)."
          parameters[3]{name,type,description}:
            identifier,str,The unique name or path of the class being analyzed.
            description,ClassDescription,"A detailed analysis object containing the class's overall purpose, constructor, and methods."
            error,"Optional[str]","An optional string indicating an error encountered during analysis, defaulting to None."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in the provided context.
          instantiated_by: The provided context does not specify where this class is instantiated.
      error: null
    schemas.types.CallInfo:
      identifier: schemas.types.CallInfo
      description:
        overall: "The CallInfo class is a Pydantic BaseModel designed to represent a specific call event within a system, likely for relationship analysis. It provides a structured format to store details such as the file path, the name of the calling function, the mode of the call (e.g., 'method', 'function'), and the line number where the call occurs. This model serves as a data container to standardize how call events are described and tracked."
        init_method:
          description: "The CallInfo class is initialized by providing values for its defined Pydantic fields: 'file', 'function', 'mode', and 'line'. Pydantic automatically handles the constructor logic, including type validation and assignment of these values as instance attributes."
          parameters[4]{name,type,description}:
            file,str,The path to the file where the call event originated.
            function,str,The name of the function or method that performed the call.
            mode,str,"The classification of the call, such as 'method', 'function', or 'module'."
            line,int,The specific line number within the file where the call event is located.
        methods[0]:
        usage_context:
          dependencies: This class primarily depends on Pydantic's BaseModel for its data structure and validation capabilities.
          instantiated_by: The instantiation points for this class are not explicitly provided in the current context.
      error: null
    schemas.types.FunctionContextInput:
      identifier: schemas.types.FunctionContextInput
      description:
        overall: "The FunctionContextInput class is a Pydantic BaseModel designed to encapsulate structured contextual information for analyzing a specific function. It serves as a data container, holding details about the external functions, methods, or classes that the target function invokes, as well as the locations from which the target function itself is called. This model facilitates a standardized way to represent a function's interaction within a larger codebase, aiding in static analysis or documentation generation."
        init_method:
          description: "This class does not explicitly define an __init__ method. Pydantic's BaseModel automatically generates a constructor based on the type-hinted attributes `calls` and `called_by`, allowing instances to be created by passing these values as keyword arguments."
          parameters[2]{name,type,description}:
            calls,"List[str]","A list of strings representing the identifiers of other functions, methods, or classes that this function calls."
            called_by,"List[CallInfo]",A list of CallInfo objects indicating where this function is called from.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in the provided context.
          instantiated_by: The provided context does not specify where this class is instantiated.
      error: null
    schemas.types.FunctionAnalysisInput:
      identifier: schemas.types.FunctionAnalysisInput
      description:
        overall: "The FunctionAnalysisInput class is a Pydantic BaseModel designed to define the structured input required for generating a FunctionAnalysis object. It serves as a data contract, ensuring that all necessary components for analyzing a function‚Äîsuch as its mode, identifier, source code, imports, and contextual information‚Äîare provided and properly typed. This class acts as a schema for data validation and parsing before a function analysis can proceed."
        init_method:
          description: "This class is a Pydantic BaseModel, meaning its `__init__` method is implicitly generated by Pydantic. It initializes instances by validating and assigning values to its defined fields: `mode`, `identifier`, `source_code`, `imports`, and `context`."
          parameters[0]:
        methods[0]:
        usage_context:
          dependencies: "This class primarily depends on Pydantic's `BaseModel` for its data validation and serialization capabilities, as well as `typing.Literal` and `typing.List` for type hinting its fields. No other explicit functional dependencies are listed in the provided context."
          instantiated_by: "The context does not explicitly list where this class is instantiated. Typically, Pydantic models like this are instantiated when parsing incoming data (e.g., from an API request body or a configuration file) to ensure it conforms to the defined schema."
      error: null
    schemas.types.MethodContextInput:
      identifier: schemas.types.MethodContextInput
      description:
        overall: "The `MethodContextInput` class is a Pydantic BaseModel designed to structure and hold contextual information about a specific method within a class. It defines fields such as the method's identifier, a list of entities it calls, a list of entities that call it, its arguments, and its docstring. This model serves as a standardized data structure for method-level analysis."
        init_method:
          description: "As a Pydantic BaseModel, the `__init__` method for `MethodContextInput` is implicitly generated by Pydantic. It handles the validation and assignment of values to the class's fields upon instantiation, ensuring type correctness and data integrity for the method's contextual information."
          parameters[5]{name,type,description}:
            identifier,str,The unique name or identifier of the method being described.
            calls,"List[str]","A list of identifiers for other methods, classes, or functions that this method calls within its implementation."
            called_by,"List[CallInfo]",A list of `CallInfo` objects (presumably another structured type) indicating the methods or functions that call this method.
            args,"List[str]",A list of argument names that the method expects in its signature.
            docstring,"Optional[str]","The docstring content of the method, if one is provided; otherwise, it can be null."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies within the provided context.
          instantiated_by: This class is not explicitly shown to be instantiated by any other components within the provided context.
      error: null
    schemas.types.ClassContextInput:
      identifier: schemas.types.ClassContextInput
      description:
        overall: "The ClassContextInput class is a Pydantic BaseModel designed to encapsulate structured context information necessary for analyzing a Python class. It serves as a data container, holding lists of external dependencies, points of instantiation, and detailed context for each method within the class. This model facilitates the organized transfer and processing of contextual data for deeper code analysis."
        init_method:
          description: "The constructor for ClassContextInput is implicitly generated by Pydantic's BaseModel. It initializes the instance attributes 'dependencies', 'instantiated_by', and 'method_context' based on the provided arguments, ensuring type validation and data integrity."
          parameters[3]{name,type,description}:
            dependencies,"List[str]",A list of strings representing external dependencies relevant to the class being analyzed.
            instantiated_by,"List[CallInfo]",A list of CallInfo objects indicating where this class is instantiated within the codebase.
            method_context,"List[MethodContextInput]","A list of MethodContextInput objects, each providing specific context for an individual method within the class."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly declare any external functional dependencies within the provided context.
          instantiated_by: This class is not explicitly shown to be instantiated by any other components in the provided context.
      error: null
    schemas.types.ClassAnalysisInput:
      identifier: schemas.types.ClassAnalysisInput
      description:
        overall: "The ClassAnalysisInput class is a Pydantic model designed to define the structured input required for generating a ClassAnalysis object. It serves as a data transfer object, ensuring that all necessary information‚Äîsuch as the operation mode, class identifier, source code, relevant imports, and contextual data‚Äîis provided in a consistent format for subsequent analysis. This model facilitates robust data validation and parsing for the class analysis pipeline."
        init_method:
          description: "This class does not explicitly define an __init__ method. It inherits from pydantic.BaseModel, and its constructor is implicitly generated by Pydantic based on the defined fields, allowing instantiation by providing values for `mode`, `identifier`, `source_code`, `imports`, and `context`."
          parameters[5]{name,type,description}:
            mode,"Literal[\"class_analysis\"]","Specifies the operation mode, which must be 'class_analysis'."
            identifier,str,The unique name or identifier of the class being analyzed.
            source_code,str,The raw source code of the entire class definition.
            imports,"List[str]",A list of import statements relevant to the source file.
            context,ClassContextInput,Additional contextual information for the class analysis.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies within its provided context.
          instantiated_by: The specific points where this class is instantiated are not provided in the current context.
      error: null