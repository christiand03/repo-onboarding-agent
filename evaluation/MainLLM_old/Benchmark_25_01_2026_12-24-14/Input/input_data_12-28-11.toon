basic_info:
  projekt_uebersicht:
    titel: analytics-application-development-uni documentation
    beschreibung: Information not found
    aktueller_status: Information not found
    key_features: Information not found
    tech_stack: Information not found
  installation:
    dependencies: "- altair==5.5.0\n- duckdb==1.4.3\n- evidently==0.7.20\n- ipython==8.12.3\n- numpy==2.3.5\n- pandas==2.3.3\n- sentence-transformers==5.2.0\n- streamlit==1.51.0\n- streamlit-option-menu==0.4.0\n- torch==2.9.1+cu126"
    setup_anleitung: Information not found
    quick_start_guide: Information not found
file_tree:
  name: root
  type: directory
  children[17]:
    - path: .DS_Store
      name: .DS_Store
      size: 6148
      type: file
    - path: .gitignore
      name: .gitignore
      size: 370
      type: file
    - name: .streamlit
      type: directory
      children[1]{path,name,size,type}:
        .streamlit/config.toml,config.toml,143,file
    - path: Notebook.ipynb
      name: Notebook.ipynb
      size: 2557414
      type: file
    - name: Notizen
      type: directory
      children[3]{path,name,size,type}:
        Notizen/Analytics Application Development.md,Analytics Application Development.md,4491,file
        Notizen/Dokumentation AAD.md,Dokumentation AAD.md,36106,file
        Notizen/Meeting_17-11-25.md,Meeting_17-11-25.md,4528,file
    - name: app_pages
      type: directory
      children[6]{path,name,size,type}:
        app_pages/__init__.py,__init__.py,0,file
        app_pages/page1.py,page1.py,14979,file
        app_pages/page2.py,page2.py,9792,file
        app_pages/page3.py,page3.py,17415,file
        app_pages/page4.py,page4.py,15184,file
        app_pages/page5.py,page5.py,6090,file
    - name: assets
      type: directory
      children[2]{path,name,size,type}:
        assets/favicon.png,favicon.png,10205,file
        assets/logo.png,logo.png,37579,file
    - path: build_db.py
      name: build_db.py
      size: 14622
      type: file
    - path: dashboard.py
      name: dashboard.py
      size: 12967
      type: file
    - path: data_cleaning.py
      name: data_cleaning.py
      size: 6790
      type: file
    - path: data_drift_metrics.py
      name: data_drift_metrics.py
      size: 4998
      type: file
    - path: data_exploration.py
      name: data_exploration.py
      size: 2586
      type: file
    - path: data_exploration2.py
      name: data_exploration2.py
      size: 4245
      type: file
    - path: db_dashboard.py
      name: db_dashboard.py
      size: 11050
      type: file
    - path: metrics.py
      name: metrics.py
      size: 39122
      type: file
    - path: readme.md
      name: readme.md
      size: 5392
      type: file
    - path: requirements.txt
      name: requirements.txt
      size: 183
      type: file
ast_schema:
  files:
    "app_pages/page1.py":
      ast_nodes:
        imports[3]: streamlit,pandas,altair
        functions[2]:
          - mode: function_analysis
            identifier: app_pages.page1.show_page
            name: show_page
            args[6]: metrics_df1,metrics_df2,metrics_combined,pot_df,comparison_df,issues_df
            docstring: null
            source_code: "def show_page(metrics_df1, metrics_df2, metrics_combined, pot_df, comparison_df, issues_df):\n\n    # Helperfunction to get delta from comparison_df\n    def get_delta(metric_name):\n        if comparison_df is None or comparison_df.empty:\n            return None\n        \n        row = comparison_df[comparison_df['Metric'] == metric_name]\n        \n        if not row.empty:\n            val = row.iloc[0]['Percent_Change']\n            return f\"{val:+.2f}%\"\n        return None\n    \n\n    # --- KPI-BEREICH ---\n    kpi_cols = st.columns(8)\n\n    row_count_df1 = metrics_df1.get(\"row_count\", pd.NA)\n    row_count_df2 = metrics_df2.get(\"row_count\", pd.NA)\n    null_rows_df1 = metrics_df1.get(\"null_ratio_rows\", pd.NA)\n    null_rows_df2 = metrics_df2.get(\"null_ratio_rows\", pd.NA)\n    proforma_count = metrics_df1.get(\"proforma_belege_count\", pd.NA)\n    empty_orders = metrics_df1.get(\"empty_orders_count\", 0)\n\n    kva_unique = metrics_combined.get(\"kvarechnung_id_is_unique\", None)\n    kva_nr_land_unique = metrics_combined.get(\"kvarechnung_nummer_land_is_unique\", None)\n    pos_unique = metrics_combined.get(\"position_id_is_unique\", None)\n\n    total_issues = issues_df[\"overall_issues\"] if issues_df is not None else 0\n\n    with kpi_cols[0]:\n        st.metric(label=\"Auffälligkeiten\", value=f\"{total_issues:,}\".replace(\",\", \".\"), help=\"Alle Fehler und Warnings im Datensatz\", delta=get_delta(\"overall_issues\"), delta_color=\"inverse\")\n    with kpi_cols[1]:\n        st.metric(label=\"Aufträge (df)\", value=f\"{row_count_df1:,}\".replace(\",\", \".\"), help=\"Anzahl Zeilen in Auftragsdaten (df).\", delta=get_delta(\"count_total_orders\"), delta_color=\"off\")\n    with kpi_cols[2]:\n        st.metric(label=\"Positionen (df2)\", value=f\"{row_count_df2:,}\".replace(\",\", \".\"), help=\"Anzahl Zeilen in Positionsdaten (df2).\", delta=get_delta(\"count_total_positions\"), delta_color=\"off\")\n    with kpi_cols[3]:\n        st.metric(label=\"Null-Anteil (Auftragsdaten)\", value=f\"{null_rows_df1:.2f}%\", help=\"Anteil der Zeilen mit mindestens einem Nullwert (außer PLZ) in Auftragsdaten.\", delta=get_delta(\"null_row_ratio_orders\"), delta_color=\"inverse\")\n    with kpi_cols[4]:\n        st.metric(label=\"Null-Anteil (Positionsdaten)\", value=f\"{null_rows_df2:.2f}%\", help=\"Anteil der Zeilen mit mindestens einem Nullwert (außer Bezeichnung) in Positionsdaten.\", delta=get_delta(\"null_row_ratio_positions\"), delta_color=\"inverse\")\n    with kpi_cols[5]:\n        st.metric(label=\"Proforma‑Belege\", value=f\"{proforma_count:,}\".replace(\",\", \".\"), help=\"Anzahl Aufträge mit Einigung_Netto zwischen 0,01 und 1 €. Details sind unter Plausibilitätscheck.\", delta=get_delta(\"count_proforma_receipts\"), delta_color=\"inverse\")\n    with kpi_cols[6]:\n        st.metric(label=\"Aufträge ohne Pos.\", value=f\"{empty_orders:,}\".replace(\",\", \".\"), help=\"Anzahl der Aufträge, denen keine Positionen zugeordnet sind (PositionsAnzahl ist leer). Details sind unter Plausibilitätscheck.\", delta=get_delta(\"count_empty_orders\"), delta_color=\"inverse\")\n    with kpi_cols[7]:\n        if kva_unique and kva_nr_land_unique and pos_unique:\n            text = \"True\"\n        else:\n            text = \"False\"\n\n\n        tooltip = (\n            \"Prüft Einzigartigkeit von:\\n\"\n            f\"- ({kva_unique}) KvaRechnung_ID (Auftragsdaten)\\n\"\n            f\"- ({kva_nr_land_unique}) KvaRechnung_Nummer pro Land (Auftragsdaten)\\n\"\n            f\"- ({pos_unique}) Position_ID (Positionsdaten)\"\n        )\n\n        st.metric(\n            label=\"Eindeutigkeit IDs\",\n            value=text,\n            help=tooltip\n        )\n\n    st.markdown(\"<div style='margin-top: 1rem;'></div>\", unsafe_allow_html=True)\n\n\n    # CHART-BEREICH\n    chart_col1, chart_col2 = st.columns(2)\n\n    # Chart 1: Nullwerte je Spalte (Top N) aus df\n    with chart_col1:\n        st.subheader(\"Nullwerte je Spalte (Top N)\")\n\n        null_ratio_cols = metrics_df1.get(\"null_ratio_cols\", None)\n\n        null_df = None\n        if isinstance(null_ratio_cols, dict) and len(null_ratio_cols) > 0:\n            tmp = pd.DataFrame(list(null_ratio_cols.items()), columns=[\"Spalte\", \"Nullquote_%\"])  # altes Format\n            null_df = tmp.copy()\n        elif isinstance(null_ratio_cols, pd.DataFrame) and not null_ratio_cols.empty:\n            # erwartete Spalten im neuen Format: 'index' (Spaltenname) und 'null_ratio' (in %)\n            df_tmp = null_ratio_cols.copy()\n            # Fallback: versuche sinnvolle Spalten zu finden\n            col_name_col = None\n            ratio_col = None\n            for cand in [\"index\", \"column\", \"column_name\", \"Spalte\"]:\n                if cand in df_tmp.columns:\n                    col_name_col = cand\n                    break\n            for cand in [\"null_ratio\", \"Nullquote_%\", \"ratio\", \"nullratio\"]:\n                if cand in df_tmp.columns:\n                    ratio_col = cand\n                    break\n            if col_name_col is None:\n                # Wenn reset_index() verwendet wurde, kann der Spaltenname in der ersten Spalte ohne Namen liegen\n                if df_tmp.columns.size >= 1:\n                    col_name_col = df_tmp.columns[0]\n            if ratio_col is None and df_tmp.columns.size >= 2:\n                # Heuristik: zweite Spalte nehmen\n                ratio_col = df_tmp.columns[1]\n\n            if col_name_col is not None and ratio_col is not None:\n                null_df = df_tmp[[col_name_col, ratio_col]].rename(columns={col_name_col: \"Spalte\", ratio_col: \"Nullquote_%\"})\n\n        if isinstance(null_df, pd.DataFrame) and not null_df.empty:\n            # Slider konfigurieren\n            max_n = int(min(30, len(null_df))) if len(null_df) > 0 else 5\n            default_n = int(min(10, len(null_df))) if len(null_df) > 0 else 5\n            n = st.slider(\"Anzahl Spalten (Top N)\", min_value=5, max_value=max(5, max_n), value=max(5, default_n), key=\"p1_topn\")\n\n            # Sortieren und Top N auswählen\n            # Stelle sicher, dass die Quote numerisch ist\n            null_df = null_df.copy()\n            null_df[\"Nullquote_%\"] = pd.to_numeric(null_df[\"Nullquote_%\"], errors=\"coerce\")\n            null_df = null_df.dropna(subset=[\"Nullquote_%\"]).sort_values(\"Nullquote_%\", ascending=False).head(n)\n\n            bar = (\n                alt.Chart(null_df)\n                .mark_bar()\n                .encode(\n                    x=alt.X(\"Nullquote_%:Q\", title=\"Nullquote [%]\"),\n                    y=alt.Y(\"Spalte:N\", sort='-x', title=\"Spalte\"),\n                    tooltip=[\"Spalte\", alt.Tooltip(\"Nullquote_%:Q\", format=\".2f\")]\n                )\n                .properties(height=28 * len(null_df), width=\"container\")\n            )\n            st.altair_chart(bar, width=\"stretch\")\n        else:\n            st.info(\"Keine Nullwert-Informationen verfügbar.\")\n\n    # Chart 2: Fehlerhäufigkeit nach Wochentag und Stunde (Heatmap)\n    with chart_col2:\n        st.subheader(\"Fehlerquote nach Wochentag und Stunde\")\n        st.caption(\"Diese Heatmap visualisiert Konzentrationen von Fehlern im Zeitverlauf. Je intensiver der Rotton, desto höher war die prozentuale Fehlerquote am jeweiligen Wochentag zu der entsprechenden Uhrzeit.\")\n        err_df = metrics_df1.get(\"error_frequency_weekday_hour\", None)\n        if isinstance(err_df, pd.DataFrame) and not err_df.empty:\n            weekday_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n            if \"weekday\" in err_df.columns:\n                err_df[\"weekday\"] = pd.Categorical(err_df[\"weekday\"], categories=weekday_order, ordered=True)\n\n            heat = (\n                alt.Chart(err_df)\n                .mark_rect()\n                .encode(\n                    x=alt.X(\"hour:O\", title=\"Stunde\"),\n                    y=alt.Y(\"weekday:O\", sort=weekday_order, title=\"Wochentag\"),\n                    color=alt.Color(\"error_rate:Q\", title=\"Fehlerquote [%]\", scale=alt.Scale(scheme=\"reds\")),\n                    tooltip=[\n                        alt.Tooltip(\"weekday:N\", title=\"Wochentag\"),\n                        alt.Tooltip(\"hour:Q\", title=\"Stunde\"),\n                        alt.Tooltip(\"total_rows:Q\", title=\"Anzahl Aufträge\"),\n                        alt.Tooltip(\"error_rows:Q\", title=\"Fehlerfälle\"),\n                        alt.Tooltip(\"error_rate:Q\", title=\"Fehlerquote\", format=\".2f\")\n                    ]\n                )\n                .properties(height=240, width=\"container\")\n            )\n            st.altair_chart(heat, width=\"stretch\")\n            st.caption(\"Hinweis: PLZ VN wird von OCR nicht gesetzt, daher extrem hohe Fehlerquoten an Wochenenden und nachts.\")\n\n        else:\n            st.info(\"Keine Fehlerfrequenz-Daten verfügbar.\")\n\n\n    st.markdown(\"<div style='margin-top: 1rem;'></div>\", unsafe_allow_html=True)\n\n\n\n# Chart 3: Avg. Positionen pro Auftrag über Monat (Trend)\n    st.subheader(\"Positionen pro Auftrag über Zeit\")\n\n    if isinstance(pot_df, pd.DataFrame) and not pot_df.empty and {\n        \"Zeitperiode\", \"Avg_Positionen_pro_Auftrag\", \"Total_Positionen\", \"Anzahl_Auftraege\"\n    }.issubset(pot_df.columns):\n        \n        # 1. Daten vorbereiten (Datum konvertieren)\n        work = pot_df.copy()\n        try:\n            work[\"Monat\"] = pd.to_datetime(work[\"Zeitperiode\"] + \"-01\", errors=\"coerce\")\n        except Exception:\n            work[\"Monat\"] = pd.to_datetime(work[\"Zeitperiode\"], errors=\"coerce\")\n        \n        work = work.dropna(subset=[\"Monat\"]).sort_values(\"Monat\").reset_index(drop=True)\n\n        if not work.empty:\n            min_date = work[\"Monat\"].min().date()\n            max_date = work[\"Monat\"].max().date()\n\n            # 2. Zeitraum-Slider einfügen\n            # Wir nutzen st.slider mit value=(min, max) für einen Bereich\n            col_filter, _ = st.columns([2, 1])\n            with col_filter:\n                selected_range = st.slider(\n                    \"Zeitraum eingrenzen:\",\n                    min_value=min_date,\n                    max_value=max_date,\n                    value=(min_date, max_date),\n                    format=\"MMM YYYY\"\n                )\n\n            # 3. Daten filtern\n            mask = (work[\"Monat\"].dt.date >= selected_range[0]) & (work[\"Monat\"].dt.date <= selected_range[1])\n            filtered_work = work.loc[mask].reset_index(drop=True)\n\n            if not filtered_work.empty:\n                # Ruhige Linie ohne Marker, dunkles Theme\n                base = alt.Chart(filtered_work).encode(\n                    x=alt.X(\"Monat:T\", title=\"Monat\")\n                )\n\n                # Y‑Achse dezent\n                y_enc = alt.Y(\n                    \"Avg_Positionen_pro_Auftrag:Q\",\n                    title=\"Ø Positionen pro Auftrag\",\n                    scale=alt.Scale(zero=False, nice=True)\n                )\n\n                line = base.mark_line(color=\"#7c3aed\", strokeWidth=2).encode(\n                    y=y_enc,\n                    tooltip=[\n                        alt.Tooltip(\"yearmonth(Monat):T\", title=\"Monat\"),\n                        alt.Tooltip(\"Avg_Positionen_pro_Auftrag:Q\", title=\"Ø Pos./Auftrag\", format=\".2f\"),\n                        alt.Tooltip(\"Anzahl_Auftraege:Q\", title=\"Anzahl Aufträge\", format=\",\"),\n                        alt.Tooltip(\"Total_Positionen:Q\", title=\"Summe Positionen\", format=\",\")\n                    ]\n                )\n\n                # Relevante Punkte: letztes, Maximum, Minimum innerhalb der Auswahl\n                last_idx = len(filtered_work) - 1\n                min_idx = int(filtered_work[\"Avg_Positionen_pro_Auftrag\"].idxmin())\n                max_idx = int(filtered_work[\"Avg_Positionen_pro_Auftrag\"].idxmax())\n\n                highlight_idx = sorted(set([i for i in [last_idx, min_idx, max_idx] if i is not None]))\n                highlights = filtered_work.iloc[highlight_idx].copy()\n\n                highlight_layer = (\n                    alt.Chart(highlights)\n                    .mark_point(filled=True, size=80, color=\"#e5e7eb\", fill=\"#e5e7eb\", opacity=0.9)\n                    .encode(x=\"Monat:T\", y=y_enc)\n                )\n\n                # Labels nur für Max/Min/Letzter\n                if len(highlights) > 0:\n                    highlights[\"Label\"] = highlights.apply(\n                        lambda r: \"Max\" if r.name == max_idx else (\"Min\" if r.name == min_idx else \"Letzter Wert\"), axis=1\n                    )\n                \n                label_layer = (\n                    alt.Chart(highlights)\n                    .mark_text(dy=-10, color=\"#9ca3af\", fontSize=11)\n                    .encode(x=\"Monat:T\", y=y_enc, text=\"Label:N\")\n                )\n\n                chart = (line + highlight_layer + label_layer).properties(height=320, width=\"container\")\n                st.altair_chart(chart, width=\"stretch\")\n\n                # --- Insight-Text Logik (angepasst auf Auswahl) ---\n                trend_text = \"\"\n                trend_metric = None\n                \n                # Wir vergleichen Start der Auswahl vs. Ende der Auswahl\n                if len(filtered_work) >= 2:\n                    start_val = filtered_work.iloc[0][\"Avg_Positionen_pro_Auftrag\"]\n                    last_val = filtered_work.iloc[-1][\"Avg_Positionen_pro_Auftrag\"]\n                    \n                    if pd.notna(start_val) and start_val != 0 and pd.notna(last_val):\n                        delta_pct = (last_val - start_val) / start_val * 100\n                        trend_metric = delta_pct\n                        abs_delta = abs(delta_pct)\n\n                        months_diff = (filtered_work.iloc[-1][\"Monat\"] - filtered_work.iloc[0][\"Monat\"]).days // 30\n                        \n                        zeitraum_str = f\"im gewählten Zeitraum ({months_diff} Monate)\"\n\n                        if abs_delta < 2:\n                            trend_text = f\"Stabiler Verlauf {zeitraum_str}\"\n                        elif abs_delta < 8:\n                            trend_text = \"Leichter \" + (\"Anstieg\" if delta_pct > 0 else \"Rückgang\") + f\" {zeitraum_str}\"\n                        else:\n                            trend_text = \"Deutlicher \" + (\"Anstieg\" if delta_pct > 0 else \"Rückgang\") + f\" {zeitraum_str}\"\n\n                if trend_text == \"\":\n                    trend_text = \"Zeitraum zu kurz für Trendanalyse\"\n\n                # Ausgabe: Insight und Trendkennzahl\n                st.caption(trend_text)\n                if trend_metric is not None:\n                    sign = \"+\" if trend_metric >= 0 else \"\"\n                    # Farbe basierend auf Vorzeichen (optional)\n                    color = \"green\" if trend_metric > 0 else \"red\" if trend_metric < 0 else \"gray\"\n                    st.markdown(f\"Veränderung Auswahl: :{color}[{sign}{trend_metric:.1f}%]\")\n\n            else:\n                st.warning(\"Keine Daten im gewählten Zeitraum verfügbar.\")\n        else:\n            st.info(\"Datensatz enthält keine gültigen Zeitangaben.\")\n    else:\n        st.info(\"Keine Zeitreihendaten zu Positionen/Auftrag verfügbar.\")"
            start_line: 5
            end_line: 304
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: app_pages.page1.get_delta
            name: get_delta
            args[1]: metric_name
            docstring: null
            source_code: "def get_delta(metric_name):\n        if comparison_df is None or comparison_df.empty:\n            return None\n        \n        row = comparison_df[comparison_df['Metric'] == metric_name]\n        \n        if not row.empty:\n            val = row.iloc[0]['Percent_Change']\n            return f\"{val:+.2f}%\"\n        return None"
            start_line: 8
            end_line: 17
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "app_pages/page2.py":
      ast_nodes:
        imports[3]: streamlit,pandas,altair
        functions[3]:
          - mode: function_analysis
            identifier: app_pages.page2.show_page
            name: show_page
            args[5]: metrics_df1,metrics_df2,metrics_combined,comparison_df,issues_df
            docstring: "This function renders page 2 of 5 of the dashboard. \n\nPage 2 visualizes metrics concerning the data quality of numeric column data.\nFor a detailed list, refer to the 'Notes' section.\n\nParameters\n----------\nmetrics_df1 : pandas.DataFrame\n    DataFrame containing values for all metrics concerning order data only\nmetrics_df2 : pandas.DataFrame\n    DataFrame containing values for all metrics concerning position data only\nmetrics_combined : pandas.DataFrame\n    DataFrame containing values for all metrics concerning order and position data\ncomparison_df : pandas.DataFrame\n    DataFrame with metric value changes over time\nissues_df : bool\n    DataFrame containing values for all metrics concerning potentially invalid data points\nReturns\n-------\nvoid        \n\nNotes\n-----\nThis page contains the following information:\n\n- KPIs\n    - numerical anomalies, total count and ratio\n    - errors in 'current value' data , ratio to orders\n    - count of orders valued over 50k\n    - check if order valuations match sum of associated positions\n    - trend data for KPIs\n- Tables:\n    - all rows with suspicous/incorrect data as table, with optional .csv download option\n        - Current Values\n        - Discrepancies in order valuation and position sum\n        - orders exceeding 50k total value\n    \n    "
            source_code: "def show_page(metrics_df1, metrics_df2, metrics_combined, comparison_df, issues_df):\n    \"\"\"This function renders page 2 of 5 of the dashboard. \n    \n    Page 2 visualizes metrics concerning the data quality of numeric column data.\n    For a detailed list, refer to the 'Notes' section.\n\n    Parameters\n    ----------\n    metrics_df1 : pandas.DataFrame\n        DataFrame containing values for all metrics concerning order data only\n    metrics_df2 : pandas.DataFrame\n        DataFrame containing values for all metrics concerning position data only\n    metrics_combined : pandas.DataFrame\n        DataFrame containing values for all metrics concerning order and position data\n    comparison_df : pandas.DataFrame\n        DataFrame with metric value changes over time\n    issues_df : bool\n        DataFrame containing values for all metrics concerning potentially invalid data points\n    Returns\n    -------\n    void        \n\n    Notes\n    -----\n    This page contains the following information:\n    \n    - KPIs\n        - numerical anomalies, total count and ratio\n        - errors in 'current value' data , ratio to orders\n        - count of orders valued over 50k\n        - check if order valuations match sum of associated positions\n        - trend data for KPIs\n    - Tables:\n        - all rows with suspicous/incorrect data as table, with optional .csv download option\n            - Current Values\n            - Discrepancies in order valuation and position sum\n            - orders exceeding 50k total value\n        \n        \"\"\"\n    # Helperfunction to get delta from comparison_df\n    def get_delta(metric_name):\n        if comparison_df is None or comparison_df.empty:\n            return None\n        \n        row = comparison_df[comparison_df['Metric'] == metric_name]\n        \n        if not row.empty:\n            val = row.iloc[0]['Percent_Change']\n            return f\"{val:+.2f}%\"\n        return None\n\n    zeitwert_error_df = metrics_df1.get(\"zeitwert_error_df\", pd.NA)\n    zeitwert_error_count = metrics_df1.get(\"zeitwert_errors_count\", pd.NA)\n    above_50k_df = metrics_df1.get(\"above_50k_df\", pd.NA)\n    above_50k_count = len(above_50k_df)\n    row_count = metrics_df1.get(\"row_count\")\n    row_count_df2 = metrics_df2.get(\"row_count\")\n    auftraege_abgleich = metrics_combined.get(\"auftraege_abgleich\")\n    numeric_issues = issues_df[\"numeric_issues\"]\n\n    anteil_zeitwert = (zeitwert_error_count / row_count) * 100 \n    anteil_above_50k = (above_50k_count / row_count) * 100 \n    anteil_summe = (auftraege_abgleich.shape[0] / row_count) * 100 \n    anteil_numeric_issues = (numeric_issues / (row_count + row_count_df2)) * 100\n\n    # --- KPIs ---\n    kpi_cols = st.columns(4)\n    with kpi_cols[0]: \n        st.metric(label=\"Numerische Auffälligkeiten\", value=f\"{numeric_issues:,}\".replace(\",\", \".\"), delta=get_delta(\"numeric_issues\"), delta_color=\"inverse\", help=\"Numerische Fehler und Warnings im Datensatz\")\n        st.caption(f\"Anteil: {anteil_numeric_issues:.2f}% an beiden Datensätzen\")\n    with kpi_cols[1]: \n        st.metric(label=\"Fehleranzahl Zeitwerte\", value=f\"{zeitwert_error_count:,}\".replace(\",\", \".\"), delta=get_delta(\"count_zeitwert_errors\"), delta_color=\"inverse\", help=\"Anzahl der Fehler in der Zeitwertspalte\")\n        st.caption(f\"Anteil: {anteil_zeitwert:.2f}% der Auftragsdaten\")\n    with kpi_cols[2]: \n        st.metric(label=\"Anzahl Aufträge über 50.000€\", value=f\"{above_50k_count:,}\".replace(\",\", \".\"), delta=get_delta(\"count_above_50k\"), delta_color=\"inverse\", help=\"Anzahl der Aufträge mit einem Wert über 50.000€\")\n        st.caption(f\"Anteil: {anteil_above_50k:.2f}% der Auftragsdaten\")\n    with kpi_cols[3]: \n        st.metric(label=\"Abweichung Summen\", value=f\"{auftraege_abgleich.shape[0] if auftraege_abgleich is not None else 0:,}\".replace(\",\", \".\"), delta=get_delta(\"count_abweichung_summen\"), delta_color=\"inverse\", help=\"Anzahl der Aufträge mit Abweichungen in den Summen (Auftragssumme = Summe der Positionen)\")\n        st.caption(f\"Anteil: {anteil_summe:.2f}% der Auftragsdaten\")\n    st.markdown(\"---\")\n\n    st.subheader(\"Fehlerverlauf im Vergleich\")\n    \n    # --- Trends for KPIs ---\n    def prepare_trend_data(df, label, time_col=\"CRMEingangszeit\"):\n        \"\"\"Helper function grouping values of passed df into monthly intervalls.\n\n        Parameters\n        ----------\n        df : pandas.DataFrame\n            DataFrame containing computed metric values over time.\n        label : string\n            label for aggregated data, written to column 'Kategorie' of the returned df\n        time_col : str, optional\n            label of the timestamp column in df, by default \"CRMEingangszeit\"\n\n        Returns\n        -------\n        pandas.DataFrame\n            df with metric values aggregated by month. Has a column 'Kategorie' for labels. This returns an empty df if no timestamp was passed.\n            \n        \"\"\"\n        if df is None or df.empty or time_col not in df.columns:\n            return pd.DataFrame()\n        \n        temp = df.copy()\n        temp[time_col] = pd.to_datetime(temp[time_col], errors=\"coerce\")\n        temp = temp.dropna(subset=[time_col])\n        \n        temp[\"Monat\"] = temp[time_col].dt.to_period(\"M\").dt.to_timestamp()\n        \n        aggregated = temp.groupby(\"Monat\").size().reset_index(name=\"Anzahl\")\n        aggregated[\"Kategorie\"] = label\n        return aggregated\n\n    df_trend_1 = prepare_trend_data(zeitwert_error_df, \"Zeitwert Fehler\")\n    df_trend_2 = prepare_trend_data(above_50k_df, \"Aufträge > 50k\")\n    df_trend_3 = prepare_trend_data(auftraege_abgleich, \"Abweichung Summen\")\n\n    combined_trend = pd.concat([df_trend_1, df_trend_2, df_trend_3], ignore_index=True)\n\n    if not combined_trend.empty:\n        min_date = combined_trend[\"Monat\"].min().date()\n        max_date = combined_trend[\"Monat\"].max().date()\n        \n        all_kategorien = combined_trend[\"Kategorie\"].unique()\n\n        col_ctrl1, col_ctrl2 = st.columns([1, 1])\n\n        with col_ctrl1:\n            if min_date != max_date:\n                selected_range = st.slider(\n                    \"Zeitraum eingrenzen:\",\n                    min_value=min_date,\n                    max_value=max_date,\n                    value=(min_date, max_date),\n                    format=\"MMM YYYY\",\n                    key=\"p2_trend_slider\"\n                )\n            else:\n                selected_range = (min_date, max_date)\n\n        with col_ctrl2:\n            selected_metrics = st.multiselect(\n                \"Metriken auswählen:\",\n                options=all_kategorien,\n                default=all_kategorien,\n                key=\"p2_trend_multiselect\"\n            )\n            \n        mask_date = (combined_trend[\"Monat\"].dt.date >= selected_range[0]) & (combined_trend[\"Monat\"].dt.date <= selected_range[1])\n        mask_cat = combined_trend[\"Kategorie\"].isin(selected_metrics)\n        \n        chart_data = combined_trend.loc[mask_date & mask_cat]\n\n        if not chart_data.empty:\n            line_chart = alt.Chart(chart_data).mark_line(point=True).encode(\n                x=alt.X(\"Monat:T\", title=\"Monat\", axis=alt.Axis(format=\"%b %Y\")),\n                y=alt.Y(\"Anzahl:Q\", title=\"Anzahl Vorfälle\"),\n                color=alt.Color(\"Kategorie:N\", title=\"Metrik\", scale=alt.Scale(scheme=\"category10\")),\n                tooltip=[\n                    alt.Tooltip(\"Monat:T\", format=\"%B %Y\"),\n                    alt.Tooltip(\"Kategorie:N\"),\n                    alt.Tooltip(\"Anzahl:Q\")\n                ]\n            ).properties(\n                height=350,\n                width=\"container\"\n            ).interactive()\n\n            st.altair_chart(line_chart, width=\"stretch\")\n        else:\n            st.warning(\"Keine Daten für die aktuelle Auswahl verfügbar.\")\n\n    else:\n        st.info(\"Nicht genügend Zeitdaten für ein Verlaufsdiagramm vorhanden.\")\n\n    st.markdown(\"---\")\n\n    # --- Tables ---\n    chart_col1, chart_col2 = st.columns(2)\n\n# Welche Spalten sollten noch rein um die Daten sinnvoll prüfen zu können? Aktuell kein Spaltenname da Series\n    with chart_col1:\n        st.subheader(\"Die inkorrekten Zeitwerte:\")\n        st.caption(\"Auflistung aller Aufträge mit inkorrekten Zeitwerten in der Zeitwert-Spalte.\")\n        st.markdown(f\"**Gefundene Einträge: {len(zeitwert_error_df)}**\")\n        st.dataframe(zeitwert_error_df)\n        csv_zeitwert = zeitwert_error_df.to_csv(index=False).encode('utf-8')\n\n        st.download_button(\n            label=\"Details als CSV herunterladen\",\n            data=csv_zeitwert,\n            file_name=\"zeitwert_fehler_details.csv\",\n            mime=\"text/csv\",\n        )\n\n\n    with chart_col2:\n        st.subheader(\"Abweichungen Auftragssumme vs. Positionssummen:\")\n        st.caption(\"Auflistung aller Aufträge, bei denen die Auftragssumme nicht mit der Summe der Positionen übereinstimmt.\")\n        st.markdown(f\"**Gefundene Einträge: {len(auftraege_abgleich)}**\")\n        st.dataframe(auftraege_abgleich)\n        csv_abweichungen = auftraege_abgleich.to_csv(index=False).encode('utf-8')\n        st.download_button(\n            label=\"Details als CSV herunterladen\",\n            data=csv_abweichungen,\n            file_name=\"auftragssumme_abweichungen_details.csv\",\n            mime=\"text/csv\",\n        )\n\n    st.subheader(\"Aufträge über 50.000€:\")\n    st.caption(\"Auflistung aller Aufträge mit einem Wert über 50.000€.\")\n    st.markdown(f\"**Gefundene Einträge: {len(above_50k_df)}**\")\n    st.dataframe(above_50k_df)\n    csv_above_50k = above_50k_df.to_csv(index=False).encode('utf-8')\n\n    st.download_button(\n        label=\"Details als CSV herunterladen\",\n        data=csv_above_50k,\n        file_name=\"auftraege_ueber_50k_details.csv\",\n        mime=\"text/csv\",\n    )"
            start_line: 5
            end_line: 227
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: app_pages.page2.get_delta
            name: get_delta
            args[1]: metric_name
            docstring: null
            source_code: "def get_delta(metric_name):\n        if comparison_df is None or comparison_df.empty:\n            return None\n        \n        row = comparison_df[comparison_df['Metric'] == metric_name]\n        \n        if not row.empty:\n            val = row.iloc[0]['Percent_Change']\n            return f\"{val:+.2f}%\"\n        return None"
            start_line: 45
            end_line: 54
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: app_pages.page2.prepare_trend_data
            name: prepare_trend_data
            args[3]: df,label,time_col
            docstring: "Helper function grouping values of passed df into monthly intervalls.\n\nParameters\n----------\ndf : pandas.DataFrame\n    DataFrame containing computed metric values over time.\nlabel : string\n    label for aggregated data, written to column 'Kategorie' of the returned df\ntime_col : str, optional\n    label of the timestamp column in df, by default \"CRMEingangszeit\"\n\nReturns\n-------\npandas.DataFrame\n    df with metric values aggregated by month. Has a column 'Kategorie' for labels. This returns an empty df if no timestamp was passed.\n    "
            source_code: "def prepare_trend_data(df, label, time_col=\"CRMEingangszeit\"):\n        \"\"\"Helper function grouping values of passed df into monthly intervalls.\n\n        Parameters\n        ----------\n        df : pandas.DataFrame\n            DataFrame containing computed metric values over time.\n        label : string\n            label for aggregated data, written to column 'Kategorie' of the returned df\n        time_col : str, optional\n            label of the timestamp column in df, by default \"CRMEingangszeit\"\n\n        Returns\n        -------\n        pandas.DataFrame\n            df with metric values aggregated by month. Has a column 'Kategorie' for labels. This returns an empty df if no timestamp was passed.\n            \n        \"\"\"\n        if df is None or df.empty or time_col not in df.columns:\n            return pd.DataFrame()\n        \n        temp = df.copy()\n        temp[time_col] = pd.to_datetime(temp[time_col], errors=\"coerce\")\n        temp = temp.dropna(subset=[time_col])\n        \n        temp[\"Monat\"] = temp[time_col].dt.to_period(\"M\").dt.to_timestamp()\n        \n        aggregated = temp.groupby(\"Monat\").size().reset_index(name=\"Anzahl\")\n        aggregated[\"Kategorie\"] = label\n        return aggregated"
            start_line: 89
            end_line: 118
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "app_pages/page3.py":
      ast_nodes:
        imports[3]: streamlit,pandas,altair
        functions[3]:
          - mode: function_analysis
            identifier: app_pages.page3.show_page
            name: show_page
            args[4]: metrics_df1,metrics_df2,comparison_df,issues_df
            docstring: "This function renders page 3 of 5 of the dashboard. \n\nPage 3 visualizes metrics concerning the data quality of textual column data.\nFor a detailed list, refer to the 'Notes' section.\n\nParameters\n----------\nmetrics_df1 : pandas.DataFrame\n    DataFrame containing values for all metrics concerning order data only\nmetrics_df2 : pandas.DataFrame\n    DataFrame containing values for all metrics concerning position data only\nmetrics_combined : pandas.DataFrame\n    DataFrame containing values for all metrics concerning order and position data\ncomparison_df : pandas.DataFrame\n    DataFrame with metric value changes over time\nissues_df : bool\n    DataFrame containing values for all metrics concerning potentially invalid data points\nReturns\n-------\nvoid\n\nNotes\n-----\nThis page contains the following information:\n\n- KPIs\n    - aggregated errors and inconsistencies in textual data\n    - amount of test data in production data sets\n    - amount of suspicious trade-company associations (heuristic-based) \n    - amount of suspicious trade-company associations (semantic-classification-based)\n- Charts\n    - detail view of trends over time for all error types\n    - detail view for outlier trade-company associations\n        - heuristic-based\n        - semantic classification       "
            source_code: "def show_page(metrics_df1, metrics_df2, comparison_df, issues_df):\n    \"\"\"This function renders page 3 of 5 of the dashboard. \n    \n    Page 3 visualizes metrics concerning the data quality of textual column data.\n    For a detailed list, refer to the 'Notes' section.\n\n    Parameters\n    ----------\n    metrics_df1 : pandas.DataFrame\n        DataFrame containing values for all metrics concerning order data only\n    metrics_df2 : pandas.DataFrame\n        DataFrame containing values for all metrics concerning position data only\n    metrics_combined : pandas.DataFrame\n        DataFrame containing values for all metrics concerning order and position data\n    comparison_df : pandas.DataFrame\n        DataFrame with metric value changes over time\n    issues_df : bool\n        DataFrame containing values for all metrics concerning potentially invalid data points\n    Returns\n    -------\n    void\n\n    Notes\n    -----\n    This page contains the following information:\n    \n    - KPIs\n        - aggregated errors and inconsistencies in textual data\n        - amount of test data in production data sets\n        - amount of suspicious trade-company associations (heuristic-based) \n        - amount of suspicious trade-company associations (semantic-classification-based)\n    - Charts\n        - detail view of trends over time for all error types\n        - detail view for outlier trade-company associations\n            - heuristic-based\n            - semantic classification       \n\n    \"\"\"\n\n    # Helperfunction to get delta from comparison_df\n    def get_delta(metric_name):\n        if comparison_df is None or comparison_df.empty:\n            return None\n        \n        row = comparison_df[comparison_df['Metric'] == metric_name]\n        \n        if not row.empty:\n            val = row.iloc[0]['Percent_Change']\n            return f\"{val:+.2f}%\"\n        return None\n\n\n    # DATEN LADEN\n    df_outlier = metrics_df1.get(\"handwerker_gewerke_outlier\")\n    df_semantic = metrics_df1.get(\"mismatched_entries\")\n    test_data_df = metrics_df1.get(\"test_data_df\", pd.DataFrame()) \n    \n    kundengruppe_containing_test = metrics_df1.get(\"test_kundengruppen_anzahl\")\n    row_count = metrics_df1.get(\"row_count\") \n    row_count_df2 = metrics_df2.get(\"row_count\")\n    text_issues = issues_df[\"text_issues\"]\n    anteil_text_issues = (text_issues / (row_count + row_count_df2)) * 100\n    anteil_testdaten = (kundengruppe_containing_test / row_count * 100)\n\n    # outlier KPI\n    outlier_count = len(df_outlier[df_outlier['is_outlier'] == True])\n    outlier_share = (outlier_count / row_count * 100)\n    semantic_count = len(df_semantic)\n    semantic_share = (semantic_count / row_count * 100)\n\n    # KPI HEADER\n    kpi_cols = st.columns(4)\n    with kpi_cols[0]:\n        st.metric(\n            label=\"Textuelle Auffälligkeiten\",\n            value=f\"{text_issues:,}\".replace(\",\", \".\"),\n            delta=get_delta(\"text_issues\"), \n            delta_color=\"inverse\",\n            help=\"Textuelle Fehler und Warnings im Datensatz\"\n        )\n        st.caption(f\"Anteil: {anteil_text_issues:.2f}% an beiden Datensätzen\")\n    with kpi_cols[1]: \n        st.metric(\n            label=\"Testdatensätze in Kundengruppe\",\n            value=f\"{kundengruppe_containing_test:,}\".replace(\",\", \".\"),\n            delta=get_delta(\"count_test_data_rows\"), \n            delta_color=\"inverse\",\n            help=\"Anzahl der Aufträge, die als Testdatensätze identifiziert wurden\"\n        )\n        st.caption(f\"Anteil: {anteil_testdaten:.2f}% der Auftragsdaten\")\n\n    with kpi_cols[2]:\n        st.metric(\n            label=\"Auffällige Gewerk-Zuordnungen (regelbasiert)\",\n            value=f\"{outlier_count:,}\".replace(\",\", \".\"),\n            delta=get_delta(\"count_handwerker_outliers\"), \n            delta_color=\"inverse\",\n            help=\"Anzahl der Aufträge mit auffälligen Handwerker-Gewerk Zuordnungen\"\n        )\n        st.caption(f\"Anteil: {outlier_share:.2f}% der Auftragsdaten\")\n    with kpi_cols[3]:\n        st.metric(\n            label=\"Auffällige Gewerk-Zuordnungen (semantisch)\",\n            value=f\"{semantic_count:,}\".replace(\",\", \".\"),\n            delta=get_delta(\"count_semantic_outliers\"), \n            delta_color=\"inverse\",\n            help=\"Anzahl der Aufträge, bei denen Handwerkername und Gewerk semantisch nicht zusammenpassen (KI-Check)\"\n        )\n        st.caption(f\"Anteil: {semantic_share:.2f}% der Auftragsdaten\")    \n\n\n    st.markdown(\"---\")\n\n    # ZEITVERLAUF DIAGRAMM (MIT RAW DATA VIEW)\n    st.subheader(\"Fehlerverlauf im Vergleich\")\n    st.caption(\"Dieses Diagramm zeigt den Verlauf der ausgewählten Fehlerkategorien über den gewählten Zeitraum. Aktuell können nur Testdatensätze visualisiert werden.\")\n\n    def prepare_trend_data(df, label, time_col=\"CRMEingangszeit\"):\n        \"\"\"Bereitet DataFrame für das Zeitreihendiagramm (Aggregation) vor.\"\"\"\n        if df is None or df.empty or time_col not in df.columns:\n            return pd.DataFrame()\n        \n        temp = df.copy()\n        temp[time_col] = pd.to_datetime(temp[time_col], errors=\"coerce\")\n        temp = temp.dropna(subset=[time_col])\n        \n        temp[\"Monat\"] = temp[time_col].dt.to_period(\"M\").dt.to_timestamp()\n        \n        aggregated = temp.groupby(\"Monat\").size().reset_index(name=\"Anzahl\")\n        aggregated[\"Kategorie\"] = label\n        return aggregated\n\n    # Daten für Chart vorbereiten (Aggregiert)\n    df_trend_test = prepare_trend_data(test_data_df, \"Testdatensätze\")\n    \n    combined_trend = pd.concat([df_trend_test], ignore_index=True)\n\n    if not combined_trend.empty:\n        min_date = combined_trend[\"Monat\"].min().date()\n        max_date = combined_trend[\"Monat\"].max().date()\n        all_kategorien = combined_trend[\"Kategorie\"].unique()\n\n        col_ctrl1, col_ctrl2 = st.columns([1, 1])\n\n        with col_ctrl1:\n            if min_date != max_date:\n                selected_range = st.slider(\n                    \"Zeitraum eingrenzen:\",\n                    min_value=min_date,\n                    max_value=max_date,\n                    value=(min_date, max_date),\n                    format=\"MMM YYYY\",\n                    key=\"p3_trend_slider\"\n                )\n            else:\n                selected_range = (min_date, max_date)\n\n        with col_ctrl2:\n            selected_metrics = st.multiselect(\n                \"Metriken auswählen:\",\n                options=all_kategorien,\n                default=all_kategorien,\n                key=\"p3_trend_multiselect\"\n            )\n\n        # View Toggle\n        view_mode_trend = st.radio(\n            \"Ansicht (Zeitverlauf):\",\n            [\"Grafische Auswertung\", \"Detail-Tabelle\"],\n            horizontal=True,\n            label_visibility=\"collapsed\",\n            key=\"p3_trend_view_toggle\"\n        )\n        \n        if view_mode_trend == \"Grafische Auswertung\":\n            mask_date = (combined_trend[\"Monat\"].dt.date >= selected_range[0]) & (combined_trend[\"Monat\"].dt.date <= selected_range[1])\n            mask_cat = combined_trend[\"Kategorie\"].isin(selected_metrics)\n            \n            chart_data = combined_trend.loc[mask_date & mask_cat]\n\n            if not chart_data.empty:\n                line_chart = alt.Chart(chart_data).mark_line(point=True).encode(\n                    x=alt.X(\"Monat:T\", title=\"Monat\", axis=alt.Axis(format=\"%b %Y\")),\n                    y=alt.Y(\"Anzahl:Q\", title=\"Anzahl Vorfälle\"),\n                    color=alt.Color(\"Kategorie:N\", title=\"Metrik\", scale=alt.Scale(scheme=\"category10\")),\n                    tooltip=[\n                        alt.Tooltip(\"Monat:T\", format=\"%B %Y\"),\n                        alt.Tooltip(\"Kategorie:N\"),\n                        alt.Tooltip(\"Anzahl:Q\")\n                    ]\n                ).properties(\n                    height=350,\n                    width=\"container\"\n                ).interactive()\n\n                st.altair_chart(line_chart, width=\"stretch\")\n\n            else:\n                st.warning(\"Keine Chart-Daten für die aktuelle Auswahl verfügbar.\")\n        \n        else:\n            \n            raw_frames = []\n\n            if \"Testdatensätze\" in selected_metrics and not test_data_df.empty:\n                temp_raw = test_data_df.copy()\n                \n                temp_col = \"CRMEingangszeit\"\n                if temp_col in temp_raw.columns:\n                    temp_raw[temp_col] = pd.to_datetime(temp_raw[temp_col], errors=\"coerce\")\n                    \n                    mask_raw = (temp_raw[temp_col].dt.date >= selected_range[0]) & \\\n                               (temp_raw[temp_col].dt.date <= selected_range[1])\n                    \n                    filtered_raw = temp_raw.loc[mask_raw].copy()\n                    \n                    filtered_raw.insert(0, \"Fehlerkategorie\", \"Testdatensatz\")\n                    \n                    raw_frames.append(filtered_raw)\n\n            if raw_frames:\n                final_raw_df = pd.concat(raw_frames, ignore_index=True)\n                \n                st.markdown(f\"**Gefundene Einträge: {len(final_raw_df)}** im Zeitraum {selected_range[0].strftime('%d.%m.%Y')} bis {selected_range[1].strftime('%d.%m.%Y')}\")\n                \n                st.dataframe(\n                    final_raw_df, \n                    width=\"stretch\", \n                    hide_index=True\n                )\n\n                csv_raw = final_raw_df.to_csv(index=False).encode('utf-8')\n                st.download_button(\n                    label=\"Rohdaten als CSV herunterladen\",\n                    data=csv_raw,\n                    file_name=\"zeitverlauf_rohdaten_text.csv\",\n                    mime=\"text/csv\"\n                )\n            else:\n                st.info(\"Keine Rohdaten für die gewählten Metriken im ausgewählten Zeitraum gefunden.\")\n\n    else:\n        st.info(\"Nicht genügend Zeitdaten für ein Verlaufsdiagramm vorhanden.\")\n\n    st.markdown(\"---\")\n\n    # DETAIL ANALYSE (OUTLIERS)\n    st.subheader(\"Statistische Auffälligkeiten (Handwerker vs. Gewerk)\")\n    st.caption(\"Diese Analyse identifiziert Handwerker und Gewerke, bei den auffällig viele Zuordnungsfehler auftreten. (Der Namensabgleich prüft, ob der Handwerkername auf das Gewerk hinweist)\")\n\n    tab1, tab2 = st.tabs(['Regelbasiert', 'Semantisch'])\n    \n    #regelabsierter Abgleich\n    with tab1:\n        if df_outlier is not None and not df_outlier.empty:\n\n            st.caption(\"Wenn das angegebene Gewerk eines Handwerkers in weniger als 20% seiner gesamten Aufträge vorkommt wird dieses als Ausreißer deklariert. \")\n            st.caption(\"In der Detail-Tabelle kann zusätzlich eingesehen werden ob ein Keyword aus einem anderen Gewerk im Namen des Handwerkers gefunden wurde als das angegebene Gewerk.\")\n\n            view_mode_outlier = st.radio(\n                \"Darstellung (Auffälligkeiten):\",\n                [\"Grafische Auswertung\", \"Detail-Tabelle\"],\n                horizontal=True,\n                label_visibility=\"collapsed\",\n                key=\"p3_outlier_view_toggle\"\n            )\n\n            if view_mode_outlier == \"Grafische Auswertung\":\n\n                st.markdown(\"#### Fehlerschwerpunkte nach Gewerk\")\n                grouped_gewerk = df_outlier['Gewerk_Name'].value_counts().reset_index()\n                grouped_gewerk.columns = ['Gewerk', 'Anzahl']\n\n                chart_gewerk = alt.Chart(grouped_gewerk.head(10)).mark_bar().encode(\n                    x=alt.X('Anzahl:Q', title=\"Anzahl Auffälligkeiten\"),\n                    y=alt.Y('Gewerk:N', sort='-x', title=\"Gewerk\"),\n                    tooltip=['Gewerk', 'Anzahl'],\n                    color=alt.value(\"#E4572E\")\n                ).properties(height=280)\n                st.altair_chart(chart_gewerk, width=\"stretch\")\n\n                st.markdown(\"---\")\n\n                st.markdown(\"#### Top-Verursacher\")\n\n                df_hw = df_outlier.copy()\n\n                grouped_hw = df_hw.groupby('Handwerker_Name', observed=True)['count'].sum().reset_index()\n                grouped_hw.columns = ['Handwerker', 'Summe_Fehler']\n\n                grouped_hw = grouped_hw.sort_values('Summe_Fehler', ascending=False).head(10)\n\n                chart_hw = alt.Chart(grouped_hw).mark_bar().encode(\n                    x=alt.X('Summe_Fehler:Q', title=\"Summe potenziell fehlerhafter Aufträge\"),\n                    y=alt.Y('Handwerker:N', sort='-x', title=\"Handwerker\"),\n                    tooltip=['Handwerker', 'Summe_Fehler'],\n                    color=alt.value(\"#442D7B\")\n                ).properties(height=280)\n                st.altair_chart(chart_hw, width=\"stretch\")\n\n            else:\n                # Tabellenansicht Outlier\n                df_display = df_outlier.copy()\n\n                if 'ratio' in df_display.columns:\n                    df_display['ratio'] = (df_display['ratio'] * 100).round(2).astype(str) + '%'\n\n                rename_map = {\n                    \"Handwerker_Name\": \"Handwerker\",\n                    \"Gewerk_Name\": \"Gewerk (Auftrag)\",\n                    \"count\": \"Anzahl Aufträge\",\n                    \"ratio\": \"Anteil am Gesamtvolumen\",\n                    \"Check_Result\": \"Namensabgleich\"\n                }\n\n                cols_to_show = [c for c in rename_map.keys() if c in df_display.columns]\n                st.markdown(f\"**Gefundene Einträge: {len(df_display)}**\")\n                st.dataframe(\n                    df_display[cols_to_show].rename(columns=rename_map),\n                    width=\"stretch\",\n                    hide_index=True\n                )\n                \n                # Download Button für die Outlier Tabelle\n                csv_outlier = df_display.to_csv(index=False).encode('utf-8')\n                st.download_button(\n                    label=\"Tabelle als CSV herunterladen\",\n                    data=csv_outlier,\n                    file_name=\"handwerker_outliers.csv\",\n                    mime=\"text/csv\"\n                )\n\n        else:\n            st.success(\"Keine statistischen Auffälligkeiten in den Daten gefunden.\")\n    #semantischer Abgleich   \n    with tab2:\n        if df_outlier is not None and not df_outlier.empty:\n\n            st.caption(\"Es wird ein semantischer Vergleich des Gewerks mit dem Handwerkernamen durchgeführt. Ein zu starker Unterschied wird als fehlerhaft ausgegeben.\")\n\n            view_mode_outlier = st.radio(\n                \"Darstellung (Auffälligkeiten):\",\n                [\"Grafische Auswertung\", \"Detail-Tabelle\"],\n                horizontal=True,\n                label_visibility=\"collapsed\",\n                key=\"p3_semantic_view_toggle\"\n            )\n\n            if view_mode_outlier == \"Grafische Auswertung\":\n\n                st.markdown(\"#### Fehlerschwerpunkte nach Gewerk\")\n                grouped_gewerk = df_semantic['Gewerk_Name'].value_counts().reset_index()\n                grouped_gewerk.columns = ['Gewerk', 'Anzahl']\n\n                chart_gewerk = alt.Chart(grouped_gewerk.head(10)).mark_bar().encode(\n                    x=alt.X('Anzahl:Q', title=\"Anzahl Auffälligkeiten\"),\n                    y=alt.Y('Gewerk:N', sort='-x', title=\"Gewerk\"),\n                    tooltip=['Gewerk', 'Anzahl'],\n                    color=alt.value(\"#E4572E\")\n                ).properties(height=280)\n                st.altair_chart(chart_gewerk, width=\"stretch\")\n\n                st.markdown(\"---\")\n\n                st.markdown(\"#### Top-Verursacher\")\n\n                df_sem = df_semantic.copy()\n\n                grouped_hw = df_sem['Handwerker_Name'].value_counts().reset_index()\n                grouped_hw.columns = ['Handwerker', 'Summe_Fehler']\n\n                grouped_hw = grouped_hw.sort_values('Summe_Fehler', ascending=False).head(10)\n\n                chart_hw = alt.Chart(grouped_hw).mark_bar().encode(\n                    x=alt.X('Summe_Fehler:Q', title=\"Summe potenziell fehlerhafter Aufträge\"),\n                    y=alt.Y('Handwerker:N', sort='-x', title=\"Handwerker\"),\n                    tooltip=['Handwerker', 'Summe_Fehler'],\n                    color=alt.value(\"#442D7B\")\n                ).properties(height=280)\n                st.altair_chart(chart_hw, width=\"stretch\")\n\n            else:\n                # Tabellenansicht Outlier\n                df_display = df_semantic[['Gewerk_Name', 'Handwerker_Name', 'Similarity_Score']].copy()\n\n                rename_map = {\n                    \"Handwerker_Name\": \"Handwerker\",\n                    \"Gewerk_Name\": \"Gewerk (Auftrag)\",\n                    \"Similarity_Score\": \"Ähnlichkeit\"\n                }\n\n                cols_to_show = [c for c in rename_map.keys() if c in df_display.columns]\n                st.markdown(f\"**Gefundene Einträge: {len(df_display)}**\")\n                st.dataframe(\n                    df_display[cols_to_show].rename(columns=rename_map),\n                    width=\"stretch\",\n                    hide_index=True\n                )\n                \n                # Download Button für die Outlier Tabelle\n                csv_semantic = df_display.to_csv(index=False).encode('utf-8')\n                st.download_button(\n                    label=\"Tabelle als CSV herunterladen\",\n                    data=csv_semantic,\n                    file_name=\"handwerker_semantic.csv\",\n                    mime=\"text/csv\"\n                )\n\n        else:\n            st.success(\"Keine statistischen Auffälligkeiten in den Daten gefunden.\")"
            start_line: 5
            end_line: 414
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: app_pages.page3.get_delta
            name: get_delta
            args[1]: metric_name
            docstring: null
            source_code: "def get_delta(metric_name):\n        if comparison_df is None or comparison_df.empty:\n            return None\n        \n        row = comparison_df[comparison_df['Metric'] == metric_name]\n        \n        if not row.empty:\n            val = row.iloc[0]['Percent_Change']\n            return f\"{val:+.2f}%\"\n        return None"
            start_line: 45
            end_line: 54
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: app_pages.page3.prepare_trend_data
            name: prepare_trend_data
            args[3]: df,label,time_col
            docstring: Bereitet DataFrame für das Zeitreihendiagramm (Aggregation) vor.
            source_code: "def prepare_trend_data(df, label, time_col=\"CRMEingangszeit\"):\n        \"\"\"Bereitet DataFrame für das Zeitreihendiagramm (Aggregation) vor.\"\"\"\n        if df is None or df.empty or time_col not in df.columns:\n            return pd.DataFrame()\n        \n        temp = df.copy()\n        temp[time_col] = pd.to_datetime(temp[time_col], errors=\"coerce\")\n        temp = temp.dropna(subset=[time_col])\n        \n        temp[\"Monat\"] = temp[time_col].dt.to_period(\"M\").dt.to_timestamp()\n        \n        aggregated = temp.groupby(\"Monat\").size().reset_index(name=\"Anzahl\")\n        aggregated[\"Kategorie\"] = label\n        return aggregated"
            start_line: 122
            end_line: 135
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "app_pages/page4.py":
      ast_nodes:
        imports[3]: streamlit,pandas,altair
        functions[2]:
          - mode: function_analysis
            identifier: app_pages.page4.show_page
            name: show_page
            args[4]: metrics_df1,metrics_df2,comparison_df,issues_df
            docstring: null
            source_code: "def show_page(metrics_df1, metrics_df2, comparison_df, issues_df):\n\n    # Helperfunction to get delta from comparison_df\n    def get_delta(metric_name):\n        if comparison_df is None or comparison_df.empty:\n            return None\n        \n        row = comparison_df[comparison_df['Metric'] == metric_name]\n        \n        if not row.empty:\n            val = row.iloc[0]['Percent_Change']\n            return f\"{val:+.2f}%\"\n        return None\n\n\n    plausi_df1 = metrics_df1.get(\"plausi_forderung_einigung_df\")\n    plausi_list_df1 = plausi_df1['Diff']\n    plausi_count_df1 = metrics_df1.get(\"plausi_forderung_einigung_count\", 0)\n    plausi_avg_df1 = metrics_df1.get(\"plausi_forderung_einigung_avg_diff\", 0.0)\n\n    plausi_count_df2 = metrics_df2.get(\"plausi_forderung_einigung_count\", 0)\n    plausi_avg_df2 = metrics_df2.get(\"plausi_forderung_einigung_avg_diff\", 0.0)\n    plausi_outliers_df2 = metrics_df2.get(\"plausi_forderung_einigung_df2\")\n    plausi_list_df2 = plausi_outliers_df2['Diff']\n\n    discount_errors = metrics_df2.get(\"discount_check_errors\", 0)\n    disc_stats = metrics_df2.get(\"discount_stats\")\n    disc_details = metrics_df2.get(\"discount_details\")\n\n    proforma_df = metrics_df1.get(\"proforma_belege_df\")\n    proforma_count = metrics_df1.get(\"proforma_belege_count\", 0)\n\n    fn_count_df1 = metrics_df1.get(\"false_negative\", 0)\n    fn_stats_df1 = metrics_df1.get(\"false_negative_stats\")\n    fn_details_df1 = metrics_df1.get(\"false_negative_details\")\n\n    fn_count_df2 = len(metrics_df2.get(\"false_negative_details\"))\n    fn_stats_df2 = metrics_df2.get(\"false_negative_stats\")\n    fn_details_df2 = metrics_df2.get(\"false_negative_details\")\n\n    total_df1 = metrics_df1.get(\"row_count\", 0)\n    total_df2 = metrics_df2.get(\"row_count\", 0)\n\n    plausi_issue = issues_df[\"plausi_issues\"] if issues_df is not None else 0\n    anteil_plausi_issues = (plausi_issue / (total_df1 + total_df2)) * 100 if (total_df1 + total_df2) > 0 else 0\n\n    empty_orders_df = metrics_df1.get(\"empty_orders_df\")\n    empty_orders = metrics_df1.get(\"empty_orders_count\", 0)\n    outliers_by_damage = metrics_df1.get(\"outliers_by_damage\")\n\n    st.markdown(\"### Plausibilitäts-Checks & Logikfehler\")\n    kpi_cols = st.columns(1)\n    with kpi_cols[0]:\n        st.metric(\n            label=\"Plausibilitäts-Auffälligkeiten\",\n            value=f\"{plausi_issue:,}\".replace(\",\", \".\"),\n            delta=get_delta(\"plausi_issues\"),\n            delta_color=\"inverse\",\n            help=\"Textuelle Fehler und Warnings im Datensatz\"\n        )\n        st.caption(f\"Anteil: {anteil_plausi_issues:.2f}% an beiden Datensätzen\")\n\n    tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([\n        \"Logikfehler: Forderung < Einigung\",\n        \"Rabatt/Vorzeichen\",\n        \"Proforma-Belege\",\n        \"Validierung der Vorzeichenlogik (Auftrag)\",\n        \"Validierung der Vorzeichenlogik (Position)\",\n        \"Aufträge ohne Positionen\",\n        \"Ausreißer in der Forderungssumme\"\n    ])\n\n    with tab1:\n        st.subheader(\"Logikfehler: Forderung < Einigung\")\n        st.caption(\"Fälle, in denen Forderung_Netto kleiner als Einigung_Netto ist\")\n\n        dataset_choice = st.radio(\"Datensatz wählen\", [\"Auftragsdaten\", \"Positionsdaten\"], horizontal=True)\n\n        if \"Auftragsdaten\" in dataset_choice:\n            count_val = plausi_count_df1\n            avg_val = plausi_avg_df1\n            diff_list = plausi_list_df1\n            total_rows = total_df1\n            outliers_view = plausi_df1\n            id_col = \"KvaRechnung_ID\"\n            file_suffix = \"auftraege\"\n            delta = get_delta(\"count_plausibility_errors_df\")\n        else:\n            count_val = plausi_count_df2\n            avg_val = plausi_avg_df2\n            diff_list = plausi_list_df2\n            total_rows = total_df2\n            outliers_view = plausi_outliers_df2\n            id_col = \"Position_ID\"\n            file_suffix = \"positionen\"\n            delta = get_delta(\"count_plausibility_errors_df2\")\n\n        c1, c2, c3, c4 = st.columns(4)\n        c1.metric(\"Anzahl Fälle\", value=f\"{count_val:,}\".replace(\",\", \".\"), delta=delta, delta_color=\"inverse\", help=\"Anzahl der Fälle, in denen Forderung_Netto < Einigung_Netto\")\n        c2.metric(\"Quote\", f\"{(count_val / total_rows) * 100:.2f}%\" if total_rows else \"NA\", help=\"Anteil der fehlerhaften Fälle am gesamten Datensatz\")\n        c3.metric(\"Ø Abweichung\", f\"{avg_val:,.2f} €\", help=\"Durchschnittliche Differenz zwischen Forderung_Netto und Einigung_Netto bei fehlerhaften Fällen\")\n\n        # Berechnung P95 für Chart-Skalierung\n        if not diff_list.empty:\n            p95 = diff_list.quantile(0.95)\n        else:\n            p95 = 0\n\n        c4.metric(\"P95\", f\"{p95:,.2f} €\", help=\"95. Perzentil der Differenzen zwischen Forderung_Netto und Einigung_Netto bei fehlerhaften Fällen\")\n\n        c_chart1, c_chart2 = st.columns(2)\n\n        # Chart 1: Histogramm\n        with c_chart1:\n            if not diff_list.empty:\n                hist_data = pd.DataFrame({\"Diff\": diff_list})\n                # Filter für bessere Ansicht im Histogramm (bis P95)\n                hist = alt.Chart(hist_data[hist_data[\"Diff\"] <= (p95 * 1.5 if p95 > 0 else 100)]).mark_bar().encode(\n                    x=alt.X(\"Diff:Q\", bin=True, title=\"Differenz\"), \n                    y=alt.Y(\"count()\", title=\"Anzahl\"),\n                ).properties(height=300, title=\"Verteilung\")\n                st.altair_chart(hist, width=\"stretch\")\n            else:\n                st.info(\"Keine Daten für Histogramm.\")\n\n        # Chart 2: Top Ausreißer\n        with c_chart2:\n            if not outliers_view.empty:\n                # Top 10 Logik für Chart\n                top_10 = outliers_view[outliers_view[\"Diff\"] > p95].head(10) if p95 > 0 else outliers_view.head(10)\n                \n                if not top_10.empty:\n                    bar = alt.Chart(top_10).mark_bar(color=\"#E4572E\").encode(\n                        x=\"Diff:Q\", y=alt.Y(f\"{id_col}:N\", sort=\"-x\")\n                    ).properties(height=300, title=\"Top 10 Ausreißer (> P95)\")\n                    st.altair_chart(bar, width=\"stretch\")\n                else:\n                    st.info(\"Keine extremen Ausreißer für Grafik vorhanden.\")\n            else:\n                st.caption(\"Keine Daten vorhanden.\")\n\n        st.markdown(\"---\")\n        if not outliers_view.empty:\n            with st.expander(f\"Details anzeigen ({dataset_choice})\"):\n                st.markdown(f\"**Gefundene Einträge: {len(outliers_view)}**\")\n                outliers_view = outliers_view.sort_values(by=\"Diff\", ascending=False)\n                st.dataframe(outliers_view, width=\"stretch\")\n\n                csv_plausi = outliers_view.to_csv(index=False).encode('utf-8')\n\n                st.download_button(\n                    label=\"Details als CSV herunterladen\",\n                    data=csv_plausi,\n                    file_name=f\"plausibilitaet_einigung_forderung_{file_suffix}.csv\",\n                    mime=\"text/csv\",\n                )\n        else:\n            st.success(\"Keine Auffälligkeiten in diesem Datensatz gefunden.\")\n\n    with tab2:\n        st.subheader(\"Rabatt-Logik & Vorzeichenprüfung\")\n        st.caption(\"Fälle, in denen Rabatte unplausibel sind. (Rabatte werden anhand von Keywords erkannt) Vorzeichen-Checks nur bei Einigung_netto.\")\n        c1, c2 = st.columns(2)\n        c1.metric(\"Unplausible Positionen\", value=f\"{discount_errors:,}\".replace(\",\", \".\"), delta=get_delta(\"count_discount_logic_errors\"), delta_color=\"inverse\", help=\"Anzahl der Positionen mit unplausiblen Rabatten\")\n\n        c2.metric(\"Anteil an Positionen\", f\"{(discount_errors / total_df2) * 100:.2f}%\" if total_df2 else \"NA\", help=\"Prozentualer Anteil der unplausiblen Positionen am gesamten Datensatz\")\n\n        if not disc_stats.empty:\n            bar = alt.Chart(disc_stats).mark_bar(color=\"#E4572E\").encode(\n                x=alt.X(\"Anzahl:Q\", axis=alt.Axis(tickMinStep=1, format='d')),\n                y=alt.Y(\"Bezeichnung:N\", sort=\"-x\"),\n                tooltip=[\"Bezeichnung\", \"Anzahl\"]\n            ).properties(height=400, title=\"Top Fehlerquellen\")\n            st.altair_chart(bar, width=\"stretch\")\n\n        if not disc_details.empty:\n            with st.expander(\"Details anzeigen\"):\n                st.markdown(f\"**Gefundene Einträge: {len(disc_details)}**\")\n                st.dataframe(disc_details, width=\"stretch\")\n\n                csv_disc = disc_details.to_csv(index=False).encode('utf-8')\n\n                st.download_button(\n                    label=\"Details als CSV herunterladen\",\n                    data=csv_disc,\n                    file_name=\"rabatt_fehler_details.csv\",\n                    mime=\"text/csv\",\n                )\n\n    with tab3:\n            st.subheader(\"Erkannte Proforma-Belege\")\n            st.caption(\"Aufträge, die als Proforma-Belege identifiziert wurden. (Performa-Beleg = Aufträge mit Einigung_Netto zwischen 0,01 und 1 €.)\")\n            c1, c2 = st.columns(2)\n            c1.metric(\"Anzahl Belege\", value=f\"{proforma_count:,}\".replace(\",\", \".\"), delta=get_delta(\"count_proforma_receipts\"), delta_color=\"inverse\", help=\"Anzahl der als Proforma-Belege identifizierten Aufträge\")\n            c2.metric(\"Anteil an Aufträgen\", f\"{(proforma_count / total_df1) * 100:.2f}%\" if total_df1 else \"NA\", help=\"Prozentualer Anteil der Proforma-Belege am gesamten Auftragsdatensatz\")\n\n            if not proforma_df.empty:\n                if \"CRMEingangszeit\" in proforma_df.columns:\n                    line = alt.Chart(proforma_df).mark_line(point=True).encode(\n                        x=alt.X(\"yearmonth(CRMEingangszeit):T\", title=\"Zeitraum\"),\n                        y=alt.Y(\"count()\", title=\"Anzahl\", axis=alt.Axis(tickMinStep=1, format='d'))\n                    ).properties(height=300)\n                    st.altair_chart(line, width=\"stretch\")\n\n                with st.expander(\"Details anzeigen\"):\n                    st.markdown(f\"**Gefundene Einträge: {len(proforma_df)}**\")\n                    st.dataframe(proforma_df, width=\"stretch\")\n                    csv_proforma = proforma_df.to_csv(index=False).encode('utf-8')\n\n                    st.download_button(\n                        label=\"Details als CSV herunterladen\",\n                        data=csv_proforma,\n                        file_name=\"proforma_belege.csv\",\n                        mime=\"text/csv\",\n                    )\n\n    with tab4:\n        st.subheader(\"Validierung der Vorzeichenlogik in Auftragsdaten\")\n        st.caption(\"Aufträge mit inkonsistenten Vorzeichen bei Forderung, Empfehlung und Einigung.\")\n        c1, c2, c3 = st.columns(3)\n        c1.metric(\"Fehlerhafte Aufträge\", value=f\"{fn_count_df1:,}\".replace(\",\", \".\"), delta=get_delta(\"count_false_negative_df\"), delta_color=\"inverse\", help=\"Anzahl der Aufträge mit inkonsistenten Vorzeichen bei Forderung, Einigung und Auftragssumme\")\n        c2.metric(\"Anteil an Aufträgen\", f\"{(fn_count_df1 / total_df1) * 100:.2f}%\" if total_df1 else \"NA\", help=\"Prozentualer Anteil der fehlerhaften Aufträge am gesamten Auftragsdatensatz\")\n        c3.metric(\"Gesamt Aufträge\", value=f\"{total_df1:,}\".replace(\",\", \".\"), help=\"Gesamtanzahl der Aufträge im Datensatz\")\n\n        if not fn_stats_df1.empty:\n            bar = alt.Chart(fn_stats_df1).mark_bar().encode(\n                x=alt.X(\"Fehler:Q\", axis=alt.Axis(tickMinStep=1, format='d')),\n                y=alt.Y(\"Spalte:N\", sort=\"-x\"),\n                tooltip=[\"Spalte\", \"Fehler\"]\n            ).properties(height=300, title=\"Fehlerverteilung\")\n            st.altair_chart(bar, width=\"stretch\")\n\n        if not fn_details_df1.empty:\n            with st.expander(\"Details anzeigen\"):\n                st.markdown(f\"**Gefundene Einträge: {len(fn_details_df1)}**\")\n                st.dataframe(fn_details_df1, width=\"stretch\")\n                csv_data = fn_details_df1.to_csv(index=False).encode('utf-8')\n\n                st.download_button(\n                    label=\"Details als CSV herunterladen\",\n                    data=csv_data,\n                    file_name=\"tripel_fehler_details.csv\",\n                    mime=\"text/csv\",\n                )\n\n    with tab5:\n        st.subheader(\"Validierung der Vorzeichenlogik in Positionsdaten\")\n        st.caption(\"Positionen mit inkonsistenten Vorzeichen bei Forderung, Einigung, Positionssumme und Menge.\")\n        c1, c2, c3 = st.columns(3)\n        c1.metric(\"Inkonsistente Positionen\", value=f\"{fn_count_df2:,}\".replace(\",\", \".\"), delta=get_delta(\"count_false_negative_df2\"), delta_color=\"inverse\", help=\"Anzahl der Positionen mit inkonsistenten Vorzeichen bei Forderung, Einigung und Positionssumme\")\n        c2.metric(\"Anteil an Positionen\", f\"{(fn_count_df2 / total_df2) * 100:.2f}%\" if total_df2 else \"NA\", help=\"Prozentualer Anteil der fehlerhaften Positionen am gesamten Positionsdatensatz\")\n        c3.metric(\"Gesamt Positionen\", value=f\"{total_df2:,}\".replace(\",\", \".\"), help=\"Gesamtanzahl der Positionen im Datensatz\")\n\n        if not fn_stats_df2.empty:\n            bar = alt.Chart(fn_stats_df2).mark_bar().encode(\n                x=\"Anzahl:Q\",\n                y=alt.Y(\"Kategorie:N\", sort=\"-x\"),\n                tooltip=[\"Kategorie\", \"Anzahl\"]\n            ).properties(height=300, title=\"Fehlerkategorien\")\n            st.altair_chart(bar, width=\"stretch\")\n\n        if not fn_details_df2.empty:\n            with st.expander(\"Details anzeigen\"):\n                st.markdown(f\"**Gefundene Einträge: {len(fn_details_df2)}**\")\n                st.dataframe(fn_details_df2, width=\"stretch\")\n                csv_data_2 = fn_details_df2.to_csv(index=False).encode('utf-8')\n\n                st.download_button(\n                    label=\"Details als CSV herunterladen\",\n                    data=csv_data_2,\n                    file_name=\"konsistenz_fehler_details.csv\",\n                    mime=\"text/csv\",\n                )\n    \n    with tab6:\n        st.subheader(\"Detailansicht für die Aufträge ohne Positionen\")\n        st.metric(label=\"Aufträge ohne Pos.\", value=f\"{empty_orders:,}\".replace(\",\", \".\"), help=\"Anzahl der Aufträge, denen keine Positionen zugeordnet sind (PositionsAnzahl ist leer).\", delta=get_delta(\"count_empty_orders\"),delta_color=\"inverse\")\n        selected_types = st.multiselect(\n        \"Nach Schadensart filtern:\",\n        options= empty_orders_df[\"Schadenart_Name\"].unique(),\n        default=None,\n        key=\"schadenart_multiselect\"\n    )\n        st.dataframe(empty_orders_df[empty_orders_df[\"Schadenart_Name\"].isin(selected_types)] if selected_types else empty_orders_df)\n\n    with tab7:\n        st.subheader(\"Ausreißer in der Forderungssumme\")\n        selected_kundengruppe = st.multiselect(\n        \"Nach Kundengruppe filtern:\",\n        options= outliers_by_damage[\"Kundengruppe\"].unique(),\n        default=None,\n        key=\"kundengruppe_multiselect\"\n    )\n        st.dataframe(outliers_by_damage[outliers_by_damage[\"Kundengruppe\"].isin(selected_kundengruppe)] if selected_kundengruppe else outliers_by_damage)\n        st.caption(\"Gibt die Aufträge aus deren Forderungssumme gruppiert nach Schadensart im 1. oder 99. Perzentil sind.\")\n\n    st.markdown(\"---\")"
            start_line: 5
            end_line: 301
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: app_pages.page4.get_delta
            name: get_delta
            args[1]: metric_name
            docstring: null
            source_code: "def get_delta(metric_name):\n        if comparison_df is None or comparison_df.empty:\n            return None\n        \n        row = comparison_df[comparison_df['Metric'] == metric_name]\n        \n        if not row.empty:\n            val = row.iloc[0]['Percent_Change']\n            return f\"{val:+.2f}%\"\n        return None"
            start_line: 8
            end_line: 17
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "app_pages/page5.py":
      ast_nodes:
        imports[6]: streamlit,streamlit.components.v1,pandas,data_drift_metrics,duckdb,pathlib.Path
        functions[4]:
          - mode: function_analysis
            identifier: app_pages.page5.load_df
            name: load_df
            args[1]: df_type
            docstring: null
            source_code: "def load_df(df_type): #wäre es schneller die nulls schon hier im SQL zu droppen?\n    con = duckdb.connect(\"resources/dashboard_data.duckdb\", read_only=True)\n    if df_type == \"df\":\n        try:\n            df = con.execute(\"SELECT * FROM auftragsdaten\").df()\n        finally:\n            con.close() \n    if df_type == \"df2\":\n        try:\n            df = con.execute(\"SELECT * FROM positionsdaten\").df()   \n        finally:\n             con.close()\n    return df"
            start_line: 9
            end_line: 21
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: app_pages.page5.fetch_reports_table
            name: fetch_reports_table
            args[0]:
            docstring: null
            source_code: "def fetch_reports_table():\n    reports_folder = Path(\"resources/reports\")\n    filenames = [f.stem for f in reports_folder.iterdir() if f.is_file()]\n    split = [name.split(\"_\")for name in filenames]\n    df_reports = pd.DataFrame(split, columns=[\"eval\", \"Quelle\", \"Start Ref.\", \"Ende Ref.\", \"Start Vergl.\", \"Ende Vergl.\"])\n    #date_col = [\"Start Ref.\", \"Ende Ref.\", \"Start Vergl.\", \"Ende Vergl.\"]\n    #df_reports[date_col]= df_reports[date_col].apply(pd.to_datetime, errors=\"coerce\")\n    df_reports= df_reports.drop([\"eval\"], axis=\"columns\")\n    df_reports=df_reports.replace({\"Quelle\":{\"df\":\"Aufträge\",\"df2\":\"Positionen\"}})\n    return df_reports"
            start_line: 23
            end_line: 32
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: app_pages.page5.refresh_table
            name: refresh_table
            args[0]:
            docstring: null
            source_code: "def refresh_table():\n     st.session_state.reports_table = fetch_reports_table()"
            start_line: 34
            end_line: 35
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: app_pages.page5.show_page
            name: show_page
            args[0]:
            docstring: null
            source_code: "def show_page():\n    con = duckdb.connect(\"resources/dashboard_data.duckdb\", read_only=True)\n   \n    try:\n        min_date = con.execute(\"SELECT MIN(CRMEingangszeit) FROM auftragsdaten\").fetchone()[0].date()\n        max_date = con.execute(\"SELECT MAX(CRMEingangszeit) FROM auftragsdaten\").fetchone()[0].date()\n    finally:\n         con.close()    \n    min_date_6m = min_date + pd.Timedelta(weeks=26)\n    min_date_12m = min_date + pd.Timedelta(weeks=52)\n    report_html= None\n    path_to_report = None\n    \n    \n    form_column, tab_column = st.columns(2)\n    dynamic_reset_ph = tab_column.empty()\n    with form_column:\n        #Date-Picker, für Referenzset und Evalset, Quelldatenset\n        with st.expander(\"Reportauswahl\", width=750):\n            with st.form(\"Vergleichszeiträume\"):\n            \n                source_designation = st.selectbox(\n                    \"Datenquelle\",\n                    (\"Auftragsdaten\", \"Positionsdaten\"),\n                    width=175\n                    )\n                \n                col1, col2 = st.columns(2)\n                with col1:\n                    start_date_reference, end_date_reference = st.date_input(\n                        \"Referenzrahmen\",\n                        (min_date,min_date_6m),\n                        min_date,max_date, \n                        width=225                                   \n                        )\n                with col2:    \n                    start_date_eval, end_date_eval = st.date_input(\n                        \"Vergleichsrahmen\",\n                        (min_date_6m,min_date_12m),\n                        min_date,max_date,\n                        width=225\n                        )\n                    \n                force_reload = st.checkbox(\"Refresh erzwingen?\")\n\n                submitted = st.form_submit_button(\"Report anzeigen\")\n\n                if submitted:\n                #Check für den Ordnerpfad\n                    st.write(\"Pfad zum Report:\")\n                    if source_designation == \"Auftragsdaten\":\n                            source_type = \"df\"\n                            path_to_report = f\"resources/reports/eval_df_{str(start_date_reference)}_{str(end_date_reference)}_{str(start_date_eval)}_{str(end_date_eval)}.html\"\n                            st.code(\"./\"+path_to_report)\n                    if source_designation == \"Positionsdaten\":\n                            source_type = \"df2\"\n                            path_to_report = (\"resources/reports/eval_df2_\"+\n                                                            str(start_date_reference)+\"_\"+\n                                                            str(end_date_reference)+\"_\"+\n                                                            str(start_date_eval)+\"_\"+\n                                                            str(end_date_eval)+\n                                                            \".html\") \n                            st.code(\"./\"+path_to_report)\n                    if force_reload:\n                            st.write(\"-> Report wird neu erstellt <-\")\n    \n    with dynamic_reset_ph:\n        with st.expander(\"Verfügbare Reports\", width= 750):\n            refresh_table()\n            st.dataframe(st.session_state.reports_table)                                 \n\n    #prevents use of uninitialized values further down the line    \n    if not path_to_report:\n         st.write(\"Warten auf Auswahl..\")\n         st.stop()\n\n         \n    try: #versuche den gewünschten report zu finden\n        if not force_reload:\n            report_html = open(path_to_report, \"r\", encoding=\"utf-8\").read()\n            components.html(report_html, height= 1000, scrolling=True)\n        else:\n             raise FileNotFoundError    \n    except FileNotFoundError: #erstelle fehlenden report, dazu feedback\n        with st.empty():\n            source_df = load_df(source_type).dropna(subset=['CRMEingangszeit'])\n            st.write(\"Report ist noch nicht vorhanden und wird erstellt. Dies kann einige Momente dauern...\")\n            ddm.data_drift_evaluation(\n                source_df, \n                start_date_reference, end_date_reference,\n                start_date_eval, end_date_eval\n                )\n            st.write(\"Report erstellt.\")\n        report_html = open(path_to_report, \"r\", encoding=\"utf-8\").read()    \n        components.html(report_html, height= 1000, scrolling=True)\n    \n    dynamic_reset_ph.empty()\n    with tab_column:\n        with st.expander(\"Verfügbare Reports\", width= 750):\n            refresh_table()\n            st.dataframe(st.session_state.reports_table)"
            start_line: 38
            end_line: 138
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    build_db.py:
      ast_nodes:
        imports[6]: pandas,duckdb,time,metrics,os,data_cleaning
        functions[1]:
          - mode: function_analysis
            identifier: build_db.calc_percent
            name: calc_percent
            args[1]: row
            docstring: null
            source_code: "def calc_percent(row):\n    old = row['Old_Value']\n    new = row['Current_Value']\n    diff = row['Absolute_Change']\n\n    if old == 0:\n        if new == 0:\n            return 0.0\n        else:\n            return 100.0\n\n    return (diff / old) * 100"
            start_line: 314
            end_line: 325
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    dashboard.py:
      ast_nodes:
        imports[10]: time,pandas,streamlit,metrics,streamlit_option_menu.option_menu,app_pages.page1,app_pages.page2,app_pages.page3,app_pages.page4,app_pages.page5
        functions[7]:
          - mode: function_analysis
            identifier: dashboard.load
            name: load
            args[0]:
            docstring: null
            source_code: "def load():\n    df = pd.read_parquet(\"resources/Auftragsdaten_konvertiert\")\n    df2 = pd.read_parquet(\"resources/Positionsdaten_konvertiert\")\n    return df, df2"
            start_line: 15
            end_line: 18
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: dashboard.compute_metrics_df1
            name: compute_metrics_df1
            args[0]:
            docstring: null
            source_code: "def compute_metrics_df1():\n    print(\"Calculating metrics for df1 (Auftragsdaten)...\")\n    df, _ = load()\n\n    calc_time_start = time.time()\n    plausi_diff_df, plausi_count, plausi_avg = mt.plausibilitaetscheck_forderung_einigung(df)\n    print(\"Calculated plausi_diff_list, plausi_count, plausi_avg in \"\n          f\"{round(time.time() - calc_time_start, 2)}s\")\n\n    calc_time_start = time.time()\n    zeitwert_errors_df = mt.check_zeitwert(df)\n    print(\"Calculated zeitwert_errors_list in \"\n          f\"{round(time.time() - calc_time_start, 2)}s\")\n\n    calc_time_start = time.time()\n    proforma_df, proforma_count = mt.proformabelege(df)\n    print(\"Calculated proforma_df, proforma_count in \"\n          f\"{round(time.time() - calc_time_start, 2)}s\")\n\n    calc_time_start = time.time()\n    grouped_col_ratios_df1, grouped_row_ratios_df1 = mt.data_cleanliness(df)\n    print(\"Calculated grouped_col_ratios_df1, grouped_row_ratios_df1 in \"\n          f\"{round(time.time() - calc_time_start, 2)}s\")\n\n    calc_time_start = time.time()\n    error_freq_df = mt.error_frequency_by_weekday_hour(\n        df,\n        time_col=\"CRMEingangszeit\",\n        relevant_columns=None\n    )\n    print(\"Calculated error_freq_df (by weekday) in \"\n          f\"{round(time.time() - calc_time_start, 2)}s\")\n    \n    calc_time_start = time.time()\n    df_added = mt.handwerker_gewerke_outlier(df)\n    df_true = df_added[df_added['is_outlier'] == True].copy()\n    df_true['Check_Result'] = mt.check_keywords(df_true)\n\n    print(\"Calculated craftsman/craft comparison in \"\n          f\"{round(time.time() - calc_time_start, 2)}s\")\n    \n    calc_time_start = time.time()\n    fn_stats_df, fn_details_df = mt.false_negative_df1(df)\n    fn_count_df = fn_stats_df['Fehler'].sum()\n    print(\"Calculated fn_stats, fn_details in \"\n          f\"{round(time.time() - calc_time_start, 2)}s\")\n\n    calc_time_start = time.time()\n    metrics_df1 = {\n        \"row_count\": mt.count_rows(df),\n        \"null_ratio_cols\": mt.ratio_null_values_column(df),\n        \"null_ratio_rows\": mt.ratio_null_values_rows(df),\n        \"test_kundengruppen_anzahl\": mt.Kundengruppe_containing_test(df),\n        \"test_data_df\": mt.Kundengruppe_containing_test(df, return_frame=True),\n        \"plausi_forderung_einigung_df\": plausi_diff_df,\n        \"plausi_forderung_einigung_count\": plausi_count,\n        \"plausi_forderung_einigung_avg_diff\": plausi_avg,\n        \"grouped_col_ratios\": grouped_col_ratios_df1,\n        \"grouped_row_ratios\": grouped_row_ratios_df1,\n        \"proforma_belege_df\": proforma_df,\n        \"proforma_belege_count\": proforma_count,\n        \"above_50k_df\": mt.above_50k(df),\n        \"zeitwert_error_df\": zeitwert_errors_df,\n        \"zeitwert_errors_count\": len(zeitwert_errors_df),\n        \"error_frequency_weekday_hour\": error_freq_df,\n        \"false_negative\": fn_count_df,\n        \"false_negative_stats\": fn_stats_df,\n        \"false_negative_details\": fn_details_df,\n        \"mismatched_entries\": mt.mismatched_entries(df),\n        \"handwerker_gewerke_outlier\": df_true,\n        \"empty_orders_count\": mt.empty_orders(df)\n    }\n    print(\"Calculated all other metrics for df1 in \"\n          f\"{round(time.time() - calc_time_start, 2)}s\")\n\n    return metrics_df1"
            start_line: 21
            end_line: 96
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: dashboard.compute_metrics_df2
            name: compute_metrics_df2
            args[0]:
            docstring: Teure Metriken für df2 (Positionsdaten) – gecached.
            source_code: "def compute_metrics_df2():\n    \"\"\"Teure Metriken für df2 (Positionsdaten) – gecached.\"\"\"\n    print(\"Calculating metrics for df2 (Positionsdaten)...\")\n    _, df2 = load()\n    calc_time_start = time.time()\n    \n    plausi_diff_df, plausi_count, plausi_avg = mt.plausibilitaetscheck_forderung_einigung(df2)\n    print(\"Calculated plausi_diff_list, plausi_count, plausi_avg in \"\n          f\"{round(time.time() - calc_time_start, 2)}s\")\n    \n    calc_time_start = time.time()\n    disc_stats, disc_details = mt.discount_details(df2)\n    print(\"Calculated discount_stats, discount_details in \"\n          f\"{round(time.time() - calc_time_start, 2)}s\")\n    \n    calc_time_start = time.time()\n    fn_stats_df2, fn_details_df2 = mt.false_negative_df2(df2)\n    print(\"Calculated fn_stats, fn_details in \"\n          f\"{round(time.time() - calc_time_start, 2)}s\")\n    \n    metrics_df2 = {\n        \"row_count\": mt.count_rows(df2),\n        \"null_ratio_cols\": mt.ratio_null_values_column(df2),\n        \"null_ratio_rows\": mt.ratio_null_values_rows(df2),\n        \"discount_check_errors\": mt.discount_check(df2), #\n        \"position_counts_per_rechnung\": mt.position_count(df2),\n        \"plausi_forderung_einigung_df2\": plausi_diff_df,\n        \"plausi_forderung_einigung_count\": plausi_count,\n        \"plausi_forderung_einigung_avg_diff\": plausi_avg,\n        \"false_negative_stats\": fn_stats_df2,\n        \"false_negative_details\": fn_details_df2,\n        \"discount_stats\": disc_stats,\n        \"discount_details\": disc_details,\n    }\n    print(\"Calculated all metrics for df2 in \"\n          f\"{round(time.time() - calc_time_start, 2)}s\")\n    return metrics_df2"
            start_line: 100
            end_line: 136
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: dashboard.compute_metrics_combined
            name: compute_metrics_combined
            args[0]:
            docstring: "Metriken, die beide DataFrames brauchen – gecached."
            source_code: "def compute_metrics_combined():\n    \"\"\"Metriken, die beide DataFrames brauchen – gecached.\"\"\"\n    print(\"Calculating combined metrics...\")\n    df, df2 = load()\n    calc_time_start = time.time()\n    kva_id_unique, pos_id_unique = mt.uniqueness_check(df, df2)\n    auftraege_abgleich = mt.abgleich_auftraege(df, df2)\n    metrics_combined = {\n        \"kvarechnung_id_is_unique\": kva_id_unique,\n        \"position_id_is_unique\": pos_id_unique,\n        \"auftraege_abgleich\": auftraege_abgleich\n    }\n    print(\"Calculated all combined metrics in \"\n          f\"{round(time.time() - calc_time_start, 2)}s\")\n    return metrics_combined"
            start_line: 140
            end_line: 154
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: dashboard.compute_positions_over_time
            name: compute_positions_over_time
            args[0]:
            docstring: Positionsanzahl pro Auftrag über Zeit – gecached.
            source_code: "def compute_positions_over_time():\n    \"\"\"Positionsanzahl pro Auftrag über Zeit – gecached.\"\"\"\n    print(\"Calculating positions per order over time...\")\n    df, df2 = load()\n    calc_time_start = time.time()\n    positions_over_time_df = mt.positions_per_order_over_time(\n        df,\n        df2,\n        time_col=\"CRMEingangszeit\"\n    )\n    print(\"Calculated positions over time in \"\n          f\"{round(time.time() - calc_time_start, 2)}s\")\n    return positions_over_time_df"
            start_line: 158
            end_line: 170
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: dashboard.compute_issues_df
            name: compute_issues_df
            args[0]:
            docstring: null
            source_code: "def compute_issues_df():\n\n    zeitwert = metrics_df1.get(\"zeitwert_errors_count\")\n    df_above_50k = metrics_df1.get(\"above_50k_df\")\n    df_semantic = metrics_df1.get(\"mismatched_entries\")\n    test_data_count = metrics_df1.get(\"test_kundengruppen_anzahl\")\n    df_mismatch = metrics_combined.get(\"auftraege_abgleich\")\n    df_outliers_true = metrics_df1.get(\"handwerker_gewerke_outlier\")\n    plausibility_error_count_df = metrics_df1.get(\"plausi_forderung_einigung_count\")\n    plausibility_error_count_df2 = metrics_df1.get(\"plausi_forderung_einigung_count\")\n    discount_logic_errors = metrics_df2.get(\"discount_check_errors\")\n    proforma_count = metrics_df1.get(\"proforma_belege_count\")\n    fn_details1 = metrics_df1.get(\"false_negative_details\")\n    fn_details2 = metrics_df2.get(\"false_negative_details\")\n    \n    numeric_issues = zeitwert + len(df_above_50k) + len(df_mismatch)\n    text_issues = test_data_count + len(df_outliers_true) + len(df_semantic)\n    plausi_issues = plausibility_error_count_df + plausibility_error_count_df2 + discount_logic_errors + proforma_count + len(fn_details1) + len(fn_details2)\n    overall_issues = numeric_issues + text_issues + plausi_issues\n\n    issues = {\n        'numeric_issues': [numeric_issues],\n        'text_issues': [text_issues],\n        'plausi_issues': [plausi_issues],\n        'overall_issues': [overall_issues],\n        'count_zeitwert_errors': [zeitwert],\n        'count_above_50k': [len(df_above_50k)],\n        'count_handwerker_outliers': [len(df_outliers_true)],\n        'count_semantic_outliers': [len(df_semantic)],\n        'count_abweichung_summen': [len(df_mismatch)],\n        'count_plausibility_errors_df': [plausibility_error_count_df],\n        'count_plausibility_errors_df2': [plausibility_error_count_df2],\n        'count_false_negative_df': [len(fn_details1)],\n        'count_false_negative_df2': [len(fn_details2)],\n    }\n\n    df_issues = pd.DataFrame(issues)\n\n    return df_issues"
            start_line: 173
            end_line: 211
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: dashboard.compute_comparison_metrics
            name: compute_comparison_metrics
            args[0]:
            docstring: null
            source_code: "def compute_comparison_metrics():\n    data = {\n    'Metric': [\n        'avg_plausibility_diff_df', \n        'avg_plausibility_diff_df2', \n        'count_above_50k', \n        'count_abweichung_summen', \n        'count_discount_logic_errors'\n    ],\n    'Current_Value': [\n        1560.530884, \n        406.184998, \n        306.000000, \n        34581.000000, \n        22110.000000\n    ],\n    'Old_Value': [\n        1560.530884, \n        406.184998, \n        306.000000, \n        34581.000000, \n        22110.000000\n    ],\n    'Percent_Change': [0.0, 0.0, 0.0, 0.0, 0.0]\n}\n\n    # Define the specific indices from your example\n    indices = [10, 13, 21, 24, 15]\n\n    df = pd.DataFrame(data, index=indices)\n\n    return df"
            start_line: 216
            end_line: 247
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    data_cleaning.py:
      ast_nodes:
        imports[3]: pandas,numpy,metrics
        functions[2]:
          - mode: function_analysis
            identifier: data_cleaning.load_data
            name: load_data
            args[0]:
            docstring: "This function loads the raw data from the programs 'resources' folder. Three .parquet files are expected: 'Auftragsdaten', 'Positionsdaten' and 'Auftragsdaten_Zeit'.\n\nReturns\n-------\npandas.DataFrame\n    Auftragsdaten\npandas.DataFrame\n    Positionsdaten\npandas.DataFrame\n    Auftragsdaten_Zeit"
            source_code: "def load_data():\n    \"\"\"This function loads the raw data from the programs 'resources' folder. Three .parquet files are expected: 'Auftragsdaten', 'Positionsdaten' and 'Auftragsdaten_Zeit'.\n\n    Returns\n    -------\n    pandas.DataFrame\n        Auftragsdaten\n    pandas.DataFrame\n        Positionsdaten\n    pandas.DataFrame\n        Auftragsdaten_Zeit\n    \"\"\"\n    print(\"Loading Data...\")\n    df = pd.read_parquet(\"resources/Auftragsdaten\")\n    df2 = pd.read_parquet(\"resources/Positionsdaten\")\n    df3 = pd.read_parquet(\"resources/Auftragsdaten_Zeit\")\n\n    return df, df2, df3"
            start_line: 7
            end_line: 24
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: data_cleaning.data_cleaning
            name: data_cleaning
            args[3]: df,df2,df3
            docstring: "This function merges the given raw data sets with appropriate timestamp data and adds columns for more expedient metric computation. \n\nParameters\n----------\ndf : pandas.DataFrame\n    DataFrame containing 'Auftragsdaten' data set\ndf2 : pandas.DataFrame\n    DataFrame containing 'Positionsdaten' data set_\ndf3 : pandas.DataFrame\n    DataFrame containing 'Auftragsdaten_Zeit' data set\n\nReturns\n-------\npandas.DataFrame, pandas.DataFrame\n   cleaned Auftrags- and Positionsdaten sets with timestamp information added\n    \nNotes\n-----\nList of steps performed:\n- (extensive) timestamps added to df from df3, cleanup after merge\n- CRMEingangszeit timestamp added to df2, grouped by order\n- New column (int16) in df: number of positions for order\n- Replace custom null indicators with standard Null\n- Correction of common typing errors from data entry\n- Tyoe conversions and downcasting as appropriate for each column\n- New column (bool) in df2 with information on row correctly reflecting a discount position"
            source_code: "def data_cleaning(df, df2, df3):\n    \"\"\"This function merges the given raw data sets with appropriate timestamp data and adds columns for more expedient metric computation. \n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing 'Auftragsdaten' data set\n    df2 : pandas.DataFrame\n        DataFrame containing 'Positionsdaten' data set_\n    df3 : pandas.DataFrame\n        DataFrame containing 'Auftragsdaten_Zeit' data set\n\n    Returns\n    -------\n    pandas.DataFrame, pandas.DataFrame\n       cleaned Auftrags- and Positionsdaten sets with timestamp information added\n        \n    Notes\n    -----\n    List of steps performed:\n    - (extensive) timestamps added to df from df3, cleanup after merge\n    - CRMEingangszeit timestamp added to df2, grouped by order\n    - New column (int16) in df: number of positions for order\n    - Replace custom null indicators with standard Null\n    - Correction of common typing errors from data entry\n    - Tyoe conversions and downcasting as appropriate for each column\n    - New column (bool) in df2 with information on row correctly reflecting a discount position\n\n    \n\n    \"\"\"\n    print(\"Starting data cleaning...\")\n    #add timestamp columns to Auftragsdaten\n    df = pd.merge(df, df3, on='KvaRechnung_ID', how='left') \n    # remove duplicate columns introduced by df3\n    df = df.drop([\"Auftrag_ID_y\", \"Schadensnummer_y\"], axis=1) \n    #restore original column naming in df\n    df = df.rename(columns={'Auftrag_ID_x': 'AuftragID', 'Schadensnummer_x': 'Schadensnummer'}) \n    #transfer timestamps for orders to associated position data\n    df2 = pd.merge(df2,df[['KvaRechnung_ID','CRMEingangszeit']], on='KvaRechnung_ID', how='left')\n   \n    #add int column with number of positions for every entry in Auftragsdaten, downcast to save space\n    df = pd.merge(df,mt.position_count(df2), on='KvaRechnung_ID', how='left')\n    df['PositionsAnzahl'] = df['PositionsAnzahl'].astype('Int16')\n\n    #checking effectivness of dtype changes, print initial state\n    print(f\"Memory usage before converting:\")\n    df.info(memory_usage='deep')\n    df2.info(memory_usage='deep')\n\n    #multiple columns contain custom indicators or empty fields, these are transformed into proper null values\n    None_placeholder = [\"-\", \"(leer)\", \"(null)\", \"wird vergeben\", \"unbekannter HW\", \"#unbekannter hw\", \"Allgemeine Standardbeschreibungen\"]\n    for placeholder in None_placeholder:\n        df = df.replace(placeholder, pd.NA)\n        df2 = df2.replace(placeholder, pd.NA)\n\n    # Replace Typing Error in Schadensart_Name\n    df = df.replace(\"Betriebsunterbrechnung\", \"Betriebsunterbrechung\")\n\n    # Replace Typing Error in Falltyp_Name\n    df = df.replace(\"Überpannung Heizung\", \"Überspannung Heizung\")\n    df = df.replace(\"Kfz\", \"KFZ\")\n    df = df.replace(\"Schliessanlagen\", \"Schließanlagen\")\n\n    # Converting Object Types for df\n    columns_to_convert = ['Land', 'PLZ_SO', 'PLZ_HW', 'PLZ_VN', 'address1_postalcode', \n                        'Schadenart_Name', 'Falltyp_Name', 'Gewerk_Name', 'Kundengruppe', \n                        'Handwerker_Name']\n\n    # Convert Column with low Cardinality to Category\n    df[columns_to_convert] = df[columns_to_convert].astype('category')\n\n    # Convert the other Object Columns to String\n    object_columns = df.select_dtypes('object').columns\n    df[object_columns] = df[object_columns].astype('string')\n\n\n    # Downcast integer columns\n    int_cols = df.select_dtypes(include=['int64']).columns\n    for col in int_cols:\n        df[col] = pd.to_numeric(df[col], downcast='integer')\n\n    # Downcast float columns\n    df_below_four_decimals = df.select_dtypes(include='float') \\\n                                        .apply(lambda col: np.isclose(col, col.round(4))) \\\n                                        .any() \\\n                                        .loc[lambda s: s] \\\n                                        .index.tolist()\n\n    df[df_below_four_decimals] = df[df_below_four_decimals].astype('float32')\n\n    # Converting Object Types for df2\n    df2_columns_to_convert = ['KvaRechnung_ID', 'KvaRechnung_Nummer', 'Mengeneinheit', 'Bemerkung']\n\n    # Converting to Category\n    df2[df2_columns_to_convert] = df2[df2_columns_to_convert].astype('category')\n\n    # Converting the rest to strings\n    object_columns = df2.select_dtypes('object').columns\n    df2[object_columns] = df2[object_columns].astype('string')\n\n    # Downcast float columns\n    df2_below_four_decimals = df2.select_dtypes(include='float') \\\n                                        .apply(lambda col: np.isclose(col, col.round(4))) \\\n                                        .any() \\\n                                        .loc[lambda s: s] \\\n                                        .index.tolist()\n\n    df2[df2_below_four_decimals] = df2[df2_below_four_decimals].astype('float32')\n\n    #checking effectivness of dtype changes, print 'after' state\n    print(f\"Memory usage after converting:\")\n    df.info(memory_usage='deep')\n    df2.info(memory_usage='deep')\n\n\n    # Add boolean column to check if row is a discount position\n    keywords = [\"Rabatt\", \"Skonto\", \"Nachlass\", \"Gutschrift\", \"Bonus\", \"Abzug\", \"Minderung\", \"Gutschein\", \"Erlass\", \"Storno\", \"Kulanz\"]\n\n    pattern = '|'.join(keywords)\n    df2['ist_Abzug'] = df2['Bezeichnung'].str.contains(pattern, case=False, regex=True, na=False)\n    normal_position = (df2['Einigung_Netto'] >= 0) & (df2['ist_Abzug'] == False)\n    discount_position = (df2['Einigung_Netto'] < 0) & (df2['ist_Abzug'] == True)\n\n    df2['Plausibel'] = normal_position | discount_position\n\n    return df, df2"
            start_line: 27
            end_line: 153
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    data_drift_metrics.py:
      ast_nodes:
        imports[5]: pandas,evidently.Dataset,evidently.DataDefinition,evidently.Report,evidently.presets.DataDriftPreset
        functions[3]:
          - mode: function_analysis
            identifier: data_drift_metrics.check_start_end_date
            name: check_start_end_date
            args[2]: start,end
            docstring: "Helper function. Checks if end follows start chronologically and reorders the two if needed. \n\nParameters\n----------\nstart : datetime\n    The assumed beginnig of the interval.\nend : datetime\n    The assumed end of the interval.\n\nReturns\n-------\ndatetime, datetime\n    Pair of chronologically sorted datetime values."
            source_code: "def check_start_end_date(start,end):\n    \"\"\"Helper function. Checks if end follows start chronologically and reorders the two if needed. \n\n    Parameters\n    ----------\n    start : datetime\n        The assumed beginnig of the interval.\n    end : datetime\n        The assumed end of the interval.\n\n    Returns\n    -------\n    datetime, datetime\n        Pair of chronologically sorted datetime values.\n    \"\"\"\n    if(start > end):\n       start, end = end, start\n    return start, end"
            start_line: 21
            end_line: 38
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: data_drift_metrics.datetime_slice_mask
            name: datetime_slice_mask
            args[3]: df,start_date,end_date
            docstring: "Helper function. Returns a chronologically sliced Dataset according to passed datetime.\n\nParameters\n----------\ndf : pandas.Dataframe\n    input df\nstart_date : date\n\nend_date : date\n  \n\nReturns\n-------\nevidently.Dataset\n    sliced DataFrame, converted to Dataset."
            source_code: "def datetime_slice_mask(df, start_date, end_date):\n    \"\"\"Helper function. Returns a chronologically sliced Dataset according to passed datetime.\n\n    Parameters\n    ----------\n    df : pandas.Dataframe\n        input df\n    start_date : date\n    \n    end_date : date\n      \n\n    Returns\n    -------\n    evidently.Dataset\n        sliced DataFrame, converted to Dataset.\n    \"\"\"\n    #Type cleanup to full datetime, due to the dashboard passing only date-level precision values\n    start_date = pd.to_datetime(start_date).replace(hour=0,minute=0,second=0)\n    end_date = pd.to_datetime(end_date).replace(hour=0,minute=0,second=0) \n\n    mask =(df[\"CRMEingangszeit\"] >= start_date) & (df[\"CRMEingangszeit\"] < end_date)\n\n    if 'Kundengruppe' in df.columns: #evaluates true if Auftragsdaten-df was passed \n        sliced_ds = Dataset.from_pandas(\n            df.loc[mask],\n            data_definition=schema_df\n        ) \n    if 'Menge' in df.columns: #evaluates true if Positionsdaten-df was passed\n        sliced_ds = Dataset.from_pandas(\n            df.loc[mask],\n            data_definition=schema_df2\n        )  \n    return sliced_ds"
            start_line: 41
            end_line: 74
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: data_drift_metrics.data_drift_evaluation
            name: data_drift_evaluation
            args[5]: df,start_date_reference,end_date_reference,start_date_eval,end_date_eval
            docstring: "Uses the standard preset in the evidentlyai framework to evaluate data drift between two samples (chosen by time interval) from the passed DataFrame. The resulting Snapshot object is saved as html for easy embedding.\n\nParameters\n----------\ndf : pandas.DataFrame\n    DataFrame to sample from\nstart_date_reference : datetime\n    starting datetime of the reference, baseline dataset\nend_date_reference : datetime\n    ending datetime of the reference, baseline dataset\nstart_date_eval : datetime\n    starting datetime of the evaluated dataset \nend_date_eval : datetime\n    starting datetime of the evaluated dataset"
            source_code: "def data_drift_evaluation(df, start_date_reference, end_date_reference, start_date_eval, end_date_eval):\n    \"\"\"Uses the standard preset in the evidentlyai framework to evaluate data drift between two samples (chosen by time interval) from the passed DataFrame. The resulting Snapshot object is saved as html for easy embedding.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to sample from\n    start_date_reference : datetime\n        starting datetime of the reference, baseline dataset\n    end_date_reference : datetime\n        ending datetime of the reference, baseline dataset\n    start_date_eval : datetime\n        starting datetime of the evaluated dataset \n    end_date_eval : datetime\n        starting datetime of the evaluated dataset\n    \n    \"\"\"\n    #check if start and end dates are in chronologicl order, switch if needed\n    start_date_reference, end_date_reference = check_start_end_date(start_date_reference, end_date_reference)\n    start_date_eval, end_date_eval = check_start_end_date(start_date_eval, end_date_eval)\n    \n    #create sliced datasets for analysis (mask-based)\n    reference_data = datetime_slice_mask(df,start_date_reference,end_date_reference)\n    eval_data = datetime_slice_mask(df, start_date_eval,end_date_eval) \n    \n    if 'Kundengruppe' in df.columns: #evaluates true if Auftragsdaten-df was passed\n        report = Report([\n            DataDriftPreset(\n                columns=[\"Forderung_Netto\", \"Empfehlung_Netto\", \"Einigung_Netto\", \"Differenz_vor_Zeitwert_Netto\",\"Land\",\"Schadenart_Name\", \"Falltyp_Name\", \"Gewerk_Name\"]\n                )\n        ])\n        my_eval = report.run(eval_data, reference_data)\n        my_eval.save_html(\"resources/reports/eval_df_\"+\n                          str(start_date_reference)+\"_\"+\n                          str(end_date_reference)+\"_\"+\n                          str(start_date_eval)+\"_\"+\n                          str(end_date_eval)+\n                          \".html\")\n    if 'Menge' in df.columns: #evaluates true if Positionsdaten-df was passed\n        report = Report([\n            DataDriftPreset(\n                columns=[\"Menge\",\"Menge_Einigung\", \"EP\", \"EP_Einigung\", \"Forderung_Netto\", \"Einigung_Netto\"]\n                )\n        ])\n        my_eval = report.run(eval_data, reference_data)\n        my_eval.save_html(\"resources/reports/eval_df2_\"+\n                          str(start_date_reference)+\"_\"+\n                          str(end_date_reference)+\"_\"+\n                          str(start_date_eval)+\"_\"+\n                          str(end_date_eval)+\n                          \".html\")"
            start_line: 77
            end_line: 127
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    data_exploration.py:
      ast_nodes:
        imports[3]: pandas,streamlit,time
        functions[1]:
          - mode: function_analysis
            identifier: data_exploration.load
            name: load
            args[0]:
            docstring: null
            source_code: "def load():\n\n    df = pd.read_parquet(\"resources/Auftragsdaten_konvertiert\")\n    df2 = pd.read_parquet(\"resources/Positionsdaten_konvertiert\")\n    return df, df2"
            start_line: 7
            end_line: 11
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    data_exploration2.py:
      ast_nodes:
        imports[1]: pandas
        functions[0]:
        classes[0]:
    db_dashboard.py:
      ast_nodes:
        imports[9]: time,streamlit,duckdb,streamlit_option_menu.option_menu,app_pages.page1,app_pages.page2,app_pages.page3,app_pages.page4,app_pages.page5
        functions[7]:
          - mode: function_analysis
            identifier: db_dashboard.get_db_connection
            name: get_db_connection
            args[0]:
            docstring: Establishes a read-only connection to the DuckDB database.
            source_code: "def get_db_connection():\n    \"\"\"Establishes a read-only connection to the DuckDB database.\"\"\"\n    DB_PATH = \"resources/dashboard_data.duckdb\"\n    con = duckdb.connect(DB_PATH, read_only=True)\n    return con"
            start_line: 17
            end_line: 21
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: db_dashboard.compute_metrics_df1
            name: compute_metrics_df1
            args[0]:
            docstring: null
            source_code: "def compute_metrics_df1(): # Vorberechnete Daten für Auftragsdaten werden aus duckdb geladen und einmalig gecached (Rohdaten werden nicht vorgehalten)\n    print(\"Loading metrics for df1 (Auftragsdaten) from DB...\")\n    start_time = time.time()\n\n    con = get_db_connection()\n    scalars = con.execute(\"SELECT * FROM scalar_metrics\").df().iloc[0]\n\n    null_ratios_cols = con.execute(\"SELECT * FROM metric_null_ratios_per_column\").df()\n    \n    test_data_df = con.execute(\"SELECT * FROM metric_test_data_entries\").df()\n\n    plausi_df = con.execute(\"SELECT * FROM metric_plausibility_diffs_auftragsdaten\").df()\n\n    grouped_col_ratios_df1 = con.execute(\"SELECT * FROM metric_cleanliness_cols_grouped_auftragsdaten\").df()\n    \n    row_ratios_df = con.execute(\"SELECT * FROM metric_cleanliness_rows_grouped_auftragsdaten\").df()\n    grouped_row_ratios_df1 = row_ratios_df.set_index('Kundengruppe')['row_null_ratio']\n\n    proforma_df = con.execute(\"SELECT * FROM metric_proforma\").df()\n\n    above_50k_df = con.execute(\"SELECT * FROM metric_above_50k\").df()\n\n    zeitwert_df = con.execute(\"SELECT * FROM metric_zeitwert_errors\").df()\n\n    error_freq_df = con.execute(\"SELECT * FROM metric_error_heatmap\").df()\n\n    handwerker_outliers = con.execute(\"SELECT * FROM metric_handwerker_outliers\").df()\n\n    fn_stats_df1 = con.execute(\"SELECT * FROM metric_fn_stats_df1\").df()\n    fn_details_df1 = con.execute(\"SELECT * FROM metric_fn_details_df1\").df()\n    fn_count_df = fn_stats_df1['Fehler'].sum()\n    semantic_mismatches = con.execute(\"SELECT * FROM metric_semantic_mismatches\").df()\n\n    empty_orders_df = con.execute(\"SELECT * FROM metric_empty_orders_dataframe\").df()\n    \n    outliers_by_damage = con.execute(\"SELECT * FROM metric_outliers_by_damage\").df()\n\n\n    metrics_df1 = {\n        \"row_count\": scalars['count_total_orders'],\n        \"null_ratio_cols\": null_ratios_cols,\n        \"null_ratio_rows\": scalars['null_row_ratio_orders'],\n        \"test_kundengruppen_anzahl\": scalars['count_test_data_rows'],\n        \"test_data_df\": test_data_df,\n        \"plausi_forderung_einigung_df\": plausi_df,\n        \"plausi_forderung_einigung_count\": scalars['count_plausibility_errors_df'],\n        \"plausi_forderung_einigung_avg_diff\": scalars['avg_plausibility_diff_df'],\n        \"grouped_col_ratios\": grouped_col_ratios_df1,\n        \"grouped_row_ratios\": grouped_row_ratios_df1,\n        \"proforma_belege_df\": proforma_df,\n        \"proforma_belege_count\": scalars['count_proforma_receipts'],\n        \"above_50k_df\": above_50k_df,\n        \"zeitwert_error_df\": zeitwert_df,\n        \"zeitwert_errors_count\": len(zeitwert_df),\n        \"error_frequency_weekday_hour\": error_freq_df,\n        \"false_negative\": fn_count_df,\n        \"handwerker_gewerke_outlier\": handwerker_outliers,\n        \"false_negative_stats\": fn_stats_df1,\n        \"false_negative_details\": fn_details_df1,\n        \"empty_orders_count\": scalars['count_empty_orders'],\n        \"mismatched_entries\": semantic_mismatches,\n        \"empty_orders_df\": empty_orders_df,\n        \"outliers_by_damage\": outliers_by_damage\n    }\n    \n    print(f\"Loaded metrics for df1 in {round(time.time() - start_time, 2)}s\")\n    return metrics_df1"
            start_line: 25
            end_line: 91
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: db_dashboard.compute_metrics_df2
            name: compute_metrics_df2
            args[0]:
            docstring: null
            source_code: "def compute_metrics_df2(): # Analog für alle Metriken zu Positionsdaten\n    print(\"Loading metrics for df2 (Positionsdaten) from DB...\")\n    start_time = time.time()\n    \n    con = get_db_connection()\n    scalars = con.execute(\"SELECT * FROM scalar_metrics\").df().iloc[0]\n\n    plausi_df2 = con.execute(\"SELECT * FROM metric_plausibility_diffs_positionsdaten\").df()\n\n    position_counts_df = con.execute(\"SELECT * FROM metric_position_count_positionsdaten\").df()\n\n    fn_stats_df2 = con.execute(\"SELECT * FROM metric_fn_stats_df2\").df()\n    fn_details_df2 = con.execute(\"SELECT * FROM metric_fn_details_df2\").df()\n    disc_stats = con.execute(\"SELECT * FROM metric_discount_stats\").df()\n    disc_details = con.execute(\"SELECT * FROM metric_discount_details\").df()\n    null_ratio_cols = con.execute(\"SELECT * FROM metric_cleanliness_cols_ungrouped_positionsdaten\").df()\n\n\n    metrics_df2 = {\n        \"row_count\": scalars['count_total_positions'],\n        \"null_ratio_cols\": null_ratio_cols,\n        \"null_ratio_rows\": scalars['null_row_ratio_positions'] if 'null_row_ratio_positions' in scalars else 0,\n        \"discount_check_errors\": scalars['count_discount_logic_errors'],\n        \"position_counts_per_rechnung\": position_counts_df,\n        \"plausi_forderung_einigung_count\": scalars[\"count_plausibility_errors_df2\"], \n        \"plausi_forderung_einigung_avg_diff\": scalars[\"avg_plausibility_diff_df2\"],\n        \"false_negative_stats\": fn_stats_df2,\n        \"false_negative_details\": fn_details_df2,\n        \"discount_stats\": disc_stats,\n        \"discount_details\": disc_details,\n        \"plausi_forderung_einigung_df2\": plausi_df2\n    }\n\n    print(f\"Loaded metrics for df2 in {round(time.time() - start_time, 2)}s\")\n    return metrics_df2"
            start_line: 95
            end_line: 129
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: db_dashboard.compute_metrics_combined
            name: compute_metrics_combined
            args[0]:
            docstring: null
            source_code: "def compute_metrics_combined(): #Analog für alle Metriken über beide Datensets\n    print(\"Loading combined metrics from DB...\")\n    start_time = time.time()\n    \n    con = get_db_connection()\n    scalars = con.execute(\"SELECT * FROM scalar_metrics\").df().iloc[0]\n    auftraege_abgleich_df = con.execute(\"SELECT * FROM metric_order_pos_mismatch\").df()\n\n    metrics_combined = {\n        \"kvarechnung_id_is_unique\": bool(scalars['is_unique_kva_id']),\n        \"kvarechnung_nummer_land_is_unique\": bool(scalars['is_unique_kva_nr_per_land']),\n        \"position_id_is_unique\": bool(scalars['is_unique_position_id']),\n        \"auftraege_abgleich\": auftraege_abgleich_df\n    }\n    \n    print(f\"Loaded combined metrics in {round(time.time() - start_time, 2)}s\")\n    return metrics_combined"
            start_line: 133
            end_line: 149
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: db_dashboard.compute_positions_over_time
            name: compute_positions_over_time
            args[0]:
            docstring: null
            source_code: "def compute_positions_over_time(): \n    print(\"Loading positions per order over time from DB...\")\n    start_time = time.time()\n    con = get_db_connection()\n    df_pos_time = con.execute(\"SELECT * FROM metric_positions_over_time\").df()\n    \n    print(f\"Loaded positions over time in {round(time.time() - start_time, 2)}s\")\n    return df_pos_time"
            start_line: 153
            end_line: 160
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: db_dashboard.compute_comparison_metrics
            name: compute_comparison_metrics
            args[0]:
            docstring: null
            source_code: "def compute_comparison_metrics():\n    print(\"Loading comparison metrics from DB...\")\n    start_time = time.time()\n    \n    con = get_db_connection()\n    comparison_df = con.execute(\"SELECT * FROM metric_comparison\").df()\n    \n    print(f\"Loaded comparison metrics in {round(time.time() - start_time, 2)}s\")\n    return comparison_df"
            start_line: 163
            end_line: 171
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: db_dashboard.compute_issues_df
            name: compute_issues_df
            args[0]:
            docstring: null
            source_code: "def compute_issues_df():\n    print(\"Loading issues metrics from DB...\")\n    start_time = time.time()\n    con = get_db_connection()\n    issues_df = con.execute(\"SELECT * FROM issues\").df().iloc[0]\n    print(f\"Loaded issues metrics in {round(time.time() - start_time, 2)}s\")\n    return issues_df"
            start_line: 174
            end_line: 180
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    metrics.py:
      ast_nodes:
        imports[5]: pandas,numpy,re,sentence_transformers.SentenceTransformer,torch
        functions[24]:
          - mode: function_analysis
            identifier: metrics.load_data
            name: load_data
            args[0]:
            docstring: null
            source_code: "def load_data():\n    df = pd.read_parquet(\"resources/Auftragsdaten_konvertiert\")\n    df2 = pd.read_parquet(\"resources/Positionsdaten_konvertiert\")\n    return df, df2"
            start_line: 7
            end_line: 10
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: metrics.ratio_null_values_column
            name: ratio_null_values_column
            args[1]: input_df
            docstring: "Helper function that calculates the null-value-ratios (in percent) for each column of the supplied DataFrame. \n\nParameters\n----------\ninput_df : pandas.DataFrame\n    DataFrame that is to be evaluated.\n\nReturns\n-------\nratio_dict: pd.DataFrame\n    DataFrame of the form\n      |column_name |  null_ratio (float)|\n    with null_ratio being the percentage amount of null entries in the column  "
            source_code: "def ratio_null_values_column(input_df):\n    \"\"\"Helper function that calculates the null-value-ratios (in percent) for each column of the supplied DataFrame. \n\n    Parameters\n    ----------\n    input_df : pandas.DataFrame\n        DataFrame that is to be evaluated.\n\n    Returns\n    -------\n    ratio_dict: pd.DataFrame\n        DataFrame of the form\n          |column_name |  null_ratio (float)|\n        with null_ratio being the percentage amount of null entries in the column  \n    \"\"\"\n    null_ratio_df = pd.DataFrame(input_df.isna()\n                             .mean()\n                             .mul(100)\n                             .round(2)\n                             .rename(\"null_ratio\")\n                             .reset_index()\n                             )\n    return null_ratio_df"
            start_line: 13
            end_line: 35
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: metrics.ratio_null_values_rows
            name: ratio_null_values_rows
            args[2]: input_df,exclude_cols
            docstring: "Helper function that calculates the ratio of rows containing null values in all columns to total number of rows.\n\nParameters\n----------\ninput_df : pandas.DataFrame\n    DataFrame that is to be evaluated.\nexclude_columns : list, optional\n    List of column identifiers; these columns will be pruned from calculations, by default None.\n\nReturns\n-------\nrow_ratio: float\n    Percentage value of rows with at least one null value in the given columns."
            source_code: "def ratio_null_values_rows(input_df, exclude_cols=None):\n    \"\"\"Helper function that calculates the ratio of rows containing null values in all columns to total number of rows.\n    \n    Parameters\n    ----------\n    input_df : pandas.DataFrame\n        DataFrame that is to be evaluated.\n    exclude_columns : list, optional\n        List of column identifiers; these columns will be pruned from calculations, by default None.\n\n    Returns\n    -------\n    row_ratio: float\n        Percentage value of rows with at least one null value in the given columns.\n    \"\"\"\n    if exclude_cols is None:\n        exclude_cols = []\n    \n    df_to_check = input_df.drop(columns=exclude_cols, errors='ignore')\n\n    total_rows = len(df_to_check)\n    if total_rows == 0:\n        return 0.0\n\n    null_rows = df_to_check.isnull().any(axis=1).sum()\n    row_ratio = (null_rows / total_rows) * 100\n\n    return row_ratio"
            start_line: 38
            end_line: 65
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: metrics.Kundengruppe_containing_test
            name: Kundengruppe_containing_test
            args[2]: df,return_frame
            docstring: "Determines the number of rows in the 'Auftragsdaten' data set that are suspected to be part of a test data set. Optionally returns a data frame with all relevant instances. A row is conidered test data if the entry in 'Kundengruppe' is named accordingly.\n\nParameters\n----------\ndf : pandas.DataFrame\n    'Auftragsdaten'-DataFrame that is to be evaluated.\nreturn_frame : bool, optional\n    If True, this function returns exclusively a DataFrame with all found test data, by default False\n\nReturns\n-------\nanzahl_test: int\n    total number of test data rows.\ntest_Kundengruppen: pandas.DataFrame or None\n    DataFrame containing all found test data, returned only if return_frame = True"
            source_code: "def Kundengruppe_containing_test(df, return_frame=False):\n    \"\"\"Determines the number of rows in the 'Auftragsdaten' data set that are suspected to be part of a test data set. Optionally returns a data frame with all relevant instances. A row is conidered test data if the entry in 'Kundengruppe' is named accordingly.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        'Auftragsdaten'-DataFrame that is to be evaluated.\n    return_frame : bool, optional\n        If True, this function returns exclusively a DataFrame with all found test data, by default False\n\n    Returns\n    -------\n    anzahl_test: int\n        total number of test data rows.\n    test_Kundengruppen: pandas.DataFrame or None\n        DataFrame containing all found test data, returned only if return_frame = True\n    \"\"\"\n    test_Kundengruppen = df[df['Kundengruppe'].str.contains('test', case=False, na=False)]\n    anzahl_test = len(test_Kundengruppen)\n    if return_frame:\n        return test_Kundengruppen\n    else:\n        return anzahl_test"
            start_line: 67
            end_line: 89
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: metrics.plausibilitaetscheck_forderung_einigung
            name: plausibilitaetscheck_forderung_einigung
            args[1]: input_df
            docstring: "Checks for diff between Einigung_Netto and Forderung_Netto for all rows in the given dataframe. Cases with Einigung > Forderung are cosidered faulty.\n\nParamters\n---------\ninput_df: pandas.DataFrame\n    DataFrame that is to be evaluated.\n\nReturns\n-------        \nresults: pandas.DataFrame\n    a DataFrame of all differences > 0 as float values alongside their ID, Forderung_Netto and Einigung_Netto \ncount: int  \n    total number of rows with difference >0\navg: float\n    average difference over all found instances    "
            source_code: "def plausibilitaetscheck_forderung_einigung(input_df):\n    \"\"\"Checks for diff between Einigung_Netto and Forderung_Netto for all rows in the given dataframe. Cases with Einigung > Forderung are cosidered faulty.\n\n        Paramters\n        ---------\n        input_df: pandas.DataFrame\n            DataFrame that is to be evaluated.\n        \n        Returns\n        -------        \n        results: pandas.DataFrame\n            a DataFrame of all differences > 0 as float values alongside their ID, Forderung_Netto and Einigung_Netto \n        count: int  \n            total number of rows with difference >0\n        avg: float\n            average difference over all found instances    \n    \"\"\"\n    if 'AuftragID' in input_df.columns:\n        cols = [\"KvaRechnung_ID\", \"Forderung_Netto\", \"Einigung_Netto\"]\n    else:\n        cols = [\"Position_ID\", \"Forderung_Netto\", \"Einigung_Netto\"]\n\n    temp_df = input_df[cols].copy()\n\n    faulty_rows_mask = temp_df['Einigung_Netto'].round(2) > temp_df['Forderung_Netto'].round(2)\n    count = faulty_rows_mask.sum()\n\n    results = temp_df.loc[faulty_rows_mask].copy()\n    results['Diff'] = (results['Einigung_Netto'] - results['Forderung_Netto']).round(2)\n    avg = results['Diff'].mean()\n\n    return results, count, avg"
            start_line: 92
            end_line: 123
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: metrics.uniqueness_check
            name: uniqueness_check
            args[2]: df,df2
            docstring: "Checks whether the assumed unique ID columns in the data sets are truly unique.   \n\nParameters\n----------\ndf : pandas.DataFrame\n    DataFrame that contains the 'Auftragsdaten' data set\ndf2 : pandas.DataFrame\n    DataFrame that contains the 'Positionsdaten' data set\n\nReturns\n-------\nkvarechnung_id_is_unique: bool\n    True if column is unique.\nposition_id_is_unique : bool    \n    True if column is unique."
            source_code: "def uniqueness_check(df, df2):\n    \"\"\"Checks whether the assumed unique ID columns in the data sets are truly unique.   \n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame that contains the 'Auftragsdaten' data set\n    df2 : pandas.DataFrame\n        DataFrame that contains the 'Positionsdaten' data set\n\n    Returns\n    -------\n    kvarechnung_id_is_unique: bool\n        True if column is unique.\n    position_id_is_unique : bool    \n        True if column is unique.\n    \"\"\"\n    kvarechnung_id_is_unique = df['KvaRechnung_ID'].is_unique\n    position_id_is_unique = df2['Position_ID'].is_unique\n\n    tmp = df[['DH_ID', 'KvaRechnung_Nummer']].dropna()\n    kvarechnung_nummer_land_is_unique = True\n\n    dup_mask = tmp.duplicated(subset=['DH_ID', 'KvaRechnung_Nummer'], keep=False)\n    is_duplicated = dup_mask.any()\n    kvarechnung_nummer_land_is_unique = not is_duplicated\n\n    df_problem = df.loc[tmp.index[dup_mask]][[\"KvaRechnung_Nummer\", \"DH_ID\"]]\n\n    return kvarechnung_id_is_unique, position_id_is_unique, kvarechnung_nummer_land_is_unique, df_problem"
            start_line: 126
            end_line: 155
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: metrics.count_rows
            name: count_rows
            args[1]: input_df
            docstring: "Helper function to calculate the number of rows in a data frame after filtering.\n\nParameters\n----------\ninput_df : pandas.DataFrame\n    DataFrame to be evaluated.\n\nReturns\n-------\ncount: int\n    _description_"
            source_code: "def count_rows(input_df):\n    \"\"\"Helper function to calculate the number of rows in a data frame after filtering.\n\n    Parameters\n    ----------\n    input_df : pandas.DataFrame\n        DataFrame to be evaluated.\n\n    Returns\n    -------\n    count: int\n        _description_\n    \"\"\"\n    count = len(input_df)\n    return count"
            start_line: 158
            end_line: 172
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: metrics.data_cleanliness
            name: data_cleanliness
            args[3]: input_df,group_by_col,specific_group
            docstring: "Determines ratio of null-values by columns and percentage of rows containing any amount of null values, with optional grouping by a given column. Also supports filtering down to ratio for a single group of interest.\n\nParameters\n----------\ninput_df : pandas.DataFrame\n    DataFrame that is to be evaluated.\ngroup_by_col: string, optional\n    Column identifier for grouping, default = 'Kundengruppe'\nspecific_group: string, optional\n    Passes a group entry to filter the result by, if any. Default = None   \n\nReturns\n-------\nnull_ratio_rows: float or None\n    Percentage value of rows with at least one null value in the given columns.\nnull_ratio_cols: DataFrame or None\n    DataFrame, with null_ratio being the percentage amount of null entries in the column.   \ngrouped_row_ratios: pandas.Series or None\n    Series containing the row ratios of all groups as float.\ngrouped_col_ratios: pandas.DataFrame or None\n    DataFrame containing groups and null-value-ratios per column for each.             "
            source_code: "def data_cleanliness(input_df,group_by_col=\"Kundengruppe\", specific_group=None):\n    \"\"\"Determines ratio of null-values by columns and percentage of rows containing any amount of null values, with optional grouping by a given column. Also supports filtering down to ratio for a single group of interest.\n\n    Parameters\n    ----------\n    input_df : pandas.DataFrame\n        DataFrame that is to be evaluated.\n    group_by_col: string, optional\n        Column identifier for grouping, default = 'Kundengruppe'\n    specific_group: string, optional\n        Passes a group entry to filter the result by, if any. Default = None   \n\n    Returns\n    -------\n    null_ratio_rows: float or None\n        Percentage value of rows with at least one null value in the given columns.\n    null_ratio_cols: DataFrame or None\n        DataFrame, with null_ratio being the percentage amount of null entries in the column.   \n    grouped_row_ratios: pandas.Series or None\n        Series containing the row ratios of all groups as float.\n    grouped_col_ratios: pandas.DataFrame or None\n        DataFrame containing groups and null-value-ratios per column for each.             \n    \"\"\"      \n\n    if group_by_col is None:\n        if 'PLZ_SO' in input_df.columns:\n            null_ratio_rows = ratio_null_values_rows(input_df, exclude_cols=['PLZ_SO', 'PLZ_HW', 'PLZ_VN', 'address1_postalcode'])\n            null_ratio_cols = ratio_null_values_column(input_df)\n            \n        else:\n            null_ratio_rows = ratio_null_values_rows(input_df, exclude_cols=['Bemerkung'])\n            null_ratio_cols = ratio_null_values_column(input_df)\n\n        return null_ratio_rows, null_ratio_cols\n\n    else:\n\n        grouped = input_df.groupby(group_by_col,observed=True)\n\n        # Group for columns & rows\n        grouped_null_counts = grouped.apply(lambda x: x.isnull().sum(), include_groups=False)\n        grouped_null_rows = grouped.apply(lambda x: x.isnull().any(axis=1).sum(), include_groups=False)\n\n        # create group sizes\n        group_sizes = grouped.size()\n\n        # calculate ratios\n        grouped_col_ratios = grouped_null_counts.div(group_sizes, axis=0)\n        grouped_row_ratios = grouped_null_rows / group_sizes\n        if specific_group:\n            grouped_col_ratios = grouped_col_ratios.loc[[specific_group]]\n            grouped_row_ratios = grouped_row_ratios.loc[[specific_group]]\n        return grouped_row_ratios, grouped_col_ratios"
            start_line: 176
            end_line: 228
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: metrics.discount_check
            name: discount_check
            args[1]: df2
            docstring: "Checks if a row in the 'Positionsdaten' data set does/doesn't describe a discount or similar and if the 'Einigung_Netto' and 'Forderung_Netto' information accurately reflects this (negative or positive values). References values in the 'Plausibel' column, which is calculated during database creation (see build_db.py). \n\nParameters\n----------\ndf2 : pandas.DataFrame\n    DataFrame containing the 'Positionsdaten' data set\n\nReturns\n-------\npotential_errors: int\n    The number of potentially faulty rows\n\nNotes\n-----        \nThis check relies on logic in data_cleaning.py that writes its results to the 'Plausibel' column. "
            source_code: "def discount_check(df2):\n    \"\"\"Checks if a row in the 'Positionsdaten' data set does/doesn't describe a discount or similar and if the 'Einigung_Netto' and 'Forderung_Netto' information accurately reflects this (negative or positive values). References values in the 'Plausibel' column, which is calculated during database creation (see build_db.py). \n    \n    Parameters\n    ----------\n    df2 : pandas.DataFrame\n        DataFrame containing the 'Positionsdaten' data set\n\n    Returns\n    -------\n    potential_errors: int\n        The number of potentially faulty rows\n\n    Notes\n    -----        \n    This check relies on logic in data_cleaning.py that writes its results to the 'Plausibel' column. \n    \n    \"\"\"\n    potential_errors = (~df2['Plausibel']).sum()\n    return potential_errors"
            start_line: 231
            end_line: 250
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: metrics.proformabelege
            name: proformabelege
            args[1]: df
            docstring: "Function that checks for pro-forma receipts in the 'Auftragsdaten' data set. \n\nParameters\n----------\ndf : pandas.DataFrame\n    DataFrame that is to be evaluated.\n\nReturns\n-------\nproforma: pandas.DataFrame\n    DataFrame containing all found pro-forma receipt rows\nproforma_count: int\n    Amount of found receipts    "
            source_code: "def proformabelege(df):\n    \"\"\"Function that checks for pro-forma receipts in the 'Auftragsdaten' data set. \n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame that is to be evaluated.\n\n    Returns\n    -------\n    proforma: pandas.DataFrame\n        DataFrame containing all found pro-forma receipt rows\n    proforma_count: int\n        Amount of found receipts    \n    \"\"\"\n    proforma = df[df['Einigung_Netto'].between(0.01, 1)]\n    proforma_count = len(proforma)\n    return proforma, proforma_count"
            start_line: 253
            end_line: 270
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: metrics.position_count
            name: position_count
            args[1]: input_df
            docstring: "Counts the number of positions for each unique KvaRechnung_ID\n\nParameters\n----------\ninput_df : input_df : pandas.DataFrame\n    DataFrame that is to be evaluated.\n\nReturns\n-------\nposition_count: pandas.DataFrame\n    DataFrame with the columns 'KvaRechnung_ID' and the amount of associated positions.  "
            source_code: "def position_count(input_df):\n    \"\"\"Counts the number of positions for each unique KvaRechnung_ID\n\n    Parameters\n    ----------\n    input_df : input_df : pandas.DataFrame\n        DataFrame that is to be evaluated.\n\n    Returns\n    -------\n    position_count: pandas.DataFrame\n        DataFrame with the columns 'KvaRechnung_ID' and the amount of associated positions.  \n    \"\"\"\n    position_count = input_df.groupby('KvaRechnung_ID', observed=False)['Position_ID'].count().reset_index().rename(columns={'Position_ID': 'PositionsAnzahl'})\n    return position_count"
            start_line: 273
            end_line: 287
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: metrics.empty_orders
            name: empty_orders
            args[1]: df
            docstring: "Function that checks if any orders do not have positions associated with them.\n\nParameters\n----------\ndf : pandas.DataFrame\n     DataFrame with 'Auftragsdaten' data set that is to be evaluated.\n\nReturns\n-------\nempty_orders: int\n    Total amount of orders that do not have any positions associated with them.\nempty_order_df: pandas.DateFrame\n    DataFrame containing all empty orders."
            source_code: "def empty_orders(df):\n    \"\"\"Function that checks if any orders do not have positions associated with them.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n         DataFrame with 'Auftragsdaten' data set that is to be evaluated.\n\n    Returns\n    -------\n    empty_orders: int\n        Total amount of orders that do not have any positions associated with them.\n    empty_order_df: pandas.DateFrame\n        DataFrame containing all empty orders.\n\n    \"\"\"\n    empty_orders_df = df[df['PositionsAnzahl'].isna()]\n    empty_orders = len(empty_orders_df)\n    return empty_orders, empty_orders_df"
            start_line: 289
            end_line: 307
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: metrics.above_50k
            name: above_50k
            args[1]: df
            docstring: "Checks for all receipts or positions that exceed a limit for suspicion of €50k in Einigung_Netto and need to be manually vetted.\n\nParameters\n----------\ndf : pandas.DataFrame\n    DataFrame that is to be evaluated.\n\nReturns\n-------\nsuspicious_data: pandas.DataFrame\n    Data frame containing suspiciously high positions"
            source_code: "def above_50k(df):\n    \"\"\"Checks for all receipts or positions that exceed a limit for suspicion of €50k in Einigung_Netto and need to be manually vetted.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame that is to be evaluated.\n\n    Returns\n    -------\n    suspicious_data: pandas.DataFrame\n        Data frame containing suspiciously high positions\n    \"\"\"\n    suspicious_data = df[df['Einigung_Netto'] >= 50000]\n    suspicious_data = suspicious_data[['KvaRechnung_ID', 'Forderung_Netto', 'Empfehlung_Netto', 'Einigung_Netto', 'Kundengruppe', 'Handwerker_Name', 'CRMEingangszeit']]\n    return suspicious_data"
            start_line: 311
            end_line: 326
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: metrics.outliers_by_damage
            name: outliers_by_damage
            args[4]: df,schadenart,set_quantile,column_choice
            docstring: "Calculates the upper and lower outliers outside the desired quantile range (symmetric over mean) for each kind of damage. Assumes 'Forderung_Netto' as column of interest.  \n\nParameters\n----------\ndf : pandas.DataFrame\n    DataFrame to be evaluated\nschadenart : string, optional\n    specific damage type label to filter for, by default None\nset_quantile : float, optional\n    desired quantile range, symmetric upper/lower bound is inferred, by default 0.99\ncolumn_choice : str, optional\n    numeric column containing outliers, by default 'Forderung_netto'\n\nReturns\n-------\ndf_outlier: pandas.DataFrame\n    df containing all suspicious rows"
            source_code: "def outliers_by_damage(df, schadenart=None, set_quantile=0.99, column_choice='Forderung_Netto'):\n    \"\"\"Calculates the upper and lower outliers outside the desired quantile range (symmetric over mean) for each kind of damage. Assumes 'Forderung_Netto' as column of interest.  \n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be evaluated\n    schadenart : string, optional\n        specific damage type label to filter for, by default None\n    set_quantile : float, optional\n        desired quantile range, symmetric upper/lower bound is inferred, by default 0.99\n    column_choice : str, optional\n        numeric column containing outliers, by default 'Forderung_netto'\n\n    Returns\n    -------\n    df_outlier: pandas.DataFrame\n        df containing all suspicious rows\n    \"\"\"\n    if set_quantile < 0.5:\n        set_quantile = 1-set_quantile\n\n    if schadenart:\n        df = df[df['Schadenart_Name'] == schadenart]\n\n    df_grouped_upper = df.groupby(['Schadenart_Name'],observed=True)[column_choice].transform('quantile',set_quantile, numeric_only=True) \n    df_grouped_lower = df.groupby(['Schadenart_Name'],observed=True)[column_choice].transform('quantile',1-set_quantile, numeric_only=True)\n\n    df_outlier = df[\n        (df[column_choice] > df_grouped_upper) | \n        (df[column_choice] < df_grouped_lower)\n    ]\n\n    return df_outlier"
            start_line: 328
            end_line: 361
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: metrics.check_zeitwert
            name: check_zeitwert
            args[1]: df
            docstring: "Checks if the value in the column 'Differenz_vor_Zeitwert_Netto' satisfies the condition [Zeitwert = Forderung-Einigung]\n   and calculates the relative error. Only valid for 'Auftragsdaten' data set.     \n\nParameters\n----------\ndf : _pandas.DataFrame\n    DataFrame containing 'Auftragsdaten' data set that is to be evaluated.\nReturns\n-------\nresult_df: pandas.DataFrame\n    DataFrame of all error values (float) alongside the ID found in the original data frame"
            source_code: "def check_zeitwert(df):\n    \"\"\"Checks if the value in the column 'Differenz_vor_Zeitwert_Netto' satisfies the condition [Zeitwert = Forderung-Einigung]\n       and calculates the relative error. Only valid for 'Auftragsdaten' data set.     \n\n    Parameters\n    ----------\n    df : _pandas.DataFrame\n        DataFrame containing 'Auftragsdaten' data set that is to be evaluated.\n    Returns\n    -------\n    result_df: pandas.DataFrame\n        DataFrame of all error values (float) alongside the ID found in the original data frame\n    \"\"\"\n\n    difference = (df['Forderung_Netto'] - df['Einigung_Netto']).round(2) - (df['Differenz_vor_Zeitwert_Netto']).round(2)\n    mask = ~np.isclose(difference, 0, atol=0.01) # positive = not enough difference, negative = too much difference\n\n    result_df = df.loc[mask, ['KvaRechnung_ID', 'CRMEingangszeit']].copy()\n    result_df['Differenz Zeitwert'] = difference[mask]\n    \n    return result_df"
            start_line: 368
            end_line: 388
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: metrics.positions_per_order_over_time
            name: positions_per_order_over_time
            args[3]: df,df2,time_col
            docstring: "Berechnet die durchschnittliche Anzahl an Positionen pro Auftrag je Monat.\n\nArgs:\n    df: Auftragsdaten mit Spalte 'KvaRechnung_ID' und einer Zeitspalte.\n    df2: Positionsdaten mit Spalten 'KvaRechnung_ID' und 'Position_ID'.\n    time_col: Name der Zeitspalte in orders_df (z.B. 'CRMEingangszeit').\n\nReturns:\n    DataFrame mit Spalten:\n    - 'Zeitperiode'\n    - 'Avg_Positionen_pro_Auftrag'\n    - 'Total_Positionen'\n    - 'Anzahl_Auftraege'\n    - 'Growth_rate_%'"
            source_code: "def positions_per_order_over_time(df, df2, time_col=\"CRMEingangszeit\"):\n    \"\"\"\n    Berechnet die durchschnittliche Anzahl an Positionen pro Auftrag je Monat.\n\n    Args:\n        df: Auftragsdaten mit Spalte 'KvaRechnung_ID' und einer Zeitspalte.\n        df2: Positionsdaten mit Spalten 'KvaRechnung_ID' und 'Position_ID'.\n        time_col: Name der Zeitspalte in orders_df (z.B. 'CRMEingangszeit').\n\n    Returns:\n        DataFrame mit Spalten:\n        - 'Zeitperiode'\n        - 'Avg_Positionen_pro_Auftrag'\n        - 'Total_Positionen'\n        - 'Anzahl_Auftraege'\n        - 'Growth_rate_%'\n    \"\"\"\n\n    # Positionen pro Auftrag zählen\n    pos_counts = (\n        df2\n        .groupby(\"KvaRechnung_ID\",observed=True)[\"Position_ID\"]\n        .count()\n        .reset_index(name=\"Positionen_pro_Auftrag\")\n    )\n\n    # Zeitspalte vorbereiten\n    orders = df[[\"KvaRechnung_ID\", time_col]].copy()\n    orders = orders.dropna(subset=[time_col])\n    orders[time_col] = pd.to_datetime(orders[time_col], errors=\"coerce\")\n    orders = orders.dropna(subset=[time_col])\n\n    # Zeitperiode (Monat) bestimmen\n    orders[\"Zeitperiode\"] = orders[time_col].dt.to_period(\"M\").astype(str)\n\n    # Positionen an Aufträge mergen\n    merged = orders.merge(pos_counts, on=\"KvaRechnung_ID\", how=\"left\")\n    merged[\"Positionen_pro_Auftrag\"] = merged[\"Positionen_pro_Auftrag\"].fillna(0)\n\n    # Aggregation je Zeitperiode\n    result = (\n        merged\n        .groupby(\"Zeitperiode\",observed=True)[\"Positionen_pro_Auftrag\"]\n        .agg([\"mean\", \"sum\", \"count\"])\n        .reset_index()\n    )\n\n    result = result.sort_values(\"Zeitperiode\")\n\n    result = result.rename(\n        columns={\n            \"mean\": \"Avg_Positionen_pro_Auftrag\",\n            \"sum\": \"Total_Positionen\",\n            \"count\": \"Anzahl_Auftraege\"\n        }\n    )\n\n    # prozentuale Veränderung der durchschnittlichen Positionsanzahl\n    result[\"Growth_rate_%\"] = result[\"Avg_Positionen_pro_Auftrag\"].pct_change() * 100\n\n    return result"
            start_line: 393
            end_line: 453
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: metrics.error_frequency_by_weekday_hour
            name: error_frequency_by_weekday_hour
            args[3]: df,time_col,relevant_columns
            docstring: "Aggregiert die Fehlerhäufigkeit (NaN-Werte) nach Wochentag und Stunde. Ein Auftrag gilt als fehlerhaft, wenn in mindestens einer der relevanten Spalten ein NaN-Wert vorkommt.\n\nParameters\n----------\n    df: pandas.DataFrame\n        Auftragsdaten-DataFrame (z.B. Auftragsdaten_konvertiert),muss 'KvaRechnung_ID' und die Zeitspalte enthalten.\n    time_col: string\n        Name der Zeitspalte in df, z.B. 'CRMEingangszeit'.\n    relevant_columns: list\n        Liste der Spalten, die auf NaN geprüft werden sollen.\n        Wenn None -> alle Spalten außer 'KvaRechnung_ID' und time_col.\n\nReturns\n-------\n    result: pandas.DataFrame\n        DataFrame mit Spalten:\n        - 'weekday'     : Name des Wochentags (Monday, Tuesday, ...)\n        - 'hour'        : Stunde (0–23)\n        - 'total_rows'  : Anzahl Aufträge in diesem Zeit-Slot\n        - 'error_rows'  : Anzahl fehlerhafter Aufträge in diesem Slot\n        - 'error_rate'  : Fehlerquote in Prozent"
            source_code: "def error_frequency_by_weekday_hour(df, time_col=\"CRMEingangszeit\", relevant_columns=None):\n    \"\"\"\n    Aggregiert die Fehlerhäufigkeit (NaN-Werte) nach Wochentag und Stunde. Ein Auftrag gilt als fehlerhaft, wenn in mindestens einer der relevanten Spalten ein NaN-Wert vorkommt.\n\n    Parameters\n    ----------\n        df: pandas.DataFrame\n            Auftragsdaten-DataFrame (z.B. Auftragsdaten_konvertiert),muss 'KvaRechnung_ID' und die Zeitspalte enthalten.\n        time_col: string\n            Name der Zeitspalte in df, z.B. 'CRMEingangszeit'.\n        relevant_columns: list\n            Liste der Spalten, die auf NaN geprüft werden sollen.\n            Wenn None -> alle Spalten außer 'KvaRechnung_ID' und time_col.\n\n    Returns\n    -------\n        result: pandas.DataFrame\n            DataFrame mit Spalten:\n            - 'weekday'     : Name des Wochentags (Monday, Tuesday, ...)\n            - 'hour'        : Stunde (0–23)\n            - 'total_rows'  : Anzahl Aufträge in diesem Zeit-Slot\n            - 'error_rows'  : Anzahl fehlerhafter Aufträge in diesem Slot\n            - 'error_rate'  : Fehlerquote in Prozent\n    \"\"\"\n\n    work_df = df.copy()\n\n    # Zeitspalte in datetime umwandeln\n    work_df[time_col] = pd.to_datetime(work_df[time_col], errors=\"coerce\")\n    work_df = work_df.dropna(subset=[time_col])\n\n    # Wochentag + Stunde extrahieren\n    work_df[\"weekday\"] = work_df[time_col].dt.day_name()\n    work_df[\"hour\"] = work_df[time_col].dt.hour\n\n    # Relevante Spalten bestimmen\n    if relevant_columns is None:\n        exclude = {time_col, \"KvaRechnung_ID\", \"weekday\", \"hour\"}\n        relevant_columns = [c for c in work_df.columns if c not in exclude]\n\n    if not relevant_columns:\n        raise ValueError(\"Keine relevanten Spalten für die Fehlerprüfung gefunden.\")\n\n    # Error = mind. ein NaN in den relevanten Spalten\n    work_df[\"has_error\"] = work_df[relevant_columns].isna().any(axis=1)\n\n    result = (\n        work_df\n        .groupby([\"weekday\", \"hour\"],observed=True)\n        .agg(\n            total_rows=(\"KvaRechnung_ID\", \"count\"),\n            error_rows=(\"has_error\", \"sum\"),\n        )\n        .reset_index()\n    )\n\n    result[\"error_rate\"] = result[\"error_rows\"] / result[\"total_rows\"] * 100\n\n    weekday_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n    result[\"weekday\"] = pd.Categorical(result[\"weekday\"], categories=weekday_order, ordered=True)\n    result = result.sort_values([\"weekday\", \"hour\"])\n\n    return result"
            start_line: 455
            end_line: 517
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: metrics.mismatched_entries
            name: mismatched_entries
            args[4]: df,threshold,process_batch_size,encode_batch_size
            docstring: "Calculates the semantic similarity between 'Gewerk_Name' and 'Handwerker_Name' using a \nSentence Transformer model on the GPU. Identifies entries where the similarity score \nfalls below the threshold.(< 0.2).\n\nParameters:\n----------\n    df: pandas.DataFrame\n        DataFrame (Auftragsdaten) that contains the columns 'Gewerk_Name' and 'Handwerker_Name'.\n    threshold: float, optional\n        Similarity threshold (default: 0.2). Values below this limit are considered mismatches.\n        The optimal threshold in a production system would need to be evaluated further. \n    process_batch_size: int, optional\n        Number of rows to be compared simultaneously (high value possible, e.g. 16384).\n    encode_batch_size: int, optional\n        Number of unique terms to be vectorized simultaneously by the model (low value recommended, e.g. 128).\n\nReturns:\n-------\n    mismatches: pandas.DataFrame\n        DataFrame containing rows where 'Similarity_Score' < threshold.\n        The results are sorted ascending by similarity and include the new column 'Similarity_Score'."
            source_code: "def mismatched_entries(df, threshold=0.2, process_batch_size=16384, encode_batch_size=128):\n    \"\"\"\n    Calculates the semantic similarity between 'Gewerk_Name' and 'Handwerker_Name' using a \n    Sentence Transformer model on the GPU. Identifies entries where the similarity score \n    falls below the threshold.(< 0.2).\n    \n    Parameters:\n    ----------\n        df: pandas.DataFrame\n            DataFrame (Auftragsdaten) that contains the columns 'Gewerk_Name' and 'Handwerker_Name'.\n        threshold: float, optional\n            Similarity threshold (default: 0.2). Values below this limit are considered mismatches.\n            The optimal threshold in a production system would need to be evaluated further. \n        process_batch_size: int, optional\n            Number of rows to be compared simultaneously (high value possible, e.g. 16384).\n        encode_batch_size: int, optional\n            Number of unique terms to be vectorized simultaneously by the model (low value recommended, e.g. 128).\n\n    Returns:\n    -------\n        mismatches: pandas.DataFrame\n            DataFrame containing rows where 'Similarity_Score' < threshold.\n            The results are sorted ascending by similarity and include the new column 'Similarity_Score'.\n    \"\"\"\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Running on: {device}\")\n    \n    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device=device)\n\n    df = df.dropna(subset=['Gewerk_Name', 'Handwerker_Name']).copy()\n    gewerk_codes, unique_gewerke = pd.factorize(df['Gewerk_Name'])\n    handwerker_codes, unique_handwerker = pd.factorize(df['Handwerker_Name'])\n\n\n    print(\"Encoding unique values...\")\n    emb_gewerke = model.encode(\n        unique_gewerke, \n        batch_size=encode_batch_size, \n        show_progress_bar=True, \n        convert_to_tensor=True, \n        device=device,\n        normalize_embeddings=True\n    )\n\n    emb_handwerker = model.encode(\n        unique_handwerker, \n        batch_size=encode_batch_size, \n        show_progress_bar=True, \n        convert_to_tensor=True, \n        device=device,\n        normalize_embeddings=True\n    )\n\n\n    print(\"Calculating similarity scores on GPU...\")\n    similarity_scores = []\n    \n    total_rows = len(df)\n    \n    t_gewerk_codes = torch.tensor(gewerk_codes, device=device, dtype=torch.long)\n    t_handwerker_codes = torch.tensor(handwerker_codes, device=device, dtype=torch.long)\n\n    with torch.no_grad():\n        for i in range(0, total_rows, process_batch_size):\n            end = min(i + process_batch_size, total_rows)\n            \n            batch_g_codes = t_gewerk_codes[i:end]\n            batch_h_codes = t_handwerker_codes[i:end]\n            \n            batch_emb_g = emb_gewerke[batch_g_codes]\n            batch_emb_h = emb_handwerker[batch_h_codes]\n            \n            sim = torch.nn.functional.cosine_similarity(batch_emb_g, batch_emb_h, dim=1)\n            \n            similarity_scores.append(sim.cpu().numpy())\n\n    full_scores = np.concatenate(similarity_scores)\n    df['Similarity_Score'] = full_scores\n    \n    mismatches = df[df['Similarity_Score'] < threshold].copy()\n    mismatches = mismatches.sort_values(by='Similarity_Score', ascending=True)\n\n    # Cleanup GPU memory\n    del emb_gewerke\n    del emb_handwerker\n    del t_gewerk_codes\n    del t_handwerker_codes\n    torch.cuda.empty_cache()\n\n    return mismatches"
            start_line: 520
            end_line: 609
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: metrics.handwerker_gewerke_outlier
            name: handwerker_gewerke_outlier
            args[1]: df
            docstring: "Determines which companies are on record with an unusual trade entry.\n\nParameters\n----------\ndf : pandas.DataFrame\n    'Auftragsdaten'-DataFrame\n\nReturns\n-------\nstats: pandas.DataFrame\n    DataFrame containing:\n    - 'Handwerker_Name': string, company name\n    - 'Gerwerk_Name': string, associated trades\n    - 'count': int, amount of observed instances of trade-company combination\n    - 'total_count': int, total amount of observations (per company)\n    - 'ratio': float, count/total_count (per company)\n    - 'anzahl_gewerke': int, absolute amount of trades (per company)  \n    - 'is_outlier': bool, True for more than 1 trade, ratio < 0.2"
            source_code: "def handwerker_gewerke_outlier(df):\n    \"\"\"Determines which companies are on record with an unusual trade entry.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        'Auftragsdaten'-DataFrame\n\n    Returns\n    -------\n    stats: pandas.DataFrame\n        DataFrame containing:\n        - 'Handwerker_Name': string, company name\n        - 'Gerwerk_Name': string, associated trades\n        - 'count': int, amount of observed instances of trade-company combination\n        - 'total_count': int, total amount of observations (per company)\n        - 'ratio': float, count/total_count (per company)\n        - 'anzahl_gewerke': int, absolute amount of trades (per company)  \n        - 'is_outlier': bool, True for more than 1 trade, ratio < 0.2\n    \"\"\"\n    df = df[[\"Handwerker_Name\", \"Gewerk_Name\"]]\n    df = df.dropna()\n    stats = df.groupby(['Handwerker_Name', 'Gewerk_Name'], observed=True).size().reset_index(name='count')\n    total_counts = df.groupby('Handwerker_Name', observed=True).size().reset_index(name='total_count')\n\n    stats = stats.merge(total_counts, on='Handwerker_Name')\n    stats['ratio'] = stats['count'] / stats['total_count']\n\n    stats['anzahl_gewerke'] = stats.groupby('Handwerker_Name', observed=True)['Gewerk_Name'].transform('count')\n    stats['is_outlier'] = (stats['anzahl_gewerke'] > 1) & (stats['ratio'] < 0.2)\n    stats = stats.dropna()\n\n    return stats"
            start_line: 611
            end_line: 643
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: metrics.check_keywords
            name: check_keywords
            args[1]: df
            docstring: "This metrics tries to check if an observed company-trade combination is valid by checking the company name for relation to a given trade.\n\nParameters\n----------\ndf : pandas.DataFrame\n    DataFrame containing orders; company and trade information\n\nReturns\n-------\nnumpy.ndarray\n    An array with length of df, with each element being one of the following strings:\n    - \"CONFIRMED_BY_NAME\" : trade coheres with company name\n    - \"CONFLICT_WITH_<TRADE>\": trade is not confirmed by company name\n    - \"NO_KEYWORD_INFO\": keyword n/a"
            source_code: "def check_keywords(df):\n    \"\"\"This metrics tries to check if an observed company-trade combination is valid by checking the company name for relation to a given trade.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing orders; company and trade information\n\n    Returns\n    -------\n    numpy.ndarray\n        An array with length of df, with each element being one of the following strings:\n        - \"CONFIRMED_BY_NAME\" : trade coheres with company name\n        - \"CONFLICT_WITH_<TRADE>\": trade is not confirmed by company name\n        - \"NO_KEYWORD_INFO\": keyword n/a\n    \"\"\"\n    keywords_mapping = {\n    'Heizung- und Sanitärinstallation': ['heizung', 'sanitär', 'bad', 'gas', 'wasser', 'hls', 'wärme', 'installateur', 'haustechnik', 'therme', 'leckage'],\n    'Metallbau- und Schlosserarbeiten': ['metall', 'schlosser', 'stahlbau', 'schweiß', 'schmiede', 'konstruktion', 'edelstahl'],\n    'Fahrrad': ['fahrrad', 'bike', 'rad', 'zweirad', 'velo', 'ebike'],\n    'Maurerarbeiten': ['maurer', 'rohbau', 'baugeschäft', 'hochbau', 'bauunternehmung', 'klinker'],\n    'Putz- und Stuckarbeiten': ['putz', 'stuck', 'verputz', 'gips', 'lehmbau', 'fassadenputz'],\n    'Fassadensysteme': ['fassade', 'verklinkerung', 'dämmung', 'bekleidung', 'wand'],\n    'Rollladen und Sonnenschutz': ['rollladen', 'sonnenschutz', 'jalousie', 'markise', 'store', 'beschattung', 'rolltor'],\n    'Dachdeckerarbeiten': ['dach', 'bedachung', 'dachdecker', 'steildach', 'flachdach', 'ziegel'],\n    'Tief- und Erdbauarbeiten': ['tiefbau', 'erdbau', 'bagger', 'aushub', 'erd bewegung', 'graben', 'schacht'],\n    'Bodenbelagsarbeiten': ['boden', 'belag', 'teppich', 'linoleum', 'vinyl', 'raumausstatter', 'laminat', 'designbelag'],\n    'Tischlerarbeiten': ['tischler', 'schreiner', 'möbel', 'holzbau', 'innenausbau', 'fensterbau'],\n    'Leckageortung und Trocknung': ['leck', 'ortung', 'trocknung', 'feuchte', 'wassersha', 'thermografie'],\n    'Fliesen- und Plattenverlegearbeiten': ['fliesen', 'platten', 'mosaik', 'keramik', 'granit', 'steinzeug'],\n    'Maler- und Tapezierarbeiten': ['maler', 'lack', 'anstrich', 'tapezier', 'farbe', 'raumdesign'],\n    'Zimmer- und Holzbauarbeiten': ['zimmerer', 'zimmerei', 'holzbau', 'sägewerk', 'abbund', 'dachstuhl'],\n    'Elektroarbeiten': ['elektro', 'strom', 'elektronik', 'schaltanlagen', 'licht', 'kabel', 'spannung'],\n    'Rohr- und Kanalbefahrung': ['kanal', 'rohr', 'tv-inspektion', 'dichtheit', 'abfluss', 'kamera'],\n    'Spenglerarbeiten': ['spengler', 'klempner', 'blech', 'flaschner', 'kupfer', 'zink'],\n    'Garten- und Landschaftsbauarbeiten': ['garten', 'landschaft', 'galabau', 'grün', 'pflanze', 'baum', 'außenanlagen'],\n    'Sachverständigenleistungen': ['sachverständig', 'gutachter', 'expert', 'bewertung', 'wertermittlung', 'analyse'],\n    'Multigewerk': ['bauunternehmung', 'generalunternehmer', 'bauservice', 'dienstleistung', 'allround', 'sanierung', 'komplettbau'],\n    'Brandschmutzbeseitigung': ['brand', 'sanierung', 'ruß', 'reinigung', 'schadenmanagement'],\n    'Verglasungsarbeiten': ['glas', 'fenster', 'vitrinen', 'spiegel', 'wintergarten'],\n    'Trockenbauarbeiten': ['trockenbau', 'akustik', 'rigips', 'innenausbau', 'montagebau'],\n    'Sicherheits- und Baustelleneinrichtung': ['sicherheit', 'baustrom', 'absperrung', 'bauzaun', 'wc-service', 'logistik'],\n    'Abfall, Entsorgung und Recycling': ['entsorgung', 'recycling', 'container', 'schrott', 'abfall', 'mulden'],\n    'Schließanlagen und Beschläge': ['schlüssel', 'schließ', 'sicherheitstechnik', 'beschlag', 'tresor'],\n    'Daten-, Melde- und Kommunikationsanlagen': ['daten', 'netzwerk', 'kommunikation', 'edv', 'it', 'telekom', 'glasfaser'],\n    'Straßen, Wege, Plätze': ['straßenbau', 'pflaster', 'asphalt', 'wege', 'hofbefestigung'],\n    'Gebäudereinigung': ['reinigung', 'clean', 'facility', 'sauber', 'glasreinigung', 'service'],\n    'Mauerarbeiten': ['mauer', 'stein', 'rohbau', 'sanierung'],\n    'Dachklempnerarbeiten': ['dachklempner', 'dachrinne', 'fallrohr', 'bauklempner'],\n    'Sanierungsarbeiten an schadstoffhaltigen Bauteilen': ['asbest', 'schadstoff', 'altlasten', 'dekontamination', 'kmf'],\n    'Lüftungsbau und Klimatechnik': ['lüftung', 'klima', 'kälte', 'air', 'ventilation', 'raumluft'],\n    'Bauleitung': ['bauleitung', 'architekt', 'ingenieur', 'planung', 'baubetreuung', 'projektsteuerung'],\n    'Werbeanlagen': ['werbe', 'reklame', 'schild', 'lichtwerbung', 'folie', 'beschriftung', 'sign'],\n    'Verkehrstechnische Anlagen': ['verkehr', 'ampel', 'signal', 'markierung', 'leitsystem'],\n    'Estricharbeiten': ['estrich', 'unterboden', 'zement', 'fließestrich'],\n    'Schwimmbadtechnik': ['schwimmbad', 'pool', 'sauna', 'wellness', 'wasseraufbereitung'],\n    'Gerüstbauarbeiten': ['gerüst', 'rüstung', 'einrüstung', 'steigtechnik'],\n    'Parkettarbeiten': ['parkett', 'dielen', 'schleif', 'holzboden'],\n    'Abbrucharbeiten': ['abbruch', 'abriss', 'rückbau', 'demontage', 'spreng'],\n    'Natursteinarbeiten': ['naturstein', 'steinmetz', 'marmor', 'granit', 'grabmale'],\n    'Medienverbrauch': ['stadtwerke', 'energie', 'versorgung', 'messdienst', 'strom', 'gas', 'wasser'],\n    'Beton- und Stahlbetonarbeiten': ['beton', 'stahlbeton', 'pump', 'fertigteil', 'schalung'],\n    'Handel': ['handel', 'vertrieb', 'shop', 'markt', 'baustoff', 'großhandel', 'verkauf'],\n    'Mietminderung': ['mieterschutz', 'anwalt', 'mietverein', 'recht'],\n    'Stahlbauarbeiten': ['stahlbau', 'halle', 'tragwerk', 'schlosserei'],\n    'Betonerhaltungsarbeiten': ['betonsanierung', 'bautenschutz', 'rissinjizierung', 'oberflächenschutz'],\n    'Wasserhaltung': ['wasserhaltung', 'grundwasser', 'absenkung', 'brunnenbau'],\n    'Solaranlagen': ['solar', 'photovoltaik', 'pv', 'sonne', 'regenerativ', 'energie'],\n    'Schutz- und Bewegungsaufwand': ['schutz', 'verpackung', 'abdeckung', 'transportschutz', 'umzug'],\n    'Rechtsanwälte': ['anwalt', 'kanzlei', 'recht', 'notar', 'law', 'jurist'],\n    'Abdichtungsarbeiten gegen Wasser/ Bauwerkstrockenlegung': ['abdichtung', 'isolierung', 'bitumen', 'injektion', 'trockenlegung', 'leckage'],\n    'Zäune und Grundstückseinfriedungen': ['zaun', 'tor', 'einfriedung', 'gitter', 'draht'],\n    'Bekämpfender Holzschutz': ['holzschutz', 'schwamm', 'schädlingsbekämpfung', 'kammerjäger'],\n    'Spengler- / Klempnerarbeiten': ['spengler', 'klempner', 'flaschner', 'blechbearbeitung', 'leckage'],\n    'Trocknung': ['trocknung', 'bautrocknung', 'entfeuchtung', 'leckage'],\n    'KFZ': ['kfz', 'auto', 'werkstatt', 'car', 'motor', 'fahrzeug', 'garage'],\n    'Leckageortung': ['leckage', 'ortung', 'rohrbruch', 'leck'],\n    'Immobilien': ['immobilien', 'makler', 'wohnbau', 'real estate', 'verwaltung', 'property'],\n    'Versicherung': ['versicherung', 'finanz', 'assekuranz', 'makler', 'agentur']\n    }\n\n\n\n    names = df['Handwerker_Name'].astype(str).str.lower()\n\n    confirmed_mask = np.zeros(len(df), dtype=bool)\n\n    conflict_series = pd.Series([None] * len(df), index=df.index)\n\n    for trade, keywords in keywords_mapping.items():\n        if not keywords:\n            continue\n\n        pattern = '|'.join(map(re.escape, keywords))\n\n        has_keyword = names.str.contains(pattern, regex=True, na=False)\n\n        is_current_trade = (df['Gewerk_Name'] == trade)\n        confirmed_mask = confirmed_mask | (has_keyword & is_current_trade)\n\n        is_conflict = (has_keyword & ~is_current_trade)\n\n        mask_to_update = is_conflict & conflict_series.isna()\n        conflict_series.loc[mask_to_update] = f\"CONFLICT_WITH_{trade.upper()}\"\n\n    final_result = np.where(\n        confirmed_mask,\n        \"CONFIRMED_BY_NAME\",\n        np.where(\n            conflict_series.notna(),\n            conflict_series,\n            \"NO_KEYWORD_INFO\"\n        )\n    )\n\n    return final_result"
            start_line: 645
            end_line: 760
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: metrics.abgleich_auftraege
            name: abgleich_auftraege
            args[2]: df1,df2
            docstring: "Vergleicht die Kopfdaten von Aufträgen (df1) mit der Summe ihrer Positionen (df2).\n\nDie Funktion gruppiert die Positionsdaten (df2) anhand der 'Kva_RechnungID', bildet\ndie Summen für 'Forderung_Netto' und 'Einigung_Netto' und vergleicht diese mit den\nin df1 hinterlegten Werten. Gleitkomma-Ungenauigkeiten werden dabei berücksichtigt.\n\nArgs:\n    df1 (pd.DataFrame): Dataframe mit den Auftragsdaten (Soll-Werte).\n        Muss zwingend folgende Spalten enthalten:\n        - 'Kva_RechnungID' (Verbindungsschlüssel)\n        - 'Forderung_Netto'\n        - 'Einigung_Netto'\n        \n    df2 (pd.DataFrame): Dataframe mit den Positionsdaten (Ist-Werte).\n        Muss zwingend folgende Spalten enthalten:\n        - 'Kva_RechnungID' (Verbindungsschlüssel)\n        - 'Forderung_Netto'\n        - 'Einigung_Netto'\n\nReturns:\n    pd.DataFrame: Eine Liste der Abweichungen. Der Dataframe enthält nur die IDs,\n    bei denen die Werte nicht übereinstimmen.\n    \n    Enthaltene Spalten:\n    - 'Kva_RechnungID': ID des betroffenen Auftrags.\n    - 'Diff_Forderung': Differenzbetrag (Wert in df1 - Summe in df2).\n    - 'Diff_Einigung': Differenzbetrag (Wert in df1 - Summe in df2).\n    - 'CRMEingangszeit': Zeitstempel des Auftrags\n    \n    Ist die Differenz positiv, ist der Wert im Auftrag höher als die Summe der Positionen."
            source_code: "def abgleich_auftraege(df1, df2):\n    \"\"\"\n    Vergleicht die Kopfdaten von Aufträgen (df1) mit der Summe ihrer Positionen (df2).\n\n    Die Funktion gruppiert die Positionsdaten (df2) anhand der 'Kva_RechnungID', bildet\n    die Summen für 'Forderung_Netto' und 'Einigung_Netto' und vergleicht diese mit den\n    in df1 hinterlegten Werten. Gleitkomma-Ungenauigkeiten werden dabei berücksichtigt.\n\n    Args:\n        df1 (pd.DataFrame): Dataframe mit den Auftragsdaten (Soll-Werte).\n            Muss zwingend folgende Spalten enthalten:\n            - 'Kva_RechnungID' (Verbindungsschlüssel)\n            - 'Forderung_Netto'\n            - 'Einigung_Netto'\n            \n        df2 (pd.DataFrame): Dataframe mit den Positionsdaten (Ist-Werte).\n            Muss zwingend folgende Spalten enthalten:\n            - 'Kva_RechnungID' (Verbindungsschlüssel)\n            - 'Forderung_Netto'\n            - 'Einigung_Netto'\n\n    Returns:\n        pd.DataFrame: Eine Liste der Abweichungen. Der Dataframe enthält nur die IDs,\n        bei denen die Werte nicht übereinstimmen.\n        \n        Enthaltene Spalten:\n        - 'Kva_RechnungID': ID des betroffenen Auftrags.\n        - 'Diff_Forderung': Differenzbetrag (Wert in df1 - Summe in df2).\n        - 'Diff_Einigung': Differenzbetrag (Wert in df1 - Summe in df2).\n        - 'CRMEingangszeit': Zeitstempel des Auftrags\n        \n        Ist die Differenz positiv, ist der Wert im Auftrag höher als die Summe der Positionen.\n    \"\"\"\n\n    df2_sum = df2.groupby('KvaRechnung_ID', observed=False)[['Forderung_Netto', 'Einigung_Netto']].sum().reset_index()\n\n    merged = pd.merge(df1, df2_sum, on='KvaRechnung_ID', how='left', suffixes=('_soll', '_ist'))\n\n    cols_to_fix = ['Forderung_Netto_ist', 'Einigung_Netto_ist']\n    merged[cols_to_fix] = merged[cols_to_fix].fillna(0)\n\n    merged['Diff_Forderung'] = (merged['Forderung_Netto_soll'] - merged['Forderung_Netto_ist']).round(2)\n    merged['Diff_Einigung'] = (merged['Einigung_Netto_soll'] - merged['Einigung_Netto_ist']).round(2)\n\n    mask_abweichung = (\n        ~np.isclose(merged['Diff_Forderung'], 0) |\n        ~np.isclose(merged['Diff_Einigung'], 0)\n    )\n\n    abweichungen = merged[mask_abweichung].copy()\n\n    result_df = abweichungen[['KvaRechnung_ID', 'Diff_Forderung', 'Diff_Einigung','CRMEingangszeit']]\n\n    return result_df"
            start_line: 763
            end_line: 816
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: metrics.false_negative_df1
            name: false_negative_df1
            args[1]: df
            docstring: "Calculates detailed statistics and specific error instances for singular sign errors in the column tuple ('Einigung', 'Empfehlung', 'Forderung') in 'Auftragsdaten'. Can not accurately detect multiple combined errors\n\nParameters\n----------\ndf : pandas.DataFrame\n    DataFrame containing 'Auftragsdaten' data set that is to be evaluated.\n\nReturns\n-------\nstats_df: pandas.DataFrame\n    Small DataFrame containing error counts per column (Einigung, Empfehlung, Forderung) for visualization.\ndetails_df: pandas.DataFrame\n    DataFrame containing the error instances."
            source_code: "def false_negative_df1(df):\n    \"\"\"Calculates detailed statistics and specific error instances for singular sign errors in the column tuple ('Einigung', 'Empfehlung', 'Forderung') in 'Auftragsdaten'. Can not accurately detect multiple combined errors\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing 'Auftragsdaten' data set that is to be evaluated.\n\n    Returns\n    -------\n    stats_df: pandas.DataFrame\n        Small DataFrame containing error counts per column (Einigung, Empfehlung, Forderung) for visualization.\n    details_df: pandas.DataFrame\n        DataFrame containing the error instances.\n    \"\"\"\n    m_ein = (df['Einigung_Netto'] < 0)\n    m_emp = (df[\"Empfehlung_Netto\"] < 0)\n    m_for = (df[\"Forderung_Netto\"] < 0)\n\n    e_ein = (m_ein != m_emp) & (m_ein != m_for)\n    e_emp = (m_emp != m_ein) & (m_emp != m_for)\n    e_for = (m_for != m_ein) & (m_for != m_emp)\n\n\n    stats_df = pd.DataFrame({\n        \"Spalte\": [\"Einigung_Netto\", \"Empfehlung_Netto\", \"Forderung_Netto\"],\n        \"Fehler\": [int(e_ein.sum()), int(e_emp.sum()), int(e_for.sum())]\n    })\n\n    full_mask = e_ein | e_emp | e_for\n    \n    details_df = df.loc[full_mask, [\n        \"KvaRechnung_ID\", \"Forderung_Netto\", \"Empfehlung_Netto\", \"Einigung_Netto\"\n    ]].copy()\n\n    return stats_df, details_df"
            start_line: 819
            end_line: 854
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: metrics.false_negative_df2
            name: false_negative_df2
            args[1]: df2
            docstring: "Führt detaillierte Konsistenzprüfungen (Plausibilität & Vorzeichen) auf dem DataFrame durch\nund gibt sowohl eine statistische Zusammenfassung als auch die betroffenen Zeilen zurück.\n\nDie Funktion prüft folgende Kriterien (sofern die Spalten existieren):\n1. 'Menge' < 0\n2. 'Menge_Einigung' < 0\n3. Vorzeichen-Widerspruch zwischen 'EP' und 'EP_Einigung' (einer negativ, einer positiv)\n4. Vorzeichen-Widerspruch zwischen 'Forderung_Netto' und 'Einigung_Netto'\n\nParameters\n----------\ndf2 : pandas.DataFrame\n    Der DataFrame mit den Positionsdaten, der überprüft werden soll.\n    Erwartet idealerweise Spalten wie 'Menge', 'EP', 'Forderung_Netto', etc.\n    Fehlende Spalten werden ignoriert (führen nicht zum Absturz).\n\nReturns\n-------\nstats_df : pandas.DataFrame\n    Eine Tabelle mit zwei Spalten: 'Kategorie' (Art des Fehlers) und 'Anzahl' (Häufigkeit).\n    Bleibt leer, wenn keine Fehler gefunden wurden.\n    \ndetails_df : pandas.DataFrame\n    Ein Auszug aus df2, der nur die Zeilen enthält, in denen mindestens ein Fehler gefunden wurde.\n    Enthält nur relevante Spalten ('Position_ID', 'Menge', 'EP', Beträge, etc.).\n    Falls 'Position_ID' fehlt, wird der DataFrame-Index als ID verwendet."
            source_code: "def false_negative_df2(df2):\n    \"\"\"\n    Führt detaillierte Konsistenzprüfungen (Plausibilität & Vorzeichen) auf dem DataFrame durch\n    und gibt sowohl eine statistische Zusammenfassung als auch die betroffenen Zeilen zurück.\n\n    Die Funktion prüft folgende Kriterien (sofern die Spalten existieren):\n    1. 'Menge' < 0\n    2. 'Menge_Einigung' < 0\n    3. Vorzeichen-Widerspruch zwischen 'EP' und 'EP_Einigung' (einer negativ, einer positiv)\n    4. Vorzeichen-Widerspruch zwischen 'Forderung_Netto' und 'Einigung_Netto'\n\n    Parameters\n    ----------\n    df2 : pandas.DataFrame\n        Der DataFrame mit den Positionsdaten, der überprüft werden soll.\n        Erwartet idealerweise Spalten wie 'Menge', 'EP', 'Forderung_Netto', etc.\n        Fehlende Spalten werden ignoriert (führen nicht zum Absturz).\n\n    Returns\n    -------\n    stats_df : pandas.DataFrame\n        Eine Tabelle mit zwei Spalten: 'Kategorie' (Art des Fehlers) und 'Anzahl' (Häufigkeit).\n        Bleibt leer, wenn keine Fehler gefunden wurden.\n        \n    details_df : pandas.DataFrame\n        Ein Auszug aus df2, der nur die Zeilen enthält, in denen mindestens ein Fehler gefunden wurde.\n        Enthält nur relevante Spalten ('Position_ID', 'Menge', 'EP', Beträge, etc.).\n        Falls 'Position_ID' fehlt, wird der DataFrame-Index als ID verwendet.\n    \"\"\"\n    errors_list = []\n    \n    n = len(df2)\n    mask_menge = pd.Series([False] * n, index=df2.index)\n    mask_menge_ein = pd.Series([False] * n, index=df2.index)\n    mask_ep = pd.Series([False] * n, index=df2.index)\n    mask_betrag = pd.Series([False] * n, index=df2.index)\n\n    if \"Menge\" in df2.columns:\n        mask_menge = (df2['Menge'] < 0) \n        cnt = mask_menge.sum()\n        if cnt > 0: \n            errors_list.append({\"Kategorie\": \"Menge < 0\", \"Anzahl\": int(cnt)})\n\n    if \"Menge_Einigung\" in df2.columns:\n        mask_menge_ein = (df2['Menge_Einigung'] < 0)\n        cnt = mask_menge_ein.sum()\n        if cnt > 0: \n            errors_list.append({\"Kategorie\": \"Menge_Einigung < 0\", \"Anzahl\": int(cnt)})\n\n    if \"EP\" in df2.columns and \"EP_Einigung\" in df2.columns:\n        mask_ep = (df2['EP'] < 0) ^ (df2['EP_Einigung'] < 0)\n        cnt = mask_ep.sum()\n        if cnt > 0:\n            errors_list.append({\"Kategorie\": \"Vorzeichen EP ungleich\", \"Anzahl\": int(cnt)})\n\n    if \"Forderung_Netto\" in df2.columns and \"Einigung_Netto\" in df2.columns:\n        mask_betrag = (df2['Forderung_Netto'] < 0) ^ (df2['Einigung_Netto'] < 0)\n        cnt = mask_betrag.sum()\n        if cnt > 0:\n            errors_list.append({\"Kategorie\": \"Vorzeichen Betrag ungleich\", \"Anzahl\": int(cnt)})\n\n    stats_df = pd.DataFrame(errors_list)\n\n    total_mask = mask_menge | mask_menge_ein | mask_ep | mask_betrag\n\n    details_df = pd.DataFrame()\n    \n    if total_mask.any():\n        cols = [\n            \"Position_ID\", \n            \"Bezeichnung\",\n            \"Menge\", \"Menge_Einigung\",\n            \"EP\", \"EP_Einigung\",\n            \"Forderung_Netto\", \"Einigung_Netto\" \n        ]\n\n        if \"Position_ID\" not in df2.columns:\n            temp_df = df2.loc[total_mask].copy()\n            if \"Position_ID\" not in temp_df.columns:\n                temp_df = temp_df.reset_index()\n                if \"index\" in temp_df.columns:\n                    temp_df = temp_df.rename(columns={\"index\": \"Position_ID\"})\n        else:\n            temp_df = df2.loc[total_mask].copy()\n\n        available_cols = [c for c in cols if c in temp_df.columns]\n        details_df = temp_df[available_cols]\n\n    return stats_df, details_df"
            start_line: 857
            end_line: 945
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: metrics.discount_details
            name: discount_details
            args[1]: df2
            docstring: "Aggregates statistics on discount logic errors and returns detailed instances based on the 'Plausibel' column.\n\nParameters\n----------\ndf2 : pandas.DataFrame\n    DataFrame containing 'Positionsdaten' data set that is to be evaluated.\n\nReturns\n-------\nstats_df: pandas.DataFrame\n    DataFrame with counts of the most frequent descriptions (Bezeichnung) among invalid entries.\ndetails_df: pandas.DataFrame\n    DataFrame containing specific invalid rows."
            source_code: "def discount_details(df2):\n    \"\"\"Aggregates statistics on discount logic errors and returns detailed instances based on the 'Plausibel' column.\n\n    Parameters\n    ----------\n    df2 : pandas.DataFrame\n        DataFrame containing 'Positionsdaten' data set that is to be evaluated.\n\n    Returns\n    -------\n    stats_df: pandas.DataFrame\n        DataFrame with counts of the most frequent descriptions (Bezeichnung) among invalid entries.\n    details_df: pandas.DataFrame\n        DataFrame containing specific invalid rows.\n    \"\"\"\n    if \"Plausibel\" not in df2.columns:\n        return pd.DataFrame(), pd.DataFrame()\n\n    mask_err = ~df2[\"Plausibel\"]\n\n    stats_df = pd.DataFrame()\n    if mask_err.any() and \"Bezeichnung\" in df2.columns:\n        stats_df = df2.loc[mask_err, \"Bezeichnung\"].value_counts().head(15).reset_index()\n        stats_df.columns = [\"Bezeichnung\", \"Anzahl\"]\n\n    cols = [\"Position_ID\", \"Bezeichnung\", \"Forderung_Netto\", \"Einigung_Netto\", \"ist_Abzug\"]\n    details_df = df2.loc[mask_err, [c for c in cols if c in df2.columns]]\n\n    return stats_df, details_df"
            start_line: 948
            end_line: 976
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
analysis_results:
  functions:
  classes: