basic_info:
  projekt_uebersicht:
    titel: Repo Onboarding Agent üöÄ
    beschreibung: ```
    aktueller_status: Information not found
    key_features: Information not found
    tech_stack: Information not found
  installation:
    dependencies: "- altair==4.2.2\n- annotated-types==0.7.0\n- anyio==4.11.0\n- attrs==25.4.0\n- bcrypt==5.0.0\n- blinker==1.9.0\n- cachetools==6.2.2\n- captcha==0.7.1\n- certifi==2025.11.12\n- cffi==2.0.0\n- charset-normalizer==3.4.4\n- click==8.3.1\n- colorama==0.4.6\n- contourpy==1.3.3\n- cryptography==46.0.3\n- cycler==0.12.1\n- distro==1.9.0\n- dnspython==2.8.0\n- dotenv==0.9.9\n- entrypoints==0.4\n- extra-streamlit-components==0.1.81\n- filetype==1.2.0\n- fonttools==4.61.0\n- gitdb==4.0.12\n- GitPython==3.1.45\n- google-ai-generativelanguage==0.9.0\n- google-api-core==2.28.1\n- google-auth==2.43.0\n- googleapis-common-protos==1.72.0\n- grpcio==1.76.0\n- grpcio-status==1.76.0\n- h11==0.16.0\n- httpcore==1.0.9\n- httpx==0.28.1\n- idna==3.11\n- Jinja2==3.1.6\n- jiter==0.12.0\n- jsonpatch==1.33\n- jsonpointer==3.0.0\n- jsonschema==4.25.1\n- jsonschema-specifications==2025.9.1\n- kiwisolver==1.4.9\n- langchain==1.0.8\n- langchain-core==1.1.0\n- langchain-google-genai==3.1.0\n- langchain-ollama==1.0.0\n- langchain-openai==1.1.0\n- langgraph==1.0.3\n- langgraph-checkpoint==3.0.1\n- langgraph-prebuilt==1.0.5\n- langgraph-sdk==0.2.9\n- langsmith==0.4.46\n- MarkupSafe==3.0.3\n- matplotlib==3.10.7\n- narwhals==2.12.0\n- networkx==3.6\n- numpy==2.3.5\n- ollama==0.6.1\n- openai==2.8.1\n- orjson==3.11.4\n- ormsgpack==1.12.0\n- packaging==25.0\n- pandas==2.3.3\n- pillow==12.0.0\n- proto-plus==1.26.1\n- protobuf==6.33.1\n- pyarrow==21.0.0\n- pyasn1==0.6.1\n- pyasn1_modules==0.4.2\n- pycparser==2.23\n- pydantic==2.12.4\n- pydantic_core==2.41.5\n- pydeck==0.9.1\n- PyJWT==2.10.1\n- pymongo==4.15.4\n- pyparsing==3.2.5\n- python-dateutil==2.9.0.post0\n- python-dotenv==1.2.1\n- pytz==2025.2\n- PyYAML==6.0.3\n- referencing==0.37.0\n- regex==2025.11.3\n- requests==2.32.5\n- requests-toolbelt==1.0.0\n- rpds-py==0.29.0\n- rsa==4.9.1\n- setuptools==75.9.1\n- six==1.17.0\n- smmap==5.0.2\n- sniffio==1.3.1\n- streamlit==1.51.0\n- streamlit-authenticator==0.4.2\n- streamlit-mermaid==0.3.0\n- tenacity==9.1.2\n- tiktoken==0.12.0\n- toml==0.10.2\n- toolz==1.1.0\n- toon_format @ git+https://github.com/toon-format/toon-python.git@9c4f0c0c24f2a0b0b376315f4b8707f8c9006de6\n- tornado==6.5.2\n- tqdm==4.67.1\n- typing-inspection==0.4.2\n- typing_extensions==4.15.0\n- tzdata==2025.2\n- urllib3==2.5.0\n- watchdog==6.0.0\n- xxhash==3.6.0\n- zstandard==0.25.0\n- nbformat"
    setup_anleitung: Information not found
    quick_start_guide: Information not found
file_tree:
  name: root
  type: directory
  children[16]:
    - path: .env.example
      name: .env.example
      size: 48
      type: file
    - path: .gitignore
      name: .gitignore
      size: 184
      type: file
    - name: SystemPrompts
      type: directory
      children[6]{path,name,size,type}:
        SystemPrompts/SystemPromptClassHelperLLM.txt,SystemPromptClassHelperLLM.txt,6645,file
        SystemPrompts/SystemPromptFunctionHelperLLM.txt,SystemPromptFunctionHelperLLM.txt,5546,file
        SystemPrompts/SystemPromptHelperLLM.txt,SystemPromptHelperLLM.txt,9350,file
        SystemPrompts/SystemPromptMainLLM.txt,SystemPromptMainLLM.txt,8380,file
        SystemPrompts/SystemPromptMainLLMToon.txt,SystemPromptMainLLMToon.txt,8456,file
        SystemPrompts/SystemPromptNotebookLLM.txt,SystemPromptNotebookLLM.txt,6921,file
    - path: analysis_output.json
      name: analysis_output.json
      size: 569396
      type: file
    - name: backend
      type: directory
      children[12]{path,name,size,type}:
        backend/AST_Schema.py,AST_Schema.py,6622,file
        backend/File_Dependency.py,File_Dependency.py,7384,file
        backend/HelperLLM.py,HelperLLM.py,17280,file
        backend/MainLLM.py,MainLLM.py,4281,file
        backend/__init__.py,__init__.py,0,file
        backend/basic_info.py,basic_info.py,7284,file
        backend/callgraph.py,callgraph.py,9020,file
        backend/converter.py,converter.py,3594,file
        backend/getRepo.py,getRepo.py,4739,file
        backend/main.py,main.py,25591,file
        backend/relationship_analyzer.py,relationship_analyzer.py,8279,file
        backend/scads_key_test.py,scads_key_test.py,663,file
    - name: database
      type: directory
      children[1]{path,name,size,type}:
        database/db.py,db.py,8222,file
    - name: frontend
      type: directory
      children[4]:
        - name: .streamlit
          type: directory
          children[1]{path,name,size,type}:
            frontend/.streamlit/config.toml,config.toml,165,file
        - path: frontend/__init__.py
          name: __init__.py
          size: 0
          type: file
        - path: frontend/frontend.py
          name: frontend.py
          size: 31176
          type: file
        - name: gifs
          type: directory
          children[1]{path,name,size,type}:
            frontend/gifs/4j.gif,4j.gif,3306723,file
    - name: notizen
      type: directory
      children[8]:
        - path: notizen/Report Agenda.txt
          name: Report Agenda.txt
          size: 3492
          type: file
        - path: notizen/Zwischenpraesentation Agenda.txt
          name: Zwischenpraesentation Agenda.txt
          size: 809
          type: file
        - path: notizen/doc_bestandteile.md
          name: doc_bestandteile.md
          size: 847
          type: file
        - name: grafiken
          type: directory
          children[4]:
            - name: "1"
              type: directory
              children[5]{path,name,size,type}:
                notizen/grafiken/1/File_Dependency_Graph_Repo.dot,File_Dependency_Graph_Repo.dot,1227,file
                notizen/grafiken/1/global_callgraph.png,global_callgraph.png,940354,file
                notizen/grafiken/1/global_graph.png,global_graph.png,4756519,file
                notizen/grafiken/1/global_graph_2.png,global_graph_2.png,242251,file
                notizen/grafiken/1/repo.dot,repo.dot,13870,file
            - name: "2"
              type: directory
              children[12]{path,name,size,type}:
                notizen/grafiken/2/FDG_repo.dot,FDG_repo.dot,9150,file
                notizen/grafiken/2/fdg_graph.png,fdg_graph.png,425374,file
                notizen/grafiken/2/fdg_graph_2.png,fdg_graph_2.png,4744718,file
                notizen/grafiken/2/filtered_callgraph_flask.png,filtered_callgraph_flask.png,614631,file
                notizen/grafiken/2/filtered_callgraph_repo-agent.png,filtered_callgraph_repo-agent.png,280428,file
                notizen/grafiken/2/filtered_callgraph_repo-agent_3.png,filtered_callgraph_repo-agent_3.png,1685838,file
                notizen/grafiken/2/filtered_repo_callgraph_flask.dot,filtered_repo_callgraph_flask.dot,19984,file
                notizen/grafiken/2/filtered_repo_callgraph_repo-agent-3.dot,filtered_repo_callgraph_repo-agent-3.dot,20120,file
                notizen/grafiken/2/filtered_repo_callgraph_repo-agent.dot,filtered_repo_callgraph_repo-agent.dot,3118,file
                notizen/grafiken/2/global_callgraph.png,global_callgraph.png,608807,file
                notizen/grafiken/2/graph_flask.md,graph_flask.md,17398,file
                notizen/grafiken/2/repo.dot,repo.dot,19984,file
            - name: Flask-Repo
              type: directory
              children[65]{path,name,size,type}:
                notizen/grafiken/Flask-Repo/__init__.dot,__init__.dot,89,file
                notizen/grafiken/Flask-Repo/__main__.dot,__main__.dot,87,file
                notizen/grafiken/Flask-Repo/app.dot,app.dot,78,file
                notizen/grafiken/Flask-Repo/auth.dot,auth.dot,1126,file
                notizen/grafiken/Flask-Repo/blog.dot,blog.dot,939,file
                notizen/grafiken/Flask-Repo/blueprints.dot,blueprints.dot,4601,file
                notizen/grafiken/Flask-Repo/cli.dot,cli.dot,8269,file
                notizen/grafiken/Flask-Repo/conf.dot,conf.dot,489,file
                notizen/grafiken/Flask-Repo/config.dot,config.dot,2130,file
                notizen/grafiken/Flask-Repo/conftest.dot,conftest.dot,1756,file
                notizen/grafiken/Flask-Repo/ctx.dot,ctx.dot,3009,file
                notizen/grafiken/Flask-Repo/db.dot,db.dot,782,file
                notizen/grafiken/Flask-Repo/debughelpers.dot,debughelpers.dot,1903,file
                notizen/grafiken/Flask-Repo/factory.dot,factory.dot,220,file
                notizen/grafiken/Flask-Repo/flask.dot,flask.dot,82,file
                notizen/grafiken/Flask-Repo/globals.dot,globals.dot,370,file
                notizen/grafiken/Flask-Repo/hello.dot,hello.dot,152,file
                notizen/grafiken/Flask-Repo/helpers.dot,helpers.dot,2510,file
                notizen/grafiken/Flask-Repo/importerrorapp.dot,importerrorapp.dot,155,file
                notizen/grafiken/Flask-Repo/logging.dot,logging.dot,564,file
                notizen/grafiken/Flask-Repo/make_celery.dot,make_celery.dot,99,file
                notizen/grafiken/Flask-Repo/multiapp.dot,multiapp.dot,88,file
                notizen/grafiken/Flask-Repo/provider.dot,provider.dot,1769,file
                notizen/grafiken/Flask-Repo/scaffold.dot,scaffold.dot,3700,file
                notizen/grafiken/Flask-Repo/sessions.dot,sessions.dot,4178,file
                notizen/grafiken/Flask-Repo/signals.dot,signals.dot,133,file
                notizen/grafiken/Flask-Repo/tag.dot,tag.dot,3750,file
                notizen/grafiken/Flask-Repo/tasks.dot,tasks.dot,312,file
                notizen/grafiken/Flask-Repo/templating.dot,templating.dot,2277,file
                notizen/grafiken/Flask-Repo/test_appctx.dot,test_appctx.dot,2470,file
                notizen/grafiken/Flask-Repo/test_async.dot,test_async.dot,1919,file
                notizen/grafiken/Flask-Repo/test_auth.dot,test_auth.dot,725,file
                notizen/grafiken/Flask-Repo/test_basic.dot,test_basic.dot,17383,file
                notizen/grafiken/Flask-Repo/test_blog.dot,test_blog.dot,1105,file
                notizen/grafiken/Flask-Repo/test_blueprints.dot,test_blueprints.dot,11515,file
                notizen/grafiken/Flask-Repo/test_cli.dot,test_cli.dot,6435,file
                notizen/grafiken/Flask-Repo/test_config.dot,test_config.dot,2854,file
                notizen/grafiken/Flask-Repo/test_config.png,test_config.png,288693,file
                notizen/grafiken/Flask-Repo/test_converters.dot,test_converters.dot,937,file
                notizen/grafiken/Flask-Repo/test_db.dot,test_db.dot,481,file
                notizen/grafiken/Flask-Repo/test_factory.dot,test_factory.dot,201,file
                notizen/grafiken/Flask-Repo/test_helpers.dot,test_helpers.dot,5857,file
                notizen/grafiken/Flask-Repo/test_instance_config.dot,test_instance_config.dot,1249,file
                notizen/grafiken/Flask-Repo/test_js_example.dot,test_js_example.dot,453,file
                notizen/grafiken/Flask-Repo/test_json.dot,test_json.dot,4021,file
                notizen/grafiken/Flask-Repo/test_json_tag.dot,test_json_tag.dot,1452,file
                notizen/grafiken/Flask-Repo/test_logging.dot,test_logging.dot,1348,file
                notizen/grafiken/Flask-Repo/test_regression.dot,test_regression.dot,763,file
                notizen/grafiken/Flask-Repo/test_reqctx.dot,test_reqctx.dot,3809,file
                notizen/grafiken/Flask-Repo/test_request.dot,test_request.dot,668,file
                notizen/grafiken/Flask-Repo/test_session_interface.dot,test_session_interface.dot,667,file
                notizen/grafiken/Flask-Repo/test_signals.dot,test_signals.dot,1671,file
                notizen/grafiken/Flask-Repo/test_subclassing.dot,test_subclassing.dot,612,file
                notizen/grafiken/Flask-Repo/test_templating.dot,test_templating.dot,5129,file
                notizen/grafiken/Flask-Repo/test_testing.dot,test_testing.dot,4081,file
                notizen/grafiken/Flask-Repo/test_user_error_handler.dot,test_user_error_handler.dot,5984,file
                notizen/grafiken/Flask-Repo/test_views.dot,test_views.dot,2641,file
                notizen/grafiken/Flask-Repo/testing.dot,testing.dot,2785,file
                notizen/grafiken/Flask-Repo/typing.dot,typing.dot,86,file
                notizen/grafiken/Flask-Repo/typing_app_decorators.dot,typing_app_decorators.dot,500,file
                notizen/grafiken/Flask-Repo/typing_error_handler.dot,typing_error_handler.dot,420,file
                notizen/grafiken/Flask-Repo/typing_route.dot,typing_route.dot,1717,file
                notizen/grafiken/Flask-Repo/views.dot,views.dot,1125,file
                notizen/grafiken/Flask-Repo/wrappers.dot,wrappers.dot,911,file
                notizen/grafiken/Flask-Repo/wsgi.dot,wsgi.dot,19,file
            - name: Repo-onboarding
              type: directory
              children[15]{path,name,size,type}:
                notizen/grafiken/Repo-onboarding/AST.dot,AST.dot,1361,file
                notizen/grafiken/Repo-onboarding/Frontend.dot,Frontend.dot,538,file
                notizen/grafiken/Repo-onboarding/HelperLLM.dot,HelperLLM.dot,1999,file
                notizen/grafiken/Repo-onboarding/HelperLLM.png,HelperLLM.png,234861,file
                notizen/grafiken/Repo-onboarding/MainLLM.dot,MainLLM.dot,2547,file
                notizen/grafiken/Repo-onboarding/agent.dot,agent.dot,2107,file
                notizen/grafiken/Repo-onboarding/basic_info.dot,basic_info.dot,1793,file
                notizen/grafiken/Repo-onboarding/callgraph.dot,callgraph.dot,1478,file
                notizen/grafiken/Repo-onboarding/getRepo.dot,getRepo.dot,1431,file
                notizen/grafiken/Repo-onboarding/graph_AST.png,graph_AST.png,132743,file
                notizen/grafiken/Repo-onboarding/graph_AST2.png,graph_AST2.png,12826,file
                notizen/grafiken/Repo-onboarding/graph_AST3.png,graph_AST3.png,136016,file
                notizen/grafiken/Repo-onboarding/main.dot,main.dot,1091,file
                notizen/grafiken/Repo-onboarding/tools.dot,tools.dot,1328,file
                notizen/grafiken/Repo-onboarding/types.dot,types.dot,133,file
        - path: notizen/notizen.md
          name: notizen.md
          size: 4937
          type: file
        - path: notizen/paul_notizen.md
          name: paul_notizen.md
          size: 2033
          type: file
        - path: notizen/praesentation_notizen.md
          name: praesentation_notizen.md
          size: 2738
          type: file
        - path: notizen/technische_notizen.md
          name: technische_notizen.md
          size: 3616
          type: file
    - path: output.json
      name: output.json
      size: 454008
      type: file
    - path: output.toon
      name: output.toon
      size: 341854
      type: file
    - path: readme.md
      name: readme.md
      size: 1905
      type: file
    - path: requirements.txt
      name: requirements.txt
      size: 4286
      type: file
    - name: result
      type: directory
      children[28]{path,name,size,type}:
        result/ast_schema_01_12_2025_11-49-24.json,ast_schema_01_12_2025_11-49-24.json,201215,file
        result/notebook_report_23_12_2025_12-56-24_NotebookLLM_gemini-2.5-flash.md,notebook_report_23_12_2025_12-56-24_NotebookLLM_gemini-2.5-flash.md,15776,file
        result/notebook_report_27_12_2025_15-06-09_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-06-09_NotebookLLM_gemini-2.5-flash.md,14804,file
        result/notebook_report_27_12_2025_15-09-29_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-09-29_NotebookLLM_gemini-2.5-flash.md,17598,file
        result/notebook_report_27_12_2025_15-26-34_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-26-34_NotebookLLM_gemini-2.5-flash.md,20458,file
        result/notebook_report_27_12_2025_15-33-06_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-33-06_NotebookLLM_gemini-2.5-flash.md,15268,file
        result/notebook_report_29_12_2025_15-03-21_NotebookLLM_gemini-2.5-flash.md,notebook_report_29_12_2025_15-03-21_NotebookLLM_gemini-2.5-flash.md,16111,file
        result/report_01_12_2025_12-26-46_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_12-26-46_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,79523,file
        result/report_01_12_2025_12-55-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_12-55-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,143883,file
        result/report_01_12_2025_13-37-30_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_13-37-30_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,141924,file
        result/report_01_12_2025_14-15-04_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_14-15-04_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,129176,file
        result/report_01_12_2025_14-42-38_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_14-42-38_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,38933,file
        result/report_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,138015,file
        result/report_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.md,report_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.md,127747,file
        result/report_03_12_2025_22-46-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_03_12_2025_22-46-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,80305,file
        result/report_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,163921,file
        result/report_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.md,report_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.md,128520,file
        result/report_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.md,report_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.md,7245,file
        result/report_14_11_2025_14-52-36.md,report_14_11_2025_14-52-36.md,16393,file
        result/report_14_11_2025_15-21-53.md,report_14_11_2025_15-21-53.md,108585,file
        result/report_14_11_2025_15-26-24.md,report_14_11_2025_15-26-24.md,11659,file
        result/report_21_11_2025_15-43-30.md,report_21_11_2025_15-43-30.md,57940,file
        result/report_21_11_2025_16-06-12.md,report_21_11_2025_16-06-12.md,76423,file
        result/report_22_11_2025_14-01-50_Helper_llama3_Main_geminipro.md,report_22_11_2025_14-01-50_Helper_llama3_Main_geminipro.md,39595,file
        result/report_22_11_2025_14-39-55_Helper_llama3_MainLLM_llama3.md,report_22_11_2025_14-39-55_Helper_llama3_MainLLM_llama3.md,2013,file
        result/result_2025-11-11_12-30-53.md,result_2025-11-11_12-30-53.md,1232,file
        result/result_2025-11-11_12-43-51.md,result_2025-11-11_12-43-51.md,33683,file
        result/result_2025-11-11_12-45-37.md,result_2025-11-11_12-45-37.md,1193,file
    - name: schemas
      type: directory
      children[1]{path,name,size,type}:
        schemas/types.py,types.py,7559,file
    - name: statistics
      type: directory
      children[5]{path,name,size,type}:
        statistics/savings_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.png,savings_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.png,23279,file
        statistics/savings_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.png,savings_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.png,23650,file
        statistics/savings_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.png,savings_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.png,23805,file
        statistics/savings_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.png,savings_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.png,22698,file
        statistics/savings_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.png,savings_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.png,22539,file
    - path: test.json
      name: test.json
      size: 241669
      type: file
ast_schema:
  files:
    "backend/AST_Schema.py":
      ast_nodes:
        imports[3]: ast,os,getRepo.GitRepository
        functions[1]:
          - mode: function_analysis
            identifier: backend.AST_Schema.path_to_module
            name: path_to_module
            args[2]: filepath,project_root
            docstring: Wandelt einen Dateipfad in einen Python-Modulpfad um.
            source_code: "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    \n    module_path = rel_path.replace(os.path.sep, '.')\n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n    return module_path"
            start_line: 5
            end_line: 18
            context:
              calls[0]:
              called_by[0]:
        classes[2]:
          - mode: class_analysis
            identifier: backend.AST_Schema.ASTVisitor
            name: ASTVisitor
            docstring: null
            source_code: "class ASTVisitor(ast.NodeVisitor):\n    def __init__(self, source_code: str, file_path: str, project_root: str):\n        self.source_code = source_code\n        self.file_path = file_path\n        self.project_root = project_root\n        self.module_path = path_to_module(self.file_path, self.project_root)\n        self.schema = {\"imports\": [], \"functions\": [], \"classes\": []}\n        self._current_class = None\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(f\"{node.module}.{alias.name}\")\n        self.generic_visit(node)\n\n    def visit_ClassDef(self, node):\n        class_identifier = f\"{self.module_path}.{node.name}\"\n        class_info = {\n            \"mode\": \"class_analysis\",\n            \"identifier\": class_identifier,\n            \"name\": node.name,\n            \"docstring\": ast.get_docstring(node),\n            \"source_code\": ast.get_source_segment(self.source_code, node),\n            \"start_line\": node.lineno,\n            \"end_line\": node.end_lineno,\n            \"context\": {\n                \"dependencies\": [],\n                \"instantiated_by\": [],\n                \"method_context\": []\n            },   \n        }\n        self.schema[\"classes\"].append(class_info)\n        self._current_class = class_info \n        self.generic_visit(node)\n        self._current_class = None\n\n    def visit_FunctionDef(self, node):\n        if self._current_class:\n            method_identifier = f\"{self._current_class['identifier']}.{node.name}\"\n            method_context_info = {\n                \"identifier\": method_identifier,\n                \"name\": node.name,\n                \"calls\": [],\n                \"called_by\": [],\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n            }\n            self._current_class[\"context\"][\"method_context\"].append(method_context_info)\n        else:\n            func_identifier = f\"{self.module_path}.{node.name}\"\n            func_info = {\n                \"mode\": \"function_analysis\",\n                \"identifier\": func_identifier,\n                \"name\": node.name,\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"source_code\": ast.get_source_segment(self.source_code, node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n                \"context\": {\n                    \"calls\": [],\n                    \"called_by\": []\n                }\n            }\n            self.schema[\"functions\"].append(func_info)\n        self.generic_visit(node)\n    \n    def visit_AsyncFunctionDef(self, node):\n        self.visit_FunctionDef(node)"
            start_line: 20
            end_line: 94
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[6]:
                - identifier: backend.AST_Schema.ASTVisitor.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[4]: self,source_code,file_path,project_root
                  docstring: null
                  start_line: 21
                  end_line: 27
                - identifier: backend.AST_Schema.ASTVisitor.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 29
                  end_line: 32
                - identifier: backend.AST_Schema.ASTVisitor.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 34
                  end_line: 37
                - identifier: backend.AST_Schema.ASTVisitor.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 39
                  end_line: 58
                - identifier: backend.AST_Schema.ASTVisitor.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 60
                  end_line: 91
                - identifier: backend.AST_Schema.ASTVisitor.visit_AsyncFunctionDef
                  name: visit_AsyncFunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 93
                  end_line: 94
          - mode: class_analysis
            identifier: backend.AST_Schema.ASTAnalyzer
            name: ASTAnalyzer
            docstring: null
            source_code: "class ASTAnalyzer:\n\n    def __init__(self):\n        pass\n\n    def merge_relationship_data(self, full_schema: dict, raw_relationships: dict) -> dict:\n\n        outgoing_calls = raw_relationships.get(\"outgoing\", {})\n        incoming_calls = raw_relationships.get(\"incoming\", {})\n\n        for file_path, file_data in full_schema.get(\"files\", {}).items():\n            ast_nodes = file_data.get(\"ast_nodes\", {})\n            \n            for func in ast_nodes.get(\"functions\", []):\n                func_id = func.get(\"identifier\")\n                if func_id:\n                    func[\"context\"][\"calls\"] = outgoing_calls.get(func_id, [])\n                    func[\"context\"][\"called_by\"] = incoming_calls.get(func_id, [])\n\n            for cls in ast_nodes.get(\"classes\", []):\n                cls_id = cls.get(\"identifier\")\n                \n                if cls_id:\n                    cls[\"context\"][\"instantiated_by\"] = incoming_calls.get(cls_id, [])\n\n                class_dependencies = set()\n                \n                for method in cls[\"context\"].get(\"method_context\", []):\n                    method_id = method.get(\"identifier\")\n                    if method_id:\n                        m_calls = outgoing_calls.get(method_id, [])\n                        method[\"calls\"] = m_calls\n                        method[\"called_by\"] = incoming_calls.get(method_id, [])\n                        \n                        for callee in m_calls:\n                            if cls_id and not callee.startswith(f\"{cls_id}.\"):\n                                class_dependencies.add(callee)\n                \n                cls[\"context\"][\"dependencies\"] = sorted(list(class_dependencies))\n        \n        return full_schema\n\n    def analyze_repository(self, files: list, repo: GitRepository) -> dict:\n        full_schema = {\n            \"files\": {}\n        }\n\n        all_paths = [file_obj.path for file_obj in files]\n        if not all_paths:\n            return full_schema\n        \n        project_root = os.path.commonpath(all_paths)\n        if os.path.isfile(project_root):\n            project_root = os.path.dirname(project_root)\n\n        for file_obj in files:\n            if not file_obj.path.endswith('.py'):\n                continue\n            \n            file_content = file_obj.content\n            if not file_content.strip():\n                continue\n\n            try:\n                tree = ast.parse(file_content)\n                visitor = ASTVisitor(\n                    source_code=file_content, \n                    file_path=file_obj.path, \n                    project_root=project_root\n                )\n                visitor.visit(tree)\n                file_schema_nodes = visitor.schema\n\n                if file_schema_nodes[\"imports\"] or file_schema_nodes[\"functions\"] or file_schema_nodes[\"classes\"]:\n                    full_schema[\"files\"][file_obj.path] = {\n                        \"ast_nodes\": file_schema_nodes\n                    }\n\n            except (SyntaxError, ValueError) as e:\n                print(f\"Warnung: Konnte Datei '{file_obj.path}' nicht parsen. Fehler: {e}\")\n\n        return full_schema"
            start_line: 97
            end_line: 178
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[3]:
                - identifier: backend.AST_Schema.ASTAnalyzer.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 99
                  end_line: 100
                - identifier: backend.AST_Schema.ASTAnalyzer.merge_relationship_data
                  name: merge_relationship_data
                  calls[0]:
                  called_by[0]:
                  args[3]: self,full_schema,raw_relationships
                  docstring: null
                  start_line: 102
                  end_line: 137
                - identifier: backend.AST_Schema.ASTAnalyzer.analyze_repository
                  name: analyze_repository
                  calls[0]:
                  called_by[0]:
                  args[3]: self,files,repo
                  docstring: null
                  start_line: 139
                  end_line: 178
    "backend/File_Dependency.py":
      ast_nodes:
        imports[17]: networkx,os,ast.Assign,ast.AST,ast.ClassDef,ast.FunctionDef,ast.Import,ast.ImportFrom,ast.Name,ast.NodeVisitor,ast.literal_eval,ast.parse,ast.walk,keyword.iskeyword,pathlib.Path,getRepo.GitRepository,callgraph.make_safe_dot
        functions[3]:
          - mode: function_analysis
            identifier: backend.File_Dependency.build_file_dependency_graph
            name: build_file_dependency_graph
            args[3]: filename,tree,repo_root
            docstring: null
            source_code: "def build_file_dependency_graph(filename: str, tree: AST, repo_root: str) -> nx.DiGraph:\n    graph = nx.DiGraph()\n\n    tree_visitor = FileDependencyGraph(filename, repo_root)\n    tree_visitor.visit(tree)\n\n    for caller, callees in tree_visitor.import_dependencies.items():\n        graph.add_node(caller)\n        graph.add_nodes_from(callees)\n        for callee in callees:\n            graph.add_edge(caller, callee)\n    \n    return graph"
            start_line: 153
            end_line: 165
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.File_Dependency.build_repository_graph
            name: build_repository_graph
            args[1]: repository
            docstring: null
            source_code: "def build_repository_graph(repository: GitRepository) -> nx.DiGraph:\n    all_files = repository.get_all_files()\n    repo_root = repository.temp_dir\n    global_graph = nx.DiGraph()\n\n    for file in all_files: \n        if not file.path.endswith(\".py\"):\n            continue\n        filename = str(os.path.basename(file.path)).removesuffix(\".py\")\n        tree = parse(file.content)\n        graph = build_file_dependency_graph(filename, tree, repo_root)\n        \n        for node in graph.nodes:\n            global_graph.add_node(node)\n\n        for caller, callee in graph.edges:\n            if callee:\n                global_graph.add_node(callee)\n                global_graph.add_edge(caller, callee)\n    \n    return global_graph"
            start_line: 167
            end_line: 187
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.File_Dependency.get_all_temp_files
            name: get_all_temp_files
            args[1]: directory
            docstring: null
            source_code: "def get_all_temp_files(directory: str) -> list[Path]:\n    root_path = Path(directory).resolve()\n    all_files = [file.relative_to(root_path) for file in root_path.rglob(\"*.py\")]\n\n    return all_files"
            start_line: 189
            end_line: 193
            context:
              calls[0]:
              called_by[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.File_Dependency.FileDependencyGraph
            name: FileDependencyGraph
            docstring: null
            source_code: "class FileDependencyGraph(NodeVisitor):\n\n    import_dependencies: dict[str, set[str]] = {}\n    def __init__(self, filename: str, repo_root):\n        \"\"\"\n        Initialisiert den File Dependency Graphen\n\n        Args:\n\n        \"\"\"\n        self.filename = filename\n        self.repo_root = repo_root\n\n    def _resolve_module_name(self, node: ImportFrom) -> list[str]:\n        \"\"\"\n        L√∂st relative Imports der Form `from .. import name1, name2` auf.\n        Liefert die Liste der tats√§chlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\n        Wirft ImportError, wenn nichts aufgel√∂st werden konnte.\n        \"\"\"\n        level_depth = node.level\n        names = [alias.name for alias in node.names]  \n        all_files = get_all_temp_files(self.repo_root)  \n\n        candidates = [p for p in all_files if p.stem == Path(self.filename).stem or p.name == f\"{self.filename}.py\"]\n\n        if not candidates:\n            raise ImportError(f\"Kann aktuelle Datei '{self.filename}' im Repo nicht finden.\")\n\n\n        candidates.sort(key=lambda p: len(p.parts))\n        current_rel_path = candidates[0]\n        base_dir = current_rel_path.parent\n        if level_depth <= 0:\n            raise ImportError(\"Erwarteter relativer Import (level >= 1).\")\n        for _ in range(level_depth - 1):\n            if base_dir == base_dir.parent and len(base_dir.parts) == 0:\n                raise ImportError(f\"Relative Import-Ebene ({level_depth}) zu gro√ü f√ºr Datei '{current_rel_path}'.\")\n            base_dir = base_dir.parent\n\n        repo_root_path = Path(self.repo_root).resolve()\n        resolved: list[str] = []\n\n        def module_file_exists(rel_base: Path, name: str) -> bool:\n            file_path = repo_root_path / rel_base / f\"{name}.py\"\n            pkg_init = repo_root_path / rel_base / name / \"__init__.py\"\n            return file_path.exists() or pkg_init.exists()\n\n        def init_exports_symbol(rel_base: Path, symbol: str) -> bool:\n            \"\"\"\n            Pr√ºft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\n            oder als Name (funktion/klasse/assign) definiert ist.\n            \"\"\"\n            init_path = repo_root_path / rel_base / \"__init__.py\"\n            if not init_path.exists():\n                return False\n            try:\n                src = init_path.read_text(encoding=\"utf-8\")\n                mod = parse(src, filename=str(init_path))\n            except Exception:\n                return False\n\n            for node_ in walk(mod):\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == \"__all__\":\n                            try:\n                                value = literal_eval(node_.value)\n                                if isinstance(value, (list, tuple)) and symbol in value:\n                                    return True\n                            except Exception:\n                                pass\n                if isinstance(node_, (FunctionDef, ClassDef)) and node_.name == symbol:\n                    return True\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == symbol:\n                            return True\n            return False\n\n        for name in names:\n            if not name.isidentifier() or iskeyword(name):\n                continue\n\n            if module_file_exists(base_dir, name):\n                resolved.append(name)\n                continue\n            if init_exports_symbol(base_dir, name):\n                resolved.append(name)\n                continue\n\n        resolved = sorted(set(resolved))\n\n        if not resolved:\n            raise ImportError(\n                f\"Kein passendes Modul/Symbol f√ºr relative Import-Aufl√∂sung gefunden \"\n                f\"(level={level_depth}, names={names}, base_dir={base_dir})\"\n            )\n\n        return resolved\n    \n    def visit_Import(self, node: Import | ImportFrom, base_name: str | None = None):\n        for alias in node.names:\n        \n            if self.filename not in self.import_dependencies:\n                self.import_dependencies[self.filename] = set()\n\n            if base_name:\n                self.import_dependencies[self.filename].add(base_name)\n            else:\n                self.import_dependencies[self.filename].add(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node: ImportFrom):\n        \"\"\"\n        Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\n        also c, und dieser wird als callee f√ºr den caller, das File, gesetzt.\n        \"\"\"\n        module_name = node.module\n        if module_name:\n            module_base = module_name.split(\".\")[-1]\n            self.visit_Import(node, module_base)\n        else:\n            try:\n                resolved = self._resolve_module_name(node)\n                for base in resolved:\n                    self.visit_Import(node, base)\n            except ImportError as e:\n                print(f\"Aufl√∂sung eines relativen Imports fehlgeschlagen: {e}\")\n\n        self.generic_visit(node)"
            start_line: 22
            end_line: 151
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[6]:
                - identifier: backend.File_Dependency.FileDependencyGraph.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[3]: self,filename,repo_root
                  docstring: "Initialisiert den File Dependency Graphen\n\nArgs:"
                  start_line: 25
                  end_line: 33
                - identifier: backend.File_Dependency.FileDependencyGraph._resolve_module_name
                  name: _resolve_module_name
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: "L√∂st relative Imports der Form `from .. import name1, name2` auf.\nLiefert die Liste der tats√§chlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\nWirft ImportError, wenn nichts aufgel√∂st werden konnte."
                  start_line: 35
                  end_line: 120
                - identifier: backend.File_Dependency.FileDependencyGraph.module_file_exists
                  name: module_file_exists
                  calls[0]:
                  called_by[0]:
                  args[2]: rel_base,name
                  docstring: null
                  start_line: 64
                  end_line: 67
                - identifier: backend.File_Dependency.FileDependencyGraph.init_exports_symbol
                  name: init_exports_symbol
                  calls[0]:
                  called_by[0]:
                  args[2]: rel_base,symbol
                  docstring: "Pr√ºft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\noder als Name (funktion/klasse/assign) definiert ist."
                  start_line: 69
                  end_line: 99
                - identifier: backend.File_Dependency.FileDependencyGraph.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[3]: self,node,base_name
                  docstring: null
                  start_line: 122
                  end_line: 132
                - identifier: backend.File_Dependency.FileDependencyGraph.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: "Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\nalso c, und dieser wird als callee f√ºr den caller, das File, gesetzt."
                  start_line: 134
                  end_line: 151
    "backend/HelperLLM.py":
      ast_nodes:
        imports[23]: os,json,logging,time,typing.List,typing.Dict,typing.Any,typing.Optional,typing.Union,dotenv.load_dotenv,langchain_google_genai.ChatGoogleGenerativeAI,langchain_ollama.ChatOllama,langchain_openai.ChatOpenAI,langchain.messages.HumanMessage,langchain.messages.SystemMessage,langchain.messages.AIMessage,pydantic.ValidationError,schemas.types.FunctionAnalysis,schemas.types.ClassAnalysis,schemas.types.FunctionAnalysisInput,schemas.types.FunctionContextInput,schemas.types.ClassAnalysisInput,schemas.types.ClassContextInput
        functions[1]:
          - mode: function_analysis
            identifier: backend.HelperLLM.main_orchestrator
            name: main_orchestrator
            args[0]:
            docstring: "Dummy Data and processing loop for testing the LLMHelper class.\nThis version is syntactically correct and logically matches the Pydantic models."
            source_code: "def main_orchestrator():\n    \"\"\"\n    Dummy Data and processing loop for testing the LLMHelper class.\n    This version is syntactically correct and logically matches the Pydantic models.\n    \"\"\"\n    \n    # --- Step 1: Define the pre-computed analysis for each method ---\n    \n    # Example Input 1: For the 'add_item' function\n    add_item_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"source_code\": \"\"\"def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\\\"\\\"\\\"\n        if not isinstance(quantity, int) or quantity <= 0:\n            raise ValueError(\"Quantity must be a positive integer.\")\n    \n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n        print(f\"Added {quantity} of {item_name}. New total: {self.inventory[item_name]}\")\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"process_shipment\", \"restock_api_endpoint\"]\n        }\n    })\n\n    # Example Input 2: For the 'check_stock' function\n    check_stock_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"source_code\": \"\"\"def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Retrieves the current stock quantity for a given item.\\\"\\\"\\\"\n        stock_level = self.inventory.get(item_name, 0)\n        return stock_level\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"fulfill_order\", \"ui_display_handler\"]\n        }\n    })\n\n    # Example Input 3: For the 'generate_report' function\n    generate_report_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"source_code\": \"\"\"def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a timestamped string summary of the current inventory state.\\\"\\\"\\\"\n        timestamp = datetime.now()\n        report_header = \"Inventory Report as of: {ts}\\\\n\".format(ts=timestamp)\n        report_lines = [report_header]\n        \n        for item, quantity in self.inventory.items():\n            line = \"- {name}: {qty}\".format(name=item, qty=quantity)\n            report_lines.append(line)\n            \n        return \"\\\\n\".join(report_lines)\"\"\",\n        \"imports\": [\"from datetime import datetime\"],\n        \"context\": {\n            \"calls\": [\"datetime.now\", \"str.format\"],\n            \"called_by\": [\"daily_cron_job\", \"admin_dashboard_export\"]\n        }\n    })\n\n    add_item_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"description\": {\n            \"overall\": \"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to add.\"},\n                {\"name\": \"quantity\", \"type\": \"int\", \"description\": \"The number of units to add. Must be a positive integer.\"}\n            ],\n            \"returns\": [],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"process_shipment and restock_api_endpoint\" }\n        },\n        \"error\": None\n    })\n\n    check_stock_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"description\": {\n            \"overall\": \"Retrieves the current stock quantity for a given item.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to check.\"}\n            ],\n            \"returns\": [\n                {\"name\": \"stock_level\", \"type\": \"int\", \"description\": \"The quantity of the item in stock. Returns 0 if the item is not found.\"}\n            ],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"fulfill_order and ui_display_handler\" }\n        },\n        \"error\": None\n    })\n\n    generate_report_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"description\": {\n            \"overall\": \"Generates a timestamped string summary of the current inventory state.\",\n            \"parameters\": [ {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"} ],\n            \"returns\": [\n                {\"name\": \"report\", \"type\": \"str\", \"description\": \"A formatted string detailing all items and their quantities.\"}\n            ],\n            \"usage_context\": { \"calls\": \"datetime.now and str.format\", \"called_by\": \"daily_cron_job and admin_dashboard_export\" }\n        },\n        \"error\": None\n    })\n\n    class_input = ClassAnalysisInput(\n        mode=\"class_analysis\",\n        identifier=\"InventoryManager\",\n        # NOTE: The indentation of this string is now fixed. It starts at column 0.\n        source_code=\"\"\"class InventoryManager:\n    \\\"\\\"\\\"Manages stock levels for products in a warehouse.\\\"\\\"\\\"\n    def __init__(self, warehouse_id: str):\n        self.warehouse_id = warehouse_id\n        self.inventory = {}  # item_name: quantity\n\n    def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds an item to the inventory.\\\"\\\"\\\"\n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n\n    def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Checks the stock of a specific item.\\\"\\\"\\\"\n        return self.inventory.get(item_name, 0)\n\n    def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a summary report of the inventory.\\\"\\\"\\\"\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        report_lines = [f\"Inventory Report for {self.warehouse_id} at {timestamp}\"]\n        if not self.inventory:\n            report_lines.append(\"Inventory is empty.\")\n        else:\n            for item, quantity in self.inventory.items():\n                report_lines.append(f\"- {item}: {quantity}\")\n        return \"\\\\n\".join(report_lines)\n\"\"\",\n        imports=[\"from datetime import datetime\"],\n        context=ClassContextInput(\n            dependencies=[\"datetime\"],\n            instantiated_by=[\"main_app_startup\", \"warehouse_worker_script\"],\n            methods_analysis=[\n                add_item_analysis,\n                check_stock_analysis,\n                generate_report_analysis\n            ]\n        )\n    )\n\n    \n    # The helper methods expect a LIST of inputs, so we wrap our single object in a list.\n    input = [add_item_input, check_stock_input, generate_report_input]\n    analysis = [add_item_analysis, check_stock_analysis, generate_report_analysis] \n\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    llm_helper = LLMHelper(api_key=GEMINI_API_KEY, function_prompt_path=function_prompt_file, class_prompt_path=class_prompt_file)\n\n    # This will be the final JSON containing all documentation\n    final_documentation = {}\n\n    # The analysis of the methods is already provided in the context.\n    logging.info(\"\\n--- Generating documentation for classes ---\")\n    \n    # The `generate_for_classes` method returns a list of results.\n    analysis_results = llm_helper.generate_for_functions(input)\n\n    # --- Step 4: Process the results ---\n    \n    # We loop through the list of results (even though there's only one in this case).\n    for doc in analysis_results:\n        if doc:\n            logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n            if \"classes\" not in final_documentation:\n                final_documentation[\"classes\"] = {}\n            # Use .model_dump() to convert the Pydantic object back to a dict for JSON serialization\n            final_documentation[\"classes\"][doc.identifier] = doc.model_dump() \n        else:\n            logging.warning(f\"Failed to generate doc for a class\")\n\n    # --- Step 5: Display the final aggregated result ---\n    logging.info(\"\\n--- Final Generated Documentation ---\")\n    print(json.dumps(final_documentation, indent=2))"
            start_line: 227
            end_line: 413
            context:
              calls[0]:
              called_by[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.HelperLLM.LLMHelper
            name: LLMHelper
            docstring: "A class to interact with Google Gemini for generating code snippet documentation.\nIt centralizes API interaction, error handling, and validates I/O using Pydantic."
            source_code: "class LLMHelper:\n    \"\"\"\n    A class to interact with Google Gemini for generating code snippet documentation.\n    It centralizes API interaction, error handling, and validates I/O using Pydantic.\n    \"\"\"\n    def __init__(self, api_key: str, function_prompt_path: str, class_prompt_path: str, model_name: str = \"gemini-2.0-flash-lite\", base_url: str = None):\n        if not api_key:\n            raise ValueError(\"Gemini API Key must be set.\")\n        \n        try:\n            with open(function_prompt_path, 'r', encoding='utf-8') as f:\n                self.function_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Function system prompt file not found at: {function_prompt_path}\")\n            raise\n\n        # Handle the second file\n        try:\n            with open(class_prompt_path, 'r', encoding='utf-8') as f:\n                self.class_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Class system prompt file not found at: {class_prompt_path}\")\n            raise\n        \n        # Batch-Size config\n        self.model_name = model_name\n        self._configure_batch_settings(model_name)\n\n\n        if model_name.startswith(\"gemini-\"):\n            base_llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=0.3, \n            )\n        \n        elif model_name.startswith(\"gpt-\") and \"openGPT\" not in model_name:\n            base_llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=0.3,\n            )\n\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            if not SCADSLLM_URL:\n                raise ValueError(f\"SCADSLLM_URL environment variable is required for model {model_name}\")\n            \n            logging.info(f\"Connecting to Custom API at {SCADSLLM_URL} for model {model_name}\")\n            \n            base_llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                base_url=SCADSLLM_URL,\n                temperature=0.3,\n            )\n\n        else:\n            target_url = base_url if base_url else OLLAMA_BASE_URL\n            logging.info(f\"Using Ollama at {target_url} for model {model_name}\")\n            base_llm = ChatOllama(\n                model=model_name,\n                temperature=0.3,\n                base_url=target_url,\n            )\n\n        self.function_llm = base_llm.with_structured_output(FunctionAnalysis, method=\"json_schema\")\n        self.class_llm = base_llm.with_structured_output(ClassAnalysis, method=\"json_schema\")\n\n        self.raw_llm = base_llm\n\n        logging.info(f\"LLMHelper initialized with model '{model_name}'. Batch Size: {self.batch_size}\")\n\n    def _configure_batch_settings(self, model_name: str):\n\n        if model_name == \"gemini-2.0-flash-lite\":\n            self.batch_size = 30\n        \n        elif model_name == \"gemini-flash-latest\":\n            self.batch_size = 10\n\n        elif model_name == \"gemini-2.5-flash\":\n            self.batch_size = 10\n            \n        elif model_name == \"gemini-2.5-pro\":\n            self.batch_size = 2\n        \n        elif model_name == \"gemini-2.0-flash\":\n            self.batch_size = 15\n\n        elif model_name == \"gemini-2.5-flash-lite\":\n            self.batch_size = 15\n\n        elif model_name == \"llama3\":\n            self.batch_size = 50\n\n        elif model_name == \"gpt-5.1\":\n            self.batch_size = 500\n\n        elif model_name == \"gpt-5-mini\":\n            self.batch_size = 500\n\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            self.batch_size = 500\n            \n        else:\n            logging.warning(f\"Unknown model '{model_name}', using conservative defaults.\")\n            self.batch_size = 2\n\n    def generate_for_functions(self, function_inputs: List[FunctionAnalysisInput]) -> List[Optional[FunctionAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of functions.\"\"\"\n\n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 62\n\n        if not function_inputs:\n            return []\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(function_input.model_dump(), indent=2) \n            for function_input in function_inputs\n        ]\n       \n        conversations = [[SystemMessage(content=self.function_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_functions = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n        \n            try:\n                batch_results = self.function_llm.batch(batch_conversations, config={\"max_concurrency\": self.batch_size})\n                all_validated_functions.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                all_validated_functions.extend([None] * len(batch_conversations))\n            \n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_functions\n        \n            \n    \n\n    def generate_for_classes(self, class_inputs: List[ClassAnalysisInput]) -> List[Optional[ClassAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of classes.\"\"\"\n        if not class_inputs:\n            return []\n        \n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 62\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(class_input.model_dump(), indent=2)\n            for class_input in class_inputs\n        ]\n\n        conversations = [[SystemMessage(content=self.class_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_classes = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n\n            try:\n                batch_results = self.class_llm.batch(batch_conversations , config={\"max_concurrency\": self.batch_size})\n                all_validated_classes.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                # Falls ein Fehler auftritt, f√ºllen wir die Liste mit None auf, um die Reihenfolge zu wahren\n                all_validated_classes.extend([None] * len(batch_conversations))\n\n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_classes"
            start_line: 31
            end_line: 221
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[4]:
                - identifier: backend.HelperLLM.LLMHelper.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[6]: self,api_key,function_prompt_path,class_prompt_path,model_name,base_url
                  docstring: null
                  start_line: 36
                  end_line: 101
                - identifier: backend.HelperLLM.LLMHelper._configure_batch_settings
                  name: _configure_batch_settings
                  calls[0]:
                  called_by[0]:
                  args[2]: self,model_name
                  docstring: null
                  start_line: 103
                  end_line: 137
                - identifier: backend.HelperLLM.LLMHelper.generate_for_functions
                  name: generate_for_functions
                  calls[0]:
                  called_by[0]:
                  args[2]: self,function_inputs
                  docstring: Generates and validates documentation for a batch of functions.
                  start_line: 139
                  end_line: 178
                - identifier: backend.HelperLLM.LLMHelper.generate_for_classes
                  name: generate_for_classes
                  calls[0]:
                  called_by[0]:
                  args[2]: self,class_inputs
                  docstring: Generates and validates documentation for a batch of classes.
                  start_line: 183
                  end_line: 221
    "backend/MainLLM.py":
      ast_nodes:
        imports[9]: os,logging,sys,dotenv.load_dotenv,langchain_google_genai.ChatGoogleGenerativeAI,langchain_ollama.ChatOllama,langchain_openai.ChatOpenAI,langchain.messages.HumanMessage,langchain.messages.SystemMessage
        functions[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.MainLLM.MainLLM
            name: MainLLM
            docstring: Hauptklasse f√ºr die Interaktion mit dem LLM.
            source_code: "class MainLLM:\n    \"\"\"\n    Hauptklasse f√ºr die Interaktion mit dem LLM.\n    \"\"\"\n    def __init__(self, api_key: str, prompt_file_path: str, model_name: str = \"gemini-2.5-pro\", base_url: str = None):\n        if not api_key:\n            raise ValueError(\"Gemini API Key must be set.\")\n        \n        try:\n            with open(prompt_file_path, 'r', encoding='utf-8') as f:\n                self.system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"System prompt file not found at: {prompt_file_path}\")\n            raise\n        \n        self.model_name = model_name\n\n        if model_name.startswith(\"gemini-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n\n        elif model_name.startswith(\"gpt-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            if not SCADSLLM_URL:\n                raise ValueError(f\"SCADSLLM_URL environment variable is required for model {model_name}\")\n            \n            logging.info(f\"Connecting to Custom API at {SCADSLLM_URL} for model {model_name}\")\n            \n            self.llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                base_url=SCADSLLM_URL,\n                temperature=1.0,\n            )\n\n        else:\n            target_url = base_url if base_url else OLLAMA_BASE_URL\n            self.llm = ChatOllama(\n                model=model_name,\n                temperature=1.0,\n                base_url=target_url,\n            )\n\n        logging.info(f\"Main LLM initialized with model '{model_name}'.\")\n    \n    def call_llm(self, user_input: str):\n\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with HelperLLM input...\")\n\n        try:\n            response = self.llm.invoke(messages)\n            logging.info(\"LLM call successful.\")\n            return response.content\n        except Exception as e:\n            logging.error(f\"Error during LLM call: {e}\")\n            return None\n\n    def stream_llm(self, user_input: str):\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with 'stream'...\")\n\n        try:\n            stream_iterator = self.llm.stream(messages)\n            \n            for chunk in stream_iterator:\n                yield chunk.content\n        except Exception as e:\n            error_message = f\"\\n--- Error during LLM stream call: {e} ---\"\n            logging.error(error_message)\n            yield error_message"
            start_line: 22
            end_line: 106
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[3]:
                - identifier: backend.MainLLM.MainLLM.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[5]: self,api_key,prompt_file_path,model_name,base_url
                  docstring: null
                  start_line: 26
                  end_line: 73
                - identifier: backend.MainLLM.MainLLM.call_llm
                  name: call_llm
                  calls[0]:
                  called_by[0]:
                  args[2]: self,user_input
                  docstring: null
                  start_line: 75
                  end_line: 89
                - identifier: backend.MainLLM.MainLLM.stream_llm
                  name: stream_llm
                  calls[0]:
                  called_by[0]:
                  args[2]: self,user_input
                  docstring: null
                  start_line: 91
                  end_line: 106
    "backend/basic_info.py":
      ast_nodes:
        imports[7]: re,os,tomllib,typing.List,typing.Dict,typing.Any,typing.Optional
        functions[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.basic_info.ProjektInfoExtractor
            name: ProjektInfoExtractor
            docstring: "Extrahiert grundlegende Projektinformationen aus g√§ngigen Projektdateien\nwie README, pyproject.toml und requirements.txt."
            source_code: "class ProjektInfoExtractor:\n    \"\"\"\n    Extrahiert grundlegende Projektinformationen aus g√§ngigen Projektdateien\n    wie README, pyproject.toml und requirements.txt.\n    \"\"\"\n    def __init__(self):\n        self.INFO_NICHT_GEFUNDEN = \"Information not found\"\n        # Initialisiert die Struktur mit Platzhaltern\n        self.info = {\n            \"projekt_uebersicht\": {\n                \"titel\": self.INFO_NICHT_GEFUNDEN,\n                \"beschreibung\": self.INFO_NICHT_GEFUNDEN,\n                \"aktueller_status\": self.INFO_NICHT_GEFUNDEN,\n                \"key_features\": self.INFO_NICHT_GEFUNDEN,\n                \"tech_stack\": self.INFO_NICHT_GEFUNDEN,\n            },\n            \"installation\": {\n                \"dependencies\": self.INFO_NICHT_GEFUNDEN,\n                \"setup_anleitung\": self.INFO_NICHT_GEFUNDEN,\n                \"quick_start_guide\": self.INFO_NICHT_GEFUNDEN,\n            }\n        }\n\n    def _clean_content(self, content: str) -> str:\n        \"\"\"Entfernt Null-Bytes, die durch Encoding-Fehler (UTF-16 als UTF-8 gelesen) entstehen.\"\"\"\n        if not content:\n            return \"\"\n        return content.replace('\\x00', '')\n\n    def _finde_datei(self, patterns: List[str], dateien: List[Any]) -> Optional[Any]:\n        \"\"\"Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht.\"\"\"\n        for datei in dateien:\n            for pattern in patterns:\n                if datei.path.lower().endswith(pattern.lower()):\n                    return datei\n        return None\n\n    def _extrahiere_sektion_aus_markdown(self, inhalt: str, keywords: List[str]) -> Optional[str]:\n        \"\"\"\n        Extrahiert den Text unter einer Markdown-√úberschrift (##).\n        \"\"\"\n        # Erstellt ein Regex-Pattern, das auf jedes der Schl√ºsselw√∂rter reagiert\n        keyword_pattern = \"|\".join(re.escape(k) for k in keywords)\n        \n        # Sucht nach \"## Schl√ºsselwort\" und erfasst alles bis zur n√§chsten \"##\" oder dem Dateiende\n        pattern = re.compile(\n            rf\"##\\s*({keyword_pattern})\\s*\\n(.*?)(?=\\n##|\\Z)\",\n            re.IGNORECASE | re.DOTALL\n        )\n        match = pattern.search(inhalt)\n        if match:\n            return match.group(2).strip()\n        return None\n\n    def _parse_readme(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer README-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        if self.info[\"projekt_uebersicht\"][\"titel\"] == self.INFO_NICHT_GEFUNDEN:\n            title_match = re.search(r\"^\\s*#\\s*(.*)\", inhalt)\n            if title_match:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = title_match.group(1).strip()\n\n        # Beschreibung (Fallback)\n        if self.info[\"projekt_uebersicht\"][\"beschreibung\"] == self.INFO_NICHT_GEFUNDEN:\n            desc_match = re.search(r\"^\\s*#\\s*.*\\n+([^#\\n].*)\", inhalt, re.DOTALL)\n            if desc_match:\n                 self.info[\"projekt_uebersicht\"][\"beschreibung\"] = desc_match.group(1).strip().split('\\n\\n')[0]\n\n        # Key Features\n        features = self._extrahiere_sektion_aus_markdown(inhalt, [\"Features\", \"Key Features\", \"Merkmale\"])\n        if features:\n            self.info[\"projekt_uebersicht\"][\"key_features\"] = features\n            \n        # Tech Stack\n        tech_stack = self._extrahiere_sektion_aus_markdown(inhalt, [\"Tech Stack\", \"Technology\", \"Technologien\"])\n        if tech_stack:\n            self.info[\"projekt_uebersicht\"][\"tech_stack\"] = tech_stack\n            \n        # Status\n        status = self._extrahiere_sektion_aus_markdown(inhalt, [\"Status\", \"Current Status\"])\n        if status:\n            self.info[\"projekt_uebersicht\"][\"aktueller_status\"] = status\n\n        # Setup-Anleitung\n        setup = self._extrahiere_sektion_aus_markdown(inhalt, [\"Installation\", \"Setup\", \"Getting Started\"])\n        if setup:\n            self.info[\"installation\"][\"setup_anleitung\"] = setup\n            \n        # Quick Start\n        quick_start = self._extrahiere_sektion_aus_markdown(inhalt, [\"Quick Start\", \"Schnellstart\"])\n        if quick_start:\n            self.info[\"installation\"][\"quick_start_guide\"] = quick_start\n\n    def _parse_toml(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer pyproject.toml-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        if not tomllib:\n            print(\"Warnung: 'tomli' ist nicht installiert. pyproject.toml kann nicht analysiert werden.\")\n            return\n            \n        try:\n            data = tomllib.loads(inhalt)\n            project_data = data.get(\"project\", {})\n            \n            if \"name\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = project_data[\"name\"]\n            if \"description\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"beschreibung\"] = project_data[\"description\"]\n            if \"dependencies\" in project_data:\n                self.info[\"installation\"][\"dependencies\"] = project_data[\"dependencies\"]\n        except tomllib.TOMLDecodeError as e:\n            print(f\"Warnung: Fehler beim Parsen der pyproject.toml: {e}\")\n\n    def _parse_requirements(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer requirements.txt-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        # Nur f√ºllen, wenn noch keine Dependencies aus toml gefunden wurden\n        if self.info[\"installation\"][\"dependencies\"] == self.INFO_NICHT_GEFUNDEN:\n            lines = inhalt.splitlines()\n            dependencies = [\n                line.strip() for line in lines \n                if line.strip() and not line.strip().startswith('#')\n            ]\n            if dependencies:\n                self.info[\"installation\"][\"dependencies\"] = dependencies\n\n    def extrahiere_info(self, dateien: List[Any], repo_url: str) -> Dict[str, Any]:\n        \"\"\"\n        Orchestriert die Extraktion von Informationen.\n        \"\"\"\n        # Relevante Dateien finden\n        readme_datei = self._finde_datei([\"readme.md\", \"readme.rst\", \"readme.txt\", \"readme\"], dateien)\n        toml_datei = self._finde_datei([\"pyproject.toml\"], dateien)\n        req_datei = self._finde_datei([\"requirements.txt\"], dateien)\n\n        # Dateien parsen (mit Priorisierung)\n        if toml_datei:\n            self._parse_toml(toml_datei.content)\n\n        if req_datei:\n            self._parse_requirements(req_datei.content)\n            \n        if readme_datei:\n            self._parse_readme(readme_datei.content)\n            \n        # Finale Formatierung der Dependencies\n        deps = self.info[\"installation\"][\"dependencies\"]\n        if isinstance(deps, list):\n            if not deps:\n                self.info[\"installation\"][\"dependencies\"] = self.INFO_NICHT_GEFUNDEN\n            else:\n                self.info[\"installation\"][\"dependencies\"] = \"\\n\".join(f\"- {dep}\" for dep in deps)\n        \n        # Titel aus URL ableiten, falls nichts gefunden wurde (oder als Zusatzinfo)\n        if repo_url:\n            repo_name = os.path.basename(repo_url.removesuffix('.git'))\n            # Wenn noch kein Titel da ist oder er generisch war\n            if self.info[\"projekt_uebersicht\"][\"titel\"] == self.INFO_NICHT_GEFUNDEN:\n                 self.info[\"projekt_uebersicht\"][\"titel\"] = f\"{repo_name} documentation\"\n\n        return self.info"
            start_line: 9
            end_line: 172
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[8]:
                - identifier: backend.basic_info.ProjektInfoExtractor.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 14
                  end_line: 30
                - identifier: backend.basic_info.ProjektInfoExtractor._clean_content
                  name: _clean_content
                  calls[0]:
                  called_by[0]:
                  args[2]: self,content
                  docstring: "Entfernt Null-Bytes, die durch Encoding-Fehler (UTF-16 als UTF-8 gelesen) entstehen."
                  start_line: 32
                  end_line: 36
                - identifier: backend.basic_info.ProjektInfoExtractor._finde_datei
                  name: _finde_datei
                  calls[0]:
                  called_by[0]:
                  args[3]: self,patterns,dateien
                  docstring: "Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht."
                  start_line: 38
                  end_line: 44
                - identifier: backend.basic_info.ProjektInfoExtractor._extrahiere_sektion_aus_markdown
                  name: _extrahiere_sektion_aus_markdown
                  calls[0]:
                  called_by[0]:
                  args[3]: self,inhalt,keywords
                  docstring: Extrahiert den Text unter einer Markdown-√úberschrift (##).
                  start_line: 46
                  end_line: 61
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_readme
                  name: _parse_readme
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer README-Datei.
                  start_line: 63
                  end_line: 101
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_toml
                  name: _parse_toml
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer pyproject.toml-Datei.
                  start_line: 103
                  end_line: 122
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_requirements
                  name: _parse_requirements
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer requirements.txt-Datei.
                  start_line: 124
                  end_line: 136
                - identifier: backend.basic_info.ProjektInfoExtractor.extrahiere_info
                  name: extrahiere_info
                  calls[0]:
                  called_by[0]:
                  args[3]: self,dateien,repo_url
                  docstring: Orchestriert die Extraktion von Informationen.
                  start_line: 138
                  end_line: 172
    "backend/callgraph.py":
      ast_nodes:
        imports[9]: ast,networkx,os,pathlib.Path,typing.Dict,getRepo.GitRepository,getRepo.GitRepository,basic_info.ProjektInfoExtractor,os
        functions[2]:
          - mode: function_analysis
            identifier: backend.callgraph.make_safe_dot
            name: make_safe_dot
            args[2]: graph,out_path
            docstring: null
            source_code: "def make_safe_dot(graph: nx.DiGraph, out_path: str):\n    mapping = {}\n    H = graph.copy()\n    for i, n in enumerate(list(graph.nodes())):\n        safe = f\"n{i}\"           \n        mapping[n] = safe\n    H = nx.relabel_nodes(H, mapping)\n    for orig, safe in mapping.items():\n        H.nodes[safe][\"label\"] = orig\n    nx.drawing.nx_pydot.write_dot(H, out_path)"
            start_line: 173
            end_line: 182
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.callgraph.build_filtered_callgraph
            name: build_filtered_callgraph
            args[1]: repo
            docstring: Baut den globalen Call-Graphen und filtert ihn auf selbst geschriebene Funktionen.
            source_code: "def build_filtered_callgraph(repo: GitRepository) -> nx.DiGraph:\n    \"\"\"\n    Baut den globalen Call-Graphen und filtert ihn auf selbst geschriebene Funktionen.\n    \"\"\"\n    all_file_objects = repo.get_all_files()\n    own_functions = set()\n    file_trees = {}\n\n    for file in all_file_objects:\n        if not file.path.endswith(\".py\"):\n            continue\n        filename = Path(file.path).stem\n        tree = ast.parse(file.content)\n        file_trees[filename] = tree\n        visitor = CallGraph(filename)\n        visitor.visit(tree)\n        own_functions.update(visitor.function_set)\n\n    # Globalen Call-Graph bauen\n    global_graph = nx.DiGraph()\n    for filename, tree in file_trees.items():\n        visitor = CallGraph(filename)\n        visitor.visit(tree)\n        for caller, callees in visitor.edges.items():\n            if caller not in own_functions:\n                continue \n            for callee in callees:\n                if callee in own_functions:\n                    global_graph.add_edge(caller, callee)\n                    global_graph.add_node(caller)\n                    global_graph.add_node(callee)\n    return global_graph"
            start_line: 185
            end_line: 216
            context:
              calls[0]:
              called_by[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.callgraph.CallGraph
            name: CallGraph
            docstring: null
            source_code: "class CallGraph(ast.NodeVisitor):\n    def __init__(self, filename: str):\n        self.filename = filename\n        self.current_function = None\n        self.current_class = None\n        \n        self.local_defs: dict[str, str] = {}\n        self.graph = nx.DiGraph()\n        self.import_mapping: dict[str, str] = {}\n        self.function_set: set[str] = set()\n        self.edges: Dict[str, set[str]] = {}\n\n    def _recursive_call(self, node):\n        \"\"\"\n        Liefert eine Liste der Namenskomponenten als dotted string z.B. ['pkg', 'mod', 'Class', 'method']\n        \"\"\"\n        if isinstance(node, ast.Call):\n            return self._recursive_call(node.func)\n        if isinstance(node, ast.Name):\n            return [node.id]\n        if isinstance(node, ast.Attribute):\n            parts = self._recursive_call(node.value)\n            parts.append(node.attr)\n            return parts\n        return []\n\n    def _resolve_all_callee_names(self, callee_nodes: list[list[str]]) -> list[str]:\n        \"\"\"\n        callee_nodes ist jetzt eine Liste von Listen (Name-Steps).\n        Wir pr√ºfen zuerst lokale Definitionen, dann import_mapping.\n        \"\"\"\n        resolved = []\n        for parts in callee_nodes:\n            if not parts:\n                continue\n            simple = parts[-1]\n            dotted = \".\".join(parts[-2:]) if len(parts) >= 2 else simple\n\n            if simple in self.local_defs:\n                resolved.append(self.local_defs[simple])\n                continue\n            if dotted in self.local_defs:\n                resolved.append(self.local_defs[dotted])\n                continue\n\n            first = parts[0]\n            if first in self.import_mapping:\n                mod = self.import_mapping[first]\n                rest = \"::\".join(parts[1:]) if len(parts) > 1 else simple\n                resolved.append(f\"{mod}::{rest}\")\n                continue\n\n            if self.current_class:\n                resolved.append(f\"{self.filename}::{parts[0]}::{simple}\" if len(parts)==1 else f\"{self.filename}::{'::'.join(parts)}\")\n            else:\n                resolved.append(f\"{self.filename}::{ '::'.join(parts)}\")\n        return resolved\n\n    def _make_full_name(self, basename: str, class_name: str | None = None) -> str:\n        if class_name:\n            return f\"{self.filename}::{class_name}::{basename}\"\n        return f\"{self.filename}::{basename}\"\n    \n    def _current_caller(self) -> str:\n        if self.current_function:\n            return self.current_function\n        return f\"<{self.filename}>\" if self.filename else \"<global-scope>\"\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            module_name = alias.name\n            module_asname = alias.asname if alias.asname else module_name\n            self.import_mapping[module_asname] = module_name\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        module_name = node.module.split(\".\")[-1] if node.module else \"\"\n        for alias in node.names:\n            self.import_mapping[alias.asname or alias.name] = f\"{module_name}\" if module_name else alias.name\n\n    def visit_ClassDef(self, node: ast.ClassDef):\n        prev_class = self.current_class\n        self.current_class = node.name\n        self.generic_visit(node)\n        self.current_class = prev_class\n        \n    def visit_FunctionDef(self, node):\n        prev_function = self.current_function\n        full_name = self._make_full_name(node.name, self.current_class)\n        self.local_defs[node.name] = full_name\n        if self.current_class:\n            self.local_defs[f\"{self.current_class}.{node.name}\"] = full_name\n\n        self.current_function = full_name\n        self.graph.add_node(self.current_function)\n        self.generic_visit(node)\n        self.function_set.add(self.current_function)\n        self.current_function = prev_function\n\n    def visit_AsyncFunctionDef(self, node):\n        self.visit_FunctionDef(node)\n\n    def visit_Call(self, node):\n        caller = self._current_caller()\n        parts = self._recursive_call(node)\n        resolved_callees = self._resolve_all_callee_names([parts])\n\n        if caller not in self.edges:\n            self.edges[caller] = set()\n\n        for callee in resolved_callees:\n            if callee:\n                self.edges[caller].add(callee)\n        self.generic_visit(node)\n\n    def visit_If(self, node):\n        if (isinstance(node.test, ast.Compare) and\n            isinstance(node.test.left, ast.Name) and \n            node.test.left.id == \"__name__\"):\n            caller_backup = self.current_function\n            self.current_function = \"<main_block>\"\n            self.generic_visit(node)\n            self.current_function = caller_backup\n        else:\n            self.generic_visit(node)"
            start_line: 10
            end_line: 134
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[12]:
                - identifier: backend.callgraph.CallGraph.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,filename
                  docstring: null
                  start_line: 11
                  end_line: 20
                - identifier: backend.callgraph.CallGraph._recursive_call
                  name: _recursive_call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: "Liefert eine Liste der Namenskomponenten als dotted string z.B. ['pkg', 'mod', 'Class', 'method']"
                  start_line: 22
                  end_line: 34
                - identifier: backend.callgraph.CallGraph._resolve_all_callee_names
                  name: _resolve_all_callee_names
                  calls[0]:
                  called_by[0]:
                  args[2]: self,callee_nodes
                  docstring: "callee_nodes ist jetzt eine Liste von Listen (Name-Steps).\nWir pr√ºfen zuerst lokale Definitionen, dann import_mapping."
                  start_line: 36
                  end_line: 66
                - identifier: backend.callgraph.CallGraph._make_full_name
                  name: _make_full_name
                  calls[0]:
                  called_by[0]:
                  args[3]: self,basename,class_name
                  docstring: null
                  start_line: 68
                  end_line: 71
                - identifier: backend.callgraph.CallGraph._current_caller
                  name: _current_caller
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 73
                  end_line: 76
                - identifier: backend.callgraph.CallGraph.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 78
                  end_line: 83
                - identifier: backend.callgraph.CallGraph.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 85
                  end_line: 88
                - identifier: backend.callgraph.CallGraph.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 90
                  end_line: 94
                - identifier: backend.callgraph.CallGraph.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 96
                  end_line: 107
                - identifier: backend.callgraph.CallGraph.visit_AsyncFunctionDef
                  name: visit_AsyncFunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 109
                  end_line: 110
                - identifier: backend.callgraph.CallGraph.visit_Call
                  name: visit_Call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 112
                  end_line: 123
                - identifier: backend.callgraph.CallGraph.visit_If
                  name: visit_If
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 125
                  end_line: 134
    "backend/converter.py":
      ast_nodes:
        imports[4]: logging,nbformat,base64,nbformat.reader.NotJSONError
        functions[5]:
          - mode: function_analysis
            identifier: backend.converter.wrap_cdata
            name: wrap_cdata
            args[1]: content
            docstring: Wraps content in CDATA tags.
            source_code: "def wrap_cdata(content):\n    \"\"\"Wraps content in CDATA tags.\"\"\"\n    return f\"<![CDATA[\\n{content}\\n]]>\""
            start_line: 8
            end_line: 10
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.converter.extract_output_content
            name: extract_output_content
            args[2]: outputs,image_list
            docstring: "Extracts text and handles images by decoding Base64 to bytes.\nReturns: A list of text strings or placeholders."
            source_code: "def extract_output_content(outputs, image_list):\n    \"\"\"\n    Extracts text and handles images by decoding Base64 to bytes.\n    Returns: A list of text strings or placeholders.\n    \"\"\"\n    extracted_xml_snippets = []\n    \n    for output in outputs:\n\n        if output.output_type in ('display_data', 'execute_result') and hasattr(output, 'data'):\n            data = output.data\n            \n            # Helper to process image\n            def process_image(mime_type):\n                if mime_type in data:\n                    try:\n                        b64_str = data[mime_type]\n                        b64_str = b64_str.replace('\\n', '')\n\n                        image_index = len(image_list)\n                        image_list.append({\n                            \"mime_type\": mime_type,\n                            \"data\": b64_str\n                        })\n\n                        return f'\\n<IMAGE_PLACEHOLDER index=\"{image_index}\" mime=\"{mime_type}\"/>\\n'\n                    except Exception as e:\n                        return f\"<ERROR>Could not decode image: {e}</ERROR>\"\n                return None\n\n            # Ignore jpeg if png is found\n            img_xml = process_image('image/png')\n            if not img_xml:\n                img_xml = process_image('image/jpeg')\n            \n            if img_xml:\n                extracted_xml_snippets.append(img_xml)\n            elif 'text/plain' in data:\n                extracted_xml_snippets.append(data['text/plain'])\n\n        elif output.output_type == 'stream':\n            extracted_xml_snippets.append(output.text)\n            \n        elif output.output_type == 'error':\n            extracted_xml_snippets.append(f\"{output.ename}: {output.evalue}\")\n\n    return extracted_xml_snippets"
            start_line: 12
            end_line: 58
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.converter.process_image
            name: process_image
            args[1]: mime_type
            docstring: null
            source_code: "def process_image(mime_type):\n                if mime_type in data:\n                    try:\n                        b64_str = data[mime_type]\n                        b64_str = b64_str.replace('\\n', '')\n\n                        image_index = len(image_list)\n                        image_list.append({\n                            \"mime_type\": mime_type,\n                            \"data\": b64_str\n                        })\n\n                        return f'\\n<IMAGE_PLACEHOLDER index=\"{image_index}\" mime=\"{mime_type}\"/>\\n'\n                    except Exception as e:\n                        return f\"<ERROR>Could not decode image: {e}</ERROR>\"\n                return None"
            start_line: 25
            end_line: 40
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.converter.convert_notebook_to_xml
            name: convert_notebook_to_xml
            args[1]: file_content
            docstring: null
            source_code: "def convert_notebook_to_xml(file_content):\n    try:\n        nb = nbformat.reads(file_content, as_version=4)\n    except NotJSONError:\n        return \"<ERROR>Could not parse file as JSON/Notebook</ERROR>\", []\n\n    xml_parts = []\n    extracted_images = [] \n\n    for cell in nb.cells:\n        if cell.cell_type == 'markdown':\n            cell_xml = f'<CELL type=\"markdown\">\\n{cell.source}\\n</CELL>'\n            xml_parts.append(cell_xml)\n\n        elif cell.cell_type == 'code':\n            source_code = wrap_cdata(cell.source)\n            xml_parts.append(f'<CELL type=\"code\">\\n{source_code}\\n</CELL>')\n\n            if hasattr(cell, 'outputs') and cell.outputs:\n                snippets = extract_output_content(cell.outputs, extracted_images)\n                \n                content = \"\\n\".join(snippets)\n                \n                if content.strip():\n                    wrapped_output = wrap_cdata(content)\n                    xml_parts.append(f'<CELL type=\"output\">\\n{wrapped_output}\\n</CELL>')\n\n    return \"\\n\\n\".join(xml_parts), extracted_images"
            start_line: 60
            end_line: 87
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.converter.process_repo_notebooks
            name: process_repo_notebooks
            args[1]: repo_files
            docstring: null
            source_code: "def process_repo_notebooks(repo_files):\n    notebook_files = [f for f in repo_files if f.path.endswith('.ipynb')]\n    logging.info(f\"Found {len(notebook_files)} notebooks.\")\n        \n    results = {}\n\n    for nb_file in notebook_files:\n        logging.info(f\"Processing: {nb_file.path}\")\n\n        xml_output, images = convert_notebook_to_xml(nb_file.content)\n        results[nb_file.path] = {\n            \"xml\": xml_output,\n            \"images\": images\n        }\n            \n    return results"
            start_line: 90
            end_line: 105
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "backend/getRepo.py":
      ast_nodes:
        imports[5]: tempfile,git.Repo,git.GitCommandError,logging,os
        functions[0]:
        classes[2]:
          - mode: class_analysis
            identifier: backend.getRepo.RepoFile
            name: RepoFile
            docstring: "Repr√§sentiert eine einzelne Datei in einem Git-Repository.\n\nDer Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tats√§chlichem Zugriff."
            source_code: "class RepoFile:\n    \"\"\"\n    Repr√§sentiert eine einzelne Datei in einem Git-Repository.\n    \n    Der Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tats√§chlichem Zugriff.\n    \"\"\"\n    def __init__(self, file_path, commit_tree):\n        \"\"\"\n        Initialisiert das RepoFile-Objekt.\n\n        Args:\n            file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n            commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt.\n        \"\"\"\n        self.path = file_path\n        self._tree = commit_tree\n        \n\n        self._blob = None\n        self._content = None\n        self._size = None\n\n    @property\n    def blob(self):\n        \"\"\"Lazy-l√§dt das Git-Blob-Objekt.\"\"\"\n        if self._blob is None:\n            try:\n                self._blob = self._tree[self.path]\n            except KeyError:\n                raise FileNotFoundError(f\"Datei '{self.path}' konnte im Commit-Tree nicht gefunden werden.\")\n        return self._blob\n\n    @property\n    def content(self):\n        \"\"\"Lazy-l√§dt und gibt den dekodierten Inhalt der Datei zur√ºck.\"\"\"\n        if self._content is None:\n            self._content = self.blob.data_stream.read().decode('utf-8', errors='ignore')\n        return self._content\n\n    @property\n    def size(self):\n        \"\"\"Lazy-l√§dt und gibt die Gr√∂√üe der Datei in Bytes zur√ºck.\"\"\"\n        if self._size is None:\n            self._size = self.blob.size\n        return self._size\n        \n    def analyze_word_count(self):\n        \"\"\"\n        Eine Beispiel-Analyse-Methode. Z√§hlt die W√∂rter im Dateiinhalt.\n        \"\"\"\n        return len(self.content.split())\n\n    def __repr__(self):\n        \"\"\"Gibt eine n√ºtzliche String-Repr√§sentation des Objekts zur√ºck.\"\"\"\n        return f\"<RepoFile(path='{self.path}')>\"\n    \n    def to_dict(self, include_content=False):\n        data = {\n            \"path\": self.path,\n            \"name\": os.path.basename(self.path),\n            \"size\": self.size,\n            \"type\": \"file\"\n        }\n        if include_content:\n            data[\"content\"] = self.content\n        return data"
            start_line: 7
            end_line: 72
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[7]:
                - identifier: backend.getRepo.RepoFile.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[3]: self,file_path,commit_tree
                  docstring: "Initialisiert das RepoFile-Objekt.\n\nArgs:\n    file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n    commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt."
                  start_line: 13
                  end_line: 27
                - identifier: backend.getRepo.RepoFile.blob
                  name: blob
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt das Git-Blob-Objekt.
                  start_line: 30
                  end_line: 37
                - identifier: backend.getRepo.RepoFile.content
                  name: content
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt und gibt den dekodierten Inhalt der Datei zur√ºck.
                  start_line: 40
                  end_line: 44
                - identifier: backend.getRepo.RepoFile.size
                  name: size
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt und gibt die Gr√∂√üe der Datei in Bytes zur√ºck.
                  start_line: 47
                  end_line: 51
                - identifier: backend.getRepo.RepoFile.analyze_word_count
                  name: analyze_word_count
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Eine Beispiel-Analyse-Methode. Z√§hlt die W√∂rter im Dateiinhalt.
                  start_line: 53
                  end_line: 57
                - identifier: backend.getRepo.RepoFile.__repr__
                  name: __repr__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Gibt eine n√ºtzliche String-Repr√§sentation des Objekts zur√ºck.
                  start_line: 59
                  end_line: 61
                - identifier: backend.getRepo.RepoFile.to_dict
                  name: to_dict
                  calls[0]:
                  called_by[0]:
                  args[2]: self,include_content
                  docstring: null
                  start_line: 63
                  end_line: 72
          - mode: class_analysis
            identifier: backend.getRepo.GitRepository
            name: GitRepository
            docstring: "Verwaltet ein Git-Repository, einschlie√ülich Klonen in ein tempor√§res\nVerzeichnis und Bereitstellung von RepoFile-Objekten."
            source_code: "class GitRepository:\n    \"\"\"\n    Verwaltet ein Git-Repository, einschlie√ülich Klonen in ein tempor√§res\n    Verzeichnis und Bereitstellung von RepoFile-Objekten.\n    \"\"\"\n    def __init__(self, repo_url):\n        self.repo_url = repo_url\n        self.temp_dir = tempfile.mkdtemp()\n        self.repo = None\n\n        self.files = []\n        \n        try:\n            logging.info(f\"Clone Repository {self.repo_url}...\")\n            self.repo = Repo.clone_from(self.repo_url, self.temp_dir)\n            self.latest_commit = self.repo.head.commit\n            self.commit_tree = self.latest_commit.tree\n            logging.info(\"Cloning successful.\")\n        except GitCommandError as e:\n            self.close()\n            raise RuntimeError(f\"Error cloning repository: {e}\") from e\n\n    def get_all_files(self):\n        \"\"\"\n        Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zur√ºck.\n\n        Returns:\n            list[RepoFile]: Eine Liste von RepoFile-Instanzen.\n        \"\"\"\n        file_paths = self.repo.git.ls_files().split('\\n')\n        self.files = [RepoFile(path, self.commit_tree) for path in file_paths if path]\n        return self.files\n\n    def close(self):\n        \"\"\"L√∂scht das tempor√§re Verzeichnis und dessen Inhalt.\"\"\"\n        if self.temp_dir:\n            print(f\"\\nL√∂sche tempor√§res Verzeichnis: {self.temp_dir}\")\n            self.temp_dir = None\n            \n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def get_file_tree(self, include_content=False):\n\n        if not self.files:\n            self.get_all_files()\n\n        tree = {\"name\": \"root\", \"type\": \"directory\", \"children\": []}\n\n        for file_obj in self.files:\n            parts = file_obj.path.split('/')\n            current_level = tree[\"children\"]\n            \n            # Iteriere durch die Ordnerstruktur\n            for part in parts[:-1]:\n                # Suche, ob Ordner schon existiert\n                found = next((item for item in current_level if item[\"name\"] == part and item[\"type\"] == \"directory\"), None)\n                if not found:\n                    new_dir = {\"name\": part, \"type\": \"directory\", \"children\": []}\n                    current_level.append(new_dir)\n                    current_level = new_dir[\"children\"]\n                else:\n                    current_level = found[\"children\"]\n            \n            # Datei am Ende hinzuf√ºgen\n            current_level.append(file_obj.to_dict(include_content=include_content))\n\n        return tree"
            start_line: 78
            end_line: 148
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[6]:
                - identifier: backend.getRepo.GitRepository.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,repo_url
                  docstring: null
                  start_line: 83
                  end_line: 98
                - identifier: backend.getRepo.GitRepository.get_all_files
                  name: get_all_files
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: "Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zur√ºck.\n\nReturns:\n    list[RepoFile]: Eine Liste von RepoFile-Instanzen."
                  start_line: 100
                  end_line: 109
                - identifier: backend.getRepo.GitRepository.close
                  name: close
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: L√∂scht das tempor√§re Verzeichnis und dessen Inhalt.
                  start_line: 111
                  end_line: 115
                - identifier: backend.getRepo.GitRepository.__enter__
                  name: __enter__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 117
                  end_line: 118
                - identifier: backend.getRepo.GitRepository.__exit__
                  name: __exit__
                  calls[0]:
                  called_by[0]:
                  args[4]: self,exc_type,exc_val,exc_tb
                  docstring: null
                  start_line: 120
                  end_line: 121
                - identifier: backend.getRepo.GitRepository.get_file_tree
                  name: get_file_tree
                  calls[0]:
                  called_by[0]:
                  args[2]: self,include_content
                  docstring: null
                  start_line: 123
                  end_line: 148
    "backend/main.py":
      ast_nodes:
        imports[28]: json,math,logging,os,re,time,math,datetime.datetime,matplotlib.pyplot,datetime.datetime,pathlib.Path,dotenv.load_dotenv,getRepo.GitRepository,AST_Schema.ASTAnalyzer,MainLLM.MainLLM,basic_info.ProjektInfoExtractor,HelperLLM.LLMHelper,relationship_analyzer.ProjectAnalyzer,schemas.types.FunctionContextInput,schemas.types.FunctionAnalysisInput,schemas.types.ClassContextInput,schemas.types.ClassAnalysisInput,schemas.types.MethodContextInput,toon_format.encode,toon_format.count_tokens,toon_format.estimate_savings,toon_format.compare_formats,converter.process_repo_notebooks
        functions[7]:
          - mode: function_analysis
            identifier: backend.main.create_savings_chart
            name: create_savings_chart
            args[4]: json_tokens,toon_tokens,savings_percent,output_path
            docstring: Erstellt ein Balkendiagramm f√ºr den Token-Vergleich und speichert es.
            source_code: "def create_savings_chart(json_tokens, toon_tokens, savings_percent, output_path):\n    \"\"\"Erstellt ein Balkendiagramm f√ºr den Token-Vergleich und speichert es.\"\"\"\n    labels = ['JSON', 'TOON']\n    values = [json_tokens, toon_tokens]\n    colors = ['#ff9999', '#66b3ff']\n\n    plt.figure(figsize=(8, 6))\n    bars = plt.bar(labels, values, color=colors, width=0.5)\n\n    # Titel und Beschriftungen\n    plt.title(f'Token-Vergleich: {savings_percent:.2f}% Einsparung', fontsize=14)\n    plt.ylabel('Anzahl Token')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n\n    # Werte √ºber den Balken anzeigen\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width() / 2.0, height, \n                 f'{int(height):,}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n\n    # Speichern\n    plt.savefig(output_path)\n    plt.close()"
            start_line: 33
            end_line: 55
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.calculate_net_time
            name: calculate_net_time
            args[5]: start_time,end_time,total_items,batch_size,model_name
            docstring: Berechnet die Dauer abz√ºglich der Sleep-Zeiten f√ºr Rate-Limits.
            source_code: "def calculate_net_time(start_time, end_time, total_items, batch_size, model_name):\n    \"\"\"Berechnet die Dauer abz√ºglich der Sleep-Zeiten f√ºr Rate-Limits.\"\"\"\n    total_duration = end_time - start_time\n    \n    if not model_name.startswith(\"gemini-\"):\n        return total_duration\n\n    if total_items == 0:\n        return 0\n\n    num_batches = math.ceil(total_items / batch_size)\n    sleep_count = max(0, num_batches - 1)\n    total_sleep_time = sleep_count * 61\n    \n    net_time = total_duration - total_sleep_time\n    return max(0, net_time)"
            start_line: 57
            end_line: 72
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.main_workflow
            name: main_workflow
            args[4]: input,api_keys,model_names,status_callback
            docstring: null
            source_code: "def main_workflow(input, api_keys: dict, model_names: dict, status_callback=None):\n    \n    def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)\n\n    update_status(\"üîç Analysiere Input...\")\n    \n    user_input = input\n    \n    # API Key & Ollama Base URL aus Frontend holen\n    gemini_api_key = api_keys.get(\"gemini\")\n    openai_api_key = api_keys.get(\"gpt\")\n    scadsllm_api_key = api_keys.get(\"scadsllm\")\n    scadsllm_base_url = api_keys.get(\"scadsllm_base_url\")\n    ollama_base_url = api_keys.get(\"ollama\")\n    base_url = None\n\n    if model_names[\"helper\"].startswith(\"gpt-\"):\n        helper_api_key = openai_api_key\n    elif model_names[\"helper\"].startswith(\"gemini-\"):\n        helper_api_key = gemini_api_key\n    elif \"/\" in model_names[\"helper\"] or model_names[\"helper\"].startswith(\"alias-\") or any(x in model_names[\"helper\"] for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        helper_api_key = scadsllm_api_key\n        base_url = scadsllm_base_url\n    else:\n        helper_api_key = None\n        base_url = ollama_base_url\n    if model_names[\"main\"].startswith(\"gpt-\"):\n        main_api_key = openai_api_key\n    elif model_names[\"main\"].startswith(\"gemini-\"):\n        main_api_key = gemini_api_key\n    elif \"/\" in model_names[\"main\"] or model_names[\"main\"].startswith(\"alias-\") or any(x in model_names[\"main\"] for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        main_api_key = scadsllm_api_key\n        base_url = scadsllm_base_url\n    else:\n        main_api_key = None\n        base_url = ollama_base_url\n\n    # Standardeinstellungen f√ºr Modelle\n    helper_model = model_names.get(\"helper\", \"gpt-5-mini\")\n    main_model = model_names.get(\"main\", \"gpt-5.1\")\n\n    # Error Handling f√ºr fehlende API Keys\n    if not gemini_api_key and \"gemini\" in helper_model:\n        raise ValueError(\"Gemini API Key was not provided in api_keys dictionary.\")\n\n    # URL Extraktion\n    repo_url = None\n    url_pattern = r\"https?://(?:www\\.)?github\\.com/[^\\s]+\"\n    match = re.search(url_pattern, user_input)\n\n    if match:\n        repo_url = match.group(0)\n        logging.info(f\"Extracted repository URL: {repo_url}\")\n    else:\n        raise ValueError(\"Could not find a valid URL in the provided input.\")\n    \n    # Repo klonen und Dateien extrahieren\n    update_status(f\"‚¨áÔ∏è Klone Repository: {repo_url} ...\")\n    \n    repo_files = []\n    local_repo_path = \"\" \n\n    try: \n\n        with GitRepository(repo_url) as repo:\n            repo_files = repo.get_all_files()\n            local_repo_path = repo.temp_dir\n\n            logging.info(f\"Total files retrieved: {len(repo_files)}\")\n\n    except Exception as e:\n        logging.error(f\"Error cloning repository: {e}\")\n        raise \n\n\n    # Extrahiere Basic Infos\n    update_status(\"‚ÑπÔ∏è Extrahiere Basis-Informationen...\")\n    try:\n        info_extractor = ProjektInfoExtractor()\n        basic_project_info = info_extractor.extrahiere_info(dateien=repo_files, repo_url=repo_url)\n        logging.info(\"Basic project info extracted\")\n    except Exception as e:\n        logging.error(f\"Error extracting basic project info: {e}\")\n        basic_project_info = \"Could not extract basic info.\"\n\n    # Erstelle Repository Dateibaum\n    update_status(\"üå≤ Erstelle Repository Dateibaum...\")\n    try:\n        repo_file_tree = repo.get_file_tree()\n        logging.info(\"Repository file tree constructed\")\n    except Exception as e:\n        logging.error(f\"Error constructing repository file tree: {e}\")\n        repo_file_tree = \"Could not create file tree.\"\n\n    # Relationship Analyse durchf√ºhren\n    update_status(\"üîó Analysiere Beziehungen (Calls & Instanziierungen)...\")\n    try:\n        rel_analyzer = ProjectAnalyzer(project_root=local_repo_path)\n        rel_analyzer.analyze()\n        \n        raw_relationships = rel_analyzer.get_raw_relationships()\n        \n        logging.info(f\"Relationships analyzed.\")\n    except Exception as e:\n        logging.error(f\"Error in relationship analyzer: {e}\")\n        raw_relationships = {\"outgoing\": {}, \"incoming\": {}}\n\n    # Erstelle AST Schema\n    update_status(\"üå≥ Erstelle Abstract Syntax Tree (AST)...\")\n    try:        \n        ast_analyzer = ASTAnalyzer()   \n        ast_schema = ast_analyzer.analyze_repository(files=repo_files, repo=repo)\n        logging.info(\"AST schema created\")\n    except Exception as e:\n        logging.error(f\"Error retrieving repository files: {e}\")\n        raise\n\n    # Anreichern des AST Schemas mit Relationship Daten\n    update_status(\"‚ûï Reiche AST mit Beziehungsdaten an...\")            \n    try:   \n        ast_schema = ast_analyzer.merge_relationship_data(ast_schema, raw_relationships)\n        logging.info(\"AST schema created and enriched\")\n\n    except Exception as e:\n        logging.error(f\"Error processing repository: {e}\")\n        raise\n\n    # Vorbereitung der HelperLLM Eingaben\n    update_status(\"‚öôÔ∏è Bereite Daten f√ºr Helper LLM vor...\")\n    \n    helper_llm_function_input = []\n    helper_llm_class_input = []\n\n    try:\n        for filename, file_data in ast_schema['files'].items():\n            ast_nodes = file_data.get('ast_nodes', {})\n            imports = ast_nodes.get('imports', [])\n            functions = ast_nodes.get('functions', [])\n            classes = ast_nodes.get('classes', [])\n\n            for function in functions:\n                context = function.get('context', {})\n                \n                raw_called_by = context.get('called_by', [])\n                clean_called_by = [cb for cb in raw_called_by if isinstance(cb, dict)]\n\n                function_context = FunctionContextInput(\n                    calls = context.get('calls', []),\n                    called_by = clean_called_by \n                )\n                \n                function_input = FunctionAnalysisInput(\n                    mode = function.get('mode', 'function_analysis'),\n                    identifier = function.get('identifier'),\n                    source_code = function.get('source_code'),\n                    imports = imports,\n                    context = function_context\n                )\n                \n                helper_llm_function_input.append(function_input)\n\n            for _class in classes:\n                context = _class.get('context', {})\n                \n                method_context_inputs = []\n                for method in context.get('method_context', []):\n                    \n                    raw_method_called_by = method.get('called_by', [])\n                    clean_method_called_by = [cb for cb in raw_method_called_by if isinstance(cb, dict)]\n\n                    method_context_inputs.append(\n                        MethodContextInput(\n                            identifier=method.get('identifier'),\n                            calls=method.get('calls', []),\n                            called_by=clean_method_called_by, \n                            args=method.get('args', []),\n                            docstring=method.get('docstring')\n                        )\n                    )\n\n                raw_instantiated_by = context.get('instantiated_by', [])\n                clean_instantiated_by = [ib for ib in raw_instantiated_by if isinstance(ib, dict)]\n\n                class_context = ClassContextInput(\n                    dependencies = context.get('dependencies', []),\n                    instantiated_by = clean_instantiated_by, \n                    method_context = method_context_inputs\n                )\n\n                class_input = ClassAnalysisInput(\n                    mode = _class.get('mode', 'class_analysis'),\n                    identifier =_class.get('identifier'),\n                    source_code = _class.get('source_code'), \n                    imports = imports, \n                    context = class_context\n                )\n                \n                helper_llm_class_input.append(class_input)\n\n    except Exception as e:\n        logging.error(f\"Error preparing inputs for Helper LLM: {e}\")\n        raise\n    \n    # Initialisiere HelperLLM\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    \n    llm_helper = LLMHelper(\n        api_key=helper_api_key, \n        function_prompt_path=function_prompt_file, \n        class_prompt_path=class_prompt_file,\n        model_name=helper_model,\n        base_url=base_url,\n    )\n    \n    if ollama_base_url:\n        os.environ[\"OLLAMA_BASE_URL\"] = ollama_base_url\n\n    # Initialisiere Ergebniscontainer\n    analysis_results = {}\n    function_analysis_results = []\n    class_analysis_results = []\n\n    # Call HelperLLM f√ºr Funktionen\n    update_status(f\"ü§ñ Helper LLM: Analysiere {len(helper_llm_function_input)} Funktionen ({helper_model})...\")\n\n    try:\n        net_time_func = 0\n        if len(helper_llm_function_input) > 0:\n\n            logging.info(\"\\n--- Generating documentation for Functions ---\")\n            t_start_func = time.time()    \n            function_analysis_results = llm_helper.generate_for_functions(helper_llm_function_input)    \n            t_end_func = time.time()\n            net_time_func = calculate_net_time(t_start_func, t_end_func, len(helper_llm_function_input), llm_helper.batch_size, helper_model)\n\n        if len(function_analysis_results) != 0:\n            for doc in function_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"functions\" not in analysis_results:\n                        analysis_results[\"functions\"] = {}\n                    analysis_results[\"functions\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(f\"Failed to generate doc for a function\") \n    except Exception as e:\n        logging.error(f\"Error during Helper LLM function analysis: {e}\")\n        raise\n    \n\n    # Call HelperLLM f√ºr Klassen\n    try:\n        net_time_class = 0\n        if len(helper_llm_class_input) > 0:\n            # Rate Limit Sleep f√ºr Gemini Modelle\n            if llm_helper.model_name.startswith(\"gemini-\") & (len(helper_llm_function_input) > 0):\n                update_status(\"üí§ Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n                time.sleep(65)\n            \n            update_status(f\"ü§ñ Helper LLM: Analysiere {len(helper_llm_class_input)} Klassen ({helper_model})...\")\n            \n            logging.info(\"\\n--- Generating documentation for Classes ---\")\n            t_start_class = time.time()\n            class_analysis_results = llm_helper.generate_for_classes(helper_llm_class_input)\n            t_end_class = time.time()\n            net_time_class = calculate_net_time(t_start_class, t_end_class, len(helper_llm_class_input), llm_helper.batch_size, helper_model)\n\n        if len(class_analysis_results) != 0:\n            for doc in class_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"classes\" not in analysis_results:\n                        analysis_results[\"classes\"] = {}\n                    analysis_results[\"classes\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(f\"Failed to generate doc for a class\")\n    except Exception as e:\n        logging.error(f\"Error during Helper LLM class analysis: {e}\")\n        raise\n\n    total_helper_time = net_time_func + net_time_class\n\n    # MainLLM Input Vorbereitung\n    main_llm_input = {\n        \"basic_info\": basic_project_info,\n        \"file_tree\": repo_file_tree,\n        \"ast_schema\": ast_schema,\n        \"analysis_results\": analysis_results\n    }\n\n    # Speichern als JSON (Optional)\n    main_llm_input_json = json.dumps(main_llm_input, indent=2)\n    # with open(\"output.json\", \"w\", encoding=\"utf-8\") as f:\n    #     f.write(main_llm_input_json)\n    #     logging.info(\"JSON-Datei wurde gespeichert.\")\n\n    # Konvertiere Input in Toon Format\n    main_llm_input_toon = encode(main_llm_input)\n\n    #Speichern in TOON Format (Optional)\n    # with open(\"output.toon\", \"w\", encoding=\"utf-8\") as f:\n    #     f.write(main_llm_input_toon)\n    #     logging.info(\"output.toon erfolgreich gespeichert.\")\n    \n    # Token Evaluation\n    savings_data = None\n    try:\n        logging.info(\"--- Evaluierung der Token-Ersparnis ---\")\n        savings_data = estimate_savings(main_llm_input)\n        logging.info(f\"JSON Tokens: {savings_data['json_tokens']}\")\n        logging.info(f\"TOON Tokens: {savings_data['toon_tokens']}\")\n        logging.info(f\"Ersparnis:   {savings_data['savings_percent']:.2f}%\")\n        \n    except Exception as e:\n        logging.warning(f\"Token evaluation could not be performed: {e}\")    \n\n\n\n    prompt_file_mainllm = \"SystemPrompts/SystemPromptMainLLM.txt\"\n    prompt_file_mainllm_toon = \"SystemPrompts/SystemPromptMainLLMToon.txt\"\n    # MainLLM Ausf√ºhrung\n    main_llm = MainLLM(\n        api_key=main_api_key, \n        prompt_file_path=prompt_file_mainllm_toon,\n        model_name=main_model,\n        base_url=base_url,\n    )\n\n\n    # RPM Limit Sleep f√ºr Gemini Modelle\n    if llm_helper.model_name == main_llm.model_name and main_llm.model_name.startswith(\"gemini-\"):\n        time.sleep(65)\n        update_status(\"üí§ Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n\n    # Call MainLLM f√ºr finalen Report\n    update_status(f\"üß† Main LLM: Generiere finalen Report ({main_model})...\")\n    try:\n        total_main_time = 0\n        logging.info(\"\\n--- Generating Final Report ---\")\n        t_start_main = time.time()\n        final_report = main_llm.call_llm(main_llm_input_toon)\n        t_end_main = time.time()\n        total_main_time = t_end_main - t_start_main\n    except Exception as e:\n        logging.error(f\"Error during Main LLM final report generation: {e}\")\n        raise\n\n    \n    # --- Speichern der Ergebnisse ---\n    output_dir = \"result\"\n    stats_dir = \"Statistics\" # Neuer Ordner\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(stats_dir, exist_ok=True) # Stelle sicher, dass Statistics Ordner existiert\n\n    total_active_time = total_helper_time + total_main_time\n    \n    # Dateiname f√ºr Report\n    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M-%S\")\n    report_filename = f\"report_{timestamp}_Helper_{llm_helper.model_name}_MainLLM_{main_llm.model_name}.md\"\n    report_filepath = os.path.join(output_dir, report_filename)\n    \n    if final_report:\n        with open(report_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(final_report)\n        logging.info(f\"Final report saved to '{report_filepath}'.\")\n        \n        if savings_data:\n            try:\n                savings_filename = report_filename.replace(\"report_\", \"savings_\").replace(\".md\", \".png\")\n                savings_filepath = os.path.join(stats_dir, savings_filename)\n                \n                logging.info(f\"Erstelle Token-Chart: {savings_filepath}\")\n                create_savings_chart(\n                    json_tokens=savings_data['json_tokens'], \n                    toon_tokens=savings_data['toon_tokens'], \n                    savings_percent=savings_data['savings_percent'],\n                    output_path=savings_filepath\n                )\n            except Exception as chart_error:\n                logging.error(f\"Konnte Diagramm nicht erstellen: {chart_error}\")\n\n    else:\n        final_report = \"Error: Report generation failed or returned empty.\"\n\n    metrics = {\n        \"helper_time\": round(total_helper_time, 2),\n        \"main_time\": round(total_main_time, 2),\n        \"total_time\": round(total_active_time, 2),\n        \"helper_model\": helper_model,\n        \"main_model\": main_model,\n        \"json_tokens\": savings_data['json_tokens'] if savings_data else None,\n        \"toon_tokens\": savings_data['toon_tokens'] if savings_data else None,\n        \"savings_percent\": round(savings_data['savings_percent'], 2) if savings_data else None\n    \n    }\n\n    return {\n        \"report\": final_report,\n        \"metrics\": metrics\n    }"
            start_line: 75
            end_line: 477
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.update_status
            name: update_status
            args[1]: msg
            docstring: null
            source_code: "def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)"
            start_line: 77
            end_line: 80
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.notebook_workflow
            name: notebook_workflow
            args[4]: input,api_keys,model,status_callback
            docstring: null
            source_code: "def notebook_workflow(input, api_keys, model, status_callback=None):\n    t_start = time.time()\n    final_report = \"keine Notebooks gefunden\"\n    def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)\n\n    def gemini_payload(basic_info, nb_path, xml_content, images):\n        \n        intro_json = json.dumps({\n            \"basic_info\": basic_info,\n            \"current_notebook_path\": nb_path\n        }, indent=2)\n        \n        payload_content = []\n\n        payload_content.append({\n            \"type\": \"text\",\n            \"text\": f\"Context Information:\\n{intro_json}\\n\\nNotebook XML Structure:\\n\"\n        })\n\n        # Regex to find: <IMAGE_PLACEHOLDER index=\"0\" mime=\"image/png\"/>\n        pattern = r'(<IMAGE_PLACEHOLDER index=\"(\\d+)\" mime=\"([^\"]+)\"/>)'\n        last_pos = 0\n        \n        for match in re.finditer(pattern, xml_content):\n            text_segment = xml_content[last_pos:match.start()]\n            if text_segment.strip():\n                payload_content.append({\n                    \"type\": \"text\",\n                    \"text\": text_segment\n                })\n                \n            image_index = int(match.group(2))\n            mime_type = match.group(3)\n            \n            if image_index < len(images):\n                img_data = images[image_index]\n                b64_string = img_data['data']\n                \n                payload_content.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:{mime_type};base64,{b64_string}\"\n                    }\n                })\n                \n            last_pos = match.end()\n\n        # Add any remaining text after the last image\n        remaining_text = xml_content[last_pos:]\n        if remaining_text.strip():\n            payload_content.append({\n                \"type\": \"text\",\n                \"text\": remaining_text\n            })\n\n        return payload_content\n\n\n    base_url = None\n\n    if model.startswith(\"gpt-\"):\n        input_api_key = api_keys.get(\"gpt\")\n    elif model.startswith(\"gemini-\"):\n        input_api_key = api_keys.get(\"gemini\")\n    elif \"/\" in model or model.startswith(\"alias-\") or any(x in model for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        input_api_key = api_keys.get(\"scadsllm\")\n        base_url = api_keys.get(\"scadsllm_base_url\")\n    else:\n        input_api_key = None\n        base_url = api_keys.get(\"ollama\")\n\n    update_status(\"üîç Analysiere Input...\")\n\n    # URL Extraktion\n    repo_url = None\n    url_pattern = r\"https?://(?:www\\.)?github\\.com/[^\\s]+\"\n    match = re.search(url_pattern, input)\n\n    if match:\n        repo_url = match.group(0)\n        logging.info(f\"Extracted repository URL: {repo_url}\")\n    else:\n        raise ValueError(\"Could not find a valid URL in the provided input.\")\n    \n    update_status(f\"‚¨áÔ∏è Klone Repository: {repo_url} ...\")\n\n    try: \n        with GitRepository(repo_url) as repo:\n            repo_files = repo.get_all_files()\n    except Exception as e:\n        logging.error(f\"Error cloning repository: {e}\")\n        raise\n    \n    update_status(f\"üîó Bereite Input vor...\")\n\n    # convert to XML\n    processed_data = process_repo_notebooks(repo_files)\n\n    \n    # Extrahiere Basic Infos\n    update_status(\"‚ÑπÔ∏è Extrahiere Basis-Informationen...\")\n    try:\n        info_extractor = ProjektInfoExtractor()\n        basic_project_info = info_extractor.extrahiere_info(dateien=repo_files, repo_url=repo_url)\n        logging.info(\"Basic project info extracted\")\n    except Exception as e:\n        logging.error(f\"Error extracting basic project info: {e}\")\n        basic_project_info = \"Could not extract basic info.\"\n\n\n\n    prompt_file_notebook_llm = \"SystemPrompts/SystemPromptNotebookLLM.txt\"\n    notebook_llm = MainLLM(\n        api_key=input_api_key, \n        prompt_file_path=prompt_file_notebook_llm,\n        model_name=model,\n        base_url=base_url,\n    )\n\n    notebook_reports = []\n    total_notebooks = len(processed_data)\n    \n    logging.info(f\"Starting sequential processing of {total_notebooks} notebooks...\")\n\n    # Iterate over each notebook file individually\n    for index, (nb_path, nb_data) in enumerate(processed_data.items(), 1):\n        \n        update_status(f\"üß† Generiere Report ({index}/{total_notebooks}): {os.path.basename(nb_path)}\")\n        \n        nb_xml = nb_data['xml']\n        nb_images = nb_data['images']\n\n        llm_payload = gemini_payload(basic_project_info, nb_path, nb_xml, nb_images)\n\n        try:\n            single_report = notebook_llm.call_llm(llm_payload)\n            if single_report and isinstance(single_report, str):\n                notebook_reports.append(single_report)\n            else:\n                logging.warning(f\"LLM lieferte kein Ergebnis f√ºr {nb_path}\")\n                notebook_reports.append(f\"## Analyse f√ºr {os.path.basename(nb_path)}\\nFehler: Das Modell lieferte keine Antwort.\")\n            \n        except Exception as e:\n            logging.error(f\"Error generating report for {nb_path}: {e}\")\n            notebook_reports.append(f\"# Error processing {nb_path}\\n\\nError: {str(e)}\\n\\n---\\n\")\n\n    # Concatenate all reports\n    final_report = \"\\n\\n<br>\\n<br>\\n<br>\\n\\n\".join([str(r) for r in notebook_reports if r is not None])\n\n    \n    # --- Speichern der Ergebnisse ---\n    output_dir = \"result\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Dateiname f√ºr Report\n    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M-%S\")\n    report_filename = f\"notebook_report_{timestamp}_NotebookLLM_{notebook_llm.model_name}.md\"\n    report_filepath = os.path.join(output_dir, report_filename)\n    \n    if final_report:\n        with open(report_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(final_report)\n        logging.info(f\"Final report saved to '{report_filepath}'.\")\n\n    # Am Ende der Funktion nach dem Speichern:\n    total_time = time.time() - t_start\n    \n    # Metriken f√ºr das Frontend bauen\n    metrics = {\n        \"helper_time\": \"0\", # Notebook-Workflow hat meist keinen separaten Helper\n        \"main_time\": round(total_time, 2),\n        \"total_time\": round(total_time, 2),\n        \"helper_model\": \"None\",\n        \"main_model\": model,\n        \"json_tokens\": 0,\n        \"toon_tokens\": 0,\n        \"savings_percent\": 0\n    }\n\n    return {\n        \"report\": final_report,\n        \"metrics\": metrics\n    }"
            start_line: 483
            end_line: 668
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.update_status
            name: update_status
            args[1]: msg
            docstring: null
            source_code: "def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)"
            start_line: 486
            end_line: 489
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.gemini_payload
            name: gemini_payload
            args[4]: basic_info,nb_path,xml_content,images
            docstring: null
            source_code: "def gemini_payload(basic_info, nb_path, xml_content, images):\n        \n        intro_json = json.dumps({\n            \"basic_info\": basic_info,\n            \"current_notebook_path\": nb_path\n        }, indent=2)\n        \n        payload_content = []\n\n        payload_content.append({\n            \"type\": \"text\",\n            \"text\": f\"Context Information:\\n{intro_json}\\n\\nNotebook XML Structure:\\n\"\n        })\n\n        # Regex to find: <IMAGE_PLACEHOLDER index=\"0\" mime=\"image/png\"/>\n        pattern = r'(<IMAGE_PLACEHOLDER index=\"(\\d+)\" mime=\"([^\"]+)\"/>)'\n        last_pos = 0\n        \n        for match in re.finditer(pattern, xml_content):\n            text_segment = xml_content[last_pos:match.start()]\n            if text_segment.strip():\n                payload_content.append({\n                    \"type\": \"text\",\n                    \"text\": text_segment\n                })\n                \n            image_index = int(match.group(2))\n            mime_type = match.group(3)\n            \n            if image_index < len(images):\n                img_data = images[image_index]\n                b64_string = img_data['data']\n                \n                payload_content.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:{mime_type};base64,{b64_string}\"\n                    }\n                })\n                \n            last_pos = match.end()\n\n        # Add any remaining text after the last image\n        remaining_text = xml_content[last_pos:]\n        if remaining_text.strip():\n            payload_content.append({\n                \"type\": \"text\",\n                \"text\": remaining_text\n            })\n\n        return payload_content"
            start_line: 491
            end_line: 541
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "backend/relationship_analyzer.py":
      ast_nodes:
        imports[4]: ast,os,logging,collections.defaultdict
        functions[1]:
          - mode: function_analysis
            identifier: backend.relationship_analyzer.path_to_module
            name: path_to_module
            args[2]: filepath,project_root
            docstring: Wandelt einen Dateipfad in einen Python-Modulpfad um.
            source_code: "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    module_path = rel_path.replace(os.path.sep, '.')\n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n    return module_path"
            start_line: 6
            end_line: 18
            context:
              calls[0]:
              called_by[0]:
        classes[2]:
          - mode: class_analysis
            identifier: backend.relationship_analyzer.ProjectAnalyzer
            name: ProjectAnalyzer
            docstring: null
            source_code: "class ProjectAnalyzer:\n    \n    def __init__(self, project_root):\n        self.project_root = os.path.abspath(project_root)\n        self.definitions = {}\n        self.call_graph = defaultdict(list)\n        self.file_asts = {} \n        self.ignore_dirs = {'.git', '.venv', 'venv', '__pycache__', 'node_modules', 'dist', 'build', 'docs'}\n\n    def analyze(self):\n        py_files = self._find_py_files()\n        \n        for filepath in py_files:\n            self._collect_definitions(filepath)\n            \n        for filepath in py_files:\n            self._resolve_calls(filepath)\n            \n        self.file_asts.clear()\n        \n        return self.call_graph\n\n    def get_raw_relationships(self):\n\n        outgoing = defaultdict(set)\n        incoming = defaultdict(set)\n\n        for callee_id, callers_info_list in self.call_graph.items():\n            for info in callers_info_list:\n                caller_id = info['caller']\n                \n                if caller_id and callee_id:\n                    outgoing[caller_id].add(callee_id)\n                    incoming[callee_id].add(caller_id)\n        \n        return {\n            \"outgoing\": {k: sorted(list(v)) for k, v in outgoing.items()},\n            \"incoming\": {k: sorted(list(v)) for k, v in incoming.items()}\n        }\n\n    def _find_py_files(self):\n        py_files = []\n        for root, dirs, files in os.walk(self.project_root):\n            dirs[:] = [d for d in dirs if d not in self.ignore_dirs]\n            for file in files:\n                if file.endswith(\".py\"):\n                    py_files.append(os.path.join(root, file))\n        return py_files\n\n    def _collect_definitions(self, filepath):\n        try:\n            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n                source = f.read()\n                tree = ast.parse(source, filename=filepath)\n                \n            self.file_asts[filepath] = tree\n            module_path = path_to_module(filepath, self.project_root)\n            \n            for node in ast.walk(tree):\n                if isinstance(node, ast.FunctionDef):\n                    parent = self._get_parent(tree, node)\n                    if isinstance(parent, ast.ClassDef):\n                        path_name = f\"{module_path}.{parent.name}.{node.name}\"\n                        def_type = 'method'\n                    else:\n                        path_name = f\"{module_path}.{node.name}\"\n                        def_type = 'function'\n                    self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': def_type}\n                elif isinstance(node, ast.ClassDef):\n                    path_name = f\"{module_path}.{node.name}\"\n                    self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': 'class'}\n        except Exception as e:\n            logging.error(f\"Error collecting definitions in {filepath}: {e}\")\n            self.file_asts[filepath] = None\n\n    def _get_parent(self, tree, node):\n        for parent in ast.walk(tree):\n            for child in ast.iter_child_nodes(parent):\n                if child is node:\n                    return parent\n        return None\n\n    def _resolve_calls(self, filepath):\n        tree = self.file_asts.get(filepath)\n        if not tree:\n            return\n\n        try:\n            resolver = CallResolverVisitor(filepath, self.project_root, self.definitions)\n            resolver.visit(tree)\n            \n            for callee_pathname, caller_info_list in resolver.calls.items():\n                self.call_graph[callee_pathname].extend(caller_info_list)\n        except Exception as e:\n            logging.error(f\"Error resolving calls in {filepath}: {e}\")"
            start_line: 20
            end_line: 114
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[7]:
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,project_root
                  docstring: null
                  start_line: 22
                  end_line: 27
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.analyze
                  name: analyze
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 29
                  end_line: 40
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.get_raw_relationships
                  name: get_raw_relationships
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 42
                  end_line: 58
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._find_py_files
                  name: _find_py_files
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 60
                  end_line: 67
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._collect_definitions
                  name: _collect_definitions
                  calls[0]:
                  called_by[0]:
                  args[2]: self,filepath
                  docstring: null
                  start_line: 69
                  end_line: 93
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._get_parent
                  name: _get_parent
                  calls[0]:
                  called_by[0]:
                  args[3]: self,tree,node
                  docstring: null
                  start_line: 95
                  end_line: 100
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._resolve_calls
                  name: _resolve_calls
                  calls[0]:
                  called_by[0]:
                  args[2]: self,filepath
                  docstring: null
                  start_line: 102
                  end_line: 114
          - mode: class_analysis
            identifier: backend.relationship_analyzer.CallResolverVisitor
            name: CallResolverVisitor
            docstring: null
            source_code: "class CallResolverVisitor(ast.NodeVisitor):\n    def __init__(self, filepath, project_root, definitions):\n        self.filepath = filepath\n        self.module_path = path_to_module(filepath, project_root)\n        self.definitions = definitions\n        self.scope = {}\n        self.instance_types = {}\n        self.current_caller_name = self.module_path\n        self.current_class_name = None\n        self.calls = defaultdict(list)\n\n    def visit_ClassDef(self, node):\n        old_class_name = self.current_class_name\n        self.current_class_name = node.name\n        self.generic_visit(node)\n        self.current_class_name = old_class_name\n\n    def visit_FunctionDef(self, node):\n        old_caller = self.current_caller_name\n        \n        if self.current_class_name:\n            full_identifier = f\"{self.module_path}.{self.current_class_name}.{node.name}\"\n        else:\n            full_identifier = f\"{self.module_path}.{node.name}\"\n            \n        self.current_caller_name = full_identifier\n        self.generic_visit(node)\n        self.current_caller_name = old_caller\n\n    def visit_Call(self, node):\n        callee_pathname = self._resolve_call_qname(node.func)\n        if callee_pathname and callee_pathname in self.definitions:\n            \n            if self.current_caller_name == self.module_path:\n                caller_type = 'module'\n            elif '.<locals>.' in self.current_caller_name:\n                caller_type = 'local_function'\n            elif self.current_class_name:\n                caller_type = 'method'\n            else:\n                caller_type = 'function'\n            \n            caller_info = {\n                'file': os.path.basename(self.filepath),\n                'line': node.lineno,\n                'caller': self.current_caller_name,\n                'caller_type': caller_type \n            }\n            self.calls[callee_pathname].append(caller_info)\n        self.generic_visit(node)\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.scope[alias.asname or alias.name] = alias.name\n        self.generic_visit(node)\n        \n    def visit_ImportFrom(self, node):\n        module = node.module or ''\n        for alias in node.names:\n            name = alias.asname or alias.name\n            if node.level > 0:\n                base = self.module_path.split('.')\n                prefix = '.'.join(base[:-node.level])\n                full_module_path = f\"{prefix}.{module}\" if module else prefix\n            else:\n                full_module_path = module\n            self.scope[name] = f\"{full_module_path}.{alias.name}\"\n        self.generic_visit(node)\n\n    def visit_Assign(self, node):\n        if isinstance(node.value, ast.Call) and isinstance(node.value.func, ast.Name):\n            class_name = node.value.func.id\n            if class_name in self.scope:\n                qualified_class_name = self.scope[class_name]\n                if qualified_class_name in self.definitions:\n                    for target in node.targets:\n                        if isinstance(target, ast.Name):\n                            self.instance_types[target.id] = qualified_class_name\n        self.generic_visit(node)\n\n    def _resolve_call_qname(self, func_node):\n        if isinstance(func_node, ast.Name):\n            name = func_node.id\n            if name in self.scope:\n                return self.scope[name]\n            local_pathname = f\"{self.module_path}.{name}\"\n            if local_pathname in self.definitions:\n                return local_pathname\n        elif isinstance(func_node, ast.Attribute) and isinstance(func_node.value, ast.Name):\n            var_name = func_node.value.id\n            method_name = func_node.attr\n            if var_name in self.instance_types:\n                class_pathname = self.instance_types[var_name]\n                return f\"{class_pathname}.{method_name}\"\n            if var_name in self.scope:\n                module_pathname = self.scope[var_name]\n                return f\"{module_pathname}.{method_name}\"\n        return None"
            start_line: 117
            end_line: 214
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[8]:
                - identifier: backend.relationship_analyzer.CallResolverVisitor.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[4]: self,filepath,project_root,definitions
                  docstring: null
                  start_line: 118
                  end_line: 126
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 128
                  end_line: 132
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 134
                  end_line: 144
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Call
                  name: visit_Call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 146
                  end_line: 166
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 168
                  end_line: 171
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 173
                  end_line: 184
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Assign
                  name: visit_Assign
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 186
                  end_line: 195
                - identifier: backend.relationship_analyzer.CallResolverVisitor._resolve_call_qname
                  name: _resolve_call_qname
                  calls[0]:
                  called_by[0]:
                  args[2]: self,func_node
                  docstring: null
                  start_line: 197
                  end_line: 214
    "backend/scads_key_test.py":
      ast_nodes:
        imports[3]: os,dotenv.load_dotenv,openai.OpenAI
        functions[0]:
        classes[0]:
    "database/db.py":
      ast_nodes:
        imports[7]: datetime.datetime,pymongo.MongoClient,dotenv.load_dotenv,streamlit_authenticator,cryptography.fernet.Fernet,uuid,os
        functions[29]:
          - mode: function_analysis
            identifier: database.db.encrypt_text
            name: encrypt_text
            args[1]: text
            docstring: null
            source_code: "def encrypt_text(text: str) -> str:\n    if not text or not cipher_suite: return text\n    return cipher_suite.encrypt(text.strip().encode()).decode()"
            start_line: 28
            end_line: 30
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.decrypt_text
            name: decrypt_text
            args[1]: text
            docstring: null
            source_code: "def decrypt_text(text: str) -> str:\n    if not text or not cipher_suite: return text\n    try:\n        return cipher_suite.decrypt(text.strip().encode()).decode()\n    except Exception:\n        return text"
            start_line: 32
            end_line: 37
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.insert_user
            name: insert_user
            args[3]: username,name,password
            docstring: null
            source_code: "def insert_user(username: str, name: str, password: str):\n    user = {\n        \"_id\": username,\n        \"name\": name, \n        \"hashed_password\": stauth.Hasher.hash(password),\n        \"gemini_api_key\": \"\",\n        \"ollama_base_url\": \"\",\n        \"gpt_api_key\": \"\"\n    }\n    result = dbusers.insert_one(user)\n    return result.inserted_id"
            start_line: 43
            end_line: 53
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_all_users
            name: fetch_all_users
            args[0]:
            docstring: null
            source_code: "def fetch_all_users():\n    return list(dbusers.find())"
            start_line: 55
            end_line: 56
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_user
            name: fetch_user
            args[1]: username
            docstring: null
            source_code: "def fetch_user(username: str):\n    return dbusers.find_one({\"_id\": username})"
            start_line: 58
            end_line: 59
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_user_name
            name: update_user_name
            args[2]: username,new_name
            docstring: null
            source_code: "def update_user_name(username: str, new_name: str):\n    # Achtung: _id kann in Mongo nicht einfach so ge√§ndert werden. \n    # Hier wird nur das Name-Feld geupdated.\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"name\": new_name}})\n    return result.modified_count"
            start_line: 61
            end_line: 65
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_gemini_key
            name: update_gemini_key
            args[2]: username,gemini_api_key
            docstring: null
            source_code: "def update_gemini_key(username: str, gemini_api_key: str):\n    encrypted_key = encrypt_text(gemini_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"gemini_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 67
            end_line: 70
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_gpt_key
            name: update_gpt_key
            args[2]: username,gpt_api_key
            docstring: null
            source_code: "def update_gpt_key(username: str, gpt_api_key: str):\n    encrypted_key = encrypt_text(gpt_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"gpt_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 72
            end_line: 75
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_ollama_url
            name: update_ollama_url
            args[2]: username,ollama_base_url
            docstring: null
            source_code: "def update_ollama_url(username: str, ollama_base_url: str):\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"ollama_base_url\": ollama_base_url.strip()}})\n    return result.modified_count"
            start_line: 77
            end_line: 79
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_opensrc_key
            name: update_opensrc_key
            args[2]: username,opensrc_api_key
            docstring: null
            source_code: "def update_opensrc_key(username: str, opensrc_api_key: str):\n    encrypted_key = encrypt_text(opensrc_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"opensrc_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 81
            end_line: 84
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_opensrc_url
            name: update_opensrc_url
            args[2]: username,opensrc_base_url
            docstring: null
            source_code: "def update_opensrc_url(username: str, opensrc_base_url: str):\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"opensrc_base_url\": opensrc_base_url.strip()}})\n    return result.modified_count"
            start_line: 86
            end_line: 88
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_gemini_key
            name: fetch_gemini_key
            args[1]: username
            docstring: null
            source_code: "def fetch_gemini_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"gemini_api_key\": 1, \"_id\": 0})\n    return user.get(\"gemini_api_key\") if user else None"
            start_line: 90
            end_line: 92
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_ollama_url
            name: fetch_ollama_url
            args[1]: username
            docstring: null
            source_code: "def fetch_ollama_url(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"ollama_base_url\": 1, \"_id\": 0})\n    return user.get(\"ollama_base_url\") if user else None"
            start_line: 94
            end_line: 96
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_gpt_key
            name: fetch_gpt_key
            args[1]: username
            docstring: null
            source_code: "def fetch_gpt_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"gpt_api_key\": 1, \"_id\": 0})\n    return user.get(\"gpt_api_key\") if user else None"
            start_line: 98
            end_line: 100
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_opensrc_key
            name: fetch_opensrc_key
            args[1]: username
            docstring: null
            source_code: "def fetch_opensrc_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"opensrc_api_key\": 1, \"_id\": 0})\n    return user.get(\"opensrc_api_key\") if user else None"
            start_line: 102
            end_line: 104
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_opensrc_url
            name: fetch_opensrc_url
            args[1]: username
            docstring: null
            source_code: "def fetch_opensrc_url(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"opensrc_base_url\": 1, \"_id\": 0})\n    return user.get(\"opensrc_base_url\") if user else None"
            start_line: 106
            end_line: 108
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.delete_user
            name: delete_user
            args[1]: username
            docstring: null
            source_code: "def delete_user(username: str):\n    return dbusers.delete_one({\"_id\": username}).deleted_count"
            start_line: 110
            end_line: 111
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.get_decrypted_api_keys
            name: get_decrypted_api_keys
            args[1]: username
            docstring: null
            source_code: "def get_decrypted_api_keys(username: str):\n    user = dbusers.find_one({\"_id\": username})\n    if not user: return None, None\n    gemini_plain = decrypt_text(user.get(\"gemini_api_key\", \"\"))\n    ollama_plain = user.get(\"ollama_base_url\", \"\")\n    gpt_plain = decrypt_text(user.get(\"gpt_api_key\", \"\"))\n    opensrc_plain = decrypt_text(user.get(\"opensrc_api_key\", \"\"))\n    opensrc_url = user.get(\"opensrc_base_url\", \"\")\n    return gemini_plain, ollama_plain, gpt_plain, opensrc_plain, opensrc_url"
            start_line: 113
            end_line: 121
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.insert_chat
            name: insert_chat
            args[2]: username,chat_name
            docstring: Erstellt einen neuen Chat-Eintrag.
            source_code: "def insert_chat(username: str, chat_name: str):\n    \"\"\"Erstellt einen neuen Chat-Eintrag.\"\"\"\n    chat = {\n        \"_id\": str(uuid.uuid4()),\n        \"username\": username,\n        \"chat_name\": chat_name,\n        \"created_at\": datetime.now()\n    }\n    result = dbchats.insert_one(chat)\n    return result.inserted_id"
            start_line: 127
            end_line: 136
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_chats_by_user
            name: fetch_chats_by_user
            args[1]: username
            docstring: Holt alle definierten Chats eines Users.
            source_code: "def fetch_chats_by_user(username: str):\n    \"\"\"Holt alle definierten Chats eines Users.\"\"\"\n    # Sortieren nach Erstellung macht Sinn\n    chats = list(dbchats.find({\"username\": username}).sort(\"created_at\", 1))\n    return chats"
            start_line: 138
            end_line: 142
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.check_chat_exists
            name: check_chat_exists
            args[2]: username,chat_name
            docstring: null
            source_code: "def check_chat_exists(username: str, chat_name: str):\n    return dbchats.find_one({\"username\": username, \"chat_name\": chat_name}) is not None"
            start_line: 144
            end_line: 145
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.rename_chat_fully
            name: rename_chat_fully
            args[3]: username,old_name,new_name
            docstring: Benennt einen Chat und alle zugeh√∂rigen Exchanges um.
            source_code: "def rename_chat_fully(username: str, old_name: str, new_name: str):\n    \"\"\"\n    Benennt einen Chat und alle zugeh√∂rigen Exchanges um.\n    \"\"\"\n    # 1. Chat-Eintrag umbenennen\n    result = dbchats.update_one(\n        {\"username\": username, \"chat_name\": old_name}, \n        {\"$set\": {\"chat_name\": new_name}}\n    )\n    \n    # 2. Alle Messages (Exchanges) umh√§ngen\n    dbexchanges.update_many(\n        {\"username\": username, \"chat_name\": old_name},\n        {\"$set\": {\"chat_name\": new_name}}\n    )\n    return result.modified_count"
            start_line: 148
            end_line: 163
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.insert_exchange
            name: insert_exchange
            args[13]: question,answer,feedback,username,chat_name,helper_used,main_used,total_time,helper_time,main_time,json_tokens,toon_tokens,savings_percent
            docstring: null
            source_code: "def insert_exchange(question: str, answer: str, feedback: str, username: str, chat_name: str, \n                    helper_used: str=\"\", main_used: str=\"\", total_time: str=\"\", helper_time: str=\"\", main_time: str=\"\",\n                    json_tokens=0, toon_tokens=0, savings_percent=0.0):\n    new_id = str(uuid.uuid4())\n    exchange = {\n        \"_id\": new_id,\n        \"question\": question,\n        \"answer\": answer,\n        \"feedback\": feedback,\n        \"feedback_message\": \"\",\n        \"chat_name\": chat_name,\n        \"username\": username,\n        \"helper_used\": helper_used,\n        \"main_used\": main_used,\n        \"total_time\": total_time,\n        \"helper_time\": helper_time,\n        \"main_time\": main_time,\n        \"json_tokens\": json_tokens,\n        \"toon_tokens\": toon_tokens,\n        \"savings_percent\": savings_percent,\n        \"created_at\": datetime.now()\n    }\n    try:\n        dbexchanges.insert_one(exchange)\n        return new_id\n    except Exception as e:\n        print(f\"DB Error: {e}\")\n        return None"
            start_line: 169
            end_line: 196
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_exchanges_by_user
            name: fetch_exchanges_by_user
            args[1]: username
            docstring: null
            source_code: "def fetch_exchanges_by_user(username: str):\n    # Sortieren nach Zeitstempel wichtig f√ºr die Anzeige\n    exchanges = list(dbexchanges.find({\"username\": username}).sort(\"created_at\", 1))\n    return exchanges"
            start_line: 198
            end_line: 201
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_exchanges_by_chat
            name: fetch_exchanges_by_chat
            args[2]: username,chat_name
            docstring: null
            source_code: "def fetch_exchanges_by_chat(username: str, chat_name: str):\n    exchanges = list(dbexchanges.find({\"username\": username, \"chat_name\": chat_name}).sort(\"created_at\", 1))\n    return exchanges"
            start_line: 203
            end_line: 205
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_exchange_feedback
            name: update_exchange_feedback
            args[2]: exchange_id,feedback
            docstring: null
            source_code: "def update_exchange_feedback(exchange_id, feedback: int):\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback\": feedback}})\n    return result.modified_count"
            start_line: 207
            end_line: 209
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_exchange_feedback_message
            name: update_exchange_feedback_message
            args[2]: exchange_id,feedback_message
            docstring: null
            source_code: "def update_exchange_feedback_message(exchange_id, feedback_message: str):\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback_message\": feedback_message}})\n    return result.modified_count"
            start_line: 211
            end_line: 213
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.delete_exchange_by_id
            name: delete_exchange_by_id
            args[1]: exchange_id
            docstring: null
            source_code: "def delete_exchange_by_id(exchange_id: str):\n    result = dbexchanges.delete_one({\"_id\": exchange_id})\n    return result.deleted_count"
            start_line: 215
            end_line: 217
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.delete_full_chat
            name: delete_full_chat
            args[2]: username,chat_name
            docstring: "L√∂scht den Chat UND alle zugeh√∂rigen Exchanges.\nDas sorgt f√ºr Konsistenz zwischen Frontend und Backend."
            source_code: "def delete_full_chat(username: str, chat_name: str):\n    \"\"\"\n    L√∂scht den Chat UND alle zugeh√∂rigen Exchanges.\n    Das sorgt f√ºr Konsistenz zwischen Frontend und Backend.\n    \"\"\"\n    # 1. Alle Nachrichten in diesem Chat l√∂schen\n    del_exchanges = dbexchanges.delete_many({\"username\": username, \"chat_name\": chat_name})\n    \n    # 2. Den Chat selbst aus der Chat-Liste l√∂schen\n    del_chat = dbchats.delete_one({\"username\": username, \"chat_name\": chat_name})\n    \n    return del_chat.deleted_count"
            start_line: 221
            end_line: 232
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "frontend/frontend.py":
      ast_nodes:
        imports[16]: numpy,datetime.datetime,time,pymongo.MongoClient,dotenv.load_dotenv,os,sys,urllib.parse.urlparse,logging,traceback,re,streamlit_mermaid.st_mermaid,backend.main,database.db,streamlit,streamlit_authenticator
        functions[12]:
          - mode: function_analysis
            identifier: frontend.frontend.clean_names
            name: clean_names
            args[1]: model_list
            docstring: null
            source_code: "def clean_names(model_list):\n    return [m.split(\"/\")[-1] for m in model_list]"
            start_line: 54
            end_line: 55
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.get_filtered_models
            name: get_filtered_models
            args[2]: source_list,category_name
            docstring: Filtert eine Liste basierend auf der gew√§hlten Kategorie.
            source_code: "def get_filtered_models(source_list, category_name):\n    \"\"\"Filtert eine Liste basierend auf der gew√§hlten Kategorie.\"\"\"\n    keywords = CATEGORY_KEYWORDS.get(category_name, [\"\"])\n    \n    if \"STANDARD\" in keywords:\n        # Nur Modelle zur√ºckgeben, die auch in der Standard-Liste sind\n        return [m for m in source_list if m in STANDARD_MODELS]\n    \n    filtered = []\n    for model in source_list:\n        # Pr√ºfen ob ein Keyword im Namen steckt\n        if any(k in model.lower() for k in keywords):\n            filtered.append(model)\n            \n    return filtered if filtered else source_list"
            start_line: 86
            end_line: 100
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.save_gemini_cb
            name: save_gemini_cb
            args[0]:
            docstring: null
            source_code: "def save_gemini_cb():\n    new_key = st.session_state.get(\"in_gemini_key\", \"\")\n    if new_key:\n        db.update_gemini_key(st.session_state[\"username\"], new_key)\n        st.session_state[\"in_gemini_key\"] = \"\"\n        st.toast(\"Gemini Key erfolgreich gespeichert! ‚úÖ\")"
            start_line: 143
            end_line: 148
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.save_ollama_cb
            name: save_ollama_cb
            args[0]:
            docstring: null
            source_code: "def save_ollama_cb():\n    new_url = st.session_state.get(\"in_ollama_url\", \"\")\n    if new_url:\n        db.update_ollama_url(st.session_state[\"username\"], new_url)\n        st.toast(\"Ollama URL gespeichert! ‚úÖ\")"
            start_line: 150
            end_line: 154
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.load_data_from_db
            name: load_data_from_db
            args[1]: username
            docstring: L√§dt Chats und Exchanges konsistent aus der DB.
            source_code: "def load_data_from_db(username: str):\n    \"\"\"L√§dt Chats und Exchanges konsistent aus der DB.\"\"\"\n    \n    # Nur laden, wenn neuer User oder noch nicht geladen\n    if \"loaded_user\" not in st.session_state or st.session_state.loaded_user != username:\n        st.session_state.chats = {}\n        \n        # 1. Erst die definierten Chats laden (damit auch leere Chats da sind)\n        db_chats = db.fetch_chats_by_user(username)\n        for c in db_chats:\n            c_name = c.get(\"chat_name\")\n            if c_name:\n                st.session_state.chats[c_name] = {\"exchanges\": []}\n\n        # 2. Dann die Exchanges laden und einsortieren\n        db_exchanges = db.fetch_exchanges_by_user(username)\n        for ex in db_exchanges:\n            c_name = ex.get(\"chat_name\", \"Unbenannt\")\n            \n            # Falls Exchanges existieren f√ºr Chats, die nicht in dbchats sind (Legacy Support)\n            if c_name not in st.session_state.chats:\n                st.session_state.chats[c_name] = {\"exchanges\": []}\n            \n            if \"feedback\" not in ex or ex[\"feedback\"] is None:\n                ex[\"feedback\"] = np.nan\n            \n            st.session_state.chats[c_name][\"exchanges\"].append(ex)\n\n        # 3. Default Chat erstellen, falls gar nichts existiert\n        if not st.session_state.chats:\n            initial_name = \"Chat 1\"\n            # Konsistent in DB anlegen\n            db.insert_chat(username, initial_name)\n            st.session_state.chats[initial_name] = {\"exchanges\": []}\n            st.session_state.active_chat = initial_name\n        else:\n            # Active Chat setzen, falls n√∂tig\n            if \"active_chat\" not in st.session_state or st.session_state.active_chat not in st.session_state.chats:\n                # Nimm den ersten verf√ºgbaren Chat\n                st.session_state.active_chat = list(st.session_state.chats.keys())[0]\n        \n        st.session_state.loaded_user = username"
            start_line: 160
            end_line: 201
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.handle_feedback_change
            name: handle_feedback_change
            args[2]: ex,val
            docstring: null
            source_code: "def handle_feedback_change(ex, val):\n    ex[\"feedback\"] = val\n    db.update_exchange_feedback(ex[\"_id\"], val)\n    st.rerun()"
            start_line: 207
            end_line: 210
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.handle_delete_exchange
            name: handle_delete_exchange
            args[2]: chat_name,ex
            docstring: null
            source_code: "def handle_delete_exchange(chat_name, ex):\n    db.delete_exchange_by_id(ex[\"_id\"])\n    if chat_name in st.session_state.chats:\n        if ex in st.session_state.chats[chat_name][\"exchanges\"]:\n            st.session_state.chats[chat_name][\"exchanges\"].remove(ex)\n    st.rerun()"
            start_line: 212
            end_line: 217
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.handle_delete_chat
            name: handle_delete_chat
            args[2]: username,chat_name
            docstring: null
            source_code: "def handle_delete_chat(username, chat_name):\n    # KONSISTENZ: Ruft jetzt delete_full_chat in DB auf\n    db.delete_full_chat(username, chat_name)\n    \n    # State bereinigen\n    if chat_name in st.session_state.chats:\n        del st.session_state.chats[chat_name]\n    \n    # Active Chat neu setzen\n    if len(st.session_state.chats) > 0:\n        st.session_state.active_chat = list(st.session_state.chats.keys())[0]\n    else:\n        # Wenn alles weg ist, neuen leeren Chat anlegen\n        new_name = \"Chat 1\"\n        db.insert_chat(username, new_name)\n        st.session_state.chats[new_name] = {\"exchanges\": []}\n        st.session_state.active_chat = new_name\n        \n    st.rerun()"
            start_line: 219
            end_line: 237
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.extract_repo_name
            name: extract_repo_name
            args[1]: text
            docstring: null
            source_code: "def extract_repo_name(text):\n    url_match = re.search(r'(https?://[^\\s]+)', text)\n    if url_match:\n        url = url_match.group(0)\n        parsed = urlparse(url)\n        path = parsed.path.strip(\"/\")\n        if path:\n            repo_name = path.split(\"/\")[-1]\n            if repo_name.endswith(\".git\"):\n                repo_name = repo_name[:-4]\n            return repo_name\n    return None"
            start_line: 243
            end_line: 254
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.stream_text_generator
            name: stream_text_generator
            args[1]: text
            docstring: null
            source_code: "def stream_text_generator(text):\n    for word in text.split(\" \"):\n        yield word + \" \"\n        time.sleep(0.01)"
            start_line: 256
            end_line: 259
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.render_text_with_mermaid
            name: render_text_with_mermaid
            args[2]: markdown_text,should_stream
            docstring: null
            source_code: "def render_text_with_mermaid(markdown_text, should_stream=False):\n    if not markdown_text:\n        return\n\n    parts = re.split(r\"```mermaid\\s+(.*?)\\s+```\", markdown_text, flags=re.DOTALL)\n\n    for i, part in enumerate(parts):\n        if i % 2 == 0:\n            if part.strip():\n                if should_stream:\n                    st.write_stream(stream_text_generator(part))\n                else:\n                    st.markdown(part)\n        else:\n            try:\n                st_mermaid(part, key=f\"mermaid_{hash(part)}_{i}\")\n            except Exception:\n                st.code(part, language=\"mermaid\")"
            start_line: 261
            end_line: 278
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.render_exchange
            name: render_exchange
            args[2]: ex,current_chat_name
            docstring: null
            source_code: "def render_exchange(ex, current_chat_name):\n    st.chat_message(\"user\").write(ex[\"question\"])\n\n    with st.chat_message(\"assistant\"):\n        answer_text = ex.get(\"answer\", \"\")\n        is_error = answer_text.startswith(\"Fehler\") or answer_text.startswith(\"Error\")\n\n        # --- 1. TOOLBAR CONTAINER ---\n        # Nutzt die neuen Parameter horizontal=True und alignment\n        # border=True rahmt die Toolbar ein\n        # horizontal_alignment=\"right\" schiebt alles nach rechts\n        with st.container(horizontal=True, horizontal_alignment=\"right\"):\n            \n            if not is_error:\n                # 1. Status Text (wird jetzt links von den Buttons angezeigt, aber rechtsb√ºndig im Container)\n                st.write(\"hier die Antwort:\")\n                if ex.get(\"feedback\") == 1:\n                    st.caption(\"‚úÖ Hilfreich\")\n                elif ex.get(\"feedback\") == 0:\n                    st.caption(\"‚ùå Nicht hilfreich\")\n                \n                # 2. Die Buttons (einfach untereinander im Code, erscheinen nebeneinander im UI)\n                \n                # Like\n                type_primary_up = ex.get(\"feedback\") == 1\n                if st.button(\"üëç\", key=f\"up_{ex['_id']}\", type=\"primary\" if type_primary_up else \"secondary\", help=\"Hilfreich\"):\n                    handle_feedback_change(ex, 1)\n\n                # Dislike\n                type_primary_down = ex.get(\"feedback\") == 0\n                if st.button(\"üëé\", key=f\"down_{ex['_id']}\", type=\"primary\" if type_primary_down else \"secondary\", help=\"Nicht hilfreich\"):\n                    handle_feedback_change(ex, 0)\n\n                # Comment Popover\n                with st.popover(\"üí¨\", help=\"Notiz hinzuf√ºgen\"):\n                    msg = st.text_area(\"Notiz:\", value=ex.get(\"feedback_message\", \"\"), key=f\"txt_{ex['_id']}\")\n                    if st.button(\"Speichern\", key=f\"save_{ex['_id']}\"):\n                        ex[\"feedback_message\"] = msg\n                        db.update_exchange_feedback_message(ex[\"_id\"], msg)\n                        st.success(\"Gespeichert!\")\n                        time.sleep(0.5)\n                        st.rerun()\n\n                # Download\n                st.download_button(\n                    label=\"üì•\",\n                    data=ex[\"answer\"],\n                    file_name=f\"response_{ex['_id']}.md\",\n                    mime=\"text/markdown\",\n                    key=f\"dl_{ex['_id']}\",\n                    help=\"Als Markdown herunterladen\"\n                )\n\n                # Delete\n                if st.button(\"üóëÔ∏è\", key=f\"del_{ex['_id']}\", help=\"Nachricht l√∂schen\", type=\"secondary\"):\n                    handle_delete_exchange(current_chat_name, ex)\n\n            else:\n                # Fehlerfall\n                st.error(\"‚ö†Ô∏è Fehler\")\n                if st.button(\"üóëÔ∏è L√∂schen\", key=f\"del_err_{ex['_id']}\"):\n                    handle_delete_exchange(current_chat_name, ex)\n\n        # --- 2. CONTENT (Antwort) ---\n        with st.container(height=500, border=True):\n             render_text_with_mermaid(ex[\"answer\"], should_stream=False)"
            start_line: 284
            end_line: 349
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "schemas/types.py":
      ast_nodes:
        imports[5]: typing.List,typing.Optional,typing.Literal,pydantic.BaseModel,pydantic.ValidationError
        functions[0]:
        classes[15]:
          - mode: class_analysis
            identifier: schemas.types.ParameterDescription
            name: ParameterDescription
            docstring: Describes a single parameter of a function.
            source_code: "class ParameterDescription(BaseModel):\n    \"\"\"Describes a single parameter of a function.\"\"\"\n    name: str\n    type: str\n    description: str"
            start_line: 6
            end_line: 10
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ReturnDescription
            name: ReturnDescription
            docstring: Describes the return value of a function.
            source_code: "class ReturnDescription(BaseModel):\n    \"\"\"Describes the return value of a function.\"\"\"\n    name: str\n    type: str\n    description: str"
            start_line: 12
            end_line: 16
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.UsageContext
            name: UsageContext
            docstring: Describes the calling context of a function.
            source_code: "class UsageContext(BaseModel):\n    \"\"\"Describes the calling context of a function.\"\"\"\n    calls: str\n    called_by: str"
            start_line: 18
            end_line: 21
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionDescription
            name: FunctionDescription
            docstring: Contains the detailed analysis of a function's purpose and signature.
            source_code: "class FunctionDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a function's purpose and signature.\"\"\"\n    overall: str\n    parameters: List[ParameterDescription]\n    returns: List[ReturnDescription]\n    usage_context: UsageContext"
            start_line: 23
            end_line: 28
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionAnalysis
            name: FunctionAnalysis
            docstring: The main model representing the entire JSON schema for a function.
            source_code: "class FunctionAnalysis(BaseModel):\n    \"\"\"The main model representing the entire JSON schema for a function.\"\"\"\n    identifier: str\n    description: FunctionDescription\n    error: Optional[str] = None"
            start_line: 30
            end_line: 34
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ConstructorDescription
            name: ConstructorDescription
            docstring: Describes the __init__ method of a class.
            source_code: "class ConstructorDescription(BaseModel):\n    \"\"\"Describes the __init__ method of a class.\"\"\"\n    description: str\n    parameters: List[ParameterDescription]"
            start_line: 39
            end_line: 42
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassContext
            name: ClassContext
            docstring: Describes the class's external dependencies and primary points of instantiation.
            source_code: "class ClassContext(BaseModel):\n    \"\"\"Describes the class's external dependencies and primary points of instantiation.\"\"\"\n    dependencies: str\n    instantiated_by: str"
            start_line: 44
            end_line: 47
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassDescription
            name: ClassDescription
            docstring: "Contains the detailed analysis of a class's purpose, constructor, and methods."
            source_code: "class ClassDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a class's purpose, constructor, and methods.\"\"\"\n    overall: str\n    init_method: ConstructorDescription\n    methods: List[FunctionAnalysis]\n    usage_context: ClassContext"
            start_line: 49
            end_line: 54
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassAnalysis
            name: ClassAnalysis
            docstring: The main model for the entire JSON schema for a class.
            source_code: "class ClassAnalysis(BaseModel):\n    \"\"\"The main model for the entire JSON schema for a class.\"\"\"\n    identifier: str\n    description: ClassDescription\n    error: Optional[str] = None"
            start_line: 56
            end_line: 60
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.CallInfo
            name: CallInfo
            docstring: "Represents a specific call event from the relationship analyzer.\nUsed in 'called_by' and 'instantiated_by' lists."
            source_code: "class CallInfo(BaseModel):\n    \"\"\"\n    Represents a specific call event from the relationship analyzer.\n    Used in 'called_by' and 'instantiated_by' lists.\n    \"\"\"\n    file: str\n    function: str  # Name des Aufrufers\n    mode: str      # z.B. 'method', 'function', 'module'\n    line: int"
            start_line: 65
            end_line: 73
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionContextInput
            name: FunctionContextInput
            docstring: Structured context for analyzing a function.
            source_code: "class FunctionContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a function.\"\"\"\n    calls: List[str]\n    called_by: List[CallInfo]"
            start_line: 78
            end_line: 81
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionAnalysisInput
            name: FunctionAnalysisInput
            docstring: The required input to generate a FunctionAnalysis object.
            source_code: "class FunctionAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a FunctionAnalysis object.\"\"\"\n    mode: Literal[\"function_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: FunctionContextInput"
            start_line: 83
            end_line: 89
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.MethodContextInput
            name: MethodContextInput
            docstring: Structured context for a classes methods
            source_code: "class MethodContextInput(BaseModel):\n    \"\"\"Structured context for a classes methods\"\"\"\n    identifier: str\n    calls: List[str]\n    called_by: List[CallInfo]\n    args: List[str]\n    docstring: Optional[str]"
            start_line: 94
            end_line: 100
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassContextInput
            name: ClassContextInput
            docstring: Structured context for analyzing a class.
            source_code: "class ClassContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a class.\"\"\"\n    dependencies: List[str]\n    instantiated_by: List[CallInfo]\n    method_context: List[MethodContextInput]"
            start_line: 102
            end_line: 106
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassAnalysisInput
            name: ClassAnalysisInput
            docstring: The required input to generate a ClassAnalysis object.
            source_code: "class ClassAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a ClassAnalysis object.\"\"\"\n    mode: Literal[\"class_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: ClassContextInput"
            start_line: 108
            end_line: 114
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
analysis_results:
  functions:
    backend.AST_Schema.path_to_module:
      identifier: backend.AST_Schema.path_to_module
      description:
        overall: "This function converts a given file system path into a Python module path string. It first attempts to determine the path relative to a specified project root. If a relative path cannot be determined (e.g., due to different drives), it falls back to using just the base filename. It then removes the '.py' extension if present and replaces file system path separators with dots. Finally, it handles '__init__.py' files by removing the '.__init__' suffix to represent the package itself."
        parameters[2]{name,type,description}:
          filepath,str,The absolute or relative path to a file that needs to be converted into a module path.
          project_root,str,"The root directory of the project, used as a reference to calculate the relative path for the module."
        returns[1]{name,type,description}:
          module_path,str,"The converted Python module path string, e.g., 'my_package.my_module'."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    backend.File_Dependency.build_file_dependency_graph:
      identifier: backend.File_Dependency.build_file_dependency_graph
      description:
        overall: "This function constructs a directed graph representing file-level import dependencies. It initializes a NetworkX directed graph and then uses a `FileDependencyGraph` visitor to traverse the provided Abstract Syntax Tree (AST). The visitor collects import relationships, which are then used to populate the graph with nodes for callers and callees, and edges representing the import dependencies. The resulting graph illustrates which files import other files."
        parameters[3]{name,type,description}:
          filename,str,The name of the file whose dependencies are being analyzed.
          tree,AST,The Abstract Syntax Tree (AST) of the file to be processed for dependencies.
          repo_root,str,"The root directory of the repository, used for resolving file paths and dependencies."
        returns[1]{name,type,description}:
          graph,nx.DiGraph,A NetworkX directed graph where nodes represent files and edges represent import dependencies between them.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.File_Dependency.build_repository_graph:
      identifier: backend.File_Dependency.build_repository_graph
      description:
        overall: "This function constructs a directed graph representing the dependencies within a given Git repository. It iterates through all Python files in the repository, parses each file's content into an Abstract Syntax Tree (AST), and then builds a file-specific dependency graph. These individual file graphs are then merged into a single global directed graph, capturing the overall repository structure and inter-file relationships."
        parameters[1]{name,type,description}:
          repository,GitRepository,The Git repository object from which to extract file dependencies.
        returns[1]{name,type,description}:
          global_graph,nx.DiGraph,A NetworkX directed graph representing the dependencies found across the repository's Python files.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.File_Dependency.get_all_temp_files:
      identifier: backend.File_Dependency.get_all_temp_files
      description:
        overall: "This function, `get_all_temp_files`, is designed to locate all Python files within a specified directory and its subdirectories. It takes a directory path as a string input. The function converts this string into an absolute `pathlib.Path` object and then recursively searches for all files ending with the \".py\" extension. For each Python file found, it calculates its path relative to the initial root directory. Finally, it returns a list containing these relative `pathlib.Path` objects."
        parameters[1]{name,type,description}:
          directory,str,The path to the root directory from which to start searching for Python files.
        returns[1]{name,type,description}:
          all_files,"list[pathlib.Path]","A list of `pathlib.Path` objects, where each object represents a Python file found within the specified directory and its subdirectories, with paths relative to the input directory."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    backend.HelperLLM.main_orchestrator:
      identifier: backend.HelperLLM.main_orchestrator
      description:
        overall: "This function serves as a testing and orchestration loop for the LLMHelper class, primarily demonstrating how to process function and class analysis inputs. It defines pre-computed dummy data conforming to Pydantic models for `FunctionAnalysisInput` and `FunctionAnalysis`. The function then initializes an `LLMHelper` instance and simulates the generation of documentation for these functions, logging the process and printing the final aggregated results."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.callgraph.make_safe_dot:
      identifier: backend.callgraph.make_safe_dot
      description:
        overall: "This function takes a NetworkX directed graph and a file path, then generates a DOT file representation of the graph. It first creates a copy of the input graph and relabels its nodes with simple, safe identifiers (e.g., 'n0', 'n1') to avoid potential issues with complex node names in DOT format. The original node names are preserved by assigning them as 'label' attributes to the newly relabeled nodes. Finally, the modified graph is written to the specified output path as a DOT file."
        parameters[2]{name,type,description}:
          graph,nx.DiGraph,The NetworkX directed graph to be converted into a DOT file.
          out_path,str,The file path where the DOT graph representation will be saved.
        returns[1]{name,type,description}:
          None,None,This function does not return any value; it performs a side effect by writing a file to disk.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.callgraph.build_filtered_callgraph:
      identifier: backend.callgraph.build_filtered_callgraph
      description:
        overall: "This function constructs a directed call graph for a given Git repository, focusing exclusively on functions identified as \"self-written\" within the repository's Python files. It first traverses all Python files to identify and collect all locally defined functions using an AST visitor. Subsequently, it re-processes these files to detect call relationships between functions. Only calls where both the caller and the callee are among the identified \"self-written\" functions are included in the final NetworkX directed graph, which is then returned."
        parameters[1]{name,type,description}:
          repo,GitRepository,The GitRepository object representing the code repository to analyze.
        returns[1]{name,type,description}:
          global_graph,networkx.DiGraph,A NetworkX directed graph representing the call relationships between self-written functions within the repository.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.wrap_cdata:
      identifier: backend.converter.wrap_cdata
      description:
        overall: "The `wrap_cdata` function takes a string `content` as input and encapsulates it within XML CDATA tags. It constructs a new string that starts with \"<![CDATA[\\n\", inserts the provided content, and concludes with \"\\n]]>\". This process is designed to ensure that the content can be safely embedded within an XML document without being parsed as XML markup."
        parameters[1]{name,type,description}:
          content,str,The string content to be wrapped inside the CDATA block.
        returns[1]{name,type,description}:
          wrapped_content,str,A new string containing the original content enclosed within CDATA tags.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.extract_output_content:
      identifier: backend.converter.extract_output_content
      description:
        overall: "This function processes a list of output objects, typically from a notebook cell, to extract their content. It iterates through each output, handling different types such as display data, execute results, streams, and errors. For image data (PNG or JPEG), it decodes the Base64 string, stores the image data in a provided list, and inserts an XML placeholder into the extracted content. Textual content is appended directly, and error outputs are formatted into a string. The function ultimately returns a consolidated list of these extracted content snippets."
        parameters[2]{name,type,description}:
          outputs,list,"A list of output objects, typically from a notebook cell execution, which can contain various data types like text, images, or errors."
          image_list,list,A list that will be populated with dictionaries containing image data (mime_type and Base64 string) as they are extracted and processed.
        returns[1]{name,type,description}:
          extracted_xml_snippets,"List[str]","A list of strings, where each string is either plain text, an XML placeholder for an image, or an error message, representing the extracted content from the outputs."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.process_image:
      identifier: backend.converter.process_image
      description:
        overall: "This function attempts to process an image identified by its MIME type. It checks if the `mime_type` exists within an external `data` object. If found, it retrieves a base64 encoded string, removes newline characters, and then appends a dictionary containing the `mime_type` and cleaned base64 data to an external `image_list`. The function then returns an HTML-like placeholder string with an index and the MIME type. Error handling is included for issues during image decoding. If the `mime_type` is not present in `data`, the function returns `None`."
        parameters[1]{name,type,description}:
          mime_type,str,"The MIME type of the image to be processed, such as 'image/png' or 'image/jpeg'."
        returns[3]{name,type,description}:
          image_placeholder_tag,str,"An HTML-like string representing a placeholder for the processed image, including its index and MIME type, if successful."
          error_message,str,An error message string indicating that the image could not be decoded if an exception occurs during processing.
          None,NoneType,Returns None if the specified mime_type is not found in the external 'data' object.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: "The function `process_image` uses undeclared variables `data` and `image_list`. Without their definition or context, the function's behavior and dependencies cannot be fully analyzed, as it relies on external state."
    backend.converter.convert_notebook_to_xml:
      identifier: backend.converter.convert_notebook_to_xml
      description:
        overall: "This function converts the content of a Jupyter Notebook into an XML string, extracting any embedded images. It processes the input `file_content` by attempting to parse it as a notebook using `nbformat`. If the parsing fails due to a `NotJSONError`, it returns an error message. Otherwise, it iterates through markdown and code cells, wrapping their content in appropriate XML tags. Code cell outputs are also processed, potentially extracting images, and included as XML output cells. The function returns the accumulated XML string and a list of extracted images."
        parameters[1]{name,type,description}:
          file_content,str,"The raw string content of a Jupyter Notebook file, expected to be in JSON format."
        returns[2]{name,type,description}:
          xml_output,str,"A string containing the XML representation of the notebook content, or an error message if the input could not be parsed."
          extracted_images,list,A list of extracted image data or references found within the notebook's output cells. This list will be empty if no images are extracted or if an error occurs.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.process_repo_notebooks:
      identifier: backend.converter.process_repo_notebooks
      description:
        overall: "This function processes a collection of repository files to identify and convert Jupyter notebooks. It filters the input list to find files ending with '.ipynb'. For each identified notebook, it logs the processing activity and then calls an external utility, 'convert_notebook_to_xml', to transform the notebook's content into an XML output and extract any associated images. The results, including the generated XML and images, are stored in a dictionary keyed by the notebook's path. Finally, this dictionary containing all processed notebook data is returned."
        parameters[1]{name,type,description}:
          repo_files,"List[object]",A list of file-like objects from a repository. Each object is expected to have a 'path' attribute (string) indicating its file path and a 'content' attribute (string or bytes) representing its content.
        returns[1]{name,type,description}:
          results,"Dict[str, Dict[str, Any]]","A dictionary where keys are the string paths of the processed notebook files. Each value is a dictionary containing 'xml' (string, the XML representation of the notebook) and 'images' (Any, extracted images from the notebook)."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.create_savings_chart:
      identifier: backend.main.create_savings_chart
      description:
        overall: "This function generates a bar chart comparing two token counts, `json_tokens` and `toon_tokens`, and displays a calculated `savings_percent` in the title. It utilizes `matplotlib.pyplot` to visualize these values with distinct colors, adds labels and a grid, and annotates the bars with their respective integer values. The function sets a title indicating the token comparison and savings percentage. Finally, the generated chart is saved to a specified file path."
        parameters[4]{name,type,description}:
          json_tokens,int,The number of tokens associated with the JSON format.
          toon_tokens,int,The number of tokens associated with the TOON format.
          savings_percent,float,The percentage of token savings to be displayed in the chart's title.
          output_path,str,The file path where the generated bar chart will be saved.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    backend.main.calculate_net_time:
      identifier: backend.main.calculate_net_time
      description:
        overall: "This function calculates the effective duration of a process by subtracting estimated sleep times, which are introduced to handle rate limits. It specifically applies this adjustment logic only if the `model_name` starts with \"gemini-\". If the model is not a 'gemini-' model, or if there are no items, the raw total duration or zero is returned, respectively. Otherwise, it calculates the number of batches, determines the total estimated sleep time based on a fixed sleep duration per batch, and then subtracts this from the total duration to yield the net time."
        parameters[5]{name,type,description}:
          start_time,Any,The starting timestamp or time value for the duration calculation.
          end_time,Any,The ending timestamp or time value for the duration calculation.
          total_items,int,"The total number of items processed, used to determine the number of batches."
          batch_size,int,"The number of items processed per batch, used in conjunction with total_items to calculate batches."
          model_name,str,"The name of the model being used, which determines if rate-limit sleep time adjustments are applied."
        returns[1]{name,type,description}:
          net_time,float,"The calculated net duration, after subtracting estimated sleep times for rate limits, or the total duration if no adjustments are made. Returns 0 if total_items is 0 or if the net time calculation results in a negative value."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.main_workflow:
      identifier: backend.main.main_workflow
      description:
        overall: "This function orchestrates a comprehensive workflow for analyzing a GitHub repository and generating a detailed report using multiple Language Model (LLM) stages. It begins by extracting API keys and model configurations from the input, then clones a specified GitHub repository. It proceeds to extract basic project information, construct a file tree, and perform a relationship analysis. An Abstract Syntax Tree (AST) schema is generated and enriched with the relationship data. The function then prepares and dispatches analysis tasks to a 'Helper LLM' for individual functions and classes, collecting their structured outputs. Finally, it consolidates all gathered information and the Helper LLM results into a structured input for a 'Main LLM', which generates the final project report. The process includes error handling, status updates, token evaluation, and saving the final report along with performance metrics and a token savings chart."
        parameters[4]{name,type,description}:
          input,str,"The user input, expected to contain a GitHub repository URL."
          api_keys,dict,"A dictionary containing various API keys (e.g., 'gemini', 'gpt', 'scadsllm') and base URLs ('scadsllm_base_url', 'ollama')."
          model_names,dict,A dictionary specifying the names of the 'helper' and 'main' LLM models to be used.
          status_callback,None,An optional callback function used to provide progress updates during the workflow execution.
        returns[1]{name,type,description}:
          result,dict,A dictionary containing the 'report' (the final generated report as a string) and 'metrics' (a dictionary of performance and token usage statistics).
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by other functions in the provided context.
      error: null
    backend.main.update_status:
      identifier: backend.main.update_status
      description:
        overall: "This function, `update_status`, is designed to handle status updates within the system. It accepts a message as input. The function first checks if a `status_callback` function is available in its scope. If `status_callback` exists, it is invoked with the provided message. Regardless of whether a callback is present, the function proceeds to log the message at the INFO level using the `logging` module."
        parameters[1]{name,type,description}:
          msg,str,The message string to be used for the status update and logging.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.notebook_workflow:
      identifier: backend.main.notebook_workflow
      description:
        overall: "This function orchestrates a workflow to analyze Jupyter notebooks from a given GitHub repository. It clones the repository, processes notebook files into an XML structure, and extracts basic project information. The function then iterates through each notebook, generating a detailed report for each using a specified Large Language Model (LLM). It handles various LLM API keys and models, constructs LLM payloads, provides status updates via a callback, and logs errors. Finally, it concatenates all individual reports into a single markdown file and returns the final report along with performance metrics."
        parameters[4]{name,type,description}:
          input,str,"The input string, expected to contain a GitHub repository URL from which notebooks will be analyzed."
          api_keys,dict,"A dictionary containing API keys for various Large Language Model (LLM) services, such as 'gpt', 'gemini', 'scadsllm', or 'ollama'."
          model,str,"The name of the LLM model to be used for generating reports (e.g., 'gpt-4', 'gemini-pro')."
          status_callback,callable | None,An optional callback function that receives status messages during the workflow execution for real-time updates.
        returns[2]{name,type,description}:
          report,str,"The concatenated final report generated from all processed notebooks, formatted in markdown."
          metrics,dict,"A dictionary containing performance metrics for the workflow, including 'main_time', 'total_time', and 'main_model'."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.gemini_payload:
      identifier: backend.main.gemini_payload
      description:
        overall: "This function `gemini_payload` is responsible for constructing a multi-modal payload suitable for the Gemini API. It first serializes `basic_info` and `nb_path` into an introductory JSON string. The core logic involves parsing the `xml_content` to identify and replace `<IMAGE_PLACEHOLDER>` tags with actual image data. It uses regular expressions to locate these placeholders, extracting image indices and MIME types. Corresponding image data from the `images` list is then base64 encoded and formatted as image URLs. The function returns a list of dictionaries, interleaving text segments from the XML with these formatted image URLs, ready for a Gemini API request."
        parameters[4]{name,type,description}:
          basic_info,Dict,"A dictionary containing general information about the project or context, which will be serialized into an introductory JSON string."
          nb_path,str,"The file path of the current notebook, included in the introductory context information."
          xml_content,str,"The XML string representing the notebook's structure, potentially containing `<IMAGE_PLACEHOLDER>` tags."
          images,"List[Dict]","A list of dictionaries, where each dictionary contains image data (e.g., base64 string) and metadata, indexed to match placeholders in `xml_content`."
        returns[1]{name,type,description}:
          payload_content,"List[Dict]","A list of dictionaries, each representing a content part for the Gemini API. Each dictionary specifies either 'text' content or an 'image_url' with base64 encoded image data."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.relationship_analyzer.path_to_module:
      identifier: backend.relationship_analyzer.path_to_module
      description:
        overall: "This function transforms a given file system path into its corresponding Python module path. It first computes the relative path of the file from the specified project root, handling potential `ValueError` by using the base filename. It then removes the '.py' extension if present and replaces directory separators with dots. Finally, it removes a trailing '.__init__' to yield the canonical module path."
        parameters[2]{name,type,description}:
          filepath,str,The file system path to be converted into a module path.
          project_root,str,"The root directory of the project, used to determine the relative path of the file."
        returns[1]{name,type,description}:
          module_path,str,The Python module path derived from the input filepath.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.encrypt_text:
      identifier: database.db.encrypt_text
      description:
        overall: "This function, `encrypt_text`, is responsible for encrypting a given string using an external `cipher_suite` object. It first performs a validation check: if the input `text` is empty or if the `cipher_suite` is not initialized, the function returns the original text without any modification. If both conditions are met, the function proceeds to strip any leading or trailing whitespace from the input string. The cleaned string is then encoded into bytes, encrypted by the `cipher_suite`, and finally decoded back into a string before being returned."
        parameters[1]{name,type,description}:
          text,str,The string value to be encrypted.
        returns[1]{name,type,description}:
          encrypted_text,str,"The encrypted version of the input string, or the original string if encryption is skipped due to empty text or uninitialized cipher_suite."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.decrypt_text:
      identifier: database.db.decrypt_text
      description:
        overall: "This function attempts to decrypt a given text string using a globally available `cipher_suite` object. It first checks if the input text or `cipher_suite` is empty; if either is, it returns the original text without modification. If both are present, it proceeds to strip whitespace from the text, encode it, decrypt it using `cipher_suite.decrypt`, and then decode the result back into a string. The function includes error handling, returning the original text if any exception occurs during the decryption process."
        parameters[1]{name,type,description}:
          text,str,The string value to be decrypted.
        returns[1]{name,type,description}:
          decrypted_text,str,"The decrypted string, or the original string if decryption is not performed (due to empty input or cipher_suite) or if an error occurs during decryption."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.insert_user:
      identifier: database.db.insert_user
      description:
        overall: "This function is designed to create a new user record and persist it into a database collection. It accepts a username, a display name, and a plain-text password as input. Before storing, the password is securely hashed using `stauth.Hasher.hash`, and several API key fields are initialized as empty strings. The constructed user dictionary is then inserted into the `dbusers` collection. The function concludes by returning the unique identifier assigned to the newly created user document."
        parameters[3]{name,type,description}:
          username,str,"The unique identifier for the user, which also serves as the document's primary key."
          name,str,The display name of the user.
          password,str,"The plain-text password provided by the user, which will be hashed before storage."
        returns[1]{name,type,description}:
          inserted_id,string,"The unique identifier (e.g., MongoDB ObjectId) of the newly inserted user document."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.fetch_all_users:
      identifier: database.db.fetch_all_users
      description:
        overall: "The `fetch_all_users` function is responsible for retrieving all user records from a database collection. It executes a `find()` operation on the `dbusers` object, which is presumed to be a database collection. The results obtained from this query are then cast into a standard Python list before being returned by the function."
        parameters[0]:
        returns[1]{name,type,description}:
          users,list,A list containing all user records retrieved from the database collection.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_user:
      identifier: database.db.fetch_user
      description:
        overall: This function is designed to retrieve a single user record from a database. It takes a username as input and uses it to query a collection named `dbusers`. The `find_one` method is employed to locate a document where the `_id` field matches the provided username. The function then returns the found user document or `None` if no matching user is located.
        parameters[1]{name,type,description}:
          username,str,"The unique identifier for the user to be fetched, which is used to match the `_id` field in the database."
        returns[1]{name,type,description}:
          user_document,dict | None,"A dictionary representing the user's document if a match is found, otherwise `None`."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_user_name:
      identifier: database.db.update_user_name
      description:
        overall: "This function updates the 'name' field of a user document in the 'dbusers' collection. It identifies the user by their `username`, which is mapped to the MongoDB `_id` field. The function sets the 'name' field to the provided `new_name` using a MongoDB `update_one` operation. It returns the count of documents that were successfully modified."
        parameters[2]{name,type,description}:
          username,str,"The unique identifier of the user, corresponding to the MongoDB `_id` field."
          new_name,str,The new name to set for the user.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_gemini_key:
      identifier: database.db.update_gemini_key
      description:
        overall: "This function is responsible for updating a user's Gemini API key within the database. It accepts a username and the new Gemini API key as input. The provided API key is first stripped of leading/trailing whitespace and then encrypted before storage. The function then performs an update operation on the `dbusers` collection, setting the `gemini_api_key` field for the specified user with the encrypted value. It returns an integer indicating the number of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose Gemini API key needs to be updated.
          gemini_api_key,str,"The new Gemini API key provided by the user, which will be encrypted and stored."
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified in the database by the update operation. Typically 0 or 1.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_gpt_key:
      identifier: database.db.update_gpt_key
      description:
        overall: "This function updates a user's GPT API key in the database. It first encrypts the provided API key, ensuring any leading or trailing whitespace is removed. The encrypted key is then stored in the \"gpt_api_key\" field for the specified username in the \"dbusers\" collection. The function returns the number of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose GPT API key is to be updated.
          gpt_api_key,str,The new GPT API key to be encrypted and stored for the user.
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents modified by the update operation, typically 1 if the user exists and the key was updated, or 0 otherwise."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_ollama_url:
      identifier: database.db.update_ollama_url
      description:
        overall: This function updates the Ollama base URL for a specific user within the database. It takes a username and a new Ollama base URL as input. The provided URL is first stripped of leading/trailing whitespace before being used to update the corresponding user's record. The function then returns the count of documents that were successfully modified.
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose Ollama base URL is to be updated.
          ollama_base_url,str,"The new base URL for Ollama, which will be stripped of whitespace before being stored."
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.update_opensrc_key:
      identifier: database.db.update_opensrc_key
      description:
        overall: "This function, `update_opensrc_key`, is responsible for securely updating a user's OpenSRC API key in a database. It takes a username and the new API key as input. The provided API key is first stripped of whitespace and then encrypted using an external `encrypt_text` function. The encrypted key is subsequently stored in the `dbusers` collection, associated with the specified username. The function returns an integer indicating the number of database documents that were modified by the update operation."
        parameters[2]{name,type,description}:
          username,str,The username of the user whose OpenSRC API key is to be updated.
          opensrc_api_key,str,The new OpenSRC API key to be encrypted and stored for the user.
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents modified by the update operation in the database, typically 0 or 1."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.update_opensrc_url:
      identifier: database.db.update_opensrc_url
      description:
        overall: "This function updates a user's `opensrc_base_url` in a database. It takes a username and a new base URL as input. The function performs an `update_one` operation on the `dbusers` collection, identifying the document by the provided username. It sets the `opensrc_base_url` field to the new URL after stripping any leading or trailing whitespace. The function then returns the count of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose `opensrc_base_url` is to be updated.
          opensrc_base_url,str,The new base URL for opensource projects to be associated with the user. This value will be stripped of whitespace before storage.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_gemini_key:
      identifier: database.db.fetch_gemini_key
      description:
        overall: "This function retrieves the Gemini API key associated with a specific user from a database. It queries the `dbusers` collection, searching for a document where the `_id` matches the provided username. If a user document is found, it extracts and returns the `gemini_api_key` field. If no user is found in the database, the function returns `None`."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose Gemini API key is to be fetched.
        returns[1]{name,type,description}:
          gemini_api_key,str | None,"The Gemini API key string if found for the user, otherwise None."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_ollama_url:
      identifier: database.db.fetch_ollama_url
      description:
        overall: "This function retrieves the Ollama base URL associated with a specific user from a database. It takes a username as input and queries the 'dbusers' collection to find the corresponding user document. If the user is found, it extracts and returns the 'ollama_base_url' field. If the user is not found or the URL is missing, it returns None."
        parameters[1]{name,type,description}:
          username,str,The unique identifier of the user whose Ollama base URL is to be fetched.
        returns[1]{name,type,description}:
          ollama_base_url,str | None,"The Ollama base URL for the specified user, or None if the user is not found or the URL is not set."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.fetch_gpt_key:
      identifier: database.db.fetch_gpt_key
      description:
        overall: "This function is designed to retrieve a user's GPT API key from a database. It accepts a username as input and performs a database query to locate the corresponding user record. The function specifically targets the 'gpt_api_key' field within the user's document. If a user is found and has an associated API key, that key is returned; otherwise, the function returns None."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose GPT API key is to be fetched from the database.
        returns[1]{name,type,description}:
          gpt_api_key,str | None,"The GPT API key associated with the specified username, or None if the user is not found or the key does not exist."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.fetch_opensrc_key:
      identifier: database.db.fetch_opensrc_key
      description:
        overall: This function is designed to retrieve a user's 'opensrc_api_key' from a database. It takes a username as input and queries the 'dbusers' collection to find a matching user document. The function specifically projects the 'opensrc_api_key' field. It returns the found API key or None if the user is not found in the database.
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose Open Source API key is to be fetched.
        returns[1]{name,type,description}:
          opensrc_api_key,str | None,"The Open Source API key associated with the provided username, or None if the user is not found."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_opensrc_url:
      identifier: database.db.fetch_opensrc_url
      description:
        overall: "This function retrieves the 'opensrc_base_url' for a specified user from a database. It queries the 'dbusers' collection, searching for a document where the '_id' matches the provided 'username'. The query is optimized to fetch only the 'opensrc_base_url' field. If a matching user document is found, the function extracts and returns the 'opensrc_base_url'. If no user is found for the given username, the function returns None."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose opensrc_base_url is to be fetched.
        returns[1]{name,type,description}:
          opensrc_base_url,str | None,"The base URL for opensrc associated with the user, or None if the user is not found in the database."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.delete_user:
      identifier: database.db.delete_user
      description:
        overall: "This function, `delete_user`, is designed to remove a specific user record from a database collection. It accepts a username as input, which serves as the primary key (`_id`) to identify the user document. The function then executes a `delete_one` operation on the `dbusers` collection and returns the count of documents that were successfully deleted."
        parameters[1]{name,type,description}:
          username,str,The unique identifier (username) of the user to be deleted from the database.
        returns[1]{name,type,description}:
          deleted_count,int,"The number of documents deleted. This will typically be 1 if a user matching the username was found and deleted, or 0 if no such user was found."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.get_decrypted_api_keys:
      identifier: database.db.get_decrypted_api_keys
      description:
        overall: "This function retrieves and decrypts various API keys and base URLs associated with a specific user from a database. It first attempts to locate the user by their username. If the user is not found, it immediately returns None for all expected values. Otherwise, it fetches the Gemini, GPT, and open-source API keys, decrypting them using `decrypt_text`, and also retrieves the Ollama and open-source base URLs, returning all five values."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose API keys and URLs are to be retrieved.
        returns[5]{name,type,description}:
          gemini_plain,str | None,"The decrypted Gemini API key. Returns an empty string if the key is not found for the user, or None if the user does not exist."
          ollama_plain,str | None,"The Ollama base URL. Returns an empty string if the URL is not found for the user, or None if the user does not exist."
          gpt_plain,str | None,"The decrypted GPT API key. Returns an empty string if the key is not found for the user, or None if the user does not exist."
          opensrc_plain,str | None,"The decrypted open-source API key. Returns an empty string if the key is not found for the user, or None if the user does not exist."
          opensrc_url,str | None,"The open-source base URL. Returns an empty string if the URL is not found for the user, or None if the user does not exist."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.insert_chat:
      identifier: database.db.insert_chat
      description:
        overall: "This function creates a new chat entry in a database. It generates a unique identifier using UUID, records the provided username and chat name, and captures the current timestamp. The constructed chat document is then inserted into the 'dbchats' collection. Finally, it returns the unique ID of the newly inserted chat document."
        parameters[2]{name,type,description}:
          username,str,The username associated with the new chat entry.
          chat_name,str,The name of the chat to be created.
        returns[1]{name,type,description}:
          inserted_id,str,The unique identifier of the newly created chat document in the database.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_chats_by_user:
      identifier: database.db.fetch_chats_by_user
      description:
        overall: "This function retrieves all chat records associated with a given username from the 'dbchats' collection. It queries the collection using the provided username, sorts the results by the 'created_at' field in ascending order, and then converts the resulting cursor into a list. The function's primary purpose is to provide a comprehensive list of chats for a specific user."
        parameters[1]{name,type,description}:
          username,str,The username for which to fetch chat records.
        returns[1]{name,type,description}:
          chats,list,"A list of chat documents (dictionaries) associated with the specified username, sorted by their creation timestamp."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.check_chat_exists:
      identifier: database.db.check_chat_exists
      description:
        overall: "This function determines if a chat entry, identified by a specific username and chat name, exists within the 'dbchats' collection. It performs a database query using `find_one` to locate a document that matches both the provided username and chat name. The function then evaluates if a matching document was found, returning a boolean result. This effectively checks for the presence of a unique chat record."
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to be checked for existence.
          chat_name,str,The name of the chat to be checked for existence.
        returns[1]{name,type,description}:
          exists,bool,True if a chat matching the specified username and chat name is found in the database; False otherwise.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.rename_chat_fully:
      identifier: database.db.rename_chat_fully
      description:
        overall: "This function renames a chat and all its associated message exchanges within a database. It first updates the `chat_name` field for a specific chat entry in the `dbchats` collection, changing it from `old_name` to `new_name` for a given `username`. Subsequently, it updates the `chat_name` for all related message exchanges in the `dbexchanges` collection, ensuring data consistency across both collections. The function returns the count of modified chat entries."
        parameters[3]{name,type,description}:
          username,str,The username associated with the chat to be renamed.
          old_name,str,The current name of the chat.
          new_name,str,The new name for the chat.
        returns[1]{name,type,description}:
          modified_count,int,The number of chat entries modified in the 'dbchats' collection.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.insert_exchange:
      identifier: database.db.insert_exchange
      description:
        overall: "The `insert_exchange` function is responsible for creating and storing a new exchange record in a database. It generates a unique identifier for the exchange and constructs a dictionary containing the question, answer, feedback, user details, chat name, and various optional metrics like time and token usage. A timestamp for creation is also added. The function attempts to insert this structured data into the `dbexchanges` collection. Upon successful insertion, it returns the unique ID of the new record; otherwise, it catches any exceptions, prints an error message, and returns `None`."
        parameters[13]{name,type,description}:
          question,str,The user's question in the exchange.
          answer,str,The generated answer for the question.
          feedback,str,The feedback provided for the exchange.
          username,str,The username associated with this exchange.
          chat_name,str,The name of the chat session where the exchange occurred.
          helper_used,str,"Optional: Identifier for the helper model used in the exchange, defaults to an empty string."
          main_used,str,"Optional: Identifier for the main model used in the exchange, defaults to an empty string."
          total_time,str,"Optional: The total time taken for the exchange, defaults to an empty string."
          helper_time,str,"Optional: The time taken by the helper model, defaults to an empty string."
          main_time,str,"Optional: The time taken by the main model, defaults to an empty string."
          json_tokens,int,"Optional: The number of JSON tokens used, defaults to 0."
          toon_tokens,int,"Optional: The number of Toon tokens used, defaults to 0."
          savings_percent,float,"Optional: The percentage of savings achieved, defaults to 0.0."
        returns[2]{name,type,description}:
          new_id,str,The unique identifier of the newly inserted exchange record upon successful insertion.
          None,"null",Returned if an exception occurs during the database insertion process.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_exchanges_by_user:
      identifier: database.db.fetch_exchanges_by_user
      description:
        overall: "This function is designed to retrieve exchange records from a database based on a given username. It queries a collection, presumably `dbexchanges`, to find all documents where the 'username' field matches the input. The retrieved records are then sorted by their 'created_at' timestamp in ascending order. Finally, the function converts these sorted records into a list and returns them."
        parameters[1]{name,type,description}:
          username,str,The username for which to fetch exchange records.
        returns[1]{name,type,description}:
          exchanges,list,"A list of exchange records (documents) associated with the specified username, sorted by their creation timestamp in ascending order."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_exchanges_by_chat:
      identifier: database.db.fetch_exchanges_by_chat
      description:
        overall: "This function retrieves a list of exchange records from a database collection, identified as `dbexchanges`. It filters these records based on a specific username and chat name provided as arguments. The retrieved exchanges are then sorted by their `created_at` timestamp in ascending order before being returned as a list."
        parameters[2]{name,type,description}:
          username,str,The username used to filter the exchange records.
          chat_name,str,The name of the chat used to filter the exchange records.
        returns[1]{name,type,description}:
          exchanges,list,"A list of exchange documents (dictionaries) that match the provided username and chat name, sorted by their creation timestamp."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_exchange_feedback:
      identifier: database.db.update_exchange_feedback
      description:
        overall: "This function updates the feedback value for a specific exchange record in a database. It takes an exchange identifier and an integer feedback value as input. The function uses a database client, `dbexchanges`, to locate the document by its `_id` and then sets the `feedback` field to the provided integer value. It returns the count of documents that were successfully modified by this operation."
        parameters[2]{name,type,description}:
          exchange_id,Any,"The unique identifier for the exchange record that needs its feedback updated. Its specific type depends on the database's ID format (e.g., ObjectId, string)."
          feedback,int,The integer value representing the feedback to be assigned to the specified exchange record.
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents that were modified by the update operation. A value of 1 indicates success if the document existed, 0 otherwise."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.update_exchange_feedback_message:
      identifier: database.db.update_exchange_feedback_message
      description:
        overall: This function updates an existing exchange record in the database. It identifies the target record using a unique `exchange_id` and then sets or updates its `feedback_message` field with the provided string. The function leverages a database collection named `dbexchanges` to perform a `update_one` operation. It returns the count of documents that were successfully modified by the update operation.
        parameters[2]{name,type,description}:
          exchange_id,Any,The unique identifier for the exchange record to be updated in the database. This is used to locate the specific document.
          feedback_message,str,The new feedback message string to be stored in the specified exchange record.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation. Typically 0 or 1 for `update_one`.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.delete_exchange_by_id:
      identifier: database.db.delete_exchange_by_id
      description:
        overall: This function is designed to remove a specific exchange record from a database collection. It takes an exchange identifier as input and uses it to locate and delete a single document. The function then reports the number of documents successfully deleted.
        parameters[1]{name,type,description}:
          exchange_id,str,The unique string identifier of the exchange document to be deleted from the database.
        returns[1]{name,type,description}:
          deleted_count,int,"The number of documents that were deleted, typically 0 or 1."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.delete_full_chat:
      identifier: database.db.delete_full_chat
      description:
        overall: "This function, `delete_full_chat`, is designed to completely remove a specific chat and all its associated message exchanges from the database. It first deletes all message exchanges linked to the provided username and chat name. Subsequently, it proceeds to delete the chat entry itself from the chat collection. This two-step process ensures data consistency by removing both the chat record and its related messages. The function returns the count of chat documents that were successfully deleted."
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to be deleted.
          chat_name,str,The name of the chat to be deleted.
        returns[1]{name,type,description}:
          deleted_chat_count,int,The number of chat documents successfully deleted from the database.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    frontend.frontend.clean_names:
      identifier: frontend.frontend.clean_names
      description:
        overall: "This function takes a list of strings, where each string is expected to represent a path or a similar structure with segments separated by '/'. It processes each string by splitting it based on the '/' delimiter and then extracts the last segment. The function returns a new list containing these extracted last segments, effectively 'cleaning' the names by removing preceding path information."
        parameters[1]{name,type,description}:
          model_list,"list[str]","A list of strings, where each string is expected to contain path-like segments separated by '/'. For example, a list of file paths or URLs."
        returns[1]{name,type,description}:
          cleaned_names,"list[str]","A new list of strings, where each string is the last segment of the corresponding input string after splitting by '/'."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    frontend.frontend.get_filtered_models:
      identifier: frontend.frontend.get_filtered_models
      description:
        overall: "This function filters a given list of models (`source_list`) based on a specified `category_name`. It retrieves associated keywords for the category from a global `CATEGORY_KEYWORDS` mapping. If the category's keywords include \"STANDARD\", the function returns only those models from the `source_list` that are also present in a `STANDARD_MODELS` list. Otherwise, it iterates through the `source_list` and includes models whose names contain any of the category's keywords (case-insensitively). If no models match the keywords, the original `source_list` is returned."
        parameters[2]{name,type,description}:
          source_list,list,The list of models to be filtered.
          category_name,str,"The name of the category to filter by, used to retrieve associated keywords."
        returns[1]{name,type,description}:
          filtered_models,list,"A list of models filtered according to the specified category's keywords, or the original list if no filters apply or no models match."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.save_gemini_cb:
      identifier: frontend.frontend.save_gemini_cb
      description:
        overall: "This function serves as a callback to save a user's Gemini API key. It retrieves the potential new key from the Streamlit session state. If a valid new key is found, it updates the Gemini key in the database for the current user. Subsequently, it clears the input field in the session state and displays a success toast message to the user."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.save_ollama_cb:
      identifier: frontend.frontend.save_ollama_cb
      description:
        overall: "This function, `save_ollama_cb`, acts as a callback to save a user-provided Ollama URL. It retrieves the potential new URL from the Streamlit session state, specifically from the key 'in_ollama_url'. If a non-empty URL is found, it proceeds to update this URL in the database using `db.update_ollama_url`, associating it with the current user's username from the session state. Upon successful update, a confirmation toast message is displayed to the user via `st.toast`."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    frontend.frontend.load_data_from_db:
      identifier: frontend.frontend.load_data_from_db
      description:
        overall: "This function loads chat and exchange data from the database for a given username. It first checks if the data for the specified user is already loaded in the Streamlit session state. If not, it initializes the session state for chats, then fetches predefined chats and their associated exchanges from the database. It handles cases where exchanges might exist for chats not initially defined and ensures feedback values are standardized. Finally, it creates a default chat if no chats are found and sets the active chat in the session state."
        parameters[1]{name,type,description}:
          username,str,The username for whom to load chat and exchange data from the database.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    frontend.frontend.handle_feedback_change:
      identifier: frontend.frontend.handle_feedback_change
      description:
        overall: "This function, `handle_feedback_change`, is designed to process and persist user feedback for an exchange object within a Streamlit application. It takes an exchange object (`ex`) and a new feedback value (`val`) as input. The function first updates the `feedback` key of the `ex` object with the provided `val`. Subsequently, it calls a database utility function, `db.update_exchange_feedback`, to store this feedback persistently using the exchange's unique identifier. Finally, it triggers a full rerun of the Streamlit application to reflect the changes in the user interface."
        parameters[2]{name,type,description}:
          ex,dict,"The exchange object, expected to be a dictionary containing at least an '_id' for database identification and a 'feedback' key to be updated."
          val,Any,The new feedback value to be assigned to the exchange object.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.handle_delete_exchange:
      identifier: frontend.frontend.handle_delete_exchange
      description:
        overall: "This function is responsible for deleting a specific exchange. It first removes the exchange from the database using its unique identifier. Subsequently, it checks if the associated chat and exchange exist within the Streamlit session state and, if found, removes the exchange from the chat's list of exchanges. Finally, it triggers a Streamlit rerun to update the UI."
        parameters[2]{name,type,description}:
          chat_name,str,The name of the chat from which the exchange is to be potentially removed in the session state.
          ex,dict,"The exchange object to be deleted, expected to contain an '_id' field for database deletion and to be present in the session state."
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.handle_delete_chat:
      identifier: frontend.frontend.handle_delete_chat
      description:
        overall: "This function handles the deletion of a specific chat for a given user. It first removes the chat from the database using `db.delete_full_chat`. Subsequently, it updates the Streamlit session state by removing the chat from `st.session_state.chats`. If other chats remain, the first one becomes the active chat; otherwise, a new default chat named \"Chat 1\" is created, inserted into the database via `db.insert_chat`, and set as the active chat. Finally, `st.rerun()` is called to refresh the Streamlit application."
        parameters[2]{name,type,description}:
          username,str,The username of the user whose chat is being deleted.
          chat_name,str,The name of the chat to be deleted.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.extract_repo_name:
      identifier: frontend.frontend.extract_repo_name
      description:
        overall: "This function extracts a repository name from an input text string. It first searches for a URL within the text using a regular expression. If a URL is found, it parses the URL to extract its path component. The function then takes the last segment of the path as the potential repository name and removes a \".git\" suffix if present. If no URL is found or no valid repository name can be derived, the function returns None."
        parameters[1]{name,type,description}:
          text,str,The input string that may contain a URL from which to extract a repository name.
        returns[1]{name,type,description}:
          repo_name,str | None,"The extracted repository name as a string, or None if no URL or repository name could be identified in the input text."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    frontend.frontend.stream_text_generator:
      identifier: frontend.frontend.stream_text_generator
      description:
        overall: "This function acts as a generator that takes a single string of text and yields its words one by one. It splits the input text by spaces and then iterates through each resulting word. After yielding each word appended with a space, it introduces a small delay using `time.sleep(0.01)` to simulate a streaming effect."
        parameters[1]{name,type,description}:
          text,str,The input string of text to be streamed word by word.
        returns[1]{name,type,description}:
          word_chunk,str,"A single word from the input text, followed by a space, yielded sequentially."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.render_text_with_mermaid:
      identifier: frontend.frontend.render_text_with_mermaid
      description:
        overall: "This function processes a given markdown string, identifying and separating standard markdown content from embedded Mermaid diagram code blocks. It then renders the standard markdown using `st.markdown` (with an option to stream via `st.write_stream`) and attempts to render the Mermaid diagrams using `st_mermaid`. In case of an error during Mermaid rendering, the Mermaid code is displayed as a code block using `st.code`. The function returns early if the input markdown text is empty."
        parameters[2]{name,type,description}:
          markdown_text,str,"The input string containing markdown content, potentially with embedded Mermaid diagram syntax."
          should_stream,bool,A flag indicating whether standard markdown content should be streamed using `st.write_stream` (if True) or rendered directly with `st.markdown` (if False). Defaults to False.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.render_exchange:
      identifier: frontend.frontend.render_exchange
      description:
        overall: "This function `render_exchange` is designed to display a single chat interaction, comprising a user's question and an assistant's answer, within a Streamlit application. It first renders the user's query, followed by the assistant's response. The assistant's message area includes an interactive toolbar with feedback options (like, dislike, comment), a download button for the response, and a delete button. These interactive elements trigger actions such as `handle_feedback_change`, `db.update_exchange_feedback_message`, `st.rerun`, and `handle_delete_exchange`. The function also manages error states by displaying an error message and a delete option. Finally, the assistant's answer content is rendered using `render_text_with_mermaid` within a bordered container."
        parameters[2]{name,type,description}:
          ex,dict,"A dictionary-like object representing a chat exchange, containing keys such as 'question', 'answer', 'feedback' status, 'feedback_message', and a unique identifier '_id'."
          current_chat_name,str,"A string indicating the name of the current chat session, used to provide context for operations like deleting exchanges."
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by other functions in the provided context.
      error: null
  classes:
    backend.AST_Schema.ASTVisitor:
      identifier: backend.AST_Schema.ASTVisitor
      description:
        overall: "The ASTVisitor class, inheriting from `ast.NodeVisitor`, is designed to traverse the Abstract Syntax Tree (AST) of a Python source file. Its primary purpose is to extract structured information about imports, classes, and functions defined within that source code. It systematically builds a `schema` dictionary that categorizes these elements, including details like identifiers, names, docstrings, and source code segments, providing a comprehensive overview of the file's structure."
        init_method:
          description: "The `__init__` method initializes an `ASTVisitor` instance by storing the raw source code, the file's absolute path, and the project's root directory. It calculates the module's qualified path and sets up an empty `schema` dictionary to accumulate discovered imports, functions, and classes. It also initializes `_current_class` to `None` to track the context of the class currently being visited during AST traversal."
          parameters[4]{name,type,description}:
            self,ASTVisitor,The instance of the ASTVisitor class.
            source_code,str,The raw source code of the file being visited.
            file_path,str,The absolute path to the file being visited.
            project_root,str,The root directory of the project.
        methods[5]:
          - identifier: visit_Import
            description:
              overall: "This method processes `ast.Import` nodes, which represent `import module` statements. It iterates through each alias defined in the import statement and appends the full module name to the `schema[\"imports\"]` list. After recording the import, it calls `self.generic_visit(node)` to ensure continued traversal of the AST for any child nodes."
              parameters[2]{name,type,description}:
                self,ASTVisitor,The instance of the ASTVisitor class.
                node,ast.Import,The AST node representing an import statement.
              returns[0]:
              usage_context:
                calls: This method calls `self.generic_visit(node)` to continue AST traversal.
                called_by: This method is called by the `ast.NodeVisitor`'s dispatch mechanism when an `ast.Import` node is encountered during AST traversal.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method processes `ast.ImportFrom` nodes, which represent `from module import name` statements. It iterates through each alias in the import statement and appends the fully qualified name (e.g., `module.alias`) to the `schema[\"imports\"]` list. Following this, it invokes `self.generic_visit(node)` to ensure that any nested AST nodes are also visited."
              parameters[2]{name,type,description}:
                self,ASTVisitor,The instance of the ASTVisitor class.
                node,ast.ImportFrom,The AST node representing a 'from ... import ...' statement.
              returns[0]:
              usage_context:
                calls: This method calls `self.generic_visit(node)` to continue AST traversal.
                called_by: This method is called by the `ast.NodeVisitor`'s dispatch mechanism when an `ast.ImportFrom` node is encountered during AST traversal.
            error: null
          - identifier: visit_ClassDef
            description:
              overall: "This method processes `ast.ClassDef` nodes, which represent class definitions. It constructs a `class_info` dictionary containing the class's fully qualified identifier, its name, docstring, the source code segment, and its start and end line numbers. This `class_info` is then appended to `self.schema[\"classes\"]`. It temporarily sets `self._current_class` to this class's information to correctly attribute nested methods, performs a generic visit to traverse the class's body, and then resets `self._current_class` to `None`."
              parameters[2]{name,type,description}:
                self,ASTVisitor,The instance of the ASTVisitor class.
                node,ast.ClassDef,The AST node representing a class definition.
              returns[0]:
              usage_context:
                calls: "This method calls `path_to_module`, `ast.get_docstring`, `ast.get_source_segment`, and `self.generic_visit(node)`."
                called_by: This method is called by the `ast.NodeVisitor`'s dispatch mechanism when an `ast.ClassDef` node is encountered during AST traversal.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method processes `ast.FunctionDef` nodes, distinguishing between methods defined within a class and standalone functions. If `_current_class` is set, it extracts method-specific details like identifier, name, arguments, docstring, and line numbers, appending them to the `method_context` of the current class. Otherwise, it extracts similar details for a standalone function and appends them to `self.schema[\"functions\"]`. Finally, it calls `self.generic_visit(node)` to continue AST traversal."
              parameters[2]{name,type,description}:
                self,ASTVisitor,The instance of the ASTVisitor class.
                node,ast.FunctionDef,The AST node representing a function definition.
              returns[0]:
              usage_context:
                calls: "This method calls `ast.get_docstring`, `ast.get_source_segment`, and `self.generic_visit(node)`. It also implicitly uses `self.module_path` which is derived from `path_to_module`."
                called_by: "This method is called by the `ast.NodeVisitor`'s dispatch mechanism when an `ast.FunctionDef` node is encountered, and explicitly by `visit_AsyncFunctionDef`."
            error: null
          - identifier: visit_AsyncFunctionDef
            description:
              overall: "This method processes `ast.AsyncFunctionDef` nodes, which represent asynchronous function definitions. It delegates the entire processing logic directly to the `visit_FunctionDef` method. This approach ensures that asynchronous functions are handled consistently with regular functions for the purpose of schema extraction, capturing their details without duplicating code."
              parameters[2]{name,type,description}:
                self,ASTVisitor,The instance of the ASTVisitor class.
                node,ast.AsyncFunctionDef,The AST node representing an asynchronous function definition.
              returns[0]:
              usage_context:
                calls: This method calls `self.visit_FunctionDef(node)`.
                called_by: This method is called by the `ast.NodeVisitor`'s dispatch mechanism when an `ast.AsyncFunctionDef` node is encountered during AST traversal.
            error: null
        usage_context:
          dependencies: "The class does not explicitly list external functional dependencies in the provided context, but it relies on the `ast` module for its core functionality and `path_to_module` for path resolution."
          instantiated_by: The provided context does not specify where this class is instantiated.
      error: null
    backend.AST_Schema.ASTAnalyzer:
      identifier: backend.AST_Schema.ASTAnalyzer
      description:
        overall: "The ASTAnalyzer class is responsible for processing a repository's Python files to build a comprehensive Abstract Syntax Tree (AST) schema and then enriching this schema with inter-component relationship data. It first parses individual Python files to extract AST nodes like functions and classes, and then integrates call and instantiation relationships to provide a holistic view of the codebase's structure and dependencies."
        init_method:
          description: "The constructor for the ASTAnalyzer class currently performs no specific initialization logic, serving as a placeholder for future setup if needed."
          parameters[0]:
        methods[2]:
          - identifier: merge_relationship_data
            description:
              overall: "This method takes a full AST schema and raw relationship data (incoming/outgoing calls) and merges the relationship information into the schema. It iterates through functions and classes within the schema, populating their respective 'calls', 'called_by', and 'instantiated_by' contexts. Additionally, it identifies and lists external dependencies for each class based on its methods' outgoing calls."
              parameters[2]{name,type,description}:
                full_schema,dict,"The complete AST schema of the repository, containing file, function, and class definitions."
                raw_relationships,dict,A dictionary containing raw 'outgoing' and 'incoming' call relationships between identifiers.
              returns[1]{name,type,description}:
                full_schema,dict,"The updated full schema dictionary, now enriched with call and instantiation relationship data."
              usage_context:
                calls: This method does not explicitly call any other functions or methods based on the provided context.
                called_by: This method is not called by any other functions or methods based on the provided context.
            error: null
          - identifier: analyze_repository
            description:
              overall: "This method analyzes a list of file objects from a Git repository to construct an initial AST schema. It filters for Python files, parses their content using the `ast` module, and then uses an `ASTVisitor` to extract structured AST nodes (imports, functions, classes). The extracted data for each file is then added to a `full_schema` dictionary, handling potential parsing errors gracefully."
              parameters[2]{name,type,description}:
                files,list,"A list of file objects, each containing a path and content, to be analyzed."
                repo,GitRepository,"An object representing the Git repository, though its direct use is not evident in the provided snippet beyond potentially providing file objects."
              returns[1]{name,type,description}:
                full_schema,dict,"A dictionary representing the AST schema of the repository, organized by file paths."
              usage_context:
                calls: This method does not explicitly call any other functions or methods based on the provided context.
                called_by: This method is not called by any other functions or methods based on the provided context.
            error: null
        usage_context:
          dependencies: This class has no listed external functional dependencies based on the provided context.
          instantiated_by: This class is not instantiated by any other components based on the provided context.
      error: null
    backend.File_Dependency.FileDependencyGraph:
      identifier: backend.File_Dependency.FileDependencyGraph
      description:
        overall: "The FileDependencyGraph class extends ast.NodeVisitor to construct a graph of file-level import dependencies within a Python repository. It traverses the Abstract Syntax Tree (AST) of Python files, specifically looking for import and from ... import ... statements. The class's core functionality involves accurately resolving both absolute and relative imports, including handling symbols exported via __init__.py files. It maintains an import_dependencies dictionary to store the relationships, mapping each file to the set of modules it imports."
        init_method:
          description: "This constructor initializes the FileDependencyGraph instance. It sets the filename attribute, representing the file being analyzed, and the repo_root attribute, indicating the root directory of the repository. These attributes are crucial for resolving file paths and dependencies within the repository."
          parameters[2]{name,type,description}:
            filename,str,The path or name of the file for which dependencies are being analyzed.
            repo_root,Any,"The root directory of the repository, used for resolving file paths."
        methods[5]:
          - identifier: _resolve_module_name
            description:
              overall: "This method is responsible for resolving relative import statements (e.g., from .. import name). It determines the actual module or symbol names based on the import level and the current file's location within the repository. It first identifies the base directory for the relative import, then checks if the imported names correspond to existing module files or symbols exported by __init__.py files using nested helper functions. If no valid modules or symbols can be resolved, it raises an ImportError."
              parameters[1]{name,type,description}:
                node,ImportFrom,An AST ImportFrom node representing the relative import statement.
              returns[1]{name,type,description}:
                resolved,"list[str]",A list of resolved module or symbol names.
              usage_context:
                calls: "This method calls get_all_temp_files, Path, Path.stem, Path.name, Path.parent, Path.resolve, Path.exists, Path.read_text, parse, walk, isinstance, literal_eval, iskeyword. It also defines and calls nested functions module_file_exists and init_exports_symbol."
                called_by: This method is called by visit_ImportFrom.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method is part of the NodeVisitor pattern and is called when an Import or ImportFrom AST node is encountered. Its primary function is to record import dependencies. It adds the imported module or symbol name to the import_dependencies dictionary, mapping the current filename to a set of its dependencies. If a base_name is provided (typically for from ... import ... statements), it uses that; otherwise, it uses the alias name from the import node."
              parameters[2]{name,type,description}:
                node,Import | ImportFrom,The AST node representing an import statement.
                base_name,str | None,"An optional base name for the module, used when resolving from ... import ... statements. Defaults to None."
              returns[0]:
              usage_context:
                calls: This method calls self.generic_visit.
                called_by: This method is called by visit_ImportFrom.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method, also part of the NodeVisitor pattern, handles ImportFrom AST nodes. It extracts the module name from the import statement. If it's a direct import (e.g., from a.b.c import d), it takes the last part of the module (c) as the base name and delegates to visit_Import. If it's a relative import (e.g., from .. import x), it uses the _resolve_module_name helper to determine the actual module names before calling visit_Import for each resolved name. It includes error handling for failed relative import resolutions."
              parameters[1]{name,type,description}:
                node,ImportFrom,The AST node representing an ImportFrom statement.
              returns[0]:
              usage_context:
                calls: "This method calls str.split, self.visit_Import, self._resolve_module_name, print, self.generic_visit."
                called_by: "This method is not explicitly called by other methods in the provided context, suggesting it is primarily invoked by the AST NodeVisitor framework."
            error: null
          - identifier: module_file_exists
            description:
              overall: "This helper function, nested within _resolve_module_name, checks if a given module or package exists relative to a base directory. It verifies if a file named name.py exists or if a directory named name contains an __init__.py file, indicating a package. This is used to validate potential import targets during relative import resolution."
              parameters[2]{name,type,description}:
                rel_base,Path,The base directory relative to the repository root.
                name,str,The name of the module or package to check.
              returns[1]{name,type,description}:
                exists,bool,"True if the module file or package __init__.py exists, False otherwise."
              usage_context:
                calls: "This method calls Path, Path.exists."
                called_by: This method is called by _resolve_module_name.
            error: null
          - identifier: init_exports_symbol
            description:
              overall: "This helper function, nested within _resolve_module_name, determines if a given symbol is exported by an __init__.py file within a specified relative base directory. It checks if the __init__.py file exists, parses its content, and then inspects for __all__ assignments (using literal_eval) or direct definitions of functions, classes, or assignments matching the symbol name. This helps resolve symbols imported from packages."
              parameters[2]{name,type,description}:
                rel_base,Path,The base directory relative to the repository root.
                symbol,str,The symbol name to check for export.
              returns[1]{name,type,description}:
                exports,bool,"True if the __init__.py exports the symbol, False otherwise."
              usage_context:
                calls: "This method calls Path, Path.exists, Path.read_text, parse, walk, isinstance, literal_eval."
                called_by: This method is called by _resolve_module_name.
            error: null
        usage_context:
          dependencies: This class does not explicitly list any functional dependencies in the provided context.
          instantiated_by: This class is not explicitly listed as being instantiated by any other components in the provided context.
      error: null
    backend.HelperLLM.LLMHelper:
      identifier: backend.HelperLLM.LLMHelper
      description:
        overall: "This class provides a centralized interface for interacting with various Large Language Models (LLMs) to generate structured documentation for Python functions and classes. It handles API key validation, loads system prompts from files, dynamically configures the LLM client based on the specified model (supporting Gemini, OpenAI, Ollama, and custom APIs), and manages batch processing with rate limiting. The class ensures that LLM outputs conform to predefined Pydantic schemas for FunctionAnalysis and ClassAnalysis."
        init_method:
          description: "The constructor initializes the LLMHelper by validating the provided API key and loading system prompts for function and class analysis from specified file paths. It then configures the appropriate LLM client (Google Gemini, OpenAI, Ollama, or a custom API) based on the `model_name` and sets up structured output parsing using Pydantic schemas. Finally, it calls a private method to configure batch processing settings for the chosen model."
          parameters[5]{name,type,description}:
            api_key,str,The API key required for authenticating with the chosen LLM service.
            function_prompt_path,str,The file path to the system prompt used for generating function documentation.
            class_prompt_path,str,The file path to the system prompt used for generating class documentation.
            model_name,str,"The name of the LLM model to use (e.g., 'gemini-2.0-flash-lite', 'gpt-4'). Defaults to 'gemini-2.0-flash-lite'."
            base_url,str,"An optional base URL for custom LLM endpoints, particularly for Ollama or other self-hosted models. Defaults to None."
        methods[3]:
          - identifier: _configure_batch_settings
            description:
              overall: "This private method configures the `batch_size` attribute of the LLMHelper instance based on the provided `model_name`. It assigns different batch sizes for various Gemini, Llama, and GPT models, as well as custom API models, to optimize API calls and respect rate limits. If an unknown model is provided, it defaults to a conservative batch size of 2."
              parameters[1]{name,type,description}:
                model_name,str,The name of the LLM model for which to configure batch settings.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions within its source code."
                called_by: This method is not explicitly called by other functions or methods.
            error: null
          - identifier: generate_for_functions
            description:
              overall: "This method generates structured documentation for a list of function inputs using the configured LLM. It processes inputs in batches, converts them to JSON payloads, and sends them to the `function_llm` for analysis. The method includes error handling for batch calls and implements a waiting period between batches to comply with API rate limits, returning a list of validated `FunctionAnalysis` objects or `None` for failed items."
              parameters[1]{name,type,description}:
                function_inputs,"List[FunctionAnalysisInput]","A list of FunctionAnalysisInput objects, each containing data for a function to be documented."
              returns[1]{name,type,description}:
                None,"List[Optional[FunctionAnalysis]]","A list of FunctionAnalysis objects, where each object represents the structured documentation for a function, or None if an error occurred during its generation."
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions within its source code."
                called_by: This method is not explicitly called by other functions or methods.
            error: null
          - identifier: generate_for_classes
            description:
              overall: "This method generates structured documentation for a list of class inputs using the configured LLM. Similar to `generate_for_functions`, it processes inputs in batches, converts them to JSON payloads, and dispatches them to the `class_llm` for analysis. It incorporates error handling for API calls and includes a rate-limiting delay between batches, returning a list of validated `ClassAnalysis` objects or `None` for failed items."
              parameters[1]{name,type,description}:
                class_inputs,"List[ClassAnalysisInput]","A list of ClassAnalysisInput objects, each containing data for a class to be documented."
              returns[1]{name,type,description}:
                None,"List[Optional[ClassAnalysis]]","A list of ClassAnalysis objects, where each object represents the structured documentation for a class, or None if an error occurred during its generation."
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions within its source code."
                called_by: This method is not explicitly called by other functions or methods.
            error: null
        usage_context:
          dependencies: This class does not have any explicit external dependencies listed in the provided context.
          instantiated_by: This class is not explicitly instantiated by any other components listed in the provided context.
      error: null
    backend.MainLLM.MainLLM:
      identifier: backend.MainLLM.MainLLM
      description:
        overall: "The MainLLM class serves as a central interface for interacting with various Large Language Models (LLMs). It abstracts away the complexities of different LLM providers by dynamically initializing the appropriate client (Gemini, OpenAI, or Ollama) based on configuration parameters like model name and base URL. The class manages a system prompt loaded from a file and offers both synchronous (`call_llm`) and asynchronous streaming (`stream_llm`) methods for sending user input and receiving LLM responses. Its primary responsibility is to provide a unified and flexible way to integrate LLM capabilities into an application."
        init_method:
          description: "The constructor initializes the MainLLM instance by setting up the system prompt from a specified file and configuring the underlying Large Language Model (LLM) client. It supports various LLM providers like Gemini, OpenAI-compatible APIs, and Ollama, dynamically selecting the appropriate client based on the `model_name` and an optional `base_url`. The method also performs validation for the API key and ensures the prompt file exists, raising `ValueError` or `FileNotFoundError` if issues are encountered."
          parameters[4]{name,type,description}:
            api_key,str,The API key required for authenticating with the chosen LLM service.
            prompt_file_path,str,The file path to the system prompt that will be used for LLM interactions.
            model_name,str,"The name of the LLM model to use, defaulting to 'gemini-2.5-pro'. This parameter dictates which LLM client (Gemini, OpenAI, Ollama) is initialized."
            base_url,str | None,"An optional base URL for custom LLM endpoints, particularly for Ollama or OpenAI-compatible services. If not provided, a default Ollama URL might be used."
        methods[2]:
          - identifier: call_llm
            description:
              overall: "This method sends a user input along with the pre-configured system prompt to the initialized LLM for a single, synchronous response. It constructs a list of messages (system and human), invokes the LLM client, and returns the content of the LLM's response. Error handling is included to catch any exceptions during the LLM call, logging them and returning None."
              parameters[1]{name,type,description}:
                user_input,str,The user's query or message to be sent to the LLM.
              returns[1]{name,type,description}:
                content,str | None,"The generated text content from the LLM, or None if an error occurred during the call."
              usage_context:
                calls: This method does not explicitly call other functions or methods according to the provided context.
                called_by: This method is not explicitly called by other functions or methods according to the provided context.
            error: null
          - identifier: stream_llm
            description:
              overall: "This method provides a streaming interface to the LLM, allowing for real-time processing of the LLM's response. It constructs the message payload (system and human messages) and then utilizes the LLM client's `stream` method. The method yields chunks of the LLM's content as they become available, enabling a responsive user experience. Error handling is included to catch exceptions during the streaming process, yielding an error message if one occurs."
              parameters[1]{name,type,description}:
                user_input,str,The user's query or message to be streamed to the LLM.
              returns[1]{name,type,description}:
                chunk.content,"Generator[str, None, None]","A generator that yields string chunks of the LLM's streamed response. In case of an error, it yields a single string containing the error message."
              usage_context:
                calls: This method does not explicitly call other functions or methods according to the provided context.
                called_by: This method is not explicitly called by other functions or methods according to the provided context.
            error: null
        usage_context:
          dependencies: This class does not have explicit external dependencies listed in the provided context.
          instantiated_by: This class is not explicitly instantiated by other components within the provided context.
      error: null
    backend.basic_info.ProjektInfoExtractor:
      identifier: backend.basic_info.ProjektInfoExtractor
      description:
        overall: "The ProjektInfoExtractor class is designed to systematically extract and consolidate fundamental project information from various common project files, including READMEs, pyproject.toml, and requirements.txt. It initializes a structured dictionary to hold project overview and installation details, populating it by parsing these files with a defined priority. The class provides utility methods for content cleaning, file location, and specific Markdown section extraction, culminating in a comprehensive information dictionary."
        init_method:
          description: "The constructor initializes the ProjektInfoExtractor instance. It sets a constant `INFO_NICHT_GEFUNDEN` to 'Information not found' and creates a nested dictionary `self.info` to store extracted project details. This `self.info` dictionary is pre-populated with placeholders using `INFO_NICHT_GEFUNDEN` for various project overview and installation fields, ensuring a consistent initial state."
          parameters[1]{name,type,description}:
            self,ProjektInfoExtractor,The instance of the class.
        methods[7]:
          - identifier: _clean_content
            description:
              overall: "This private utility method is designed to sanitize string content by removing null bytes (\\x00). Null bytes can appear in text due to encoding errors, such as reading a UTF-16 encoded file as UTF-8. The method first checks if the input content is empty; if so, it returns an empty string. Otherwise, it performs a global replacement of all null bytes with an empty string, effectively deleting them, and returns the cleaned string."
              parameters[1]{name,type,description}:
                content,str,The string content to be cleaned.
              returns[1]{name,type,description}:
                cleaned_content,str,The input string with all null bytes removed.
              usage_context:
                calls: This method does not explicitly call other functions or methods.
                called_by: "This method is called by _parse_readme, _parse_toml, and _parse_requirements to preprocess file contents."
            error: null
          - identifier: _finde_datei
            description:
              overall: "This private helper method searches for a specific file within a list of given files based on a list of filename patterns. It iterates through each file in the provided `dateien` list and then through each `pattern` in the `patterns` list. It performs a case-insensitive check to see if the file's path ends with any of the specified patterns. The first file that matches any pattern is returned. If no matching file is found after checking all files against all patterns, the method returns None."
              parameters[2]{name,type,description}:
                patterns,"List[str]","A list of string patterns (e.g., file extensions or names) to search for."
                dateien,"List[Any]","A list of file objects, where each object is expected to have a 'path' attribute."
              returns[1]{name,type,description}:
                matching_file,"Optional[Any]","The first file object that matches any of the provided patterns, or None if no match is found."
              usage_context:
                calls: This method does not explicitly call other functions or methods.
                called_by: "This method is called by extrahiere_info to locate specific project files like README, pyproject.toml, and requirements.txt."
            error: null
          - identifier: _extrahiere_sektion_aus_markdown
            description:
              overall: "This private method extracts text content located under a specific Markdown heading (level 2, '##') within a given string. It takes the full content and a list of keywords. It constructs a regular expression to find '##' followed by any of the keywords, capturing the text that follows until the next '##' heading or the end of the content. The search is case-insensitive and considers newlines. If a match is found, the extracted section is returned after stripping leading/trailing whitespace; otherwise, None is returned."
              parameters[2]{name,type,description}:
                inhalt,str,The full Markdown content string to search within.
                keywords,"List[str]",A list of keywords that identify the target Markdown heading.
              returns[1]{name,type,description}:
                extracted_section,"Optional[str]","The text content found under the matching Markdown heading, or None if no such section is found."
              usage_context:
                calls: "This method calls re.escape, re.compile, and re.search from the 're' module."
                called_by: "This method is called by _parse_readme to extract specific sections like features, tech stack, status, installation instructions, and quick start guides."
            error: null
          - identifier: _parse_readme
            description:
              overall: "This private method parses the content of a README file to extract various project information, populating the `self.info` dictionary. It first cleans the content using `_clean_content`. It then attempts to extract the project title from a level 1 Markdown heading ('#'). Subsequently, it tries to find a general project description. The method heavily relies on `_extrahiere_sektion_aus_markdown` to find and extract specific sections like 'Key Features', 'Tech Stack', 'Status', 'Installation', and 'Quick Start' based on predefined keyword lists. Information is only updated if the corresponding field in `self.info` is still set to `INFO_NICHT_GEFUNDEN`, ensuring that information from higher-priority sources (like pyproject.toml) is not overwritten."
              parameters[1]{name,type,description}:
                inhalt,str,The raw string content of the README file.
              returns[0]:
              usage_context:
                calls: "This method calls _clean_content and _extrahiere_sektion_aus_markdown from the same class, and re.search from the 're' module."
                called_by: "This method is called by extrahiere_info after pyproject.toml and requirements.txt have been parsed, giving it lower priority for certain fields."
            error: null
          - identifier: _parse_toml
            description:
              overall: "This private method parses the content of a pyproject.toml file to extract project metadata. It first cleans the input content using `_clean_content`. It checks for the availability of the `tomllib` module and returns early with a warning if it's not installed. If `tomllib` is available, it attempts to load the TOML content into a Python dictionary. It then extracts the project 'name', 'description', and 'dependencies' from the '[project]' section of the TOML data, updating the `self.info` dictionary. Error handling is included to catch `tomllib.TOMLDecodeError` during parsing."
              parameters[1]{name,type,description}:
                inhalt,str,The raw string content of the pyproject.toml file.
              returns[0]:
              usage_context:
                calls: "This method calls _clean_content from the same class, tomllib.loads, and tomllib.TOMLDecodeError from the 'tomllib' module."
                called_by: "This method is called by extrahiere_info to process pyproject.toml content, typically with higher priority for certain fields."
            error: null
          - identifier: _parse_requirements
            description:
              overall: "This private method parses the content of a requirements.txt file to extract project dependencies. It first cleans the input content using `_clean_content`. It only proceeds to extract dependencies if the 'dependencies' field in `self.info` is still at its default `INFO_NICHT_GEFUNDEN` value, ensuring that pyproject.toml dependencies take precedence. It splits the content into lines, filters out empty lines and comments (lines starting with '#'), and then stores the cleaned dependency strings in the `self.info` dictionary."
              parameters[1]{name,type,description}:
                inhalt,str,The raw string content of the requirements.txt file.
              returns[0]:
              usage_context:
                calls: This method calls _clean_content from the same class.
                called_by: "This method is called by extrahiere_info to process requirements.txt content, with lower priority than pyproject.toml for dependencies."
            error: null
          - identifier: extrahiere_info
            description:
              overall: "This public method orchestrates the entire information extraction process from various project files and a repository URL. It first uses `_finde_datei` to locate README, pyproject.toml, and requirements.txt files within the provided list of `dateien`. It then parses these files in a specific order of priority: pyproject.toml first, then requirements.txt, and finally README.md. After parsing, it formats the extracted dependencies into a human-readable string if they were found as a list. Finally, if a `repo_url` is provided and a project title hasn't been found or is generic, it attempts to derive a title from the URL. The method returns the fully populated `self.info` dictionary containing all extracted project details."
              parameters[2]{name,type,description}:
                dateien,"List[Any]","A list of file objects (e.g., from a repository scan), each expected to have 'path' and 'content' attributes."
                repo_url,str,"The URL of the repository, used as a fallback to derive a project title."
              returns[1]{name,type,description}:
                project_info,"Dict[str, Any]","A dictionary containing all extracted project information, including overview, installation details, and dependencies."
              usage_context:
                calls: "This method calls _finde_datei, _parse_toml, _parse_requirements, and _parse_readme from the same class, and os.path.basename and str.removesuffix."
                called_by: "The input context does not specify any callers for this method, implying it's a primary entry point for the class's functionality."
            error: null
        usage_context:
          dependencies: "The class depends on the 're' module for regular expression operations, the 'os' module for path manipulation, and the 'tomllib' module for parsing TOML files. It also uses 'typing' for type hints."
          instantiated_by: "The input context does not specify where this class is instantiated, implying it's a standalone utility or its instantiation points are not tracked in this context."
      error: null
    backend.callgraph.CallGraph:
      identifier: backend.callgraph.CallGraph
      description:
        overall: "The CallGraph class is an ast.NodeVisitor designed to construct a call graph for a given Python source file. It traverses the Abstract Syntax Tree (AST) of a file, identifying function definitions, class definitions, import statements, and function calls. Through its visit_* methods, it maps local definitions, resolves imported names, and records caller-callee relationships in a NetworkX directed graph, ultimately providing a structured representation of how functions and methods interact within the codebase."
        init_method:
          description: "This constructor initializes the CallGraph instance by setting up various internal state variables. It stores the filename, initializes context trackers for the current function and class, and sets up dictionaries and sets for managing local definitions, the NetworkX graph, import mappings, discovered functions, and recorded call edges."
          parameters[1]{name,type,description}:
            filename,str,The path to the source file being analyzed.
        methods[11]:
          - identifier: _recursive_call
            description:
              overall: "This private helper method recursively traverses an AST node to extract the full dotted name components of a function or method call. It handles ast.Call, ast.Name, and ast.Attribute nodes, returning a list of strings representing these components. This is crucial for identifying the target of a call expression."
              parameters[1]{name,type,description}:
                node,ast.AST,"The AST node to be recursively processed, typically representing a call, name, or attribute."
              returns[1]{name,type,description}:
                name_components,"list[str]","A list of string components forming the fully qualified name of the called entity (e.g., ['pkg', 'mod', 'Class', 'method'])."
              usage_context:
                calls: This method calls itself recursively to traverse the AST node structure.
                called_by: This method is called by visit_Call.
            error: null
          - identifier: _resolve_all_callee_names
            description:
              overall: "This private helper method takes a list of potential callee name components and attempts to resolve them to their fully qualified names. It prioritizes resolution by checking local definitions, then import mappings, and finally constructs a full name based on the current file and class context. This ensures that call targets are correctly identified regardless of their origin (local, imported, or within the same file/class)."
              parameters[1]{name,type,description}:
                callee_nodes,"list[list[str]]",A list where each inner list represents the name components of a potential callee.
              returns[1]{name,type,description}:
                resolved_names,"list[str]","A list of fully resolved, fully qualified names for the callees."
              usage_context:
                calls: "This method accesses self.local_defs, self.import_mapping, self.current_class, and self.filename."
                called_by: This method is called by visit_Call.
            error: null
          - identifier: _make_full_name
            description:
              overall: "This private helper method constructs a fully qualified name for a function or method. It prepends the filename and optionally the class_name to the given basename, using '::' as a separator. This ensures a consistent and unique identifier for each callable entity within the call graph."
              parameters[2]{name,type,description}:
                basename,str,The base name of the function or method.
                class_name,str | None,"The name of the class if the entity is a method; otherwise, None."
              returns[1]{name,type,description}:
                full_name,str,The fully qualified name of the function or method.
              usage_context:
                calls: This method accesses self.filename.
                called_by: This method is called by visit_FunctionDef.
            error: null
          - identifier: _current_caller
            description:
              overall: "This private helper method determines the identifier of the current calling context. If a function is currently being visited (i.e., self.current_function is set), it returns that function's full name. Otherwise, it returns a placeholder indicating the global scope within the current file, ensuring all calls are attributed to a source."
              parameters[0]:
              returns[1]{name,type,description}:
                caller_identifier,str,The fully qualified name of the current function or a global scope identifier.
              usage_context:
                calls: This method accesses self.current_function and self.filename.
                called_by: This method is called by visit_Call.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method, overriding ast.NodeVisitor.visit_Import, processes 'import' statements. It extracts the module name and any alias, then stores this mapping in self.import_mapping. This mapping is crucial for resolving fully qualified names of imported modules and functions later during call analysis. After processing, it continues the AST traversal."
              parameters[1]{name,type,description}:
                node,ast.Import,The AST node representing an 'import' statement.
              returns[0]:
              usage_context:
                calls: This method calls self.generic_visit.
                called_by: This method is called by the ast.NodeVisitor traversal mechanism.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method, overriding ast.NodeVisitor.visit_ImportFrom, processes 'from ... import ...' statements. It extracts the module name and any aliases for the imported names, storing them in self.import_mapping. This helps in resolving names that are directly imported from a module, ensuring accurate call graph construction. It then continues the AST traversal."
              parameters[1]{name,type,description}:
                node,ast.ImportFrom,The AST node representing a 'from ... import ...' statement.
              returns[0]:
              usage_context:
                calls: This method accesses self.import_mapping.
                called_by: This method is called by the ast.NodeVisitor traversal mechanism.
            error: null
          - identifier: visit_ClassDef
            description:
              overall: "This method, overriding ast.NodeVisitor.visit_ClassDef, processes class definitions. It temporarily updates self.current_class to the name of the current class before traversing its body. This ensures that any methods defined within the class are correctly associated with their parent class. Upon exiting the class definition, it restores the previous class context."
              parameters[1]{name,type,description}:
                node,ast.ClassDef,The AST node representing a class definition.
              returns[0]:
              usage_context:
                calls: This method calls self.generic_visit.
                called_by: This method is called by the ast.NodeVisitor traversal mechanism.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method, overriding ast.NodeVisitor.visit_FunctionDef, processes function definitions (including methods). It constructs the full qualified name of the function, registers it in self.local_defs and adds it as a node to the NetworkX graph. It updates self.current_function for context, traverses the function's body, adds the function to self.function_set, and then restores the previous function context."
              parameters[1]{name,type,description}:
                node,ast.FunctionDef,The AST node representing a function definition.
              returns[0]:
              usage_context:
                calls: "This method calls self._make_full_name and self.generic_visit. It also interacts with self.local_defs, self.current_class, self.current_function, self.graph.add_node, and self.function_set."
                called_by: This method is called by the ast.NodeVisitor traversal mechanism and by visit_AsyncFunctionDef.
            error: null
          - identifier: visit_AsyncFunctionDef
            description:
              overall: "This method, overriding ast.NodeVisitor.visit_AsyncFunctionDef, processes asynchronous function definitions. It delegates the actual processing to visit_FunctionDef, effectively treating async functions similarly to regular functions for the purpose of call graph construction. This simplifies the logic by reusing the existing function definition handling."
              parameters[1]{name,type,description}:
                node,ast.AsyncFunctionDef,The AST node representing an asynchronous function definition.
              returns[0]:
              usage_context:
                calls: This method calls self.visit_FunctionDef.
                called_by: This method is called by the ast.NodeVisitor traversal mechanism.
            error: null
          - identifier: visit_Call
            description:
              overall: "This method, overriding ast.NodeVisitor.visit_Call, processes function call expressions. It identifies the current caller using _current_caller, extracts the callee's name components using _recursive_call, and resolves the full callee names using _resolve_all_callee_names. Finally, it records the call as an edge in the self.edges dictionary, mapping the caller to its callees. It ensures the caller is initialized in the edges dictionary before adding the callee."
              parameters[1]{name,type,description}:
                node,ast.Call,The AST node representing a function call.
              returns[0]:
              usage_context:
                calls: "This method calls self._current_caller, self._recursive_call, self._resolve_all_callee_names, and self.generic_visit. It also interacts with self.edges."
                called_by: This method is called by the ast.NodeVisitor traversal mechanism.
            error: null
          - identifier: visit_If
            description:
              overall: "This method, overriding ast.NodeVisitor.visit_If, processes 'if' statements. It includes special handling for the 'if __name__ == \"__main__\":' block by temporarily setting self.current_function to '<main_block>'. This ensures that calls within this entry point are correctly attributed. For all other 'if' statements, it simply continues the generic AST traversal."
              parameters[1]{name,type,description}:
                node,ast.If,The AST node representing an 'if' statement.
              returns[0]:
              usage_context:
                calls: This method calls self.generic_visit. It also interacts with self.current_function.
                called_by: This method is called by the ast.NodeVisitor traversal mechanism.
            error: null
        usage_context:
          dependencies: "The class depends on 'ast' for parsing Python code, 'networkx' (aliased as nx) for graph representation, and 'typing.Dict' for type hinting."
          instantiated_by: This class is not explicitly shown to be instantiated by other components in the provided context.
      error: null
    backend.getRepo.RepoFile:
      identifier: backend.getRepo.RepoFile
      description:
        overall: "The RepoFile class represents a single file within a Git repository, providing a structured way to access its metadata and content. It implements lazy loading for the Git blob object, file content, and file size, ensuring that these resources are only retrieved and processed when explicitly requested. This design pattern optimizes performance by avoiding unnecessary data retrieval until needed, making it efficient for handling potentially large repositories or files."
        init_method:
          description: "The `__init__` method initializes a RepoFile object by setting its file path and linking it to a specific Git commit tree. It also sets up internal attributes (`_blob`, `_content`, `_size`) to `None`, indicating that the file's blob, content, and size are to be loaded lazily upon first access."
          parameters[2]{name,type,description}:
            file_path,str,The path to the file within the Git repository.
            commit_tree,git.Tree,The Git Tree object corresponding to the commit from which the file originates.
        methods[6]:
          - identifier: blob
            description:
              overall: "This property provides lazy loading for the Git blob object associated with the file. Upon first access, it checks if the `_blob` attribute is `None`. If so, it attempts to retrieve the corresponding blob from the `_tree` using the file's path. If the file is not found within the commit tree, a `FileNotFoundError` is raised, otherwise the retrieved blob is stored and returned."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",git.Blob,The Git blob object representing the file.
              usage_context:
                calls: "Based on the provided context, this method does not explicitly call any other functions or methods."
                called_by: "Based on the provided context, this method is not explicitly called by any other functions or methods."
            error: null
          - identifier: content
            description:
              overall: "This property provides lazy loading for the decoded content of the file. When accessed for the first time, it ensures that the Git blob object is loaded via the `blob` property. It then reads the data stream from the blob, decodes it using UTF-8 (with error ignoring), and stores the resulting string in the `_content` attribute for subsequent accesses."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",str,The decoded content of the file as a string.
              usage_context:
                calls: "Based on the provided context, this method does not explicitly call any other functions or methods."
                called_by: "Based on the provided context, this method is not explicitly called by any other functions or methods."
            error: null
          - identifier: size
            description:
              overall: "This property provides lazy loading for the size of the file in bytes. Upon its initial access, it ensures that the Git blob object is loaded by calling the `blob` property. It then retrieves the `size` attribute directly from the loaded blob object and stores it in `_size` for future requests, returning the integer value."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",int,The size of the file in bytes.
              usage_context:
                calls: "Based on the provided context, this method does not explicitly call any other functions or methods."
                called_by: "Based on the provided context, this method is not explicitly called by any other functions or methods."
            error: null
          - identifier: analyze_word_count
            description:
              overall: "This method serves as an example analysis function, designed to count the total number of words present within the file's content. It achieves this by accessing the `content` property, which lazily loads the file's text. The content string is then split into words using whitespace as a delimiter, and the length of the resulting list of words is returned."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",int,The total count of words in the file content.
              usage_context:
                calls: "Based on the provided context, this method does not explicitly call any other functions or methods."
                called_by: "Based on the provided context, this method is not explicitly called by any other functions or methods."
            error: null
          - identifier: __repr__
            description:
              overall: "This special method defines the official string representation of the RepoFile object, primarily for debugging and logging purposes. It returns a concise, unambiguous string that includes the class name and the file's path, allowing developers to easily identify the object's state."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",str,"A string representation of the RepoFile object, including its path."
              usage_context:
                calls: "Based on the provided context, this method does not explicitly call any other functions or methods."
                called_by: "Based on the provided context, this method is not explicitly called by any other functions or methods."
            error: null
          - identifier: to_dict
            description:
              overall: "This method converts the RepoFile object into a dictionary representation, facilitating serialization or structured data exchange. It includes essential metadata such as the file's path, its base name, its size (lazy-loaded), and a fixed type 'file'. An optional `include_content` parameter allows for the file's content to be included in the dictionary if set to `True`."
              parameters[1]{name,type,description}:
                include_content,bool,"If True, the file's content will be included in the dictionary."
              returns[1]{name,type,description}:
                data,dict,A dictionary containing the file's metadata and optionally its content.
              usage_context:
                calls: "Based on the provided context, this method does not explicitly call any other functions or methods."
                called_by: "Based on the provided context, this method is not explicitly called by any other functions or methods."
            error: null
        usage_context:
          dependencies: The class has no explicit external dependencies listed in the provided context.
          instantiated_by: The class is not explicitly shown to be instantiated by any other components in the provided context.
      error: null
    backend.getRepo.GitRepository:
      identifier: backend.getRepo.GitRepository
      description:
        overall: "The GitRepository class provides a robust mechanism for interacting with Git repositories. It handles the cloning of a remote repository into a temporary local directory upon instantiation and ensures proper cleanup of this directory when the object is no longer needed, especially when used as a context manager. It offers functionalities to list all files within the repository and to construct a hierarchical representation of the file structure, optionally including file contents."
        init_method:
          description: "The constructor initializes a GitRepository object by cloning the specified remote Git repository into a newly created temporary directory. It sets up various instance attributes such as the repository URL, the path to the temporary directory, the GitPython Repo object, and an empty list to store RepoFile objects. It also captures the latest commit and its tree for subsequent file operations. If cloning fails, it cleans up the temporary directory and raises a RuntimeError."
          parameters[1]{name,type,description}:
            repo_url,str,The URL of the Git repository to be cloned.
        methods[5]:
          - identifier: get_all_files
            description:
              overall: "This method retrieves a list of all file paths present in the cloned Git repository using the `git ls-files` command. For each identified file path, it instantiates a `RepoFile` object, which presumably handles file-specific operations or data. These `RepoFile` instances are then stored in the `self.files` attribute and returned as a list."
              parameters[0]:
              returns[1]{name,type,description}:
                files,"list[RepoFile]","A list of RepoFile instances, each representing a file in the repository."
              usage_context:
                calls: This method calls `self.repo.git.ls_files()` to list files and `RepoFile()` to create file objects.
                called_by: This method is called by `get_file_tree` if the `self.files` attribute is empty.
            error: null
          - identifier: close
            description:
              overall: "This method is responsible for cleaning up the resources used by the GitRepository instance. Specifically, it deletes the temporary directory where the Git repository was cloned. It first checks if `self.temp_dir` is set to ensure there's a directory to remove, then prints a message indicating the deletion, and finally sets `self.temp_dir` to `None` to mark it as cleaned."
              parameters[0]:
              returns[0]:
              usage_context:
                calls: This method calls `print()` for logging purposes.
                called_by: This method is called by the `__exit__` context manager method and by the `__init__` method in case of a cloning error.
            error: null
          - identifier: __enter__
            description:
              overall: "This method implements the `__enter__` part of Python's context manager protocol. When a `GitRepository` object is used in a `with` statement, this method is automatically invoked. Its purpose is to return the instance of the object itself, allowing it to be bound to a variable in the `with` statement for use within the context block."
              parameters[0]:
              returns[1]{name,type,description}:
                self,GitRepository,The instance of the GitRepository class.
              usage_context:
                calls: This method does not explicitly call other methods or functions.
                called_by: This method is implicitly called when the `GitRepository` object is used in a `with` statement.
            error: null
          - identifier: __exit__
            description:
              overall: "This method implements the `__exit__` part of Python's context manager protocol. It is automatically called when the execution leaves the `with` statement block, regardless of whether an exception occurred. Its primary function is to ensure that the `close()` method is invoked to clean up the temporary repository directory, thereby releasing system resources."
              parameters[3]{name,type,description}:
                exc_type,type,"The type of the exception that caused the exit, or None if no exception occurred."
                exc_val,Exception,"The exception instance that caused the exit, or None."
                exc_tb,traceback,"The traceback object associated with the exception, or None."
              returns[0]:
              usage_context:
                calls: This method calls `self.close()` to perform cleanup.
                called_by: This method is implicitly called when exiting a `with` statement where the `GitRepository` object is used.
            error: null
          - identifier: get_file_tree
            description:
              overall: "This method generates a hierarchical dictionary representation of the files within the repository, mimicking a file system tree structure. If the `self.files` list is not already populated, it first calls `get_all_files()` to retrieve them. It then iterates through these `RepoFile` objects, parsing their paths to build a nested dictionary where directories are represented by 'name', 'type', and 'children' keys, and files are added using their `to_dict` method, optionally including their content."
              parameters[1]{name,type,description}:
                include_content,bool,A flag indicating whether the content of each file should be included in its dictionary representation. Defaults to False.
              returns[1]{name,type,description}:
                tree,dict,"A dictionary representing the file tree, with nested dictionaries for directories and file-specific data."
              usage_context:
                calls: This method calls `self.get_all_files()` if `self.files` is empty and `file_obj.to_dict()` for each file.
                called_by: This method is not explicitly called by other methods within the provided class definition.
            error: null
        usage_context:
          dependencies: "This class depends on `tempfile` for temporary directory management, `git.Repo` and `git.GitCommandError` from the GitPython library for Git operations, and `logging` for informational messages."
          instantiated_by: This class is not explicitly instantiated by other components within the provided context.
      error: null
    backend.relationship_analyzer.ProjectAnalyzer:
      identifier: backend.relationship_analyzer.ProjectAnalyzer
      description:
        overall: "The ProjectAnalyzer class is designed to analyze a Python project's source code to build a comprehensive call graph and identify relationships between functions, methods, and classes. It systematically traverses the project directory, parses Python files into Abstract Syntax Trees, collects definitions of callable entities, and then resolves calls made between these entities. The class provides methods to initiate the analysis and retrieve the raw outgoing and incoming call relationships, making it a core component for understanding code structure and dependencies within a project."
        init_method:
          description: "The constructor initializes the ProjectAnalyzer instance by setting up the project's root directory, various data structures to store definitions and call graphs, and a set of directories to ignore during file traversal. It converts the provided project_root to an absolute path and initializes dictionaries for definitions, call graphs, file ASTs, and a set of common directories to exclude from analysis."
          parameters[1]{name,type,description}:
            project_root,str,The root directory of the project to be analyzed.
        methods[6]:
          - identifier: analyze
            description:
              overall: "This method orchestrates the entire project analysis process. It first identifies all Python files within the project root, excluding ignored directories. Then, it iterates through these files twice: once to collect all function, method, and class definitions, and a second time to resolve calls between these definitions. Finally, it clears the stored ASTs to free memory and returns the populated call graph."
              parameters[0]:
              returns[1]{name,type,description}:
                call_graph,defaultdict(list),"A dictionary representing the call graph, where keys are callee identifiers and values are lists of caller information."
              usage_context:
                calls: "This method calls _find_py_files, _collect_definitions, and _resolve_calls."
                called_by: This method is not explicitly called by any other functions or methods in the provided context.
            error: null
          - identifier: get_raw_relationships
            description:
              overall: "This method processes the internal call_graph to generate structured outgoing and incoming relationship dictionaries. It iterates through the call_graph to identify caller-callee pairs. For each valid pair, it populates 'outgoing' with calls made from a caller_id and 'incoming' with calls received by a callee_id. The final output sorts and converts the sets of relationships into lists for consistent representation."
              parameters[0]:
              returns[1]{name,type,description}:
                relationships,dict,"A dictionary containing two keys: 'outgoing' (mapping caller IDs to a sorted list of callee IDs) and 'incoming' (mapping callee IDs to a sorted list of caller IDs)."
              usage_context:
                calls: This method does not explicitly call other functions or methods in the provided context.
                called_by: This method is not explicitly called by any other functions or methods in the provided context.
            error: null
          - identifier: _find_py_files
            description:
              overall: "This private helper method is responsible for recursively traversing the project_root to locate all Python files. It uses os.walk to navigate the directory structure, filtering out directories specified in self.ignore_dirs. For each directory, it updates the 'dirs' list in place to skip ignored directories, and then checks if files end with '.py' before adding their full paths to a list."
              parameters[0]:
              returns[1]{name,type,description}:
                py_files,"list[str]","A list of absolute file paths to all Python files found within the project, excluding ignored directories."
              usage_context:
                calls: This method calls os.walk and os.path.join.
                called_by: This method is called by analyze.
            error: null
          - identifier: _collect_definitions
            description:
              overall: "This private method parses a given Python file to identify and store all function, method, and class definitions. It reads the file, parses its source code into an Abstract Syntax Tree (AST) using ast.parse, and stores the AST in self.file_asts. It then walks the AST to find FunctionDef and ClassDef nodes, determines their fully qualified path names, and stores their file, line number, and type ('function', 'method', 'class') in self.definitions. Error handling is included for file operations and AST parsing."
              parameters[1]{name,type,description}:
                filepath,str,The path to the Python file to be analyzed.
              returns[0]:
              usage_context:
                calls: "This method calls open, ast.parse, path_to_module, ast.walk, _get_parent, and logging.error."
                called_by: This method is called by analyze.
            error: null
          - identifier: _get_parent
            description:
              overall: "This private helper method is designed to find the direct parent AST node of a given child node within a larger AST. It iterates through all nodes in the provided 'tree' and, for each node, checks its children. If a child matches the 'node' parameter, the current parent node is returned. If no parent is found after traversing the entire tree, it returns None."
              parameters[2]{name,type,description}:
                tree,ast.AST,The root of the Abstract Syntax Tree to search within.
                node,ast.AST,The child node for which to find the parent.
              returns[1]{name,type,description}:
                parent_node,ast.AST | None,"The parent AST node if found, otherwise None."
              usage_context:
                calls: This method calls ast.walk and ast.iter_child_nodes.
                called_by: This method is called by _collect_definitions.
            error: null
          - identifier: _resolve_calls
            description:
              overall: "This private method processes a file's AST to identify and resolve function and method calls. It retrieves the AST for the given 'filepath' from self.file_asts. It then instantiates a CallResolverVisitor with the file context, project root, and collected definitions. The visitor traverses the AST to find calls, and the identified calls are then extended into the self.call_graph structure. Error logging is implemented for issues during call resolution."
              parameters[1]{name,type,description}:
                filepath,str,The path to the Python file whose calls need to be resolved.
              returns[0]:
              usage_context:
                calls: "This method calls CallResolverVisitor, resolver.visit, and logging.error."
                called_by: This method is called by analyze.
            error: null
        usage_context:
          dependencies: "This class depends on os for file system operations, ast for parsing Python code, logging for error reporting, and collections.defaultdict for managing graph data structures. It also implicitly depends on path_to_module and CallResolverVisitor which are not defined in the provided source but are used."
          instantiated_by: This class is not explicitly instantiated by any other functions or methods in the provided context.
      error: null
    backend.relationship_analyzer.CallResolverVisitor:
      identifier: backend.relationship_analyzer.CallResolverVisitor
      description:
        overall: "The CallResolverVisitor class is an AST NodeVisitor designed to traverse Python abstract syntax trees to identify and resolve function and method calls. It maintains a scope for imported modules and tracks instance types to accurately determine the fully qualified names of call targets. The visitor records detailed information about each call, including the caller, file, line number, and caller type, facilitating a comprehensive analysis of inter-component relationships within a codebase."
        init_method:
          description: "The constructor initializes the visitor with the file path of the code being analyzed, the project's root directory, and a dictionary of known definitions. It sets up internal state variables for tracking module paths, local scopes, instance types, current class and caller names, and a defaultdict to store identified calls."
          parameters[3]{name,type,description}:
            filepath,str,The absolute path to the Python file currently being processed by the visitor.
            project_root,str,"The root directory of the entire project, used to derive module paths."
            definitions,dict,"A dictionary containing all known qualified definitions (functions, classes, methods) within the project, used for validating call targets."
        methods[7]:
          - identifier: visit_ClassDef
            description:
              overall: "This method is invoked when the AST visitor encounters a class definition. It temporarily updates the `current_class_name` attribute to reflect the class being visited, allowing subsequent nested function definitions to correctly form their fully qualified names. After processing the class's children, it restores the previous class name."
              parameters[1]{name,type,description}:
                node,ast.ClassDef,The AST node representing the class definition.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call any other functions or methods according to the provided context.
                called_by: This method is not explicitly called by any other functions or methods according to the provided context.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method is called when the AST visitor encounters a function definition. It constructs the full qualified identifier for the function, considering whether it's a method within a class or a top-level function. It then updates `current_caller_name` to this identifier for the duration of the function's traversal, restoring the previous caller name afterwards."
              parameters[1]{name,type,description}:
                node,ast.FunctionDef,The AST node representing the function definition.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call any other functions or methods according to the provided context.
                called_by: This method is not explicitly called by any other functions or methods according to the provided context.
            error: null
          - identifier: visit_Call
            description:
              overall: "This method processes function and method call expressions within the AST. It attempts to resolve the fully qualified name of the callee using a helper method. If the callee's qualified name is found within the known project definitions, it records the call, including the file, line number, the current caller's qualified name, and the type of the caller (module, local function, method, or function)."
              parameters[1]{name,type,description}:
                node,ast.Call,The AST node representing the function or method call.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call any other functions or methods according to the provided context.
                called_by: This method is not explicitly called by any other functions or methods according to the provided context.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method handles `import` statements, such as `import module_name` or `import module_name as alias`. It populates the `self.scope` dictionary, mapping the imported name (or alias) to its original module name, which aids in resolving qualified names later."
              parameters[1]{name,type,description}:
                node,ast.Import,The AST node representing an import statement.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call any other functions or methods according to the provided context.
                called_by: This method is not explicitly called by any other functions or methods according to the provided context.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method processes `from ... import ...` statements. It calculates the full module path for the imported names, correctly handling relative imports based on the `node.level`. It then adds these resolved qualified names to the `self.scope` dictionary for subsequent lookup."
              parameters[1]{name,type,description}:
                node,ast.ImportFrom,The AST node representing a 'from ... import ...' statement.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call any other functions or methods according to the provided context.
                called_by: This method is not explicitly called by any other functions or methods according to the provided context.
            error: null
          - identifier: visit_Assign
            description:
              overall: "This method inspects assignment statements to identify class instantiations. If an assignment involves calling a constructor (e.g., `variable = ClassName()`), it resolves the qualified name of the class and stores it in `self.instance_types`, mapping the assigned variable's name to its class's qualified name. This helps in resolving method calls on instances."
              parameters[1]{name,type,description}:
                node,ast.Assign,The AST node representing an assignment statement.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call any other functions or methods according to the provided context.
                called_by: This method is not explicitly called by any other functions or methods according to the provided context.
            error: null
          - identifier: _resolve_call_qname
            description:
              overall: "This private helper method attempts to determine the fully qualified name (QName) of a function or method being called. It handles direct name calls, resolving them against the current scope or local module definitions. It also resolves attribute calls (e.g., `instance.method` or `module.function`) by checking `instance_types` for object instances or `scope` for imported modules."
              parameters[1]{name,type,description}:
                func_node,ast.expr,"The AST node representing the function or method being called, typically an `ast.Name` or `ast.Attribute` node."
              returns[1]{name,type,description}:
                qualified_name,str | None,"The fully qualified name of the called function or method if resolvable, otherwise None."
              usage_context:
                calls: This method does not explicitly call any other functions or methods according to the provided context.
                called_by: This method is not explicitly called by any other functions or methods according to the provided context.
            error: null
        usage_context:
          dependencies: This class does not explicitly depend on any external components according to the provided context.
          instantiated_by: This class is not explicitly instantiated by any known components according to the provided context.
      error: null
    schemas.types.ParameterDescription:
      identifier: schemas.types.ParameterDescription
      description:
        overall: "The ParameterDescription class is a Pydantic BaseModel designed to encapsulate the essential details of a function parameter. It provides a structured format for its name, type, and a descriptive text, ensuring that parameter information is consistently represented and validated. This class serves as a fundamental data structure for describing function signatures within a broader system."
        init_method:
          description: "This class, being a Pydantic BaseModel, automatically generates an `__init__` method. This constructor initializes the instance attributes `name`, `type`, and `description` based on the provided arguments, leveraging Pydantic's validation capabilities to ensure data integrity upon instantiation."
          parameters[3]{name,type,description}:
            name,str,The name of the parameter being described.
            type,str,"The data type of the parameter, typically as a string representation."
            description,str,A textual explanation of the parameter's purpose or role.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies.
          instantiated_by: The instantiation points for this class are not specified in the provided context.
      error: null
    schemas.types.ReturnDescription:
      identifier: schemas.types.ReturnDescription
      description:
        overall: "The ReturnDescription class is a Pydantic BaseModel designed to provide a standardized structure for describing the return value of a function. It encapsulates three essential pieces of information: the name of the return value, its type, and a description explaining its purpose or content. This model is likely used within a larger system to generate documentation or validate function signatures."
        init_method:
          description: "This class, being a Pydantic BaseModel, implicitly generates an __init__ method. This constructor is used to initialize instances of ReturnDescription by providing values for its `name`, `type`, and `description` fields, ensuring data validation according to their specified types."
          parameters[3]{name,type,description}:
            name,str,The name or identifier of the return value.
            type,str,The data type of the return value.
            description,str,A textual description of what the return value represents or its purpose.
        methods[0]:
        usage_context:
          dependencies: The class primarily depends on pydantic.BaseModel for its structural definition and validation capabilities. No other explicit functional dependencies are identified.
          instantiated_by: The context provided does not specify where this class is instantiated.
      error: null
    schemas.types.UsageContext:
      identifier: schemas.types.UsageContext
      description:
        overall: "The UsageContext class is a Pydantic BaseModel designed to describe the calling context of a function or method. It serves as a structured data container, holding two string fields: 'calls' to summarize what the entity calls, and 'called_by' to summarize where the entity is utilized. This class is primarily used for metadata aggregation within a larger system."
        init_method:
          description: "The `__init__` method for `UsageContext` is implicitly generated by Pydantic's BaseModel. It initializes an instance of `UsageContext` by accepting `calls` and `called_by` as string arguments, validating them according to their type hints to ensure data integrity."
          parameters[2]{name,type,description}:
            calls,str,"A string describing the functions, methods, or external entities that this context's subject calls."
            called_by,str,"A string describing the functions, methods, or external entities that call this context's subject."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in the provided context.
          instantiated_by: The provided context does not specify where this class is instantiated.
      error: null
    schemas.types.FunctionDescription:
      identifier: schemas.types.FunctionDescription
      description:
        overall: "The `FunctionDescription` class is a Pydantic BaseModel designed to encapsulate a comprehensive analysis of a Python function. It provides a structured format for detailing a function's high-level purpose, its input parameters, its expected return values, and its operational context within a larger system. This model serves as a standardized data structure for representing and exchanging function analysis results, facilitating automated documentation and understanding."
        init_method:
          description: "As a Pydantic BaseModel, the `__init__` method for `FunctionDescription` is implicitly generated by Pydantic. It initializes an instance of the class by accepting values for its defined fields: `overall`, `parameters`, `returns`, and `usage_context`, ensuring type validation and data integrity upon instantiation."
          parameters[4]{name,type,description}:
            overall,str,A high-level summary describing the function's primary purpose and functionality.
            parameters,"List[ParameterDescription]","A list of `ParameterDescription` objects, each detailing an input parameter of the function, including its name, type, and description."
            returns,"List[ReturnDescription]","A list of `ReturnDescription` objects, each detailing a possible return value of the function, including its type and description."
            usage_context,UsageContext,An object containing information about where the function is called and what other functions or methods it calls internally.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in its provided context.
          instantiated_by: The specific locations where this class is instantiated are not provided in the context.
      error: null
    schemas.types.FunctionAnalysis:
      identifier: schemas.types.FunctionAnalysis
      description:
        overall: "The FunctionAnalysis class serves as a Pydantic model designed to represent the structured analysis of a single function. It encapsulates the function's unique identifier, a detailed `FunctionDescription` object (an external type), and an optional `error` field for reporting issues during analysis. This model is fundamental for standardizing the output of function analysis within a larger system, ensuring a consistent data structure for function-level insights."
        init_method:
          description: "This class does not explicitly define an `__init__` method. As a Pydantic BaseModel, its constructor is implicitly generated to accept and validate values for its defined fields: `identifier`, `description`, and `error`."
          parameters[0]:
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies within the provided context.
          instantiated_by: The instantiation points for this class are not specified within the provided context.
      error: null
    schemas.types.ConstructorDescription:
      identifier: schemas.types.ConstructorDescription
      description:
        overall: The ConstructorDescription class is a Pydantic BaseModel designed to provide a structured representation of a Python class's `__init__` method. It encapsulates a textual summary of the constructor's behavior and a list of detailed descriptions for each of its parameters. This model is crucial for generating comprehensive documentation or for static analysis of class constructors.
        init_method:
          description: "As a Pydantic BaseModel, the `__init__` method for ConstructorDescription is automatically generated. It initializes an instance of the class by accepting and validating the `description` string and a list of `ParameterDescription` objects, assigning them to the corresponding instance attributes."
          parameters[2]{name,type,description}:
            description,str,A string summary detailing the functionality and purpose of the constructor.
            parameters,"List[ParameterDescription]","A list of ParameterDescription objects, each providing details about a specific parameter of the constructor."
        methods[0]:
        usage_context:
          dependencies: "This class depends on `pydantic.BaseModel` for its data structure and validation, and `typing.List` for type hinting. It also implicitly depends on the `ParameterDescription` class, which defines the structure of individual parameters."
          instantiated_by: No specific instantiation points for this class were provided in the input context.
      error: null
    schemas.types.ClassContext:
      identifier: schemas.types.ClassContext
      description:
        overall: The ClassContext class is a Pydantic BaseModel designed to encapsulate metadata about a Python class's operational context. It specifically tracks external dependencies and the locations or components responsible for its instantiation. This model provides a structured way to describe how a class interacts with its environment and where it is utilized within a larger system.
        init_method:
          description: "The `__init__` method for ClassContext, implicitly generated by Pydantic, initializes an instance with two string attributes: `dependencies` and `instantiated_by`. These attributes are directly mapped from the constructor's arguments, setting up the class's contextual description upon creation."
          parameters[2]{name,type,description}:
            dependencies,str,A string describing the external dependencies required by the class.
            instantiated_by,str,A string describing the primary points or modules where this class is instantiated.
        methods[0]:
        usage_context:
          dependencies: No specific external dependencies are listed in the provided context for this class.
          instantiated_by: No specific instantiation points are listed in the provided context for this class.
      error: null
    schemas.types.ClassDescription:
      identifier: schemas.types.ClassDescription
      description:
        overall: "The ClassDescription class is a Pydantic BaseModel designed to encapsulate a comprehensive analysis of another Python class. It acts as a structured data container, holding a high-level overall description, a detailed init_method analysis, a list of methods analyses, and usage_context information regarding dependencies and instantiation points. This model is crucial for standardizing the output of class analysis within a larger system."
        init_method:
          description: "This class, being a Pydantic BaseModel, implicitly generates its constructor. The constructor is responsible for initializing instances of ClassDescription by validating and assigning values to its defined fields: overall, init_method, methods, and usage_context."
          parameters[4]{name,type,description}:
            overall,str,A high-level summary of the class's purpose and functionality.
            init_method,ConstructorDescription,An object containing the description and parameters of the class's constructor.
            methods,"List[FunctionAnalysis]",A list of detailed analyses for each method defined within the class.
            usage_context,ClassContext,An object describing the class's external dependencies and where it is instantiated.
        methods[0]:
        usage_context:
          dependencies: "This class does not explicitly declare external functional dependencies within its source code, relying on its Pydantic BaseModel inheritance and type hints."
          instantiated_by: Information about where this class is instantiated is not provided in the current context.
      error: null
    schemas.types.ClassAnalysis:
      identifier: schemas.types.ClassAnalysis
      description:
        overall: "The `ClassAnalysis` class serves as the main Pydantic model for structuring the comprehensive analysis of a Python class. It encapsulates the class's unique identifier, a detailed `ClassDescription` object containing its constructor and method analyses, and an optional error field. This model is designed to provide a standardized, machine-readable format for representing the output of a class analysis process."
        init_method:
          description: "This class does not explicitly define an `__init__` method. It inherits from `pydantic.BaseModel`, and its constructor is implicitly generated by Pydantic, accepting keyword arguments corresponding to its fields for initialization."
          parameters[3]{name,type,description}:
            identifier,str,A unique string identifying the class being analyzed.
            description,ClassDescription,"An object containing the detailed analysis of the class, including its overall purpose, constructor, and methods."
            error,"Optional[str]",An optional string field to store any error messages encountered during the analysis of the class. Defaults to `None`.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies.
          instantiated_by: The specific points of instantiation for this class are not provided in the current context.
      error: null
    schemas.types.CallInfo:
      identifier: schemas.types.CallInfo
      description:
        overall: "The CallInfo class is a Pydantic BaseModel designed to encapsulate details about a specific code call event. It serves as a structured data container for tracking where a function, method, or module is called or instantiated. This class helps in analyzing code relationships by providing a standardized format for call information, including the file path, the name of the calling entity, the type of call, and the line number."
        init_method:
          description: "The __init__ method for CallInfo is implicitly generated by Pydantic's BaseModel. It initializes an instance of CallInfo by validating and assigning the provided file, function, mode, and line attributes based on their type annotations."
          parameters[4]{name,type,description}:
            file,str,The file path where the call event occurred.
            function,str,The name of the calling function or method.
            mode,str,"The type of call, e.g., 'method', 'function', 'module'."
            line,int,The line number in the file where the call event occurred.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies.
          instantiated_by: The instantiation points for this class are not explicitly provided in the context.
      error: null
    schemas.types.FunctionContextInput:
      identifier: schemas.types.FunctionContextInput
      description:
        overall: "The FunctionContextInput class serves as a Pydantic data model designed to encapsulate the contextual information surrounding a function's interactions. It defines a structured schema for tracking which other functions or methods a given function 'calls' and by which other functions or methods it is 'called_by'. This model is crucial for providing a clear, machine-readable representation of a function's dependencies and usage within a larger system, facilitating analysis and documentation."
        init_method:
          description: "This class, inheriting from Pydantic's BaseModel, implicitly initializes its attributes 'calls' and 'called_by' upon instantiation. Pydantic handles validation and assignment of these fields based on the provided input data, ensuring type correctness for the list of called entities and the list of CallInfo objects for callers."
          parameters[0]:
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list external functional dependencies within the provided context.
          instantiated_by: The specific points of instantiation for this class are not provided in the current context.
      error: null
    schemas.types.FunctionAnalysisInput:
      identifier: schemas.types.FunctionAnalysisInput
      description:
        overall: "This class serves as a Pydantic model defining the structured input required for a function analysis operation. It specifies the expected data fields, including the analysis mode, the function's identifier, its source code, a list of imports, and a specific function context. This model ensures that all necessary data for analysis is provided in a consistent format for further processing."
        init_method:
          description: "The `__init__` method, implicitly provided by Pydantic's BaseModel, initializes an instance of `FunctionAnalysisInput`. It validates and assigns the provided `mode`, `identifier`, `source_code`, `imports`, and `context` attributes based on their defined types and constraints."
          parameters[5]{name,type,description}:
            mode,"Literal[\"function_analysis\"]","Specifies the mode of operation, which must be 'function_analysis'."
            identifier,str,The unique name or identifier of the function to be analyzed.
            source_code,str,The raw source code of the function to be analyzed.
            imports,"List[str]",A list of import statements relevant to the function's source code.
            context,FunctionContextInput,Additional contextual information required for the function analysis.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in the provided context.
          instantiated_by: The provided context does not specify where this class is instantiated.
      error: null
    schemas.types.MethodContextInput:
      identifier: schemas.types.MethodContextInput
      description:
        overall: "The MethodContextInput class is a Pydantic BaseModel designed to encapsulate structured contextual information for a method. It defines fields such as the method's unique identifier, a list of other functions or methods it calls, a list of entities that call it, its arguments, and its docstring. This class serves as a data structure to standardize and transfer method-specific metadata within a larger analysis or documentation generation system."
        init_method:
          description: "The constructor for MethodContextInput is automatically generated by Pydantic. It initializes an instance of the class by validating and assigning values to its defined fields: identifier, calls, called_by, args, and docstring. This ensures that all method context data conforms to the specified types upon instantiation."
          parameters[5]{name,type,description}:
            identifier,str,A unique string identifier for the method.
            calls,"List[str]","A list of identifiers for other methods, classes, or functions that this method calls."
            called_by,"List[CallInfo]",A list of CallInfo objects representing other functions or methods that call this method.
            args,"List[str]",A list of string representations of the arguments accepted by this method.
            docstring,"Optional[str]","The docstring of the method, if available; otherwise, it can be null."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly depend on other components within the provided context.
          instantiated_by: This class is not explicitly shown to be instantiated by other components within the provided context.
      error: null
    schemas.types.ClassContextInput:
      identifier: schemas.types.ClassContextInput
      description:
        overall: "The ClassContextInput class is a Pydantic BaseModel designed to encapsulate structured context information for the analysis of a Python class. It defines the data structure for dependencies, instantiation points, and method-specific contexts. This model serves as an input schema for processes that require detailed contextual data about a class."
        init_method:
          description: "This class does not define an explicit `__init__` method. Pydantic's `BaseModel` handles the initialization of its fields based on the provided type hints, automatically creating attributes for `dependencies`, `instantiated_by`, and `method_context` upon instantiation."
          parameters[0]:
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly declare external functional dependencies within its definition.
          instantiated_by: There is no explicit information provided about where this class is instantiated.
      error: null
    schemas.types.ClassAnalysisInput:
      identifier: schemas.types.ClassAnalysisInput
      description:
        overall: "The ClassAnalysisInput class serves as a Pydantic data model, defining the precise structure and validation rules for input data required to initiate a class analysis process. It encapsulates all necessary information, including the class's identifier, its full source code, associated import statements, and contextual details, ensuring that any incoming data conforms to the expected schema before processing. This model is crucial for robust data handling in a larger AI system focused on code analysis."
        init_method:
          description: "The ClassAnalysisInput class inherits from pydantic.BaseModel and therefore does not define an explicit __init__ method. Pydantic automatically generates a constructor that accepts keyword arguments corresponding to the class's defined fields, enabling robust data validation and parsing upon instantiation."
          parameters[5]{name,type,description}:
            mode,"Literal['class_analysis']","A literal string indicating the type of analysis to be performed, which must be 'class_analysis'."
            identifier,str,The unique name or identifier of the Python class that is the subject of the analysis.
            source_code,str,The complete raw source code string of the class definition to be analyzed.
            imports,"List[str]","A list of strings, each representing an import statement relevant to the source file where the class is defined."
            context,ClassContextInput,"An object containing additional contextual information pertinent to the class, such as its dependencies and where it is instantiated."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in the provided context.
          instantiated_by: The provided context does not specify where this class is instantiated.
      error: null