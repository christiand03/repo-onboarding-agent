basic_info:
  projekt_uebersicht:
    titel: Repo Onboarding Agent üöÄ
    beschreibung: ```
    aktueller_status: Information not found
    key_features: Information not found
    tech_stack: Information not found
  installation:
    dependencies: "- altair==4.2.2\n- annotated-types==0.7.0\n- anyio==4.11.0\n- attrs==25.4.0\n- bcrypt==5.0.0\n- blinker==1.9.0\n- cachetools==6.2.2\n- captcha==0.7.1\n- certifi==2025.11.12\n- cffi==2.0.0\n- charset-normalizer==3.4.4\n- click==8.3.1\n- colorama==0.4.6\n- contourpy==1.3.3\n- cryptography==46.0.3\n- cycler==0.12.1\n- distro==1.9.0\n- dnspython==2.8.0\n- dotenv==0.9.9\n- entrypoints==0.4\n- extra-streamlit-components==0.1.81\n- filetype==1.2.0\n- fonttools==4.61.0\n- gitdb==4.0.12\n- GitPython==3.1.45\n- google-ai-generativelanguage==0.9.0\n- google-api-core==2.28.1\n- google-auth==2.43.0\n- googleapis-common-protos==1.72.0\n- grpcio==1.76.0\n- grpcio-status==1.76.0\n- h11==0.16.0\n- httpcore==1.0.9\n- httpx==0.28.1\n- idna==3.11\n- Jinja2==3.1.6\n- jiter==0.12.0\n- jsonpatch==1.33\n- jsonpointer==3.0.0\n- jsonschema==4.25.1\n- jsonschema-specifications==2025.9.1\n- kiwisolver==1.4.9\n- langchain==1.0.8\n- langchain-core==1.1.0\n- langchain-google-genai==3.1.0\n- langchain-ollama==1.0.0\n- langchain-openai==1.1.0\n- langgraph==1.0.3\n- langgraph-checkpoint==3.0.1\n- langgraph-prebuilt==1.0.5\n- langgraph-sdk==0.2.9\n- langsmith==0.4.46\n- MarkupSafe==3.0.3\n- matplotlib==3.10.7\n- narwhals==2.12.0\n- networkx==3.6\n- numpy==2.3.5\n- ollama==0.6.1\n- openai==2.8.1\n- orjson==3.11.4\n- ormsgpack==1.12.0\n- packaging==25.0\n- pandas==2.3.3\n- pillow==12.0.0\n- proto-plus==1.26.1\n- protobuf==6.33.1\n- pyarrow==21.0.0\n- pyasn1==0.6.1\n- pyasn1_modules==0.4.2\n- pycparser==2.23\n- pydantic==2.12.4\n- pydantic_core==2.41.5\n- pydeck==0.9.1\n- PyJWT==2.10.1\n- pymongo==4.15.4\n- pyparsing==3.2.5\n- python-dateutil==2.9.0.post0\n- python-dotenv==1.2.1\n- pytz==2025.2\n- PyYAML==6.0.3\n- referencing==0.37.0\n- regex==2025.11.3\n- requests==2.32.5\n- requests-toolbelt==1.0.0\n- rpds-py==0.29.0\n- rsa==4.9.1\n- setuptools==75.9.1\n- six==1.17.0\n- smmap==5.0.2\n- sniffio==1.3.1\n- streamlit==1.51.0\n- streamlit-authenticator==0.4.2\n- streamlit-mermaid==0.3.0\n- tenacity==9.1.2\n- tiktoken==0.12.0\n- toml==0.10.2\n- toolz==1.1.0\n- toon_format @ git+https://github.com/toon-format/toon-python.git@9c4f0c0c24f2a0b0b376315f4b8707f8c9006de6\n- tornado==6.5.2\n- tqdm==4.67.1\n- typing-inspection==0.4.2\n- typing_extensions==4.15.0\n- tzdata==2025.2\n- urllib3==2.5.0\n- watchdog==6.0.0\n- xxhash==3.6.0\n- zstandard==0.25.0\n- nbformat"
    setup_anleitung: Information not found
    quick_start_guide: Information not found
file_tree:
  name: root
  type: directory
  children[16]:
    - path: .env.example
      name: .env.example
      size: 48
      type: file
    - path: .gitignore
      name: .gitignore
      size: 184
      type: file
    - name: SystemPrompts
      type: directory
      children[6]{path,name,size,type}:
        SystemPrompts/SystemPromptClassHelperLLM.txt,SystemPromptClassHelperLLM.txt,6645,file
        SystemPrompts/SystemPromptFunctionHelperLLM.txt,SystemPromptFunctionHelperLLM.txt,5546,file
        SystemPrompts/SystemPromptHelperLLM.txt,SystemPromptHelperLLM.txt,9350,file
        SystemPrompts/SystemPromptMainLLM.txt,SystemPromptMainLLM.txt,8380,file
        SystemPrompts/SystemPromptMainLLMToon.txt,SystemPromptMainLLMToon.txt,8456,file
        SystemPrompts/SystemPromptNotebookLLM.txt,SystemPromptNotebookLLM.txt,6921,file
    - path: analysis_output.json
      name: analysis_output.json
      size: 569396
      type: file
    - name: backend
      type: directory
      children[12]{path,name,size,type}:
        backend/AST_Schema.py,AST_Schema.py,6622,file
        backend/File_Dependency.py,File_Dependency.py,7384,file
        backend/HelperLLM.py,HelperLLM.py,17280,file
        backend/MainLLM.py,MainLLM.py,4281,file
        backend/__init__.py,__init__.py,0,file
        backend/basic_info.py,basic_info.py,7284,file
        backend/callgraph.py,callgraph.py,9020,file
        backend/converter.py,converter.py,3594,file
        backend/getRepo.py,getRepo.py,4739,file
        backend/main.py,main.py,25591,file
        backend/relationship_analyzer.py,relationship_analyzer.py,8279,file
        backend/scads_key_test.py,scads_key_test.py,663,file
    - name: database
      type: directory
      children[1]{path,name,size,type}:
        database/db.py,db.py,8222,file
    - name: frontend
      type: directory
      children[4]:
        - name: .streamlit
          type: directory
          children[1]{path,name,size,type}:
            frontend/.streamlit/config.toml,config.toml,165,file
        - path: frontend/__init__.py
          name: __init__.py
          size: 0
          type: file
        - path: frontend/frontend.py
          name: frontend.py
          size: 31176
          type: file
        - name: gifs
          type: directory
          children[1]{path,name,size,type}:
            frontend/gifs/4j.gif,4j.gif,3306723,file
    - name: notizen
      type: directory
      children[8]:
        - path: notizen/Report Agenda.txt
          name: Report Agenda.txt
          size: 3492
          type: file
        - path: notizen/Zwischenpraesentation Agenda.txt
          name: Zwischenpraesentation Agenda.txt
          size: 809
          type: file
        - path: notizen/doc_bestandteile.md
          name: doc_bestandteile.md
          size: 847
          type: file
        - name: grafiken
          type: directory
          children[4]:
            - name: "1"
              type: directory
              children[5]{path,name,size,type}:
                notizen/grafiken/1/File_Dependency_Graph_Repo.dot,File_Dependency_Graph_Repo.dot,1227,file
                notizen/grafiken/1/global_callgraph.png,global_callgraph.png,940354,file
                notizen/grafiken/1/global_graph.png,global_graph.png,4756519,file
                notizen/grafiken/1/global_graph_2.png,global_graph_2.png,242251,file
                notizen/grafiken/1/repo.dot,repo.dot,13870,file
            - name: "2"
              type: directory
              children[12]{path,name,size,type}:
                notizen/grafiken/2/FDG_repo.dot,FDG_repo.dot,9150,file
                notizen/grafiken/2/fdg_graph.png,fdg_graph.png,425374,file
                notizen/grafiken/2/fdg_graph_2.png,fdg_graph_2.png,4744718,file
                notizen/grafiken/2/filtered_callgraph_flask.png,filtered_callgraph_flask.png,614631,file
                notizen/grafiken/2/filtered_callgraph_repo-agent.png,filtered_callgraph_repo-agent.png,280428,file
                notizen/grafiken/2/filtered_callgraph_repo-agent_3.png,filtered_callgraph_repo-agent_3.png,1685838,file
                notizen/grafiken/2/filtered_repo_callgraph_flask.dot,filtered_repo_callgraph_flask.dot,19984,file
                notizen/grafiken/2/filtered_repo_callgraph_repo-agent-3.dot,filtered_repo_callgraph_repo-agent-3.dot,20120,file
                notizen/grafiken/2/filtered_repo_callgraph_repo-agent.dot,filtered_repo_callgraph_repo-agent.dot,3118,file
                notizen/grafiken/2/global_callgraph.png,global_callgraph.png,608807,file
                notizen/grafiken/2/graph_flask.md,graph_flask.md,17398,file
                notizen/grafiken/2/repo.dot,repo.dot,19984,file
            - name: Flask-Repo
              type: directory
              children[65]{path,name,size,type}:
                notizen/grafiken/Flask-Repo/__init__.dot,__init__.dot,89,file
                notizen/grafiken/Flask-Repo/__main__.dot,__main__.dot,87,file
                notizen/grafiken/Flask-Repo/app.dot,app.dot,78,file
                notizen/grafiken/Flask-Repo/auth.dot,auth.dot,1126,file
                notizen/grafiken/Flask-Repo/blog.dot,blog.dot,939,file
                notizen/grafiken/Flask-Repo/blueprints.dot,blueprints.dot,4601,file
                notizen/grafiken/Flask-Repo/cli.dot,cli.dot,8269,file
                notizen/grafiken/Flask-Repo/conf.dot,conf.dot,489,file
                notizen/grafiken/Flask-Repo/config.dot,config.dot,2130,file
                notizen/grafiken/Flask-Repo/conftest.dot,conftest.dot,1756,file
                notizen/grafiken/Flask-Repo/ctx.dot,ctx.dot,3009,file
                notizen/grafiken/Flask-Repo/db.dot,db.dot,782,file
                notizen/grafiken/Flask-Repo/debughelpers.dot,debughelpers.dot,1903,file
                notizen/grafiken/Flask-Repo/factory.dot,factory.dot,220,file
                notizen/grafiken/Flask-Repo/flask.dot,flask.dot,82,file
                notizen/grafiken/Flask-Repo/globals.dot,globals.dot,370,file
                notizen/grafiken/Flask-Repo/hello.dot,hello.dot,152,file
                notizen/grafiken/Flask-Repo/helpers.dot,helpers.dot,2510,file
                notizen/grafiken/Flask-Repo/importerrorapp.dot,importerrorapp.dot,155,file
                notizen/grafiken/Flask-Repo/logging.dot,logging.dot,564,file
                notizen/grafiken/Flask-Repo/make_celery.dot,make_celery.dot,99,file
                notizen/grafiken/Flask-Repo/multiapp.dot,multiapp.dot,88,file
                notizen/grafiken/Flask-Repo/provider.dot,provider.dot,1769,file
                notizen/grafiken/Flask-Repo/scaffold.dot,scaffold.dot,3700,file
                notizen/grafiken/Flask-Repo/sessions.dot,sessions.dot,4178,file
                notizen/grafiken/Flask-Repo/signals.dot,signals.dot,133,file
                notizen/grafiken/Flask-Repo/tag.dot,tag.dot,3750,file
                notizen/grafiken/Flask-Repo/tasks.dot,tasks.dot,312,file
                notizen/grafiken/Flask-Repo/templating.dot,templating.dot,2277,file
                notizen/grafiken/Flask-Repo/test_appctx.dot,test_appctx.dot,2470,file
                notizen/grafiken/Flask-Repo/test_async.dot,test_async.dot,1919,file
                notizen/grafiken/Flask-Repo/test_auth.dot,test_auth.dot,725,file
                notizen/grafiken/Flask-Repo/test_basic.dot,test_basic.dot,17383,file
                notizen/grafiken/Flask-Repo/test_blog.dot,test_blog.dot,1105,file
                notizen/grafiken/Flask-Repo/test_blueprints.dot,test_blueprints.dot,11515,file
                notizen/grafiken/Flask-Repo/test_cli.dot,test_cli.dot,6435,file
                notizen/grafiken/Flask-Repo/test_config.dot,test_config.dot,2854,file
                notizen/grafiken/Flask-Repo/test_config.png,test_config.png,288693,file
                notizen/grafiken/Flask-Repo/test_converters.dot,test_converters.dot,937,file
                notizen/grafiken/Flask-Repo/test_db.dot,test_db.dot,481,file
                notizen/grafiken/Flask-Repo/test_factory.dot,test_factory.dot,201,file
                notizen/grafiken/Flask-Repo/test_helpers.dot,test_helpers.dot,5857,file
                notizen/grafiken/Flask-Repo/test_instance_config.dot,test_instance_config.dot,1249,file
                notizen/grafiken/Flask-Repo/test_js_example.dot,test_js_example.dot,453,file
                notizen/grafiken/Flask-Repo/test_json.dot,test_json.dot,4021,file
                notizen/grafiken/Flask-Repo/test_json_tag.dot,test_json_tag.dot,1452,file
                notizen/grafiken/Flask-Repo/test_logging.dot,test_logging.dot,1348,file
                notizen/grafiken/Flask-Repo/test_regression.dot,test_regression.dot,763,file
                notizen/grafiken/Flask-Repo/test_reqctx.dot,test_reqctx.dot,3809,file
                notizen/grafiken/Flask-Repo/test_request.dot,test_request.dot,668,file
                notizen/grafiken/Flask-Repo/test_session_interface.dot,test_session_interface.dot,667,file
                notizen/grafiken/Flask-Repo/test_signals.dot,test_signals.dot,1671,file
                notizen/grafiken/Flask-Repo/test_subclassing.dot,test_subclassing.dot,612,file
                notizen/grafiken/Flask-Repo/test_templating.dot,test_templating.dot,5129,file
                notizen/grafiken/Flask-Repo/test_testing.dot,test_testing.dot,4081,file
                notizen/grafiken/Flask-Repo/test_user_error_handler.dot,test_user_error_handler.dot,5984,file
                notizen/grafiken/Flask-Repo/test_views.dot,test_views.dot,2641,file
                notizen/grafiken/Flask-Repo/testing.dot,testing.dot,2785,file
                notizen/grafiken/Flask-Repo/typing.dot,typing.dot,86,file
                notizen/grafiken/Flask-Repo/typing_app_decorators.dot,typing_app_decorators.dot,500,file
                notizen/grafiken/Flask-Repo/typing_error_handler.dot,typing_error_handler.dot,420,file
                notizen/grafiken/Flask-Repo/typing_route.dot,typing_route.dot,1717,file
                notizen/grafiken/Flask-Repo/views.dot,views.dot,1125,file
                notizen/grafiken/Flask-Repo/wrappers.dot,wrappers.dot,911,file
                notizen/grafiken/Flask-Repo/wsgi.dot,wsgi.dot,19,file
            - name: Repo-onboarding
              type: directory
              children[15]{path,name,size,type}:
                notizen/grafiken/Repo-onboarding/AST.dot,AST.dot,1361,file
                notizen/grafiken/Repo-onboarding/Frontend.dot,Frontend.dot,538,file
                notizen/grafiken/Repo-onboarding/HelperLLM.dot,HelperLLM.dot,1999,file
                notizen/grafiken/Repo-onboarding/HelperLLM.png,HelperLLM.png,234861,file
                notizen/grafiken/Repo-onboarding/MainLLM.dot,MainLLM.dot,2547,file
                notizen/grafiken/Repo-onboarding/agent.dot,agent.dot,2107,file
                notizen/grafiken/Repo-onboarding/basic_info.dot,basic_info.dot,1793,file
                notizen/grafiken/Repo-onboarding/callgraph.dot,callgraph.dot,1478,file
                notizen/grafiken/Repo-onboarding/getRepo.dot,getRepo.dot,1431,file
                notizen/grafiken/Repo-onboarding/graph_AST.png,graph_AST.png,132743,file
                notizen/grafiken/Repo-onboarding/graph_AST2.png,graph_AST2.png,12826,file
                notizen/grafiken/Repo-onboarding/graph_AST3.png,graph_AST3.png,136016,file
                notizen/grafiken/Repo-onboarding/main.dot,main.dot,1091,file
                notizen/grafiken/Repo-onboarding/tools.dot,tools.dot,1328,file
                notizen/grafiken/Repo-onboarding/types.dot,types.dot,133,file
        - path: notizen/notizen.md
          name: notizen.md
          size: 4937
          type: file
        - path: notizen/paul_notizen.md
          name: paul_notizen.md
          size: 2033
          type: file
        - path: notizen/praesentation_notizen.md
          name: praesentation_notizen.md
          size: 2738
          type: file
        - path: notizen/technische_notizen.md
          name: technische_notizen.md
          size: 3616
          type: file
    - path: output.json
      name: output.json
      size: 454008
      type: file
    - path: output.toon
      name: output.toon
      size: 341854
      type: file
    - path: readme.md
      name: readme.md
      size: 1905
      type: file
    - path: requirements.txt
      name: requirements.txt
      size: 4286
      type: file
    - name: result
      type: directory
      children[28]{path,name,size,type}:
        result/ast_schema_01_12_2025_11-49-24.json,ast_schema_01_12_2025_11-49-24.json,201215,file
        result/notebook_report_23_12_2025_12-56-24_NotebookLLM_gemini-2.5-flash.md,notebook_report_23_12_2025_12-56-24_NotebookLLM_gemini-2.5-flash.md,15776,file
        result/notebook_report_27_12_2025_15-06-09_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-06-09_NotebookLLM_gemini-2.5-flash.md,14804,file
        result/notebook_report_27_12_2025_15-09-29_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-09-29_NotebookLLM_gemini-2.5-flash.md,17598,file
        result/notebook_report_27_12_2025_15-26-34_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-26-34_NotebookLLM_gemini-2.5-flash.md,20458,file
        result/notebook_report_27_12_2025_15-33-06_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-33-06_NotebookLLM_gemini-2.5-flash.md,15268,file
        result/notebook_report_29_12_2025_15-03-21_NotebookLLM_gemini-2.5-flash.md,notebook_report_29_12_2025_15-03-21_NotebookLLM_gemini-2.5-flash.md,16111,file
        result/report_01_12_2025_12-26-46_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_12-26-46_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,79523,file
        result/report_01_12_2025_12-55-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_12-55-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,143883,file
        result/report_01_12_2025_13-37-30_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_13-37-30_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,141924,file
        result/report_01_12_2025_14-15-04_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_14-15-04_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,129176,file
        result/report_01_12_2025_14-42-38_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_14-42-38_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,38933,file
        result/report_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,138015,file
        result/report_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.md,report_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.md,127747,file
        result/report_03_12_2025_22-46-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_03_12_2025_22-46-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,80305,file
        result/report_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,163921,file
        result/report_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.md,report_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.md,128520,file
        result/report_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.md,report_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.md,7245,file
        result/report_14_11_2025_14-52-36.md,report_14_11_2025_14-52-36.md,16393,file
        result/report_14_11_2025_15-21-53.md,report_14_11_2025_15-21-53.md,108585,file
        result/report_14_11_2025_15-26-24.md,report_14_11_2025_15-26-24.md,11659,file
        result/report_21_11_2025_15-43-30.md,report_21_11_2025_15-43-30.md,57940,file
        result/report_21_11_2025_16-06-12.md,report_21_11_2025_16-06-12.md,76423,file
        result/report_22_11_2025_14-01-50_Helper_llama3_Main_geminipro.md,report_22_11_2025_14-01-50_Helper_llama3_Main_geminipro.md,39595,file
        result/report_22_11_2025_14-39-55_Helper_llama3_MainLLM_llama3.md,report_22_11_2025_14-39-55_Helper_llama3_MainLLM_llama3.md,2013,file
        result/result_2025-11-11_12-30-53.md,result_2025-11-11_12-30-53.md,1232,file
        result/result_2025-11-11_12-43-51.md,result_2025-11-11_12-43-51.md,33683,file
        result/result_2025-11-11_12-45-37.md,result_2025-11-11_12-45-37.md,1193,file
    - name: schemas
      type: directory
      children[1]{path,name,size,type}:
        schemas/types.py,types.py,7559,file
    - name: statistics
      type: directory
      children[5]{path,name,size,type}:
        statistics/savings_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.png,savings_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.png,23279,file
        statistics/savings_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.png,savings_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.png,23650,file
        statistics/savings_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.png,savings_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.png,23805,file
        statistics/savings_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.png,savings_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.png,22698,file
        statistics/savings_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.png,savings_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.png,22539,file
    - path: test.json
      name: test.json
      size: 241669
      type: file
ast_schema:
  files:
    "backend/AST_Schema.py":
      ast_nodes:
        imports[3]: ast,os,getRepo.GitRepository
        functions[1]:
          - mode: function_analysis
            identifier: backend.AST_Schema.path_to_module
            name: path_to_module
            args[2]: filepath,project_root
            docstring: Wandelt einen Dateipfad in einen Python-Modulpfad um.
            source_code: "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    \n    module_path = rel_path.replace(os.path.sep, '.')\n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n    return module_path"
            start_line: 5
            end_line: 18
            context:
              calls[0]:
              called_by[0]:
        classes[2]:
          - mode: class_analysis
            identifier: backend.AST_Schema.ASTVisitor
            name: ASTVisitor
            docstring: null
            source_code: "class ASTVisitor(ast.NodeVisitor):\n    def __init__(self, source_code: str, file_path: str, project_root: str):\n        self.source_code = source_code\n        self.file_path = file_path\n        self.project_root = project_root\n        self.module_path = path_to_module(self.file_path, self.project_root)\n        self.schema = {\"imports\": [], \"functions\": [], \"classes\": []}\n        self._current_class = None\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(f\"{node.module}.{alias.name}\")\n        self.generic_visit(node)\n\n    def visit_ClassDef(self, node):\n        class_identifier = f\"{self.module_path}.{node.name}\"\n        class_info = {\n            \"mode\": \"class_analysis\",\n            \"identifier\": class_identifier,\n            \"name\": node.name,\n            \"docstring\": ast.get_docstring(node),\n            \"source_code\": ast.get_source_segment(self.source_code, node),\n            \"start_line\": node.lineno,\n            \"end_line\": node.end_lineno,\n            \"context\": {\n                \"dependencies\": [],\n                \"instantiated_by\": [],\n                \"method_context\": []\n            },   \n        }\n        self.schema[\"classes\"].append(class_info)\n        self._current_class = class_info \n        self.generic_visit(node)\n        self._current_class = None\n\n    def visit_FunctionDef(self, node):\n        if self._current_class:\n            method_identifier = f\"{self._current_class['identifier']}.{node.name}\"\n            method_context_info = {\n                \"identifier\": method_identifier,\n                \"name\": node.name,\n                \"calls\": [],\n                \"called_by\": [],\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n            }\n            self._current_class[\"context\"][\"method_context\"].append(method_context_info)\n        else:\n            func_identifier = f\"{self.module_path}.{node.name}\"\n            func_info = {\n                \"mode\": \"function_analysis\",\n                \"identifier\": func_identifier,\n                \"name\": node.name,\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"source_code\": ast.get_source_segment(self.source_code, node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n                \"context\": {\n                    \"calls\": [],\n                    \"called_by\": []\n                }\n            }\n            self.schema[\"functions\"].append(func_info)\n        self.generic_visit(node)\n    \n    def visit_AsyncFunctionDef(self, node):\n        self.visit_FunctionDef(node)"
            start_line: 20
            end_line: 94
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[6]:
                - identifier: backend.AST_Schema.ASTVisitor.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[4]: self,source_code,file_path,project_root
                  docstring: null
                  start_line: 21
                  end_line: 27
                - identifier: backend.AST_Schema.ASTVisitor.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 29
                  end_line: 32
                - identifier: backend.AST_Schema.ASTVisitor.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 34
                  end_line: 37
                - identifier: backend.AST_Schema.ASTVisitor.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 39
                  end_line: 58
                - identifier: backend.AST_Schema.ASTVisitor.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 60
                  end_line: 91
                - identifier: backend.AST_Schema.ASTVisitor.visit_AsyncFunctionDef
                  name: visit_AsyncFunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 93
                  end_line: 94
          - mode: class_analysis
            identifier: backend.AST_Schema.ASTAnalyzer
            name: ASTAnalyzer
            docstring: null
            source_code: "class ASTAnalyzer:\n\n    def __init__(self):\n        pass\n\n    def merge_relationship_data(self, full_schema: dict, raw_relationships: dict) -> dict:\n\n        outgoing_calls = raw_relationships.get(\"outgoing\", {})\n        incoming_calls = raw_relationships.get(\"incoming\", {})\n\n        for file_path, file_data in full_schema.get(\"files\", {}).items():\n            ast_nodes = file_data.get(\"ast_nodes\", {})\n            \n            for func in ast_nodes.get(\"functions\", []):\n                func_id = func.get(\"identifier\")\n                if func_id:\n                    func[\"context\"][\"calls\"] = outgoing_calls.get(func_id, [])\n                    func[\"context\"][\"called_by\"] = incoming_calls.get(func_id, [])\n\n            for cls in ast_nodes.get(\"classes\", []):\n                cls_id = cls.get(\"identifier\")\n                \n                if cls_id:\n                    cls[\"context\"][\"instantiated_by\"] = incoming_calls.get(cls_id, [])\n\n                class_dependencies = set()\n                \n                for method in cls[\"context\"].get(\"method_context\", []):\n                    method_id = method.get(\"identifier\")\n                    if method_id:\n                        m_calls = outgoing_calls.get(method_id, [])\n                        method[\"calls\"] = m_calls\n                        method[\"called_by\"] = incoming_calls.get(method_id, [])\n                        \n                        for callee in m_calls:\n                            if cls_id and not callee.startswith(f\"{cls_id}.\"):\n                                class_dependencies.add(callee)\n                \n                cls[\"context\"][\"dependencies\"] = sorted(list(class_dependencies))\n        \n        return full_schema\n\n    def analyze_repository(self, files: list, repo: GitRepository) -> dict:\n        full_schema = {\n            \"files\": {}\n        }\n\n        all_paths = [file_obj.path for file_obj in files]\n        if not all_paths:\n            return full_schema\n        \n        project_root = os.path.commonpath(all_paths)\n        if os.path.isfile(project_root):\n            project_root = os.path.dirname(project_root)\n\n        for file_obj in files:\n            if not file_obj.path.endswith('.py'):\n                continue\n            \n            file_content = file_obj.content\n            if not file_content.strip():\n                continue\n\n            try:\n                tree = ast.parse(file_content)\n                visitor = ASTVisitor(\n                    source_code=file_content, \n                    file_path=file_obj.path, \n                    project_root=project_root\n                )\n                visitor.visit(tree)\n                file_schema_nodes = visitor.schema\n\n                if file_schema_nodes[\"imports\"] or file_schema_nodes[\"functions\"] or file_schema_nodes[\"classes\"]:\n                    full_schema[\"files\"][file_obj.path] = {\n                        \"ast_nodes\": file_schema_nodes\n                    }\n\n            except (SyntaxError, ValueError) as e:\n                print(f\"Warnung: Konnte Datei '{file_obj.path}' nicht parsen. Fehler: {e}\")\n\n        return full_schema"
            start_line: 97
            end_line: 178
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[3]:
                - identifier: backend.AST_Schema.ASTAnalyzer.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 99
                  end_line: 100
                - identifier: backend.AST_Schema.ASTAnalyzer.merge_relationship_data
                  name: merge_relationship_data
                  calls[0]:
                  called_by[0]:
                  args[3]: self,full_schema,raw_relationships
                  docstring: null
                  start_line: 102
                  end_line: 137
                - identifier: backend.AST_Schema.ASTAnalyzer.analyze_repository
                  name: analyze_repository
                  calls[0]:
                  called_by[0]:
                  args[3]: self,files,repo
                  docstring: null
                  start_line: 139
                  end_line: 178
    "backend/File_Dependency.py":
      ast_nodes:
        imports[17]: networkx,os,ast.Assign,ast.AST,ast.ClassDef,ast.FunctionDef,ast.Import,ast.ImportFrom,ast.Name,ast.NodeVisitor,ast.literal_eval,ast.parse,ast.walk,keyword.iskeyword,pathlib.Path,getRepo.GitRepository,callgraph.make_safe_dot
        functions[3]:
          - mode: function_analysis
            identifier: backend.File_Dependency.build_file_dependency_graph
            name: build_file_dependency_graph
            args[3]: filename,tree,repo_root
            docstring: null
            source_code: "def build_file_dependency_graph(filename: str, tree: AST, repo_root: str) -> nx.DiGraph:\n    graph = nx.DiGraph()\n\n    tree_visitor = FileDependencyGraph(filename, repo_root)\n    tree_visitor.visit(tree)\n\n    for caller, callees in tree_visitor.import_dependencies.items():\n        graph.add_node(caller)\n        graph.add_nodes_from(callees)\n        for callee in callees:\n            graph.add_edge(caller, callee)\n    \n    return graph"
            start_line: 153
            end_line: 165
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.File_Dependency.build_repository_graph
            name: build_repository_graph
            args[1]: repository
            docstring: null
            source_code: "def build_repository_graph(repository: GitRepository) -> nx.DiGraph:\n    all_files = repository.get_all_files()\n    repo_root = repository.temp_dir\n    global_graph = nx.DiGraph()\n\n    for file in all_files: \n        if not file.path.endswith(\".py\"):\n            continue\n        filename = str(os.path.basename(file.path)).removesuffix(\".py\")\n        tree = parse(file.content)\n        graph = build_file_dependency_graph(filename, tree, repo_root)\n        \n        for node in graph.nodes:\n            global_graph.add_node(node)\n\n        for caller, callee in graph.edges:\n            if callee:\n                global_graph.add_node(callee)\n                global_graph.add_edge(caller, callee)\n    \n    return global_graph"
            start_line: 167
            end_line: 187
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.File_Dependency.get_all_temp_files
            name: get_all_temp_files
            args[1]: directory
            docstring: null
            source_code: "def get_all_temp_files(directory: str) -> list[Path]:\n    root_path = Path(directory).resolve()\n    all_files = [file.relative_to(root_path) for file in root_path.rglob(\"*.py\")]\n\n    return all_files"
            start_line: 189
            end_line: 193
            context:
              calls[0]:
              called_by[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.File_Dependency.FileDependencyGraph
            name: FileDependencyGraph
            docstring: null
            source_code: "class FileDependencyGraph(NodeVisitor):\n\n    import_dependencies: dict[str, set[str]] = {}\n    def __init__(self, filename: str, repo_root):\n        \"\"\"\n        Initialisiert den File Dependency Graphen\n\n        Args:\n\n        \"\"\"\n        self.filename = filename\n        self.repo_root = repo_root\n\n    def _resolve_module_name(self, node: ImportFrom) -> list[str]:\n        \"\"\"\n        L√∂st relative Imports der Form `from .. import name1, name2` auf.\n        Liefert die Liste der tats√§chlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\n        Wirft ImportError, wenn nichts aufgel√∂st werden konnte.\n        \"\"\"\n        level_depth = node.level\n        names = [alias.name for alias in node.names]  \n        all_files = get_all_temp_files(self.repo_root)  \n\n        candidates = [p for p in all_files if p.stem == Path(self.filename).stem or p.name == f\"{self.filename}.py\"]\n\n        if not candidates:\n            raise ImportError(f\"Kann aktuelle Datei '{self.filename}' im Repo nicht finden.\")\n\n\n        candidates.sort(key=lambda p: len(p.parts))\n        current_rel_path = candidates[0]\n        base_dir = current_rel_path.parent\n        if level_depth <= 0:\n            raise ImportError(\"Erwarteter relativer Import (level >= 1).\")\n        for _ in range(level_depth - 1):\n            if base_dir == base_dir.parent and len(base_dir.parts) == 0:\n                raise ImportError(f\"Relative Import-Ebene ({level_depth}) zu gro√ü f√ºr Datei '{current_rel_path}'.\")\n            base_dir = base_dir.parent\n\n        repo_root_path = Path(self.repo_root).resolve()\n        resolved: list[str] = []\n\n        def module_file_exists(rel_base: Path, name: str) -> bool:\n            file_path = repo_root_path / rel_base / f\"{name}.py\"\n            pkg_init = repo_root_path / rel_base / name / \"__init__.py\"\n            return file_path.exists() or pkg_init.exists()\n\n        def init_exports_symbol(rel_base: Path, symbol: str) -> bool:\n            \"\"\"\n            Pr√ºft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\n            oder als Name (funktion/klasse/assign) definiert ist.\n            \"\"\"\n            init_path = repo_root_path / rel_base / \"__init__.py\"\n            if not init_path.exists():\n                return False\n            try:\n                src = init_path.read_text(encoding=\"utf-8\")\n                mod = parse(src, filename=str(init_path))\n            except Exception:\n                return False\n\n            for node_ in walk(mod):\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == \"__all__\":\n                            try:\n                                value = literal_eval(node_.value)\n                                if isinstance(value, (list, tuple)) and symbol in value:\n                                    return True\n                            except Exception:\n                                pass\n                if isinstance(node_, (FunctionDef, ClassDef)) and node_.name == symbol:\n                    return True\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == symbol:\n                            return True\n            return False\n\n        for name in names:\n            if not name.isidentifier() or iskeyword(name):\n                continue\n\n            if module_file_exists(base_dir, name):\n                resolved.append(name)\n                continue\n            if init_exports_symbol(base_dir, name):\n                resolved.append(name)\n                continue\n\n        resolved = sorted(set(resolved))\n\n        if not resolved:\n            raise ImportError(\n                f\"Kein passendes Modul/Symbol f√ºr relative Import-Aufl√∂sung gefunden \"\n                f\"(level={level_depth}, names={names}, base_dir={base_dir})\"\n            )\n\n        return resolved\n    \n    def visit_Import(self, node: Import | ImportFrom, base_name: str | None = None):\n        for alias in node.names:\n        \n            if self.filename not in self.import_dependencies:\n                self.import_dependencies[self.filename] = set()\n\n            if base_name:\n                self.import_dependencies[self.filename].add(base_name)\n            else:\n                self.import_dependencies[self.filename].add(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node: ImportFrom):\n        \"\"\"\n        Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\n        also c, und dieser wird als callee f√ºr den caller, das File, gesetzt.\n        \"\"\"\n        module_name = node.module\n        if module_name:\n            module_base = module_name.split(\".\")[-1]\n            self.visit_Import(node, module_base)\n        else:\n            try:\n                resolved = self._resolve_module_name(node)\n                for base in resolved:\n                    self.visit_Import(node, base)\n            except ImportError as e:\n                print(f\"Aufl√∂sung eines relativen Imports fehlgeschlagen: {e}\")\n\n        self.generic_visit(node)"
            start_line: 22
            end_line: 151
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[6]:
                - identifier: backend.File_Dependency.FileDependencyGraph.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[3]: self,filename,repo_root
                  docstring: "Initialisiert den File Dependency Graphen\n\nArgs:"
                  start_line: 25
                  end_line: 33
                - identifier: backend.File_Dependency.FileDependencyGraph._resolve_module_name
                  name: _resolve_module_name
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: "L√∂st relative Imports der Form `from .. import name1, name2` auf.\nLiefert die Liste der tats√§chlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\nWirft ImportError, wenn nichts aufgel√∂st werden konnte."
                  start_line: 35
                  end_line: 120
                - identifier: backend.File_Dependency.FileDependencyGraph.module_file_exists
                  name: module_file_exists
                  calls[0]:
                  called_by[0]:
                  args[2]: rel_base,name
                  docstring: null
                  start_line: 64
                  end_line: 67
                - identifier: backend.File_Dependency.FileDependencyGraph.init_exports_symbol
                  name: init_exports_symbol
                  calls[0]:
                  called_by[0]:
                  args[2]: rel_base,symbol
                  docstring: "Pr√ºft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\noder als Name (funktion/klasse/assign) definiert ist."
                  start_line: 69
                  end_line: 99
                - identifier: backend.File_Dependency.FileDependencyGraph.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[3]: self,node,base_name
                  docstring: null
                  start_line: 122
                  end_line: 132
                - identifier: backend.File_Dependency.FileDependencyGraph.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: "Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\nalso c, und dieser wird als callee f√ºr den caller, das File, gesetzt."
                  start_line: 134
                  end_line: 151
    "backend/HelperLLM.py":
      ast_nodes:
        imports[23]: os,json,logging,time,typing.List,typing.Dict,typing.Any,typing.Optional,typing.Union,dotenv.load_dotenv,langchain_google_genai.ChatGoogleGenerativeAI,langchain_ollama.ChatOllama,langchain_openai.ChatOpenAI,langchain.messages.HumanMessage,langchain.messages.SystemMessage,langchain.messages.AIMessage,pydantic.ValidationError,schemas.types.FunctionAnalysis,schemas.types.ClassAnalysis,schemas.types.FunctionAnalysisInput,schemas.types.FunctionContextInput,schemas.types.ClassAnalysisInput,schemas.types.ClassContextInput
        functions[1]:
          - mode: function_analysis
            identifier: backend.HelperLLM.main_orchestrator
            name: main_orchestrator
            args[0]:
            docstring: "Dummy Data and processing loop for testing the LLMHelper class.\nThis version is syntactically correct and logically matches the Pydantic models."
            source_code: "def main_orchestrator():\n    \"\"\"\n    Dummy Data and processing loop for testing the LLMHelper class.\n    This version is syntactically correct and logically matches the Pydantic models.\n    \"\"\"\n    \n    # --- Step 1: Define the pre-computed analysis for each method ---\n    \n    # Example Input 1: For the 'add_item' function\n    add_item_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"source_code\": \"\"\"def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\\\"\\\"\\\"\n        if not isinstance(quantity, int) or quantity <= 0:\n            raise ValueError(\"Quantity must be a positive integer.\")\n    \n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n        print(f\"Added {quantity} of {item_name}. New total: {self.inventory[item_name]}\")\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"process_shipment\", \"restock_api_endpoint\"]\n        }\n    })\n\n    # Example Input 2: For the 'check_stock' function\n    check_stock_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"source_code\": \"\"\"def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Retrieves the current stock quantity for a given item.\\\"\\\"\\\"\n        stock_level = self.inventory.get(item_name, 0)\n        return stock_level\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"fulfill_order\", \"ui_display_handler\"]\n        }\n    })\n\n    # Example Input 3: For the 'generate_report' function\n    generate_report_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"source_code\": \"\"\"def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a timestamped string summary of the current inventory state.\\\"\\\"\\\"\n        timestamp = datetime.now()\n        report_header = \"Inventory Report as of: {ts}\\\\n\".format(ts=timestamp)\n        report_lines = [report_header]\n        \n        for item, quantity in self.inventory.items():\n            line = \"- {name}: {qty}\".format(name=item, qty=quantity)\n            report_lines.append(line)\n            \n        return \"\\\\n\".join(report_lines)\"\"\",\n        \"imports\": [\"from datetime import datetime\"],\n        \"context\": {\n            \"calls\": [\"datetime.now\", \"str.format\"],\n            \"called_by\": [\"daily_cron_job\", \"admin_dashboard_export\"]\n        }\n    })\n\n    add_item_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"description\": {\n            \"overall\": \"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to add.\"},\n                {\"name\": \"quantity\", \"type\": \"int\", \"description\": \"The number of units to add. Must be a positive integer.\"}\n            ],\n            \"returns\": [],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"process_shipment and restock_api_endpoint\" }\n        },\n        \"error\": None\n    })\n\n    check_stock_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"description\": {\n            \"overall\": \"Retrieves the current stock quantity for a given item.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to check.\"}\n            ],\n            \"returns\": [\n                {\"name\": \"stock_level\", \"type\": \"int\", \"description\": \"The quantity of the item in stock. Returns 0 if the item is not found.\"}\n            ],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"fulfill_order and ui_display_handler\" }\n        },\n        \"error\": None\n    })\n\n    generate_report_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"description\": {\n            \"overall\": \"Generates a timestamped string summary of the current inventory state.\",\n            \"parameters\": [ {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"} ],\n            \"returns\": [\n                {\"name\": \"report\", \"type\": \"str\", \"description\": \"A formatted string detailing all items and their quantities.\"}\n            ],\n            \"usage_context\": { \"calls\": \"datetime.now and str.format\", \"called_by\": \"daily_cron_job and admin_dashboard_export\" }\n        },\n        \"error\": None\n    })\n\n    class_input = ClassAnalysisInput(\n        mode=\"class_analysis\",\n        identifier=\"InventoryManager\",\n        # NOTE: The indentation of this string is now fixed. It starts at column 0.\n        source_code=\"\"\"class InventoryManager:\n    \\\"\\\"\\\"Manages stock levels for products in a warehouse.\\\"\\\"\\\"\n    def __init__(self, warehouse_id: str):\n        self.warehouse_id = warehouse_id\n        self.inventory = {}  # item_name: quantity\n\n    def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds an item to the inventory.\\\"\\\"\\\"\n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n\n    def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Checks the stock of a specific item.\\\"\\\"\\\"\n        return self.inventory.get(item_name, 0)\n\n    def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a summary report of the inventory.\\\"\\\"\\\"\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        report_lines = [f\"Inventory Report for {self.warehouse_id} at {timestamp}\"]\n        if not self.inventory:\n            report_lines.append(\"Inventory is empty.\")\n        else:\n            for item, quantity in self.inventory.items():\n                report_lines.append(f\"- {item}: {quantity}\")\n        return \"\\\\n\".join(report_lines)\n\"\"\",\n        imports=[\"from datetime import datetime\"],\n        context=ClassContextInput(\n            dependencies=[\"datetime\"],\n            instantiated_by=[\"main_app_startup\", \"warehouse_worker_script\"],\n            methods_analysis=[\n                add_item_analysis,\n                check_stock_analysis,\n                generate_report_analysis\n            ]\n        )\n    )\n\n    \n    # The helper methods expect a LIST of inputs, so we wrap our single object in a list.\n    input = [add_item_input, check_stock_input, generate_report_input]\n    analysis = [add_item_analysis, check_stock_analysis, generate_report_analysis] \n\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    llm_helper = LLMHelper(api_key=GEMINI_API_KEY, function_prompt_path=function_prompt_file, class_prompt_path=class_prompt_file)\n\n    # This will be the final JSON containing all documentation\n    final_documentation = {}\n\n    # The analysis of the methods is already provided in the context.\n    logging.info(\"\\n--- Generating documentation for classes ---\")\n    \n    # The `generate_for_classes` method returns a list of results.\n    analysis_results = llm_helper.generate_for_functions(input)\n\n    # --- Step 4: Process the results ---\n    \n    # We loop through the list of results (even though there's only one in this case).\n    for doc in analysis_results:\n        if doc:\n            logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n            if \"classes\" not in final_documentation:\n                final_documentation[\"classes\"] = {}\n            # Use .model_dump() to convert the Pydantic object back to a dict for JSON serialization\n            final_documentation[\"classes\"][doc.identifier] = doc.model_dump() \n        else:\n            logging.warning(f\"Failed to generate doc for a class\")\n\n    # --- Step 5: Display the final aggregated result ---\n    logging.info(\"\\n--- Final Generated Documentation ---\")\n    print(json.dumps(final_documentation, indent=2))"
            start_line: 227
            end_line: 413
            context:
              calls[0]:
              called_by[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.HelperLLM.LLMHelper
            name: LLMHelper
            docstring: "A class to interact with Google Gemini for generating code snippet documentation.\nIt centralizes API interaction, error handling, and validates I/O using Pydantic."
            source_code: "class LLMHelper:\n    \"\"\"\n    A class to interact with Google Gemini for generating code snippet documentation.\n    It centralizes API interaction, error handling, and validates I/O using Pydantic.\n    \"\"\"\n    def __init__(self, api_key: str, function_prompt_path: str, class_prompt_path: str, model_name: str = \"gemini-2.0-flash-lite\", base_url: str = None):\n        if not api_key:\n            raise ValueError(\"Gemini API Key must be set.\")\n        \n        try:\n            with open(function_prompt_path, 'r', encoding='utf-8') as f:\n                self.function_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Function system prompt file not found at: {function_prompt_path}\")\n            raise\n\n        # Handle the second file\n        try:\n            with open(class_prompt_path, 'r', encoding='utf-8') as f:\n                self.class_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Class system prompt file not found at: {class_prompt_path}\")\n            raise\n        \n        # Batch-Size config\n        self.model_name = model_name\n        self._configure_batch_settings(model_name)\n\n\n        if model_name.startswith(\"gemini-\"):\n            base_llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=0.3, \n            )\n        \n        elif model_name.startswith(\"gpt-\") and \"openGPT\" not in model_name:\n            base_llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=0.3,\n            )\n\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            if not SCADSLLM_URL:\n                raise ValueError(f\"SCADSLLM_URL environment variable is required for model {model_name}\")\n            \n            logging.info(f\"Connecting to Custom API at {SCADSLLM_URL} for model {model_name}\")\n            \n            base_llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                base_url=SCADSLLM_URL,\n                temperature=0.3,\n            )\n\n        else:\n            target_url = base_url if base_url else OLLAMA_BASE_URL\n            logging.info(f\"Using Ollama at {target_url} for model {model_name}\")\n            base_llm = ChatOllama(\n                model=model_name,\n                temperature=0.3,\n                base_url=target_url,\n            )\n\n        self.function_llm = base_llm.with_structured_output(FunctionAnalysis, method=\"json_schema\")\n        self.class_llm = base_llm.with_structured_output(ClassAnalysis, method=\"json_schema\")\n\n        self.raw_llm = base_llm\n\n        logging.info(f\"LLMHelper initialized with model '{model_name}'. Batch Size: {self.batch_size}\")\n\n    def _configure_batch_settings(self, model_name: str):\n\n        if model_name == \"gemini-2.0-flash-lite\":\n            self.batch_size = 30\n        \n        elif model_name == \"gemini-flash-latest\":\n            self.batch_size = 10\n\n        elif model_name == \"gemini-2.5-flash\":\n            self.batch_size = 10\n            \n        elif model_name == \"gemini-2.5-pro\":\n            self.batch_size = 2\n        \n        elif model_name == \"gemini-2.0-flash\":\n            self.batch_size = 15\n\n        elif model_name == \"gemini-2.5-flash-lite\":\n            self.batch_size = 15\n\n        elif model_name == \"llama3\":\n            self.batch_size = 50\n\n        elif model_name == \"gpt-5.1\":\n            self.batch_size = 500\n\n        elif model_name == \"gpt-5-mini\":\n            self.batch_size = 500\n\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            self.batch_size = 500\n            \n        else:\n            logging.warning(f\"Unknown model '{model_name}', using conservative defaults.\")\n            self.batch_size = 2\n\n    def generate_for_functions(self, function_inputs: List[FunctionAnalysisInput]) -> List[Optional[FunctionAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of functions.\"\"\"\n\n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 62\n\n        if not function_inputs:\n            return []\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(function_input.model_dump(), indent=2) \n            for function_input in function_inputs\n        ]\n       \n        conversations = [[SystemMessage(content=self.function_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_functions = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n        \n            try:\n                batch_results = self.function_llm.batch(batch_conversations, config={\"max_concurrency\": self.batch_size})\n                all_validated_functions.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                all_validated_functions.extend([None] * len(batch_conversations))\n            \n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_functions\n        \n            \n    \n\n    def generate_for_classes(self, class_inputs: List[ClassAnalysisInput]) -> List[Optional[ClassAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of classes.\"\"\"\n        if not class_inputs:\n            return []\n        \n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 62\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(class_input.model_dump(), indent=2)\n            for class_input in class_inputs\n        ]\n\n        conversations = [[SystemMessage(content=self.class_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_classes = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n\n            try:\n                batch_results = self.class_llm.batch(batch_conversations , config={\"max_concurrency\": self.batch_size})\n                all_validated_classes.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                # Falls ein Fehler auftritt, f√ºllen wir die Liste mit None auf, um die Reihenfolge zu wahren\n                all_validated_classes.extend([None] * len(batch_conversations))\n\n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_classes"
            start_line: 31
            end_line: 221
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[4]:
                - identifier: backend.HelperLLM.LLMHelper.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[6]: self,api_key,function_prompt_path,class_prompt_path,model_name,base_url
                  docstring: null
                  start_line: 36
                  end_line: 101
                - identifier: backend.HelperLLM.LLMHelper._configure_batch_settings
                  name: _configure_batch_settings
                  calls[0]:
                  called_by[0]:
                  args[2]: self,model_name
                  docstring: null
                  start_line: 103
                  end_line: 137
                - identifier: backend.HelperLLM.LLMHelper.generate_for_functions
                  name: generate_for_functions
                  calls[0]:
                  called_by[0]:
                  args[2]: self,function_inputs
                  docstring: Generates and validates documentation for a batch of functions.
                  start_line: 139
                  end_line: 178
                - identifier: backend.HelperLLM.LLMHelper.generate_for_classes
                  name: generate_for_classes
                  calls[0]:
                  called_by[0]:
                  args[2]: self,class_inputs
                  docstring: Generates and validates documentation for a batch of classes.
                  start_line: 183
                  end_line: 221
    "backend/MainLLM.py":
      ast_nodes:
        imports[9]: os,logging,sys,dotenv.load_dotenv,langchain_google_genai.ChatGoogleGenerativeAI,langchain_ollama.ChatOllama,langchain_openai.ChatOpenAI,langchain.messages.HumanMessage,langchain.messages.SystemMessage
        functions[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.MainLLM.MainLLM
            name: MainLLM
            docstring: Hauptklasse f√ºr die Interaktion mit dem LLM.
            source_code: "class MainLLM:\n    \"\"\"\n    Hauptklasse f√ºr die Interaktion mit dem LLM.\n    \"\"\"\n    def __init__(self, api_key: str, prompt_file_path: str, model_name: str = \"gemini-2.5-pro\", base_url: str = None):\n        if not api_key:\n            raise ValueError(\"Gemini API Key must be set.\")\n        \n        try:\n            with open(prompt_file_path, 'r', encoding='utf-8') as f:\n                self.system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"System prompt file not found at: {prompt_file_path}\")\n            raise\n        \n        self.model_name = model_name\n\n        if model_name.startswith(\"gemini-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n\n        elif model_name.startswith(\"gpt-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            if not SCADSLLM_URL:\n                raise ValueError(f\"SCADSLLM_URL environment variable is required for model {model_name}\")\n            \n            logging.info(f\"Connecting to Custom API at {SCADSLLM_URL} for model {model_name}\")\n            \n            self.llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                base_url=SCADSLLM_URL,\n                temperature=1.0,\n            )\n\n        else:\n            target_url = base_url if base_url else OLLAMA_BASE_URL\n            self.llm = ChatOllama(\n                model=model_name,\n                temperature=1.0,\n                base_url=target_url,\n            )\n\n        logging.info(f\"Main LLM initialized with model '{model_name}'.\")\n    \n    def call_llm(self, user_input: str):\n\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with HelperLLM input...\")\n\n        try:\n            response = self.llm.invoke(messages)\n            logging.info(\"LLM call successful.\")\n            return response.content\n        except Exception as e:\n            logging.error(f\"Error during LLM call: {e}\")\n            return None\n\n    def stream_llm(self, user_input: str):\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with 'stream'...\")\n\n        try:\n            stream_iterator = self.llm.stream(messages)\n            \n            for chunk in stream_iterator:\n                yield chunk.content\n        except Exception as e:\n            error_message = f\"\\n--- Error during LLM stream call: {e} ---\"\n            logging.error(error_message)\n            yield error_message"
            start_line: 22
            end_line: 106
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[3]:
                - identifier: backend.MainLLM.MainLLM.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[5]: self,api_key,prompt_file_path,model_name,base_url
                  docstring: null
                  start_line: 26
                  end_line: 73
                - identifier: backend.MainLLM.MainLLM.call_llm
                  name: call_llm
                  calls[0]:
                  called_by[0]:
                  args[2]: self,user_input
                  docstring: null
                  start_line: 75
                  end_line: 89
                - identifier: backend.MainLLM.MainLLM.stream_llm
                  name: stream_llm
                  calls[0]:
                  called_by[0]:
                  args[2]: self,user_input
                  docstring: null
                  start_line: 91
                  end_line: 106
    "backend/basic_info.py":
      ast_nodes:
        imports[7]: re,os,tomllib,typing.List,typing.Dict,typing.Any,typing.Optional
        functions[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.basic_info.ProjektInfoExtractor
            name: ProjektInfoExtractor
            docstring: "Extrahiert grundlegende Projektinformationen aus g√§ngigen Projektdateien\nwie README, pyproject.toml und requirements.txt."
            source_code: "class ProjektInfoExtractor:\n    \"\"\"\n    Extrahiert grundlegende Projektinformationen aus g√§ngigen Projektdateien\n    wie README, pyproject.toml und requirements.txt.\n    \"\"\"\n    def __init__(self):\n        self.INFO_NICHT_GEFUNDEN = \"Information not found\"\n        # Initialisiert die Struktur mit Platzhaltern\n        self.info = {\n            \"projekt_uebersicht\": {\n                \"titel\": self.INFO_NICHT_GEFUNDEN,\n                \"beschreibung\": self.INFO_NICHT_GEFUNDEN,\n                \"aktueller_status\": self.INFO_NICHT_GEFUNDEN,\n                \"key_features\": self.INFO_NICHT_GEFUNDEN,\n                \"tech_stack\": self.INFO_NICHT_GEFUNDEN,\n            },\n            \"installation\": {\n                \"dependencies\": self.INFO_NICHT_GEFUNDEN,\n                \"setup_anleitung\": self.INFO_NICHT_GEFUNDEN,\n                \"quick_start_guide\": self.INFO_NICHT_GEFUNDEN,\n            }\n        }\n\n    def _clean_content(self, content: str) -> str:\n        \"\"\"Entfernt Null-Bytes, die durch Encoding-Fehler (UTF-16 als UTF-8 gelesen) entstehen.\"\"\"\n        if not content:\n            return \"\"\n        return content.replace('\\x00', '')\n\n    def _finde_datei(self, patterns: List[str], dateien: List[Any]) -> Optional[Any]:\n        \"\"\"Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht.\"\"\"\n        for datei in dateien:\n            for pattern in patterns:\n                if datei.path.lower().endswith(pattern.lower()):\n                    return datei\n        return None\n\n    def _extrahiere_sektion_aus_markdown(self, inhalt: str, keywords: List[str]) -> Optional[str]:\n        \"\"\"\n        Extrahiert den Text unter einer Markdown-√úberschrift (##).\n        \"\"\"\n        # Erstellt ein Regex-Pattern, das auf jedes der Schl√ºsselw√∂rter reagiert\n        keyword_pattern = \"|\".join(re.escape(k) for k in keywords)\n        \n        # Sucht nach \"## Schl√ºsselwort\" und erfasst alles bis zur n√§chsten \"##\" oder dem Dateiende\n        pattern = re.compile(\n            rf\"##\\s*({keyword_pattern})\\s*\\n(.*?)(?=\\n##|\\Z)\",\n            re.IGNORECASE | re.DOTALL\n        )\n        match = pattern.search(inhalt)\n        if match:\n            return match.group(2).strip()\n        return None\n\n    def _parse_readme(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer README-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        if self.info[\"projekt_uebersicht\"][\"titel\"] == self.INFO_NICHT_GEFUNDEN:\n            title_match = re.search(r\"^\\s*#\\s*(.*)\", inhalt)\n            if title_match:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = title_match.group(1).strip()\n\n        # Beschreibung (Fallback)\n        if self.info[\"projekt_uebersicht\"][\"beschreibung\"] == self.INFO_NICHT_GEFUNDEN:\n            desc_match = re.search(r\"^\\s*#\\s*.*\\n+([^#\\n].*)\", inhalt, re.DOTALL)\n            if desc_match:\n                 self.info[\"projekt_uebersicht\"][\"beschreibung\"] = desc_match.group(1).strip().split('\\n\\n')[0]\n\n        # Key Features\n        features = self._extrahiere_sektion_aus_markdown(inhalt, [\"Features\", \"Key Features\", \"Merkmale\"])\n        if features:\n            self.info[\"projekt_uebersicht\"][\"key_features\"] = features\n            \n        # Tech Stack\n        tech_stack = self._extrahiere_sektion_aus_markdown(inhalt, [\"Tech Stack\", \"Technology\", \"Technologien\"])\n        if tech_stack:\n            self.info[\"projekt_uebersicht\"][\"tech_stack\"] = tech_stack\n            \n        # Status\n        status = self._extrahiere_sektion_aus_markdown(inhalt, [\"Status\", \"Current Status\"])\n        if status:\n            self.info[\"projekt_uebersicht\"][\"aktueller_status\"] = status\n\n        # Setup-Anleitung\n        setup = self._extrahiere_sektion_aus_markdown(inhalt, [\"Installation\", \"Setup\", \"Getting Started\"])\n        if setup:\n            self.info[\"installation\"][\"setup_anleitung\"] = setup\n            \n        # Quick Start\n        quick_start = self._extrahiere_sektion_aus_markdown(inhalt, [\"Quick Start\", \"Schnellstart\"])\n        if quick_start:\n            self.info[\"installation\"][\"quick_start_guide\"] = quick_start\n\n    def _parse_toml(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer pyproject.toml-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        if not tomllib:\n            print(\"Warnung: 'tomli' ist nicht installiert. pyproject.toml kann nicht analysiert werden.\")\n            return\n            \n        try:\n            data = tomllib.loads(inhalt)\n            project_data = data.get(\"project\", {})\n            \n            if \"name\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = project_data[\"name\"]\n            if \"description\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"beschreibung\"] = project_data[\"description\"]\n            if \"dependencies\" in project_data:\n                self.info[\"installation\"][\"dependencies\"] = project_data[\"dependencies\"]\n        except tomllib.TOMLDecodeError as e:\n            print(f\"Warnung: Fehler beim Parsen der pyproject.toml: {e}\")\n\n    def _parse_requirements(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer requirements.txt-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        # Nur f√ºllen, wenn noch keine Dependencies aus toml gefunden wurden\n        if self.info[\"installation\"][\"dependencies\"] == self.INFO_NICHT_GEFUNDEN:\n            lines = inhalt.splitlines()\n            dependencies = [\n                line.strip() for line in lines \n                if line.strip() and not line.strip().startswith('#')\n            ]\n            if dependencies:\n                self.info[\"installation\"][\"dependencies\"] = dependencies\n\n    def extrahiere_info(self, dateien: List[Any], repo_url: str) -> Dict[str, Any]:\n        \"\"\"\n        Orchestriert die Extraktion von Informationen.\n        \"\"\"\n        # Relevante Dateien finden\n        readme_datei = self._finde_datei([\"readme.md\", \"readme.rst\", \"readme.txt\", \"readme\"], dateien)\n        toml_datei = self._finde_datei([\"pyproject.toml\"], dateien)\n        req_datei = self._finde_datei([\"requirements.txt\"], dateien)\n\n        # Dateien parsen (mit Priorisierung)\n        if toml_datei:\n            self._parse_toml(toml_datei.content)\n\n        if req_datei:\n            self._parse_requirements(req_datei.content)\n            \n        if readme_datei:\n            self._parse_readme(readme_datei.content)\n            \n        # Finale Formatierung der Dependencies\n        deps = self.info[\"installation\"][\"dependencies\"]\n        if isinstance(deps, list):\n            if not deps:\n                self.info[\"installation\"][\"dependencies\"] = self.INFO_NICHT_GEFUNDEN\n            else:\n                self.info[\"installation\"][\"dependencies\"] = \"\\n\".join(f\"- {dep}\" for dep in deps)\n        \n        # Titel aus URL ableiten, falls nichts gefunden wurde (oder als Zusatzinfo)\n        if repo_url:\n            repo_name = os.path.basename(repo_url.removesuffix('.git'))\n            # Wenn noch kein Titel da ist oder er generisch war\n            if self.info[\"projekt_uebersicht\"][\"titel\"] == self.INFO_NICHT_GEFUNDEN:\n                 self.info[\"projekt_uebersicht\"][\"titel\"] = f\"{repo_name} documentation\"\n\n        return self.info"
            start_line: 9
            end_line: 172
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[8]:
                - identifier: backend.basic_info.ProjektInfoExtractor.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 14
                  end_line: 30
                - identifier: backend.basic_info.ProjektInfoExtractor._clean_content
                  name: _clean_content
                  calls[0]:
                  called_by[0]:
                  args[2]: self,content
                  docstring: "Entfernt Null-Bytes, die durch Encoding-Fehler (UTF-16 als UTF-8 gelesen) entstehen."
                  start_line: 32
                  end_line: 36
                - identifier: backend.basic_info.ProjektInfoExtractor._finde_datei
                  name: _finde_datei
                  calls[0]:
                  called_by[0]:
                  args[3]: self,patterns,dateien
                  docstring: "Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht."
                  start_line: 38
                  end_line: 44
                - identifier: backend.basic_info.ProjektInfoExtractor._extrahiere_sektion_aus_markdown
                  name: _extrahiere_sektion_aus_markdown
                  calls[0]:
                  called_by[0]:
                  args[3]: self,inhalt,keywords
                  docstring: Extrahiert den Text unter einer Markdown-√úberschrift (##).
                  start_line: 46
                  end_line: 61
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_readme
                  name: _parse_readme
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer README-Datei.
                  start_line: 63
                  end_line: 101
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_toml
                  name: _parse_toml
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer pyproject.toml-Datei.
                  start_line: 103
                  end_line: 122
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_requirements
                  name: _parse_requirements
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer requirements.txt-Datei.
                  start_line: 124
                  end_line: 136
                - identifier: backend.basic_info.ProjektInfoExtractor.extrahiere_info
                  name: extrahiere_info
                  calls[0]:
                  called_by[0]:
                  args[3]: self,dateien,repo_url
                  docstring: Orchestriert die Extraktion von Informationen.
                  start_line: 138
                  end_line: 172
    "backend/callgraph.py":
      ast_nodes:
        imports[9]: ast,networkx,os,pathlib.Path,typing.Dict,getRepo.GitRepository,getRepo.GitRepository,basic_info.ProjektInfoExtractor,os
        functions[2]:
          - mode: function_analysis
            identifier: backend.callgraph.make_safe_dot
            name: make_safe_dot
            args[2]: graph,out_path
            docstring: null
            source_code: "def make_safe_dot(graph: nx.DiGraph, out_path: str):\n    mapping = {}\n    H = graph.copy()\n    for i, n in enumerate(list(graph.nodes())):\n        safe = f\"n{i}\"           \n        mapping[n] = safe\n    H = nx.relabel_nodes(H, mapping)\n    for orig, safe in mapping.items():\n        H.nodes[safe][\"label\"] = orig\n    nx.drawing.nx_pydot.write_dot(H, out_path)"
            start_line: 173
            end_line: 182
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.callgraph.build_filtered_callgraph
            name: build_filtered_callgraph
            args[1]: repo
            docstring: Baut den globalen Call-Graphen und filtert ihn auf selbst geschriebene Funktionen.
            source_code: "def build_filtered_callgraph(repo: GitRepository) -> nx.DiGraph:\n    \"\"\"\n    Baut den globalen Call-Graphen und filtert ihn auf selbst geschriebene Funktionen.\n    \"\"\"\n    all_file_objects = repo.get_all_files()\n    own_functions = set()\n    file_trees = {}\n\n    for file in all_file_objects:\n        if not file.path.endswith(\".py\"):\n            continue\n        filename = Path(file.path).stem\n        tree = ast.parse(file.content)\n        file_trees[filename] = tree\n        visitor = CallGraph(filename)\n        visitor.visit(tree)\n        own_functions.update(visitor.function_set)\n\n    # Globalen Call-Graph bauen\n    global_graph = nx.DiGraph()\n    for filename, tree in file_trees.items():\n        visitor = CallGraph(filename)\n        visitor.visit(tree)\n        for caller, callees in visitor.edges.items():\n            if caller not in own_functions:\n                continue \n            for callee in callees:\n                if callee in own_functions:\n                    global_graph.add_edge(caller, callee)\n                    global_graph.add_node(caller)\n                    global_graph.add_node(callee)\n    return global_graph"
            start_line: 185
            end_line: 216
            context:
              calls[0]:
              called_by[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.callgraph.CallGraph
            name: CallGraph
            docstring: null
            source_code: "class CallGraph(ast.NodeVisitor):\n    def __init__(self, filename: str):\n        self.filename = filename\n        self.current_function = None\n        self.current_class = None\n        \n        self.local_defs: dict[str, str] = {}\n        self.graph = nx.DiGraph()\n        self.import_mapping: dict[str, str] = {}\n        self.function_set: set[str] = set()\n        self.edges: Dict[str, set[str]] = {}\n\n    def _recursive_call(self, node):\n        \"\"\"\n        Liefert eine Liste der Namenskomponenten als dotted string z.B. ['pkg', 'mod', 'Class', 'method']\n        \"\"\"\n        if isinstance(node, ast.Call):\n            return self._recursive_call(node.func)\n        if isinstance(node, ast.Name):\n            return [node.id]\n        if isinstance(node, ast.Attribute):\n            parts = self._recursive_call(node.value)\n            parts.append(node.attr)\n            return parts\n        return []\n\n    def _resolve_all_callee_names(self, callee_nodes: list[list[str]]) -> list[str]:\n        \"\"\"\n        callee_nodes ist jetzt eine Liste von Listen (Name-Steps).\n        Wir pr√ºfen zuerst lokale Definitionen, dann import_mapping.\n        \"\"\"\n        resolved = []\n        for parts in callee_nodes:\n            if not parts:\n                continue\n            simple = parts[-1]\n            dotted = \".\".join(parts[-2:]) if len(parts) >= 2 else simple\n\n            if simple in self.local_defs:\n                resolved.append(self.local_defs[simple])\n                continue\n            if dotted in self.local_defs:\n                resolved.append(self.local_defs[dotted])\n                continue\n\n            first = parts[0]\n            if first in self.import_mapping:\n                mod = self.import_mapping[first]\n                rest = \"::\".join(parts[1:]) if len(parts) > 1 else simple\n                resolved.append(f\"{mod}::{rest}\")\n                continue\n\n            if self.current_class:\n                resolved.append(f\"{self.filename}::{parts[0]}::{simple}\" if len(parts)==1 else f\"{self.filename}::{'::'.join(parts)}\")\n            else:\n                resolved.append(f\"{self.filename}::{ '::'.join(parts)}\")\n        return resolved\n\n    def _make_full_name(self, basename: str, class_name: str | None = None) -> str:\n        if class_name:\n            return f\"{self.filename}::{class_name}::{basename}\"\n        return f\"{self.filename}::{basename}\"\n    \n    def _current_caller(self) -> str:\n        if self.current_function:\n            return self.current_function\n        return f\"<{self.filename}>\" if self.filename else \"<global-scope>\"\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            module_name = alias.name\n            module_asname = alias.asname if alias.asname else module_name\n            self.import_mapping[module_asname] = module_name\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        module_name = node.module.split(\".\")[-1] if node.module else \"\"\n        for alias in node.names:\n            self.import_mapping[alias.asname or alias.name] = f\"{module_name}\" if module_name else alias.name\n\n    def visit_ClassDef(self, node: ast.ClassDef):\n        prev_class = self.current_class\n        self.current_class = node.name\n        self.generic_visit(node)\n        self.current_class = prev_class\n        \n    def visit_FunctionDef(self, node):\n        prev_function = self.current_function\n        full_name = self._make_full_name(node.name, self.current_class)\n        self.local_defs[node.name] = full_name\n        if self.current_class:\n            self.local_defs[f\"{self.current_class}.{node.name}\"] = full_name\n\n        self.current_function = full_name\n        self.graph.add_node(self.current_function)\n        self.generic_visit(node)\n        self.function_set.add(self.current_function)\n        self.current_function = prev_function\n\n    def visit_AsyncFunctionDef(self, node):\n        self.visit_FunctionDef(node)\n\n    def visit_Call(self, node):\n        caller = self._current_caller()\n        parts = self._recursive_call(node)\n        resolved_callees = self._resolve_all_callee_names([parts])\n\n        if caller not in self.edges:\n            self.edges[caller] = set()\n\n        for callee in resolved_callees:\n            if callee:\n                self.edges[caller].add(callee)\n        self.generic_visit(node)\n\n    def visit_If(self, node):\n        if (isinstance(node.test, ast.Compare) and\n            isinstance(node.test.left, ast.Name) and \n            node.test.left.id == \"__name__\"):\n            caller_backup = self.current_function\n            self.current_function = \"<main_block>\"\n            self.generic_visit(node)\n            self.current_function = caller_backup\n        else:\n            self.generic_visit(node)"
            start_line: 10
            end_line: 134
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[12]:
                - identifier: backend.callgraph.CallGraph.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,filename
                  docstring: null
                  start_line: 11
                  end_line: 20
                - identifier: backend.callgraph.CallGraph._recursive_call
                  name: _recursive_call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: "Liefert eine Liste der Namenskomponenten als dotted string z.B. ['pkg', 'mod', 'Class', 'method']"
                  start_line: 22
                  end_line: 34
                - identifier: backend.callgraph.CallGraph._resolve_all_callee_names
                  name: _resolve_all_callee_names
                  calls[0]:
                  called_by[0]:
                  args[2]: self,callee_nodes
                  docstring: "callee_nodes ist jetzt eine Liste von Listen (Name-Steps).\nWir pr√ºfen zuerst lokale Definitionen, dann import_mapping."
                  start_line: 36
                  end_line: 66
                - identifier: backend.callgraph.CallGraph._make_full_name
                  name: _make_full_name
                  calls[0]:
                  called_by[0]:
                  args[3]: self,basename,class_name
                  docstring: null
                  start_line: 68
                  end_line: 71
                - identifier: backend.callgraph.CallGraph._current_caller
                  name: _current_caller
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 73
                  end_line: 76
                - identifier: backend.callgraph.CallGraph.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 78
                  end_line: 83
                - identifier: backend.callgraph.CallGraph.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 85
                  end_line: 88
                - identifier: backend.callgraph.CallGraph.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 90
                  end_line: 94
                - identifier: backend.callgraph.CallGraph.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 96
                  end_line: 107
                - identifier: backend.callgraph.CallGraph.visit_AsyncFunctionDef
                  name: visit_AsyncFunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 109
                  end_line: 110
                - identifier: backend.callgraph.CallGraph.visit_Call
                  name: visit_Call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 112
                  end_line: 123
                - identifier: backend.callgraph.CallGraph.visit_If
                  name: visit_If
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 125
                  end_line: 134
    "backend/converter.py":
      ast_nodes:
        imports[4]: logging,nbformat,base64,nbformat.reader.NotJSONError
        functions[5]:
          - mode: function_analysis
            identifier: backend.converter.wrap_cdata
            name: wrap_cdata
            args[1]: content
            docstring: Wraps content in CDATA tags.
            source_code: "def wrap_cdata(content):\n    \"\"\"Wraps content in CDATA tags.\"\"\"\n    return f\"<![CDATA[\\n{content}\\n]]>\""
            start_line: 8
            end_line: 10
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.converter.extract_output_content
            name: extract_output_content
            args[2]: outputs,image_list
            docstring: "Extracts text and handles images by decoding Base64 to bytes.\nReturns: A list of text strings or placeholders."
            source_code: "def extract_output_content(outputs, image_list):\n    \"\"\"\n    Extracts text and handles images by decoding Base64 to bytes.\n    Returns: A list of text strings or placeholders.\n    \"\"\"\n    extracted_xml_snippets = []\n    \n    for output in outputs:\n\n        if output.output_type in ('display_data', 'execute_result') and hasattr(output, 'data'):\n            data = output.data\n            \n            # Helper to process image\n            def process_image(mime_type):\n                if mime_type in data:\n                    try:\n                        b64_str = data[mime_type]\n                        b64_str = b64_str.replace('\\n', '')\n\n                        image_index = len(image_list)\n                        image_list.append({\n                            \"mime_type\": mime_type,\n                            \"data\": b64_str\n                        })\n\n                        return f'\\n<IMAGE_PLACEHOLDER index=\"{image_index}\" mime=\"{mime_type}\"/>\\n'\n                    except Exception as e:\n                        return f\"<ERROR>Could not decode image: {e}</ERROR>\"\n                return None\n\n            # Ignore jpeg if png is found\n            img_xml = process_image('image/png')\n            if not img_xml:\n                img_xml = process_image('image/jpeg')\n            \n            if img_xml:\n                extracted_xml_snippets.append(img_xml)\n            elif 'text/plain' in data:\n                extracted_xml_snippets.append(data['text/plain'])\n\n        elif output.output_type == 'stream':\n            extracted_xml_snippets.append(output.text)\n            \n        elif output.output_type == 'error':\n            extracted_xml_snippets.append(f\"{output.ename}: {output.evalue}\")\n\n    return extracted_xml_snippets"
            start_line: 12
            end_line: 58
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.converter.process_image
            name: process_image
            args[1]: mime_type
            docstring: null
            source_code: "def process_image(mime_type):\n                if mime_type in data:\n                    try:\n                        b64_str = data[mime_type]\n                        b64_str = b64_str.replace('\\n', '')\n\n                        image_index = len(image_list)\n                        image_list.append({\n                            \"mime_type\": mime_type,\n                            \"data\": b64_str\n                        })\n\n                        return f'\\n<IMAGE_PLACEHOLDER index=\"{image_index}\" mime=\"{mime_type}\"/>\\n'\n                    except Exception as e:\n                        return f\"<ERROR>Could not decode image: {e}</ERROR>\"\n                return None"
            start_line: 25
            end_line: 40
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.converter.convert_notebook_to_xml
            name: convert_notebook_to_xml
            args[1]: file_content
            docstring: null
            source_code: "def convert_notebook_to_xml(file_content):\n    try:\n        nb = nbformat.reads(file_content, as_version=4)\n    except NotJSONError:\n        return \"<ERROR>Could not parse file as JSON/Notebook</ERROR>\", []\n\n    xml_parts = []\n    extracted_images = [] \n\n    for cell in nb.cells:\n        if cell.cell_type == 'markdown':\n            cell_xml = f'<CELL type=\"markdown\">\\n{cell.source}\\n</CELL>'\n            xml_parts.append(cell_xml)\n\n        elif cell.cell_type == 'code':\n            source_code = wrap_cdata(cell.source)\n            xml_parts.append(f'<CELL type=\"code\">\\n{source_code}\\n</CELL>')\n\n            if hasattr(cell, 'outputs') and cell.outputs:\n                snippets = extract_output_content(cell.outputs, extracted_images)\n                \n                content = \"\\n\".join(snippets)\n                \n                if content.strip():\n                    wrapped_output = wrap_cdata(content)\n                    xml_parts.append(f'<CELL type=\"output\">\\n{wrapped_output}\\n</CELL>')\n\n    return \"\\n\\n\".join(xml_parts), extracted_images"
            start_line: 60
            end_line: 87
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.converter.process_repo_notebooks
            name: process_repo_notebooks
            args[1]: repo_files
            docstring: null
            source_code: "def process_repo_notebooks(repo_files):\n    notebook_files = [f for f in repo_files if f.path.endswith('.ipynb')]\n    logging.info(f\"Found {len(notebook_files)} notebooks.\")\n        \n    results = {}\n\n    for nb_file in notebook_files:\n        logging.info(f\"Processing: {nb_file.path}\")\n\n        xml_output, images = convert_notebook_to_xml(nb_file.content)\n        results[nb_file.path] = {\n            \"xml\": xml_output,\n            \"images\": images\n        }\n            \n    return results"
            start_line: 90
            end_line: 105
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "backend/getRepo.py":
      ast_nodes:
        imports[5]: tempfile,git.Repo,git.GitCommandError,logging,os
        functions[0]:
        classes[2]:
          - mode: class_analysis
            identifier: backend.getRepo.RepoFile
            name: RepoFile
            docstring: "Repr√§sentiert eine einzelne Datei in einem Git-Repository.\n\nDer Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tats√§chlichem Zugriff."
            source_code: "class RepoFile:\n    \"\"\"\n    Repr√§sentiert eine einzelne Datei in einem Git-Repository.\n    \n    Der Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tats√§chlichem Zugriff.\n    \"\"\"\n    def __init__(self, file_path, commit_tree):\n        \"\"\"\n        Initialisiert das RepoFile-Objekt.\n\n        Args:\n            file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n            commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt.\n        \"\"\"\n        self.path = file_path\n        self._tree = commit_tree\n        \n\n        self._blob = None\n        self._content = None\n        self._size = None\n\n    @property\n    def blob(self):\n        \"\"\"Lazy-l√§dt das Git-Blob-Objekt.\"\"\"\n        if self._blob is None:\n            try:\n                self._blob = self._tree[self.path]\n            except KeyError:\n                raise FileNotFoundError(f\"Datei '{self.path}' konnte im Commit-Tree nicht gefunden werden.\")\n        return self._blob\n\n    @property\n    def content(self):\n        \"\"\"Lazy-l√§dt und gibt den dekodierten Inhalt der Datei zur√ºck.\"\"\"\n        if self._content is None:\n            self._content = self.blob.data_stream.read().decode('utf-8', errors='ignore')\n        return self._content\n\n    @property\n    def size(self):\n        \"\"\"Lazy-l√§dt und gibt die Gr√∂√üe der Datei in Bytes zur√ºck.\"\"\"\n        if self._size is None:\n            self._size = self.blob.size\n        return self._size\n        \n    def analyze_word_count(self):\n        \"\"\"\n        Eine Beispiel-Analyse-Methode. Z√§hlt die W√∂rter im Dateiinhalt.\n        \"\"\"\n        return len(self.content.split())\n\n    def __repr__(self):\n        \"\"\"Gibt eine n√ºtzliche String-Repr√§sentation des Objekts zur√ºck.\"\"\"\n        return f\"<RepoFile(path='{self.path}')>\"\n    \n    def to_dict(self, include_content=False):\n        data = {\n            \"path\": self.path,\n            \"name\": os.path.basename(self.path),\n            \"size\": self.size,\n            \"type\": \"file\"\n        }\n        if include_content:\n            data[\"content\"] = self.content\n        return data"
            start_line: 7
            end_line: 72
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[7]:
                - identifier: backend.getRepo.RepoFile.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[3]: self,file_path,commit_tree
                  docstring: "Initialisiert das RepoFile-Objekt.\n\nArgs:\n    file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n    commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt."
                  start_line: 13
                  end_line: 27
                - identifier: backend.getRepo.RepoFile.blob
                  name: blob
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt das Git-Blob-Objekt.
                  start_line: 30
                  end_line: 37
                - identifier: backend.getRepo.RepoFile.content
                  name: content
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt und gibt den dekodierten Inhalt der Datei zur√ºck.
                  start_line: 40
                  end_line: 44
                - identifier: backend.getRepo.RepoFile.size
                  name: size
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt und gibt die Gr√∂√üe der Datei in Bytes zur√ºck.
                  start_line: 47
                  end_line: 51
                - identifier: backend.getRepo.RepoFile.analyze_word_count
                  name: analyze_word_count
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Eine Beispiel-Analyse-Methode. Z√§hlt die W√∂rter im Dateiinhalt.
                  start_line: 53
                  end_line: 57
                - identifier: backend.getRepo.RepoFile.__repr__
                  name: __repr__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Gibt eine n√ºtzliche String-Repr√§sentation des Objekts zur√ºck.
                  start_line: 59
                  end_line: 61
                - identifier: backend.getRepo.RepoFile.to_dict
                  name: to_dict
                  calls[0]:
                  called_by[0]:
                  args[2]: self,include_content
                  docstring: null
                  start_line: 63
                  end_line: 72
          - mode: class_analysis
            identifier: backend.getRepo.GitRepository
            name: GitRepository
            docstring: "Verwaltet ein Git-Repository, einschlie√ülich Klonen in ein tempor√§res\nVerzeichnis und Bereitstellung von RepoFile-Objekten."
            source_code: "class GitRepository:\n    \"\"\"\n    Verwaltet ein Git-Repository, einschlie√ülich Klonen in ein tempor√§res\n    Verzeichnis und Bereitstellung von RepoFile-Objekten.\n    \"\"\"\n    def __init__(self, repo_url):\n        self.repo_url = repo_url\n        self.temp_dir = tempfile.mkdtemp()\n        self.repo = None\n\n        self.files = []\n        \n        try:\n            logging.info(f\"Clone Repository {self.repo_url}...\")\n            self.repo = Repo.clone_from(self.repo_url, self.temp_dir)\n            self.latest_commit = self.repo.head.commit\n            self.commit_tree = self.latest_commit.tree\n            logging.info(\"Cloning successful.\")\n        except GitCommandError as e:\n            self.close()\n            raise RuntimeError(f\"Error cloning repository: {e}\") from e\n\n    def get_all_files(self):\n        \"\"\"\n        Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zur√ºck.\n\n        Returns:\n            list[RepoFile]: Eine Liste von RepoFile-Instanzen.\n        \"\"\"\n        file_paths = self.repo.git.ls_files().split('\\n')\n        self.files = [RepoFile(path, self.commit_tree) for path in file_paths if path]\n        return self.files\n\n    def close(self):\n        \"\"\"L√∂scht das tempor√§re Verzeichnis und dessen Inhalt.\"\"\"\n        if self.temp_dir:\n            print(f\"\\nL√∂sche tempor√§res Verzeichnis: {self.temp_dir}\")\n            self.temp_dir = None\n            \n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def get_file_tree(self, include_content=False):\n\n        if not self.files:\n            self.get_all_files()\n\n        tree = {\"name\": \"root\", \"type\": \"directory\", \"children\": []}\n\n        for file_obj in self.files:\n            parts = file_obj.path.split('/')\n            current_level = tree[\"children\"]\n            \n            # Iteriere durch die Ordnerstruktur\n            for part in parts[:-1]:\n                # Suche, ob Ordner schon existiert\n                found = next((item for item in current_level if item[\"name\"] == part and item[\"type\"] == \"directory\"), None)\n                if not found:\n                    new_dir = {\"name\": part, \"type\": \"directory\", \"children\": []}\n                    current_level.append(new_dir)\n                    current_level = new_dir[\"children\"]\n                else:\n                    current_level = found[\"children\"]\n            \n            # Datei am Ende hinzuf√ºgen\n            current_level.append(file_obj.to_dict(include_content=include_content))\n\n        return tree"
            start_line: 78
            end_line: 148
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[6]:
                - identifier: backend.getRepo.GitRepository.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,repo_url
                  docstring: null
                  start_line: 83
                  end_line: 98
                - identifier: backend.getRepo.GitRepository.get_all_files
                  name: get_all_files
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: "Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zur√ºck.\n\nReturns:\n    list[RepoFile]: Eine Liste von RepoFile-Instanzen."
                  start_line: 100
                  end_line: 109
                - identifier: backend.getRepo.GitRepository.close
                  name: close
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: L√∂scht das tempor√§re Verzeichnis und dessen Inhalt.
                  start_line: 111
                  end_line: 115
                - identifier: backend.getRepo.GitRepository.__enter__
                  name: __enter__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 117
                  end_line: 118
                - identifier: backend.getRepo.GitRepository.__exit__
                  name: __exit__
                  calls[0]:
                  called_by[0]:
                  args[4]: self,exc_type,exc_val,exc_tb
                  docstring: null
                  start_line: 120
                  end_line: 121
                - identifier: backend.getRepo.GitRepository.get_file_tree
                  name: get_file_tree
                  calls[0]:
                  called_by[0]:
                  args[2]: self,include_content
                  docstring: null
                  start_line: 123
                  end_line: 148
    "backend/main.py":
      ast_nodes:
        imports[28]: json,math,logging,os,re,time,math,datetime.datetime,matplotlib.pyplot,datetime.datetime,pathlib.Path,dotenv.load_dotenv,getRepo.GitRepository,AST_Schema.ASTAnalyzer,MainLLM.MainLLM,basic_info.ProjektInfoExtractor,HelperLLM.LLMHelper,relationship_analyzer.ProjectAnalyzer,schemas.types.FunctionContextInput,schemas.types.FunctionAnalysisInput,schemas.types.ClassContextInput,schemas.types.ClassAnalysisInput,schemas.types.MethodContextInput,toon_format.encode,toon_format.count_tokens,toon_format.estimate_savings,toon_format.compare_formats,converter.process_repo_notebooks
        functions[7]:
          - mode: function_analysis
            identifier: backend.main.create_savings_chart
            name: create_savings_chart
            args[4]: json_tokens,toon_tokens,savings_percent,output_path
            docstring: Erstellt ein Balkendiagramm f√ºr den Token-Vergleich und speichert es.
            source_code: "def create_savings_chart(json_tokens, toon_tokens, savings_percent, output_path):\n    \"\"\"Erstellt ein Balkendiagramm f√ºr den Token-Vergleich und speichert es.\"\"\"\n    labels = ['JSON', 'TOON']\n    values = [json_tokens, toon_tokens]\n    colors = ['#ff9999', '#66b3ff']\n\n    plt.figure(figsize=(8, 6))\n    bars = plt.bar(labels, values, color=colors, width=0.5)\n\n    # Titel und Beschriftungen\n    plt.title(f'Token-Vergleich: {savings_percent:.2f}% Einsparung', fontsize=14)\n    plt.ylabel('Anzahl Token')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n\n    # Werte √ºber den Balken anzeigen\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width() / 2.0, height, \n                 f'{int(height):,}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n\n    # Speichern\n    plt.savefig(output_path)\n    plt.close()"
            start_line: 33
            end_line: 55
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.calculate_net_time
            name: calculate_net_time
            args[5]: start_time,end_time,total_items,batch_size,model_name
            docstring: Berechnet die Dauer abz√ºglich der Sleep-Zeiten f√ºr Rate-Limits.
            source_code: "def calculate_net_time(start_time, end_time, total_items, batch_size, model_name):\n    \"\"\"Berechnet die Dauer abz√ºglich der Sleep-Zeiten f√ºr Rate-Limits.\"\"\"\n    total_duration = end_time - start_time\n    \n    if not model_name.startswith(\"gemini-\"):\n        return total_duration\n\n    if total_items == 0:\n        return 0\n\n    num_batches = math.ceil(total_items / batch_size)\n    sleep_count = max(0, num_batches - 1)\n    total_sleep_time = sleep_count * 61\n    \n    net_time = total_duration - total_sleep_time\n    return max(0, net_time)"
            start_line: 57
            end_line: 72
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.main_workflow
            name: main_workflow
            args[4]: input,api_keys,model_names,status_callback
            docstring: null
            source_code: "def main_workflow(input, api_keys: dict, model_names: dict, status_callback=None):\n    \n    def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)\n\n    update_status(\"üîç Analysiere Input...\")\n    \n    user_input = input\n    \n    # API Key & Ollama Base URL aus Frontend holen\n    gemini_api_key = api_keys.get(\"gemini\")\n    openai_api_key = api_keys.get(\"gpt\")\n    scadsllm_api_key = api_keys.get(\"scadsllm\")\n    scadsllm_base_url = api_keys.get(\"scadsllm_base_url\")\n    ollama_base_url = api_keys.get(\"ollama\")\n    base_url = None\n\n    if model_names[\"helper\"].startswith(\"gpt-\"):\n        helper_api_key = openai_api_key\n    elif model_names[\"helper\"].startswith(\"gemini-\"):\n        helper_api_key = gemini_api_key\n    elif \"/\" in model_names[\"helper\"] or model_names[\"helper\"].startswith(\"alias-\") or any(x in model_names[\"helper\"] for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        helper_api_key = scadsllm_api_key\n        base_url = scadsllm_base_url\n    else:\n        helper_api_key = None\n        base_url = ollama_base_url\n    if model_names[\"main\"].startswith(\"gpt-\"):\n        main_api_key = openai_api_key\n    elif model_names[\"main\"].startswith(\"gemini-\"):\n        main_api_key = gemini_api_key\n    elif \"/\" in model_names[\"main\"] or model_names[\"main\"].startswith(\"alias-\") or any(x in model_names[\"main\"] for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        main_api_key = scadsllm_api_key\n        base_url = scadsllm_base_url\n    else:\n        main_api_key = None\n        base_url = ollama_base_url\n\n    # Standardeinstellungen f√ºr Modelle\n    helper_model = model_names.get(\"helper\", \"gpt-5-mini\")\n    main_model = model_names.get(\"main\", \"gpt-5.1\")\n\n    # Error Handling f√ºr fehlende API Keys\n    if not gemini_api_key and \"gemini\" in helper_model:\n        raise ValueError(\"Gemini API Key was not provided in api_keys dictionary.\")\n\n    # URL Extraktion\n    repo_url = None\n    url_pattern = r\"https?://(?:www\\.)?github\\.com/[^\\s]+\"\n    match = re.search(url_pattern, user_input)\n\n    if match:\n        repo_url = match.group(0)\n        logging.info(f\"Extracted repository URL: {repo_url}\")\n    else:\n        raise ValueError(\"Could not find a valid URL in the provided input.\")\n    \n    # Repo klonen und Dateien extrahieren\n    update_status(f\"‚¨áÔ∏è Klone Repository: {repo_url} ...\")\n    \n    repo_files = []\n    local_repo_path = \"\" \n\n    try: \n\n        with GitRepository(repo_url) as repo:\n            repo_files = repo.get_all_files()\n            local_repo_path = repo.temp_dir\n\n            logging.info(f\"Total files retrieved: {len(repo_files)}\")\n\n    except Exception as e:\n        logging.error(f\"Error cloning repository: {e}\")\n        raise \n\n\n    # Extrahiere Basic Infos\n    update_status(\"‚ÑπÔ∏è Extrahiere Basis-Informationen...\")\n    try:\n        info_extractor = ProjektInfoExtractor()\n        basic_project_info = info_extractor.extrahiere_info(dateien=repo_files, repo_url=repo_url)\n        logging.info(\"Basic project info extracted\")\n    except Exception as e:\n        logging.error(f\"Error extracting basic project info: {e}\")\n        basic_project_info = \"Could not extract basic info.\"\n\n    # Erstelle Repository Dateibaum\n    update_status(\"üå≤ Erstelle Repository Dateibaum...\")\n    try:\n        repo_file_tree = repo.get_file_tree()\n        logging.info(\"Repository file tree constructed\")\n    except Exception as e:\n        logging.error(f\"Error constructing repository file tree: {e}\")\n        repo_file_tree = \"Could not create file tree.\"\n\n    # Relationship Analyse durchf√ºhren\n    update_status(\"üîó Analysiere Beziehungen (Calls & Instanziierungen)...\")\n    try:\n        rel_analyzer = ProjectAnalyzer(project_root=local_repo_path)\n        rel_analyzer.analyze()\n        \n        raw_relationships = rel_analyzer.get_raw_relationships()\n        \n        logging.info(f\"Relationships analyzed.\")\n    except Exception as e:\n        logging.error(f\"Error in relationship analyzer: {e}\")\n        raw_relationships = {\"outgoing\": {}, \"incoming\": {}}\n\n    # Erstelle AST Schema\n    update_status(\"üå≥ Erstelle Abstract Syntax Tree (AST)...\")\n    try:        \n        ast_analyzer = ASTAnalyzer()   \n        ast_schema = ast_analyzer.analyze_repository(files=repo_files, repo=repo)\n        logging.info(\"AST schema created\")\n    except Exception as e:\n        logging.error(f\"Error retrieving repository files: {e}\")\n        raise\n\n    # Anreichern des AST Schemas mit Relationship Daten\n    update_status(\"‚ûï Reiche AST mit Beziehungsdaten an...\")            \n    try:   \n        ast_schema = ast_analyzer.merge_relationship_data(ast_schema, raw_relationships)\n        logging.info(\"AST schema created and enriched\")\n\n    except Exception as e:\n        logging.error(f\"Error processing repository: {e}\")\n        raise\n\n    # Vorbereitung der HelperLLM Eingaben\n    update_status(\"‚öôÔ∏è Bereite Daten f√ºr Helper LLM vor...\")\n    \n    helper_llm_function_input = []\n    helper_llm_class_input = []\n\n    try:\n        for filename, file_data in ast_schema['files'].items():\n            ast_nodes = file_data.get('ast_nodes', {})\n            imports = ast_nodes.get('imports', [])\n            functions = ast_nodes.get('functions', [])\n            classes = ast_nodes.get('classes', [])\n\n            for function in functions:\n                context = function.get('context', {})\n                \n                raw_called_by = context.get('called_by', [])\n                clean_called_by = [cb for cb in raw_called_by if isinstance(cb, dict)]\n\n                function_context = FunctionContextInput(\n                    calls = context.get('calls', []),\n                    called_by = clean_called_by \n                )\n                \n                function_input = FunctionAnalysisInput(\n                    mode = function.get('mode', 'function_analysis'),\n                    identifier = function.get('identifier'),\n                    source_code = function.get('source_code'),\n                    imports = imports,\n                    context = function_context\n                )\n                \n                helper_llm_function_input.append(function_input)\n\n            for _class in classes:\n                context = _class.get('context', {})\n                \n                method_context_inputs = []\n                for method in context.get('method_context', []):\n                    \n                    raw_method_called_by = method.get('called_by', [])\n                    clean_method_called_by = [cb for cb in raw_method_called_by if isinstance(cb, dict)]\n\n                    method_context_inputs.append(\n                        MethodContextInput(\n                            identifier=method.get('identifier'),\n                            calls=method.get('calls', []),\n                            called_by=clean_method_called_by, \n                            args=method.get('args', []),\n                            docstring=method.get('docstring')\n                        )\n                    )\n\n                raw_instantiated_by = context.get('instantiated_by', [])\n                clean_instantiated_by = [ib for ib in raw_instantiated_by if isinstance(ib, dict)]\n\n                class_context = ClassContextInput(\n                    dependencies = context.get('dependencies', []),\n                    instantiated_by = clean_instantiated_by, \n                    method_context = method_context_inputs\n                )\n\n                class_input = ClassAnalysisInput(\n                    mode = _class.get('mode', 'class_analysis'),\n                    identifier =_class.get('identifier'),\n                    source_code = _class.get('source_code'), \n                    imports = imports, \n                    context = class_context\n                )\n                \n                helper_llm_class_input.append(class_input)\n\n    except Exception as e:\n        logging.error(f\"Error preparing inputs for Helper LLM: {e}\")\n        raise\n    \n    # Initialisiere HelperLLM\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    \n    llm_helper = LLMHelper(\n        api_key=helper_api_key, \n        function_prompt_path=function_prompt_file, \n        class_prompt_path=class_prompt_file,\n        model_name=helper_model,\n        base_url=base_url,\n    )\n    \n    if ollama_base_url:\n        os.environ[\"OLLAMA_BASE_URL\"] = ollama_base_url\n\n    # Initialisiere Ergebniscontainer\n    analysis_results = {}\n    function_analysis_results = []\n    class_analysis_results = []\n\n    # Call HelperLLM f√ºr Funktionen\n    update_status(f\"ü§ñ Helper LLM: Analysiere {len(helper_llm_function_input)} Funktionen ({helper_model})...\")\n\n    try:\n        net_time_func = 0\n        if len(helper_llm_function_input) > 0:\n\n            logging.info(\"\\n--- Generating documentation for Functions ---\")\n            t_start_func = time.time()    \n            function_analysis_results = llm_helper.generate_for_functions(helper_llm_function_input)    \n            t_end_func = time.time()\n            net_time_func = calculate_net_time(t_start_func, t_end_func, len(helper_llm_function_input), llm_helper.batch_size, helper_model)\n\n        if len(function_analysis_results) != 0:\n            for doc in function_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"functions\" not in analysis_results:\n                        analysis_results[\"functions\"] = {}\n                    analysis_results[\"functions\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(f\"Failed to generate doc for a function\") \n    except Exception as e:\n        logging.error(f\"Error during Helper LLM function analysis: {e}\")\n        raise\n    \n\n    # Call HelperLLM f√ºr Klassen\n    try:\n        net_time_class = 0\n        if len(helper_llm_class_input) > 0:\n            # Rate Limit Sleep f√ºr Gemini Modelle\n            if llm_helper.model_name.startswith(\"gemini-\") & (len(helper_llm_function_input) > 0):\n                update_status(\"üí§ Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n                time.sleep(65)\n            \n            update_status(f\"ü§ñ Helper LLM: Analysiere {len(helper_llm_class_input)} Klassen ({helper_model})...\")\n            \n            logging.info(\"\\n--- Generating documentation for Classes ---\")\n            t_start_class = time.time()\n            class_analysis_results = llm_helper.generate_for_classes(helper_llm_class_input)\n            t_end_class = time.time()\n            net_time_class = calculate_net_time(t_start_class, t_end_class, len(helper_llm_class_input), llm_helper.batch_size, helper_model)\n\n        if len(class_analysis_results) != 0:\n            for doc in class_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"classes\" not in analysis_results:\n                        analysis_results[\"classes\"] = {}\n                    analysis_results[\"classes\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(f\"Failed to generate doc for a class\")\n    except Exception as e:\n        logging.error(f\"Error during Helper LLM class analysis: {e}\")\n        raise\n\n    total_helper_time = net_time_func + net_time_class\n\n    # MainLLM Input Vorbereitung\n    main_llm_input = {\n        \"basic_info\": basic_project_info,\n        \"file_tree\": repo_file_tree,\n        \"ast_schema\": ast_schema,\n        \"analysis_results\": analysis_results\n    }\n\n    # Speichern als JSON (Optional)\n    main_llm_input_json = json.dumps(main_llm_input, indent=2)\n    # with open(\"output.json\", \"w\", encoding=\"utf-8\") as f:\n    #     f.write(main_llm_input_json)\n    #     logging.info(\"JSON-Datei wurde gespeichert.\")\n\n    # Konvertiere Input in Toon Format\n    main_llm_input_toon = encode(main_llm_input)\n\n    #Speichern in TOON Format (Optional)\n    # with open(\"output.toon\", \"w\", encoding=\"utf-8\") as f:\n    #     f.write(main_llm_input_toon)\n    #     logging.info(\"output.toon erfolgreich gespeichert.\")\n    \n    # Token Evaluation\n    savings_data = None\n    try:\n        logging.info(\"--- Evaluierung der Token-Ersparnis ---\")\n        savings_data = estimate_savings(main_llm_input)\n        logging.info(f\"JSON Tokens: {savings_data['json_tokens']}\")\n        logging.info(f\"TOON Tokens: {savings_data['toon_tokens']}\")\n        logging.info(f\"Ersparnis:   {savings_data['savings_percent']:.2f}%\")\n        \n    except Exception as e:\n        logging.warning(f\"Token evaluation could not be performed: {e}\")    \n\n\n\n    prompt_file_mainllm = \"SystemPrompts/SystemPromptMainLLM.txt\"\n    prompt_file_mainllm_toon = \"SystemPrompts/SystemPromptMainLLMToon.txt\"\n    # MainLLM Ausf√ºhrung\n    main_llm = MainLLM(\n        api_key=main_api_key, \n        prompt_file_path=prompt_file_mainllm_toon,\n        model_name=main_model,\n        base_url=base_url,\n    )\n\n\n    # RPM Limit Sleep f√ºr Gemini Modelle\n    if llm_helper.model_name == main_llm.model_name and main_llm.model_name.startswith(\"gemini-\"):\n        time.sleep(65)\n        update_status(\"üí§ Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n\n    # Call MainLLM f√ºr finalen Report\n    update_status(f\"üß† Main LLM: Generiere finalen Report ({main_model})...\")\n    try:\n        total_main_time = 0\n        logging.info(\"\\n--- Generating Final Report ---\")\n        t_start_main = time.time()\n        final_report = main_llm.call_llm(main_llm_input_toon)\n        t_end_main = time.time()\n        total_main_time = t_end_main - t_start_main\n    except Exception as e:\n        logging.error(f\"Error during Main LLM final report generation: {e}\")\n        raise\n\n    \n    # --- Speichern der Ergebnisse ---\n    output_dir = \"result\"\n    stats_dir = \"Statistics\" # Neuer Ordner\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(stats_dir, exist_ok=True) # Stelle sicher, dass Statistics Ordner existiert\n\n    total_active_time = total_helper_time + total_main_time\n    \n    # Dateiname f√ºr Report\n    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M-%S\")\n    report_filename = f\"report_{timestamp}_Helper_{llm_helper.model_name}_MainLLM_{main_llm.model_name}.md\"\n    report_filepath = os.path.join(output_dir, report_filename)\n    \n    if final_report:\n        with open(report_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(final_report)\n        logging.info(f\"Final report saved to '{report_filepath}'.\")\n        \n        if savings_data:\n            try:\n                savings_filename = report_filename.replace(\"report_\", \"savings_\").replace(\".md\", \".png\")\n                savings_filepath = os.path.join(stats_dir, savings_filename)\n                \n                logging.info(f\"Erstelle Token-Chart: {savings_filepath}\")\n                create_savings_chart(\n                    json_tokens=savings_data['json_tokens'], \n                    toon_tokens=savings_data['toon_tokens'], \n                    savings_percent=savings_data['savings_percent'],\n                    output_path=savings_filepath\n                )\n            except Exception as chart_error:\n                logging.error(f\"Konnte Diagramm nicht erstellen: {chart_error}\")\n\n    else:\n        final_report = \"Error: Report generation failed or returned empty.\"\n\n    metrics = {\n        \"helper_time\": round(total_helper_time, 2),\n        \"main_time\": round(total_main_time, 2),\n        \"total_time\": round(total_active_time, 2),\n        \"helper_model\": helper_model,\n        \"main_model\": main_model,\n        \"json_tokens\": savings_data['json_tokens'] if savings_data else None,\n        \"toon_tokens\": savings_data['toon_tokens'] if savings_data else None,\n        \"savings_percent\": round(savings_data['savings_percent'], 2) if savings_data else None\n    \n    }\n\n    return {\n        \"report\": final_report,\n        \"metrics\": metrics\n    }"
            start_line: 75
            end_line: 477
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.update_status
            name: update_status
            args[1]: msg
            docstring: null
            source_code: "def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)"
            start_line: 77
            end_line: 80
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.notebook_workflow
            name: notebook_workflow
            args[4]: input,api_keys,model,status_callback
            docstring: null
            source_code: "def notebook_workflow(input, api_keys, model, status_callback=None):\n    t_start = time.time()\n    final_report = \"keine Notebooks gefunden\"\n    def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)\n\n    def gemini_payload(basic_info, nb_path, xml_content, images):\n        \n        intro_json = json.dumps({\n            \"basic_info\": basic_info,\n            \"current_notebook_path\": nb_path\n        }, indent=2)\n        \n        payload_content = []\n\n        payload_content.append({\n            \"type\": \"text\",\n            \"text\": f\"Context Information:\\n{intro_json}\\n\\nNotebook XML Structure:\\n\"\n        })\n\n        # Regex to find: <IMAGE_PLACEHOLDER index=\"0\" mime=\"image/png\"/>\n        pattern = r'(<IMAGE_PLACEHOLDER index=\"(\\d+)\" mime=\"([^\"]+)\"/>)'\n        last_pos = 0\n        \n        for match in re.finditer(pattern, xml_content):\n            text_segment = xml_content[last_pos:match.start()]\n            if text_segment.strip():\n                payload_content.append({\n                    \"type\": \"text\",\n                    \"text\": text_segment\n                })\n                \n            image_index = int(match.group(2))\n            mime_type = match.group(3)\n            \n            if image_index < len(images):\n                img_data = images[image_index]\n                b64_string = img_data['data']\n                \n                payload_content.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:{mime_type};base64,{b64_string}\"\n                    }\n                })\n                \n            last_pos = match.end()\n\n        # Add any remaining text after the last image\n        remaining_text = xml_content[last_pos:]\n        if remaining_text.strip():\n            payload_content.append({\n                \"type\": \"text\",\n                \"text\": remaining_text\n            })\n\n        return payload_content\n\n\n    base_url = None\n\n    if model.startswith(\"gpt-\"):\n        input_api_key = api_keys.get(\"gpt\")\n    elif model.startswith(\"gemini-\"):\n        input_api_key = api_keys.get(\"gemini\")\n    elif \"/\" in model or model.startswith(\"alias-\") or any(x in model for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        input_api_key = api_keys.get(\"scadsllm\")\n        base_url = api_keys.get(\"scadsllm_base_url\")\n    else:\n        input_api_key = None\n        base_url = api_keys.get(\"ollama\")\n\n    update_status(\"üîç Analysiere Input...\")\n\n    # URL Extraktion\n    repo_url = None\n    url_pattern = r\"https?://(?:www\\.)?github\\.com/[^\\s]+\"\n    match = re.search(url_pattern, input)\n\n    if match:\n        repo_url = match.group(0)\n        logging.info(f\"Extracted repository URL: {repo_url}\")\n    else:\n        raise ValueError(\"Could not find a valid URL in the provided input.\")\n    \n    update_status(f\"‚¨áÔ∏è Klone Repository: {repo_url} ...\")\n\n    try: \n        with GitRepository(repo_url) as repo:\n            repo_files = repo.get_all_files()\n    except Exception as e:\n        logging.error(f\"Error cloning repository: {e}\")\n        raise\n    \n    update_status(f\"üîó Bereite Input vor...\")\n\n    # convert to XML\n    processed_data = process_repo_notebooks(repo_files)\n\n    \n    # Extrahiere Basic Infos\n    update_status(\"‚ÑπÔ∏è Extrahiere Basis-Informationen...\")\n    try:\n        info_extractor = ProjektInfoExtractor()\n        basic_project_info = info_extractor.extrahiere_info(dateien=repo_files, repo_url=repo_url)\n        logging.info(\"Basic project info extracted\")\n    except Exception as e:\n        logging.error(f\"Error extracting basic project info: {e}\")\n        basic_project_info = \"Could not extract basic info.\"\n\n\n\n    prompt_file_notebook_llm = \"SystemPrompts/SystemPromptNotebookLLM.txt\"\n    notebook_llm = MainLLM(\n        api_key=input_api_key, \n        prompt_file_path=prompt_file_notebook_llm,\n        model_name=model,\n        base_url=base_url,\n    )\n\n    notebook_reports = []\n    total_notebooks = len(processed_data)\n    \n    logging.info(f\"Starting sequential processing of {total_notebooks} notebooks...\")\n\n    # Iterate over each notebook file individually\n    for index, (nb_path, nb_data) in enumerate(processed_data.items(), 1):\n        \n        update_status(f\"üß† Generiere Report ({index}/{total_notebooks}): {os.path.basename(nb_path)}\")\n        \n        nb_xml = nb_data['xml']\n        nb_images = nb_data['images']\n\n        llm_payload = gemini_payload(basic_project_info, nb_path, nb_xml, nb_images)\n\n        try:\n            single_report = notebook_llm.call_llm(llm_payload)\n            if single_report and isinstance(single_report, str):\n                notebook_reports.append(single_report)\n            else:\n                logging.warning(f\"LLM lieferte kein Ergebnis f√ºr {nb_path}\")\n                notebook_reports.append(f\"## Analyse f√ºr {os.path.basename(nb_path)}\\nFehler: Das Modell lieferte keine Antwort.\")\n            \n        except Exception as e:\n            logging.error(f\"Error generating report for {nb_path}: {e}\")\n            notebook_reports.append(f\"# Error processing {nb_path}\\n\\nError: {str(e)}\\n\\n---\\n\")\n\n    # Concatenate all reports\n    final_report = \"\\n\\n<br>\\n<br>\\n<br>\\n\\n\".join([str(r) for r in notebook_reports if r is not None])\n\n    \n    # --- Speichern der Ergebnisse ---\n    output_dir = \"result\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Dateiname f√ºr Report\n    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M-%S\")\n    report_filename = f\"notebook_report_{timestamp}_NotebookLLM_{notebook_llm.model_name}.md\"\n    report_filepath = os.path.join(output_dir, report_filename)\n    \n    if final_report:\n        with open(report_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(final_report)\n        logging.info(f\"Final report saved to '{report_filepath}'.\")\n\n    # Am Ende der Funktion nach dem Speichern:\n    total_time = time.time() - t_start\n    \n    # Metriken f√ºr das Frontend bauen\n    metrics = {\n        \"helper_time\": \"0\", # Notebook-Workflow hat meist keinen separaten Helper\n        \"main_time\": round(total_time, 2),\n        \"total_time\": round(total_time, 2),\n        \"helper_model\": \"None\",\n        \"main_model\": model,\n        \"json_tokens\": 0,\n        \"toon_tokens\": 0,\n        \"savings_percent\": 0\n    }\n\n    return {\n        \"report\": final_report,\n        \"metrics\": metrics\n    }"
            start_line: 483
            end_line: 668
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.update_status
            name: update_status
            args[1]: msg
            docstring: null
            source_code: "def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)"
            start_line: 486
            end_line: 489
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.gemini_payload
            name: gemini_payload
            args[4]: basic_info,nb_path,xml_content,images
            docstring: null
            source_code: "def gemini_payload(basic_info, nb_path, xml_content, images):\n        \n        intro_json = json.dumps({\n            \"basic_info\": basic_info,\n            \"current_notebook_path\": nb_path\n        }, indent=2)\n        \n        payload_content = []\n\n        payload_content.append({\n            \"type\": \"text\",\n            \"text\": f\"Context Information:\\n{intro_json}\\n\\nNotebook XML Structure:\\n\"\n        })\n\n        # Regex to find: <IMAGE_PLACEHOLDER index=\"0\" mime=\"image/png\"/>\n        pattern = r'(<IMAGE_PLACEHOLDER index=\"(\\d+)\" mime=\"([^\"]+)\"/>)'\n        last_pos = 0\n        \n        for match in re.finditer(pattern, xml_content):\n            text_segment = xml_content[last_pos:match.start()]\n            if text_segment.strip():\n                payload_content.append({\n                    \"type\": \"text\",\n                    \"text\": text_segment\n                })\n                \n            image_index = int(match.group(2))\n            mime_type = match.group(3)\n            \n            if image_index < len(images):\n                img_data = images[image_index]\n                b64_string = img_data['data']\n                \n                payload_content.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:{mime_type};base64,{b64_string}\"\n                    }\n                })\n                \n            last_pos = match.end()\n\n        # Add any remaining text after the last image\n        remaining_text = xml_content[last_pos:]\n        if remaining_text.strip():\n            payload_content.append({\n                \"type\": \"text\",\n                \"text\": remaining_text\n            })\n\n        return payload_content"
            start_line: 491
            end_line: 541
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "backend/relationship_analyzer.py":
      ast_nodes:
        imports[4]: ast,os,logging,collections.defaultdict
        functions[1]:
          - mode: function_analysis
            identifier: backend.relationship_analyzer.path_to_module
            name: path_to_module
            args[2]: filepath,project_root
            docstring: Wandelt einen Dateipfad in einen Python-Modulpfad um.
            source_code: "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    module_path = rel_path.replace(os.path.sep, '.')\n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n    return module_path"
            start_line: 6
            end_line: 18
            context:
              calls[0]:
              called_by[0]:
        classes[2]:
          - mode: class_analysis
            identifier: backend.relationship_analyzer.ProjectAnalyzer
            name: ProjectAnalyzer
            docstring: null
            source_code: "class ProjectAnalyzer:\n    \n    def __init__(self, project_root):\n        self.project_root = os.path.abspath(project_root)\n        self.definitions = {}\n        self.call_graph = defaultdict(list)\n        self.file_asts = {} \n        self.ignore_dirs = {'.git', '.venv', 'venv', '__pycache__', 'node_modules', 'dist', 'build', 'docs'}\n\n    def analyze(self):\n        py_files = self._find_py_files()\n        \n        for filepath in py_files:\n            self._collect_definitions(filepath)\n            \n        for filepath in py_files:\n            self._resolve_calls(filepath)\n            \n        self.file_asts.clear()\n        \n        return self.call_graph\n\n    def get_raw_relationships(self):\n\n        outgoing = defaultdict(set)\n        incoming = defaultdict(set)\n\n        for callee_id, callers_info_list in self.call_graph.items():\n            for info in callers_info_list:\n                caller_id = info['caller']\n                \n                if caller_id and callee_id:\n                    outgoing[caller_id].add(callee_id)\n                    incoming[callee_id].add(caller_id)\n        \n        return {\n            \"outgoing\": {k: sorted(list(v)) for k, v in outgoing.items()},\n            \"incoming\": {k: sorted(list(v)) for k, v in incoming.items()}\n        }\n\n    def _find_py_files(self):\n        py_files = []\n        for root, dirs, files in os.walk(self.project_root):\n            dirs[:] = [d for d in dirs if d not in self.ignore_dirs]\n            for file in files:\n                if file.endswith(\".py\"):\n                    py_files.append(os.path.join(root, file))\n        return py_files\n\n    def _collect_definitions(self, filepath):\n        try:\n            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n                source = f.read()\n                tree = ast.parse(source, filename=filepath)\n                \n            self.file_asts[filepath] = tree\n            module_path = path_to_module(filepath, self.project_root)\n            \n            for node in ast.walk(tree):\n                if isinstance(node, ast.FunctionDef):\n                    parent = self._get_parent(tree, node)\n                    if isinstance(parent, ast.ClassDef):\n                        path_name = f\"{module_path}.{parent.name}.{node.name}\"\n                        def_type = 'method'\n                    else:\n                        path_name = f\"{module_path}.{node.name}\"\n                        def_type = 'function'\n                    self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': def_type}\n                elif isinstance(node, ast.ClassDef):\n                    path_name = f\"{module_path}.{node.name}\"\n                    self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': 'class'}\n        except Exception as e:\n            logging.error(f\"Error collecting definitions in {filepath}: {e}\")\n            self.file_asts[filepath] = None\n\n    def _get_parent(self, tree, node):\n        for parent in ast.walk(tree):\n            for child in ast.iter_child_nodes(parent):\n                if child is node:\n                    return parent\n        return None\n\n    def _resolve_calls(self, filepath):\n        tree = self.file_asts.get(filepath)\n        if not tree:\n            return\n\n        try:\n            resolver = CallResolverVisitor(filepath, self.project_root, self.definitions)\n            resolver.visit(tree)\n            \n            for callee_pathname, caller_info_list in resolver.calls.items():\n                self.call_graph[callee_pathname].extend(caller_info_list)\n        except Exception as e:\n            logging.error(f\"Error resolving calls in {filepath}: {e}\")"
            start_line: 20
            end_line: 114
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[7]:
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,project_root
                  docstring: null
                  start_line: 22
                  end_line: 27
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.analyze
                  name: analyze
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 29
                  end_line: 40
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.get_raw_relationships
                  name: get_raw_relationships
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 42
                  end_line: 58
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._find_py_files
                  name: _find_py_files
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 60
                  end_line: 67
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._collect_definitions
                  name: _collect_definitions
                  calls[0]:
                  called_by[0]:
                  args[2]: self,filepath
                  docstring: null
                  start_line: 69
                  end_line: 93
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._get_parent
                  name: _get_parent
                  calls[0]:
                  called_by[0]:
                  args[3]: self,tree,node
                  docstring: null
                  start_line: 95
                  end_line: 100
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._resolve_calls
                  name: _resolve_calls
                  calls[0]:
                  called_by[0]:
                  args[2]: self,filepath
                  docstring: null
                  start_line: 102
                  end_line: 114
          - mode: class_analysis
            identifier: backend.relationship_analyzer.CallResolverVisitor
            name: CallResolverVisitor
            docstring: null
            source_code: "class CallResolverVisitor(ast.NodeVisitor):\n    def __init__(self, filepath, project_root, definitions):\n        self.filepath = filepath\n        self.module_path = path_to_module(filepath, project_root)\n        self.definitions = definitions\n        self.scope = {}\n        self.instance_types = {}\n        self.current_caller_name = self.module_path\n        self.current_class_name = None\n        self.calls = defaultdict(list)\n\n    def visit_ClassDef(self, node):\n        old_class_name = self.current_class_name\n        self.current_class_name = node.name\n        self.generic_visit(node)\n        self.current_class_name = old_class_name\n\n    def visit_FunctionDef(self, node):\n        old_caller = self.current_caller_name\n        \n        if self.current_class_name:\n            full_identifier = f\"{self.module_path}.{self.current_class_name}.{node.name}\"\n        else:\n            full_identifier = f\"{self.module_path}.{node.name}\"\n            \n        self.current_caller_name = full_identifier\n        self.generic_visit(node)\n        self.current_caller_name = old_caller\n\n    def visit_Call(self, node):\n        callee_pathname = self._resolve_call_qname(node.func)\n        if callee_pathname and callee_pathname in self.definitions:\n            \n            if self.current_caller_name == self.module_path:\n                caller_type = 'module'\n            elif '.<locals>.' in self.current_caller_name:\n                caller_type = 'local_function'\n            elif self.current_class_name:\n                caller_type = 'method'\n            else:\n                caller_type = 'function'\n            \n            caller_info = {\n                'file': os.path.basename(self.filepath),\n                'line': node.lineno,\n                'caller': self.current_caller_name,\n                'caller_type': caller_type \n            }\n            self.calls[callee_pathname].append(caller_info)\n        self.generic_visit(node)\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.scope[alias.asname or alias.name] = alias.name\n        self.generic_visit(node)\n        \n    def visit_ImportFrom(self, node):\n        module = node.module or ''\n        for alias in node.names:\n            name = alias.asname or alias.name\n            if node.level > 0:\n                base = self.module_path.split('.')\n                prefix = '.'.join(base[:-node.level])\n                full_module_path = f\"{prefix}.{module}\" if module else prefix\n            else:\n                full_module_path = module\n            self.scope[name] = f\"{full_module_path}.{alias.name}\"\n        self.generic_visit(node)\n\n    def visit_Assign(self, node):\n        if isinstance(node.value, ast.Call) and isinstance(node.value.func, ast.Name):\n            class_name = node.value.func.id\n            if class_name in self.scope:\n                qualified_class_name = self.scope[class_name]\n                if qualified_class_name in self.definitions:\n                    for target in node.targets:\n                        if isinstance(target, ast.Name):\n                            self.instance_types[target.id] = qualified_class_name\n        self.generic_visit(node)\n\n    def _resolve_call_qname(self, func_node):\n        if isinstance(func_node, ast.Name):\n            name = func_node.id\n            if name in self.scope:\n                return self.scope[name]\n            local_pathname = f\"{self.module_path}.{name}\"\n            if local_pathname in self.definitions:\n                return local_pathname\n        elif isinstance(func_node, ast.Attribute) and isinstance(func_node.value, ast.Name):\n            var_name = func_node.value.id\n            method_name = func_node.attr\n            if var_name in self.instance_types:\n                class_pathname = self.instance_types[var_name]\n                return f\"{class_pathname}.{method_name}\"\n            if var_name in self.scope:\n                module_pathname = self.scope[var_name]\n                return f\"{module_pathname}.{method_name}\"\n        return None"
            start_line: 117
            end_line: 214
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[8]:
                - identifier: backend.relationship_analyzer.CallResolverVisitor.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[4]: self,filepath,project_root,definitions
                  docstring: null
                  start_line: 118
                  end_line: 126
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 128
                  end_line: 132
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 134
                  end_line: 144
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Call
                  name: visit_Call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 146
                  end_line: 166
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 168
                  end_line: 171
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 173
                  end_line: 184
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Assign
                  name: visit_Assign
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 186
                  end_line: 195
                - identifier: backend.relationship_analyzer.CallResolverVisitor._resolve_call_qname
                  name: _resolve_call_qname
                  calls[0]:
                  called_by[0]:
                  args[2]: self,func_node
                  docstring: null
                  start_line: 197
                  end_line: 214
    "backend/scads_key_test.py":
      ast_nodes:
        imports[3]: os,dotenv.load_dotenv,openai.OpenAI
        functions[0]:
        classes[0]:
    "database/db.py":
      ast_nodes:
        imports[7]: datetime.datetime,pymongo.MongoClient,dotenv.load_dotenv,streamlit_authenticator,cryptography.fernet.Fernet,uuid,os
        functions[29]:
          - mode: function_analysis
            identifier: database.db.encrypt_text
            name: encrypt_text
            args[1]: text
            docstring: null
            source_code: "def encrypt_text(text: str) -> str:\n    if not text or not cipher_suite: return text\n    return cipher_suite.encrypt(text.strip().encode()).decode()"
            start_line: 28
            end_line: 30
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.decrypt_text
            name: decrypt_text
            args[1]: text
            docstring: null
            source_code: "def decrypt_text(text: str) -> str:\n    if not text or not cipher_suite: return text\n    try:\n        return cipher_suite.decrypt(text.strip().encode()).decode()\n    except Exception:\n        return text"
            start_line: 32
            end_line: 37
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.insert_user
            name: insert_user
            args[3]: username,name,password
            docstring: null
            source_code: "def insert_user(username: str, name: str, password: str):\n    user = {\n        \"_id\": username,\n        \"name\": name, \n        \"hashed_password\": stauth.Hasher.hash(password),\n        \"gemini_api_key\": \"\",\n        \"ollama_base_url\": \"\",\n        \"gpt_api_key\": \"\"\n    }\n    result = dbusers.insert_one(user)\n    return result.inserted_id"
            start_line: 43
            end_line: 53
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_all_users
            name: fetch_all_users
            args[0]:
            docstring: null
            source_code: "def fetch_all_users():\n    return list(dbusers.find())"
            start_line: 55
            end_line: 56
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_user
            name: fetch_user
            args[1]: username
            docstring: null
            source_code: "def fetch_user(username: str):\n    return dbusers.find_one({\"_id\": username})"
            start_line: 58
            end_line: 59
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_user_name
            name: update_user_name
            args[2]: username,new_name
            docstring: null
            source_code: "def update_user_name(username: str, new_name: str):\n    # Achtung: _id kann in Mongo nicht einfach so ge√§ndert werden. \n    # Hier wird nur das Name-Feld geupdated.\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"name\": new_name}})\n    return result.modified_count"
            start_line: 61
            end_line: 65
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_gemini_key
            name: update_gemini_key
            args[2]: username,gemini_api_key
            docstring: null
            source_code: "def update_gemini_key(username: str, gemini_api_key: str):\n    encrypted_key = encrypt_text(gemini_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"gemini_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 67
            end_line: 70
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_gpt_key
            name: update_gpt_key
            args[2]: username,gpt_api_key
            docstring: null
            source_code: "def update_gpt_key(username: str, gpt_api_key: str):\n    encrypted_key = encrypt_text(gpt_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"gpt_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 72
            end_line: 75
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_ollama_url
            name: update_ollama_url
            args[2]: username,ollama_base_url
            docstring: null
            source_code: "def update_ollama_url(username: str, ollama_base_url: str):\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"ollama_base_url\": ollama_base_url.strip()}})\n    return result.modified_count"
            start_line: 77
            end_line: 79
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_opensrc_key
            name: update_opensrc_key
            args[2]: username,opensrc_api_key
            docstring: null
            source_code: "def update_opensrc_key(username: str, opensrc_api_key: str):\n    encrypted_key = encrypt_text(opensrc_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"opensrc_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 81
            end_line: 84
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_opensrc_url
            name: update_opensrc_url
            args[2]: username,opensrc_base_url
            docstring: null
            source_code: "def update_opensrc_url(username: str, opensrc_base_url: str):\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"opensrc_base_url\": opensrc_base_url.strip()}})\n    return result.modified_count"
            start_line: 86
            end_line: 88
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_gemini_key
            name: fetch_gemini_key
            args[1]: username
            docstring: null
            source_code: "def fetch_gemini_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"gemini_api_key\": 1, \"_id\": 0})\n    return user.get(\"gemini_api_key\") if user else None"
            start_line: 90
            end_line: 92
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_ollama_url
            name: fetch_ollama_url
            args[1]: username
            docstring: null
            source_code: "def fetch_ollama_url(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"ollama_base_url\": 1, \"_id\": 0})\n    return user.get(\"ollama_base_url\") if user else None"
            start_line: 94
            end_line: 96
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_gpt_key
            name: fetch_gpt_key
            args[1]: username
            docstring: null
            source_code: "def fetch_gpt_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"gpt_api_key\": 1, \"_id\": 0})\n    return user.get(\"gpt_api_key\") if user else None"
            start_line: 98
            end_line: 100
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_opensrc_key
            name: fetch_opensrc_key
            args[1]: username
            docstring: null
            source_code: "def fetch_opensrc_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"opensrc_api_key\": 1, \"_id\": 0})\n    return user.get(\"opensrc_api_key\") if user else None"
            start_line: 102
            end_line: 104
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_opensrc_url
            name: fetch_opensrc_url
            args[1]: username
            docstring: null
            source_code: "def fetch_opensrc_url(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"opensrc_base_url\": 1, \"_id\": 0})\n    return user.get(\"opensrc_base_url\") if user else None"
            start_line: 106
            end_line: 108
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.delete_user
            name: delete_user
            args[1]: username
            docstring: null
            source_code: "def delete_user(username: str):\n    return dbusers.delete_one({\"_id\": username}).deleted_count"
            start_line: 110
            end_line: 111
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.get_decrypted_api_keys
            name: get_decrypted_api_keys
            args[1]: username
            docstring: null
            source_code: "def get_decrypted_api_keys(username: str):\n    user = dbusers.find_one({\"_id\": username})\n    if not user: return None, None\n    gemini_plain = decrypt_text(user.get(\"gemini_api_key\", \"\"))\n    ollama_plain = user.get(\"ollama_base_url\", \"\")\n    gpt_plain = decrypt_text(user.get(\"gpt_api_key\", \"\"))\n    opensrc_plain = decrypt_text(user.get(\"opensrc_api_key\", \"\"))\n    opensrc_url = user.get(\"opensrc_base_url\", \"\")\n    return gemini_plain, ollama_plain, gpt_plain, opensrc_plain, opensrc_url"
            start_line: 113
            end_line: 121
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.insert_chat
            name: insert_chat
            args[2]: username,chat_name
            docstring: Erstellt einen neuen Chat-Eintrag.
            source_code: "def insert_chat(username: str, chat_name: str):\n    \"\"\"Erstellt einen neuen Chat-Eintrag.\"\"\"\n    chat = {\n        \"_id\": str(uuid.uuid4()),\n        \"username\": username,\n        \"chat_name\": chat_name,\n        \"created_at\": datetime.now()\n    }\n    result = dbchats.insert_one(chat)\n    return result.inserted_id"
            start_line: 127
            end_line: 136
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_chats_by_user
            name: fetch_chats_by_user
            args[1]: username
            docstring: Holt alle definierten Chats eines Users.
            source_code: "def fetch_chats_by_user(username: str):\n    \"\"\"Holt alle definierten Chats eines Users.\"\"\"\n    # Sortieren nach Erstellung macht Sinn\n    chats = list(dbchats.find({\"username\": username}).sort(\"created_at\", 1))\n    return chats"
            start_line: 138
            end_line: 142
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.check_chat_exists
            name: check_chat_exists
            args[2]: username,chat_name
            docstring: null
            source_code: "def check_chat_exists(username: str, chat_name: str):\n    return dbchats.find_one({\"username\": username, \"chat_name\": chat_name}) is not None"
            start_line: 144
            end_line: 145
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.rename_chat_fully
            name: rename_chat_fully
            args[3]: username,old_name,new_name
            docstring: Benennt einen Chat und alle zugeh√∂rigen Exchanges um.
            source_code: "def rename_chat_fully(username: str, old_name: str, new_name: str):\n    \"\"\"\n    Benennt einen Chat und alle zugeh√∂rigen Exchanges um.\n    \"\"\"\n    # 1. Chat-Eintrag umbenennen\n    result = dbchats.update_one(\n        {\"username\": username, \"chat_name\": old_name}, \n        {\"$set\": {\"chat_name\": new_name}}\n    )\n    \n    # 2. Alle Messages (Exchanges) umh√§ngen\n    dbexchanges.update_many(\n        {\"username\": username, \"chat_name\": old_name},\n        {\"$set\": {\"chat_name\": new_name}}\n    )\n    return result.modified_count"
            start_line: 148
            end_line: 163
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.insert_exchange
            name: insert_exchange
            args[13]: question,answer,feedback,username,chat_name,helper_used,main_used,total_time,helper_time,main_time,json_tokens,toon_tokens,savings_percent
            docstring: null
            source_code: "def insert_exchange(question: str, answer: str, feedback: str, username: str, chat_name: str, \n                    helper_used: str=\"\", main_used: str=\"\", total_time: str=\"\", helper_time: str=\"\", main_time: str=\"\",\n                    json_tokens=0, toon_tokens=0, savings_percent=0.0):\n    new_id = str(uuid.uuid4())\n    exchange = {\n        \"_id\": new_id,\n        \"question\": question,\n        \"answer\": answer,\n        \"feedback\": feedback,\n        \"feedback_message\": \"\",\n        \"chat_name\": chat_name,\n        \"username\": username,\n        \"helper_used\": helper_used,\n        \"main_used\": main_used,\n        \"total_time\": total_time,\n        \"helper_time\": helper_time,\n        \"main_time\": main_time,\n        \"json_tokens\": json_tokens,\n        \"toon_tokens\": toon_tokens,\n        \"savings_percent\": savings_percent,\n        \"created_at\": datetime.now()\n    }\n    try:\n        dbexchanges.insert_one(exchange)\n        return new_id\n    except Exception as e:\n        print(f\"DB Error: {e}\")\n        return None"
            start_line: 169
            end_line: 196
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_exchanges_by_user
            name: fetch_exchanges_by_user
            args[1]: username
            docstring: null
            source_code: "def fetch_exchanges_by_user(username: str):\n    # Sortieren nach Zeitstempel wichtig f√ºr die Anzeige\n    exchanges = list(dbexchanges.find({\"username\": username}).sort(\"created_at\", 1))\n    return exchanges"
            start_line: 198
            end_line: 201
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_exchanges_by_chat
            name: fetch_exchanges_by_chat
            args[2]: username,chat_name
            docstring: null
            source_code: "def fetch_exchanges_by_chat(username: str, chat_name: str):\n    exchanges = list(dbexchanges.find({\"username\": username, \"chat_name\": chat_name}).sort(\"created_at\", 1))\n    return exchanges"
            start_line: 203
            end_line: 205
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_exchange_feedback
            name: update_exchange_feedback
            args[2]: exchange_id,feedback
            docstring: null
            source_code: "def update_exchange_feedback(exchange_id, feedback: int):\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback\": feedback}})\n    return result.modified_count"
            start_line: 207
            end_line: 209
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_exchange_feedback_message
            name: update_exchange_feedback_message
            args[2]: exchange_id,feedback_message
            docstring: null
            source_code: "def update_exchange_feedback_message(exchange_id, feedback_message: str):\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback_message\": feedback_message}})\n    return result.modified_count"
            start_line: 211
            end_line: 213
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.delete_exchange_by_id
            name: delete_exchange_by_id
            args[1]: exchange_id
            docstring: null
            source_code: "def delete_exchange_by_id(exchange_id: str):\n    result = dbexchanges.delete_one({\"_id\": exchange_id})\n    return result.deleted_count"
            start_line: 215
            end_line: 217
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.delete_full_chat
            name: delete_full_chat
            args[2]: username,chat_name
            docstring: "L√∂scht den Chat UND alle zugeh√∂rigen Exchanges.\nDas sorgt f√ºr Konsistenz zwischen Frontend und Backend."
            source_code: "def delete_full_chat(username: str, chat_name: str):\n    \"\"\"\n    L√∂scht den Chat UND alle zugeh√∂rigen Exchanges.\n    Das sorgt f√ºr Konsistenz zwischen Frontend und Backend.\n    \"\"\"\n    # 1. Alle Nachrichten in diesem Chat l√∂schen\n    del_exchanges = dbexchanges.delete_many({\"username\": username, \"chat_name\": chat_name})\n    \n    # 2. Den Chat selbst aus der Chat-Liste l√∂schen\n    del_chat = dbchats.delete_one({\"username\": username, \"chat_name\": chat_name})\n    \n    return del_chat.deleted_count"
            start_line: 221
            end_line: 232
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "frontend/frontend.py":
      ast_nodes:
        imports[16]: numpy,datetime.datetime,time,pymongo.MongoClient,dotenv.load_dotenv,os,sys,urllib.parse.urlparse,logging,traceback,re,streamlit_mermaid.st_mermaid,backend.main,database.db,streamlit,streamlit_authenticator
        functions[12]:
          - mode: function_analysis
            identifier: frontend.frontend.clean_names
            name: clean_names
            args[1]: model_list
            docstring: null
            source_code: "def clean_names(model_list):\n    return [m.split(\"/\")[-1] for m in model_list]"
            start_line: 54
            end_line: 55
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.get_filtered_models
            name: get_filtered_models
            args[2]: source_list,category_name
            docstring: Filtert eine Liste basierend auf der gew√§hlten Kategorie.
            source_code: "def get_filtered_models(source_list, category_name):\n    \"\"\"Filtert eine Liste basierend auf der gew√§hlten Kategorie.\"\"\"\n    keywords = CATEGORY_KEYWORDS.get(category_name, [\"\"])\n    \n    if \"STANDARD\" in keywords:\n        # Nur Modelle zur√ºckgeben, die auch in der Standard-Liste sind\n        return [m for m in source_list if m in STANDARD_MODELS]\n    \n    filtered = []\n    for model in source_list:\n        # Pr√ºfen ob ein Keyword im Namen steckt\n        if any(k in model.lower() for k in keywords):\n            filtered.append(model)\n            \n    return filtered if filtered else source_list"
            start_line: 86
            end_line: 100
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.save_gemini_cb
            name: save_gemini_cb
            args[0]:
            docstring: null
            source_code: "def save_gemini_cb():\n    new_key = st.session_state.get(\"in_gemini_key\", \"\")\n    if new_key:\n        db.update_gemini_key(st.session_state[\"username\"], new_key)\n        st.session_state[\"in_gemini_key\"] = \"\"\n        st.toast(\"Gemini Key erfolgreich gespeichert! ‚úÖ\")"
            start_line: 143
            end_line: 148
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.save_ollama_cb
            name: save_ollama_cb
            args[0]:
            docstring: null
            source_code: "def save_ollama_cb():\n    new_url = st.session_state.get(\"in_ollama_url\", \"\")\n    if new_url:\n        db.update_ollama_url(st.session_state[\"username\"], new_url)\n        st.toast(\"Ollama URL gespeichert! ‚úÖ\")"
            start_line: 150
            end_line: 154
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.load_data_from_db
            name: load_data_from_db
            args[1]: username
            docstring: L√§dt Chats und Exchanges konsistent aus der DB.
            source_code: "def load_data_from_db(username: str):\n    \"\"\"L√§dt Chats und Exchanges konsistent aus der DB.\"\"\"\n    \n    # Nur laden, wenn neuer User oder noch nicht geladen\n    if \"loaded_user\" not in st.session_state or st.session_state.loaded_user != username:\n        st.session_state.chats = {}\n        \n        # 1. Erst die definierten Chats laden (damit auch leere Chats da sind)\n        db_chats = db.fetch_chats_by_user(username)\n        for c in db_chats:\n            c_name = c.get(\"chat_name\")\n            if c_name:\n                st.session_state.chats[c_name] = {\"exchanges\": []}\n\n        # 2. Dann die Exchanges laden und einsortieren\n        db_exchanges = db.fetch_exchanges_by_user(username)\n        for ex in db_exchanges:\n            c_name = ex.get(\"chat_name\", \"Unbenannt\")\n            \n            # Falls Exchanges existieren f√ºr Chats, die nicht in dbchats sind (Legacy Support)\n            if c_name not in st.session_state.chats:\n                st.session_state.chats[c_name] = {\"exchanges\": []}\n            \n            if \"feedback\" not in ex or ex[\"feedback\"] is None:\n                ex[\"feedback\"] = np.nan\n            \n            st.session_state.chats[c_name][\"exchanges\"].append(ex)\n\n        # 3. Default Chat erstellen, falls gar nichts existiert\n        if not st.session_state.chats:\n            initial_name = \"Chat 1\"\n            # Konsistent in DB anlegen\n            db.insert_chat(username, initial_name)\n            st.session_state.chats[initial_name] = {\"exchanges\": []}\n            st.session_state.active_chat = initial_name\n        else:\n            # Active Chat setzen, falls n√∂tig\n            if \"active_chat\" not in st.session_state or st.session_state.active_chat not in st.session_state.chats:\n                # Nimm den ersten verf√ºgbaren Chat\n                st.session_state.active_chat = list(st.session_state.chats.keys())[0]\n        \n        st.session_state.loaded_user = username"
            start_line: 160
            end_line: 201
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.handle_feedback_change
            name: handle_feedback_change
            args[2]: ex,val
            docstring: null
            source_code: "def handle_feedback_change(ex, val):\n    ex[\"feedback\"] = val\n    db.update_exchange_feedback(ex[\"_id\"], val)\n    st.rerun()"
            start_line: 207
            end_line: 210
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.handle_delete_exchange
            name: handle_delete_exchange
            args[2]: chat_name,ex
            docstring: null
            source_code: "def handle_delete_exchange(chat_name, ex):\n    db.delete_exchange_by_id(ex[\"_id\"])\n    if chat_name in st.session_state.chats:\n        if ex in st.session_state.chats[chat_name][\"exchanges\"]:\n            st.session_state.chats[chat_name][\"exchanges\"].remove(ex)\n    st.rerun()"
            start_line: 212
            end_line: 217
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.handle_delete_chat
            name: handle_delete_chat
            args[2]: username,chat_name
            docstring: null
            source_code: "def handle_delete_chat(username, chat_name):\n    # KONSISTENZ: Ruft jetzt delete_full_chat in DB auf\n    db.delete_full_chat(username, chat_name)\n    \n    # State bereinigen\n    if chat_name in st.session_state.chats:\n        del st.session_state.chats[chat_name]\n    \n    # Active Chat neu setzen\n    if len(st.session_state.chats) > 0:\n        st.session_state.active_chat = list(st.session_state.chats.keys())[0]\n    else:\n        # Wenn alles weg ist, neuen leeren Chat anlegen\n        new_name = \"Chat 1\"\n        db.insert_chat(username, new_name)\n        st.session_state.chats[new_name] = {\"exchanges\": []}\n        st.session_state.active_chat = new_name\n        \n    st.rerun()"
            start_line: 219
            end_line: 237
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.extract_repo_name
            name: extract_repo_name
            args[1]: text
            docstring: null
            source_code: "def extract_repo_name(text):\n    url_match = re.search(r'(https?://[^\\s]+)', text)\n    if url_match:\n        url = url_match.group(0)\n        parsed = urlparse(url)\n        path = parsed.path.strip(\"/\")\n        if path:\n            repo_name = path.split(\"/\")[-1]\n            if repo_name.endswith(\".git\"):\n                repo_name = repo_name[:-4]\n            return repo_name\n    return None"
            start_line: 243
            end_line: 254
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.stream_text_generator
            name: stream_text_generator
            args[1]: text
            docstring: null
            source_code: "def stream_text_generator(text):\n    for word in text.split(\" \"):\n        yield word + \" \"\n        time.sleep(0.01)"
            start_line: 256
            end_line: 259
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.render_text_with_mermaid
            name: render_text_with_mermaid
            args[2]: markdown_text,should_stream
            docstring: null
            source_code: "def render_text_with_mermaid(markdown_text, should_stream=False):\n    if not markdown_text:\n        return\n\n    parts = re.split(r\"```mermaid\\s+(.*?)\\s+```\", markdown_text, flags=re.DOTALL)\n\n    for i, part in enumerate(parts):\n        if i % 2 == 0:\n            if part.strip():\n                if should_stream:\n                    st.write_stream(stream_text_generator(part))\n                else:\n                    st.markdown(part)\n        else:\n            try:\n                st_mermaid(part, key=f\"mermaid_{hash(part)}_{i}\")\n            except Exception:\n                st.code(part, language=\"mermaid\")"
            start_line: 261
            end_line: 278
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.render_exchange
            name: render_exchange
            args[2]: ex,current_chat_name
            docstring: null
            source_code: "def render_exchange(ex, current_chat_name):\n    st.chat_message(\"user\").write(ex[\"question\"])\n\n    with st.chat_message(\"assistant\"):\n        answer_text = ex.get(\"answer\", \"\")\n        is_error = answer_text.startswith(\"Fehler\") or answer_text.startswith(\"Error\")\n\n        # --- 1. TOOLBAR CONTAINER ---\n        # Nutzt die neuen Parameter horizontal=True und alignment\n        # border=True rahmt die Toolbar ein\n        # horizontal_alignment=\"right\" schiebt alles nach rechts\n        with st.container(horizontal=True, horizontal_alignment=\"right\"):\n            \n            if not is_error:\n                # 1. Status Text (wird jetzt links von den Buttons angezeigt, aber rechtsb√ºndig im Container)\n                st.write(\"hier die Antwort:\")\n                if ex.get(\"feedback\") == 1:\n                    st.caption(\"‚úÖ Hilfreich\")\n                elif ex.get(\"feedback\") == 0:\n                    st.caption(\"‚ùå Nicht hilfreich\")\n                \n                # 2. Die Buttons (einfach untereinander im Code, erscheinen nebeneinander im UI)\n                \n                # Like\n                type_primary_up = ex.get(\"feedback\") == 1\n                if st.button(\"üëç\", key=f\"up_{ex['_id']}\", type=\"primary\" if type_primary_up else \"secondary\", help=\"Hilfreich\"):\n                    handle_feedback_change(ex, 1)\n\n                # Dislike\n                type_primary_down = ex.get(\"feedback\") == 0\n                if st.button(\"üëé\", key=f\"down_{ex['_id']}\", type=\"primary\" if type_primary_down else \"secondary\", help=\"Nicht hilfreich\"):\n                    handle_feedback_change(ex, 0)\n\n                # Comment Popover\n                with st.popover(\"üí¨\", help=\"Notiz hinzuf√ºgen\"):\n                    msg = st.text_area(\"Notiz:\", value=ex.get(\"feedback_message\", \"\"), key=f\"txt_{ex['_id']}\")\n                    if st.button(\"Speichern\", key=f\"save_{ex['_id']}\"):\n                        ex[\"feedback_message\"] = msg\n                        db.update_exchange_feedback_message(ex[\"_id\"], msg)\n                        st.success(\"Gespeichert!\")\n                        time.sleep(0.5)\n                        st.rerun()\n\n                # Download\n                st.download_button(\n                    label=\"üì•\",\n                    data=ex[\"answer\"],\n                    file_name=f\"response_{ex['_id']}.md\",\n                    mime=\"text/markdown\",\n                    key=f\"dl_{ex['_id']}\",\n                    help=\"Als Markdown herunterladen\"\n                )\n\n                # Delete\n                if st.button(\"üóëÔ∏è\", key=f\"del_{ex['_id']}\", help=\"Nachricht l√∂schen\", type=\"secondary\"):\n                    handle_delete_exchange(current_chat_name, ex)\n\n            else:\n                # Fehlerfall\n                st.error(\"‚ö†Ô∏è Fehler\")\n                if st.button(\"üóëÔ∏è L√∂schen\", key=f\"del_err_{ex['_id']}\"):\n                    handle_delete_exchange(current_chat_name, ex)\n\n        # --- 2. CONTENT (Antwort) ---\n        with st.container(height=500, border=True):\n             render_text_with_mermaid(ex[\"answer\"], should_stream=False)"
            start_line: 284
            end_line: 349
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "schemas/types.py":
      ast_nodes:
        imports[5]: typing.List,typing.Optional,typing.Literal,pydantic.BaseModel,pydantic.ValidationError
        functions[0]:
        classes[15]:
          - mode: class_analysis
            identifier: schemas.types.ParameterDescription
            name: ParameterDescription
            docstring: Describes a single parameter of a function.
            source_code: "class ParameterDescription(BaseModel):\n    \"\"\"Describes a single parameter of a function.\"\"\"\n    name: str\n    type: str\n    description: str"
            start_line: 6
            end_line: 10
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ReturnDescription
            name: ReturnDescription
            docstring: Describes the return value of a function.
            source_code: "class ReturnDescription(BaseModel):\n    \"\"\"Describes the return value of a function.\"\"\"\n    name: str\n    type: str\n    description: str"
            start_line: 12
            end_line: 16
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.UsageContext
            name: UsageContext
            docstring: Describes the calling context of a function.
            source_code: "class UsageContext(BaseModel):\n    \"\"\"Describes the calling context of a function.\"\"\"\n    calls: str\n    called_by: str"
            start_line: 18
            end_line: 21
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionDescription
            name: FunctionDescription
            docstring: Contains the detailed analysis of a function's purpose and signature.
            source_code: "class FunctionDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a function's purpose and signature.\"\"\"\n    overall: str\n    parameters: List[ParameterDescription]\n    returns: List[ReturnDescription]\n    usage_context: UsageContext"
            start_line: 23
            end_line: 28
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionAnalysis
            name: FunctionAnalysis
            docstring: The main model representing the entire JSON schema for a function.
            source_code: "class FunctionAnalysis(BaseModel):\n    \"\"\"The main model representing the entire JSON schema for a function.\"\"\"\n    identifier: str\n    description: FunctionDescription\n    error: Optional[str] = None"
            start_line: 30
            end_line: 34
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ConstructorDescription
            name: ConstructorDescription
            docstring: Describes the __init__ method of a class.
            source_code: "class ConstructorDescription(BaseModel):\n    \"\"\"Describes the __init__ method of a class.\"\"\"\n    description: str\n    parameters: List[ParameterDescription]"
            start_line: 39
            end_line: 42
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassContext
            name: ClassContext
            docstring: Describes the class's external dependencies and primary points of instantiation.
            source_code: "class ClassContext(BaseModel):\n    \"\"\"Describes the class's external dependencies and primary points of instantiation.\"\"\"\n    dependencies: str\n    instantiated_by: str"
            start_line: 44
            end_line: 47
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassDescription
            name: ClassDescription
            docstring: "Contains the detailed analysis of a class's purpose, constructor, and methods."
            source_code: "class ClassDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a class's purpose, constructor, and methods.\"\"\"\n    overall: str\n    init_method: ConstructorDescription\n    methods: List[FunctionAnalysis]\n    usage_context: ClassContext"
            start_line: 49
            end_line: 54
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassAnalysis
            name: ClassAnalysis
            docstring: The main model for the entire JSON schema for a class.
            source_code: "class ClassAnalysis(BaseModel):\n    \"\"\"The main model for the entire JSON schema for a class.\"\"\"\n    identifier: str\n    description: ClassDescription\n    error: Optional[str] = None"
            start_line: 56
            end_line: 60
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.CallInfo
            name: CallInfo
            docstring: "Represents a specific call event from the relationship analyzer.\nUsed in 'called_by' and 'instantiated_by' lists."
            source_code: "class CallInfo(BaseModel):\n    \"\"\"\n    Represents a specific call event from the relationship analyzer.\n    Used in 'called_by' and 'instantiated_by' lists.\n    \"\"\"\n    file: str\n    function: str  # Name des Aufrufers\n    mode: str      # z.B. 'method', 'function', 'module'\n    line: int"
            start_line: 65
            end_line: 73
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionContextInput
            name: FunctionContextInput
            docstring: Structured context for analyzing a function.
            source_code: "class FunctionContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a function.\"\"\"\n    calls: List[str]\n    called_by: List[CallInfo]"
            start_line: 78
            end_line: 81
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionAnalysisInput
            name: FunctionAnalysisInput
            docstring: The required input to generate a FunctionAnalysis object.
            source_code: "class FunctionAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a FunctionAnalysis object.\"\"\"\n    mode: Literal[\"function_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: FunctionContextInput"
            start_line: 83
            end_line: 89
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.MethodContextInput
            name: MethodContextInput
            docstring: Structured context for a classes methods
            source_code: "class MethodContextInput(BaseModel):\n    \"\"\"Structured context for a classes methods\"\"\"\n    identifier: str\n    calls: List[str]\n    called_by: List[CallInfo]\n    args: List[str]\n    docstring: Optional[str]"
            start_line: 94
            end_line: 100
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassContextInput
            name: ClassContextInput
            docstring: Structured context for analyzing a class.
            source_code: "class ClassContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a class.\"\"\"\n    dependencies: List[str]\n    instantiated_by: List[CallInfo]\n    method_context: List[MethodContextInput]"
            start_line: 102
            end_line: 106
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassAnalysisInput
            name: ClassAnalysisInput
            docstring: The required input to generate a ClassAnalysis object.
            source_code: "class ClassAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a ClassAnalysis object.\"\"\"\n    mode: Literal[\"class_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: ClassContextInput"
            start_line: 108
            end_line: 114
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
analysis_results:
  functions:
    backend.AST_Schema.path_to_module:
      identifier: backend.AST_Schema.path_to_module
      description:
        overall: "This function converts a file system path into a Python module import path. It first determines the relative path of the given `filepath` to the `project_root`. If a `ValueError` occurs during this process, it defaults to using only the base name of the file. It then removes the `.py` extension and replaces all operating system path separators with dots. Finally, it removes the `.__init__` suffix if the resulting module path represents an `__init__.py` file."
        parameters[2]{name,type,description}:
          filepath,str,The absolute or relative path to a Python file.
          project_root,str,"The root directory of the project, used to calculate the relative path."
        returns[1]{name,type,description}:
          module_path,str,The Python module path derived from the filepath.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.File_Dependency.build_file_dependency_graph:
      identifier: backend.File_Dependency.build_file_dependency_graph
      description:
        overall: "This function constructs a directed graph representing file-level import dependencies within a given repository. It initializes a NetworkX directed graph and then employs a `FileDependencyGraph` visitor to traverse the Abstract Syntax Tree (AST) of a specified file. The visitor collects all import dependencies, which are subsequently used to populate the graph. For each identified caller file and its imported callees, nodes are added to the graph, and directed edges are created to illustrate these relationships. The function ultimately returns the fully constructed NetworkX DiGraph."
        parameters[3]{name,type,description}:
          filename,str,The path to the file whose dependencies are to be analyzed.
          tree,AST,The Abstract Syntax Tree (AST) object representing the source code of the file.
          repo_root,str,"The root directory of the repository, used for resolving relative import paths."
        returns[1]{name,type,description}:
          graph,nx.DiGraph,A NetworkX directed graph where nodes represent files and edges indicate import dependencies from one file to another.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.File_Dependency.build_repository_graph:
      identifier: backend.File_Dependency.build_repository_graph
      description:
        overall: "This function constructs a directed graph representing the dependencies within a given Git repository. It iterates through all Python files in the repository, parses each file into an Abstract Syntax Tree (AST), and then uses a helper function, `build_file_dependency_graph`, to create a dependency graph for that individual file. Finally, it merges all these file-level graphs into a single global directed graph, which is then returned."
        parameters[1]{name,type,description}:
          repository,GitRepository,The GitRepository object containing the files to be analyzed for dependencies.
        returns[1]{name,type,description}:
          global_graph,nx.DiGraph,A NetworkX directed graph representing the aggregated dependencies across all Python files in the repository.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.File_Dependency.get_all_temp_files:
      identifier: backend.File_Dependency.get_all_temp_files
      description:
        overall: "This function identifies all Python files within a specified directory and its subdirectories. It takes a directory path as input, resolves it to an absolute path, and then recursively searches for all files ending with '.py'. The function returns a list of these Python files, with each file's path expressed relative to the initial root directory."
        parameters[1]{name,type,description}:
          directory,str,The path to the root directory from which to begin the search for Python files.
        returns[1]{name,type,description}:
          all_files,"list[Path]","A list of pathlib.Path objects, where each object represents a Python file found within the specified directory, with its path relative to the root directory."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.HelperLLM.main_orchestrator:
      identifier: backend.HelperLLM.main_orchestrator
      description:
        overall: "This function serves as a dummy data and processing loop for testing the LLMHelper class. It defines pre-computed analysis inputs and outputs for several example functions (add_item, check_stock, generate_report) using Pydantic models. It then initializes an LLMHelper instance and simulates the process of generating documentation for these functions, logging the results. The function demonstrates the expected data flow and structure for the LLMHelper's operations."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by other functions in the provided context.
      error: null
    backend.callgraph.make_safe_dot:
      identifier: backend.callgraph.make_safe_dot
      description:
        overall: "This function takes a NetworkX directed graph and an output file path. Its primary purpose is to create a DOT file representation of the graph where node names are 'safe' for DOT rendering, preventing issues with special characters. It achieves this by creating a copy of the graph, relabeling all nodes with simple, sequential identifiers (e.g., 'n0', 'n1'), and then storing the original node names as 'label' attributes for these new safe nodes. Finally, the modified graph is written to the specified path in DOT format."
        parameters[2]{name,type,description}:
          graph,nx.DiGraph,The NetworkX directed graph object to be processed and saved.
          out_path,str,The file path where the DOT representation of the graph will be written.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions in the provided context.
      error: null
    backend.callgraph.build_filtered_callgraph:
      identifier: backend.callgraph.build_filtered_callgraph
      description:
        overall: "This function constructs a directed call graph for a given Git repository. It first iterates through all Python files in the repository, parsing their Abstract Syntax Trees (ASTs) to identify and collect all functions defined within these files into a set of 'own functions'. Subsequently, it re-processes the parsed files to detect caller-callee relationships. The function then builds a NetworkX directed graph, adding an edge only if both the calling and called functions are part of the previously identified 'own functions' set. This process effectively filters the call graph to include only internal calls within the repository's codebase, excluding external library or framework calls."
        parameters[1]{name,type,description}:
          repo,GitRepository,The Git repository object from which to extract Python files and build the call graph.
        returns[1]{name,type,description}:
          global_graph,nx.DiGraph,"A NetworkX directed graph representing the filtered call graph, containing only edges between functions defined within the repository."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    backend.converter.wrap_cdata:
      identifier: backend.converter.wrap_cdata
      description:
        overall: "This function, `wrap_cdata`, takes a single string argument and encapsulates it within XML CDATA tags. It constructs a new string by prepending \"<![CDATA[\\n\" and appending \"\\n]]>\" to the provided content. This utility is typically used to ensure that arbitrary text, which might contain characters that would otherwise be interpreted as XML markup, is treated as plain character data by an XML parser. The function directly returns the newly formatted string."
        parameters[1]{name,type,description}:
          content,str,The string content to be wrapped within CDATA tags.
        returns[1]{name,type,description}:
          wrapped_content,str,"The input content string enclosed within `<![CDATA[\\n...\\n]]>` tags."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.extract_output_content:
      identifier: backend.converter.extract_output_content
      description:
        overall: "This function processes a list of output objects, typically from a notebook execution, to extract relevant content. It iterates through each output, categorizing it by type. For display data or execution results, it prioritizes extracting PNG images, falling back to JPEG, and generates an image placeholder while storing the base64 image data in a provided list. If no image is found, it extracts plain text. Stream outputs are appended directly, and error outputs are formatted as error messages. The function ultimately returns a list of these extracted text strings or image placeholders."
        parameters[2]{name,type,description}:
          outputs,list,"A list of output objects, likely from a notebook execution, each containing data or text."
          image_list,list,A list to which dictionaries containing image metadata and Base64 data will be appended.
        returns[1]{name,type,description}:
          extracted_xml_snippets,"List[str]","A list of strings, where each string is either extracted plain text, an XML-like image placeholder, or an error message."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.process_image:
      identifier: backend.converter.process_image
      description:
        overall: "This function processes image data based on a provided MIME type. It checks if the MIME type exists within an external 'data' dictionary. If found, it retrieves and cleans a base64 encoded string, appends a dictionary containing the MIME type and cleaned data to an external 'image_list', and returns a formatted placeholder string. Error handling is included to catch decoding issues, returning an error message string in such cases. If the MIME type is not present in 'data', the function returns None."
        parameters[1]{name,type,description}:
          mime_type,str,"The MIME type of the image to be processed, used as a key to retrieve its base64 encoded data from an external source."
        returns[3]{name,type,description}:
          image_placeholder_tag,str,"A formatted string representing an image placeholder, including its assigned index and MIME type, if processing is successful."
          error_message,str,An error message string indicating that the image could not be decoded due to an exception.
          None,NoneType,Returns None if the specified MIME type is not found in the 'data' dictionary.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.convert_notebook_to_xml:
      identifier: backend.converter.convert_notebook_to_xml
      description:
        overall: "This function takes the raw content of a Jupyter notebook as a string and converts it into an XML representation. It processes each cell, distinguishing between markdown and code cells. Markdown cell content is directly embedded, while code cell source is wrapped. If code cells have outputs, these are also processed and included in the XML, and any embedded images are extracted. The function handles potential parsing errors by returning a specific error message."
        parameters[1]{name,type,description}:
          file_content,str,"The raw content of a Jupyter notebook file, expected to be in JSON format."
        returns[2]{name,type,description}:
          xml_output,str,"A string containing the XML representation of the notebook cells, or an error message if parsing fails."
          extracted_images,list,A list of extracted images from the notebook outputs.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.process_repo_notebooks:
      identifier: backend.converter.process_repo_notebooks
      description:
        overall: "This function processes a list of repository files to identify Jupyter notebooks. It iterates through these identified notebooks, converting each one into an XML representation and extracting any associated images. The function then compiles these conversion results into a dictionary, where each notebook's path maps to its generated XML and extracted images."
        parameters[1]{name,type,description}:
          repo_files,"List[object]",A list of file objects from a repository. Each object is expected to have a 'path' attribute (string) for file identification and a 'content' attribute for the notebook's raw content.
        returns[1]{name,type,description}:
          results,"Dict[str, Dict[str, Any]]",A dictionary where keys are the paths of the processed Jupyter notebooks (string). Each value is another dictionary containing the 'xml' output (string) and 'images' (list of any type) generated from the notebook conversion.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.create_savings_chart:
      identifier: backend.main.create_savings_chart
      description:
        overall: "This function generates a bar chart comparing two token counts, `json_tokens` and `toon_tokens`. It visualizes these values using distinct colors and includes a title that displays a calculated `savings_percent`. The chart features a y-axis label, a grid, and explicitly shows the token count values above each bar for clarity. Finally, the function saves the generated chart to the specified `output_path` and closes the plot to release resources."
        parameters[4]{name,type,description}:
          json_tokens,int,The number of tokens representing the JSON format.
          toon_tokens,int,The number of tokens representing the TOON format.
          savings_percent,float,The percentage of savings to be displayed in the chart's title.
          output_path,str,The file path where the generated bar chart will be saved.
        returns[1]{name,type,description}:
          None,None,This function does not explicitly return any value; it saves a file as a side effect.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    backend.main.calculate_net_time:
      identifier: backend.main.calculate_net_time
      description:
        overall: "This function calculates the net processing time, accounting for potential sleep durations introduced by rate-limiting, specifically for 'gemini-' models. It first determines the total elapsed time between a start and end timestamp. If the model_name does not begin with 'gemini-', the total duration is returned directly. For 'gemini-' models, it calculates the number of batches based on total_items and batch_size, then estimates the total sleep time. The net time is derived by subtracting this estimated sleep time from the total duration, ensuring the result is non-negative."
        parameters[5]{name,type,description}:
          start_time,float,The timestamp when the process began.
          end_time,float,The timestamp when the process concluded.
          total_items,int,The total number of items processed.
          batch_size,int,The number of items processed per batch.
          model_name,str,"The name of the model used, which influences rate-limit calculations."
        returns[1]{name,type,description}:
          net_time,float,"The calculated duration in seconds, adjusted for rate-limiting sleep times, or the total duration if no rate-limiting adjustment is needed."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    backend.main.main_workflow:
      identifier: backend.main.main_workflow
      description:
        overall: "The `main_workflow` function orchestrates a comprehensive analysis pipeline for a given GitHub repository. It begins by extracting API keys and model names from the provided inputs, then clones the specified repository URL. The function proceeds to extract basic project information, construct a file tree, analyze code relationships, and build an Abstract Syntax Tree (AST) schema, which is subsequently enriched with relationship data. It then prepares and dispatches analysis tasks for individual functions and classes to a Helper LLM, before finally utilizing a Main LLM to synthesize a comprehensive report based on all collected and analyzed data. The process includes status updates, error handling, token evaluation, and saving the final report along with performance metrics."
        parameters[4]{name,type,description}:
          input,str,"The initial user input, expected to contain a GitHub repository URL for analysis."
          api_keys,dict,"A dictionary containing various API keys (e.g., 'gemini', 'gpt', 'scadsllm') and base URLs required for LLM interactions and other services."
          model_names,dict,A dictionary specifying the names of the models to be used for the 'helper' and 'main' LLMs in the workflow.
          status_callback,callable | None,An optional callable function used to provide progress updates throughout the workflow.
        returns[2]{name,type,description}:
          report,str,"The final generated report from the Main LLM, detailing the repository analysis."
          metrics,dict,"A dictionary containing performance metrics such as helper_time, main_time, total_time, helper_model, main_model, json_tokens, toon_tokens, and savings_percent."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.update_status:
      identifier: backend.main.update_status
      description:
        overall: "This function is designed to handle status updates. It accepts a message string as input. It first checks if a 'status_callback' is defined in the current scope; if so, it invokes this callback with the provided message. Subsequently, it logs the message at the INFO level using the 'logging' module, ensuring the message is recorded."
        parameters[1]{name,type,description}:
          msg,str,"The message string to be processed, potentially passed to a status callback, and logged."
        returns[1]{name,type,description}:
          None,None,This function performs side effects by potentially invoking a callback and logging a message; it does not return any explicit value.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.notebook_workflow:
      identifier: backend.main.notebook_workflow
      description:
        overall: "The notebook_workflow function orchestrates the analysis of Jupyter notebooks from a given GitHub repository. It clones the repository, extracts basic project information, converts notebooks to an XML-like structure with image placeholders, and then iteratively processes each notebook using a specified Large Language Model (LLM). It constructs a payload for the LLM, handles different LLM API key configurations, and aggregates individual notebook reports into a final markdown report, which is then saved to a file. Finally, it returns the combined report and execution metrics."
        parameters[4]{name,type,description}:
          input,str,"The input string, expected to contain a GitHub repository URL."
          api_keys,dict,"A dictionary containing API keys for various LLM services (e.g., 'gpt', 'gemini', 'scadsllm', 'ollama')."
          model,str,"The name of the LLM model to be used for notebook analysis (e.g., 'gpt-4', 'gemini-pro')."
          status_callback,callable | None,"An optional callback function to provide status updates during the workflow execution. If provided, it will be called with status messages."
        returns[2]{name,type,description}:
          report,str,A concatenated markdown string containing the analysis reports for all processed notebooks.
          metrics,dict,"A dictionary containing execution metrics such as 'helper_time', 'main_time', 'total_time', 'helper_model', 'main_model', 'json_tokens', 'toon_tokens', and 'savings_percent'."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by other functions in the provided context.
      error: null
    backend.main.gemini_payload:
      identifier: backend.main.gemini_payload
      description:
        overall: "This function constructs a multi-part payload suitable for the Gemini API, integrating textual content with base64-encoded images. It begins by serializing basic information and a notebook path into an introductory JSON string. The core logic involves parsing an XML-like string that may contain image placeholders. It extracts text segments and replaces image placeholders with corresponding base64 image data from a provided list. The function ultimately returns a list of dictionaries, each representing either a text or an image part, formatted for a generative AI model's input."
        parameters[4]{name,type,description}:
          basic_info,object,A dictionary or object containing basic contextual information to be included in the payload.
          nb_path,str,"The file path of the current notebook, used as part of the contextual information."
          xml_content,str,"An XML-like string representing the notebook's content, which may include special image placeholder tags."
          images,list,"A list of image data objects, where each object is expected to contain 'data' (base64 string) and 'mime' type."
        returns[1]{name,type,description}:
          payload_content,"list[dict]","A list of dictionaries, each formatted as a content part (either 'text' or 'image_url') for the Gemini API."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    backend.relationship_analyzer.path_to_module:
      identifier: backend.relationship_analyzer.path_to_module
      description:
        overall: "This function converts a given file system path into a Python module path string. It first attempts to calculate the path relative to a specified project root. If that fails, it uses the base name of the file. It then removes the '.py' extension if present, replaces path separators with dots, and finally adjusts for '__init__.py' files to represent their parent package."
        parameters[2]{name,type,description}:
          filepath,str,The absolute or relative path to a Python file.
          project_root,str,"The root directory of the project, used to calculate the relative path."
        returns[1]{name,type,description}:
          module_path,str,The converted Python module path string.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.encrypt_text:
      identifier: database.db.encrypt_text
      description:
        overall: "The `encrypt_text` function is designed to encrypt a given string using a `cipher_suite` object. It first performs a check: if the input `text` is empty or if `cipher_suite` is not initialized, the function bypasses encryption and returns the original text. Otherwise, it prepares the text by stripping leading/trailing whitespace, encodes it to bytes, performs the encryption, and then decodes the encrypted bytes back into a string before returning it."
        parameters[1]{name,type,description}:
          text,str,The string value to be encrypted.
        returns[1]{name,type,description}:
          encrypted_text,str,"The encrypted string. If the input text is empty or `cipher_suite` is unavailable, the original text is returned."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.decrypt_text:
      identifier: database.db.decrypt_text
      description:
        overall: "This function attempts to decrypt a given string using an external `cipher_suite` object. It first checks if the input `text` is empty or if `cipher_suite` is not initialized, returning the original text immediately in such cases. If conditions are met, it proceeds to decrypt the text by stripping leading/trailing whitespace, encoding it to bytes, applying the `cipher_suite.decrypt` method, and then decoding the result back into a string. The function includes error handling, returning the original, unencrypted text if any exception occurs during the decryption process."
        parameters[1]{name,type,description}:
          text,str,The string value that needs to be decrypted.
        returns[1]{name,type,description}:
          decrypted_or_original_text,str,"The decrypted string if the operation is successful, or the original string if decryption fails or if the initial conditions (non-empty text and initialized cipher_suite) are not met."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.insert_user:
      identifier: database.db.insert_user
      description:
        overall: "This function creates a new user document and inserts it into a database collection. It accepts a username, display name, and a plain-text password. The provided password is first hashed using `stauth.Hasher.hash` for security before being stored. The user document also includes initialized empty fields for Gemini, Ollama, and GPT API keys. Finally, the function inserts this prepared user document into the `dbusers` collection and returns the unique identifier of the newly created document."
        parameters[3]{name,type,description}:
          username,str,The unique identifier for the user.
          name,str,The display name of the user.
          password,str,"The plain-text password for the user, which will be hashed before storage."
        returns[1]{name,type,description}:
          inserted_id,Any,The unique identifier of the newly inserted user document.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_all_users:
      identifier: database.db.fetch_all_users
      description:
        overall: "This function is designed to retrieve all user records from a database collection. It executes a find operation on the `dbusers` collection, which is presumed to be a database interface. The results, typically a cursor of documents, are then converted into a standard Python list. This list, containing all fetched user data, is subsequently returned by the function."
        parameters[0]:
        returns[1]{name,type,description}:
          users,list,A list containing all user documents fetched from the 'dbusers' collection. Each item in the list represents a user record.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.fetch_user:
      identifier: database.db.fetch_user
      description:
        overall: "The `fetch_user` function is designed to retrieve a single user record from a database collection. It takes a username as input and uses it to query the `dbusers` collection. The function specifically searches for a document where the `_id` field matches the provided username, returning the first matching document found or `None` if no user is found."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user to be retrieved from the database.
        returns[1]{name,type,description}:
          user_document,dict | None,"The user document as a dictionary if a matching user is found, otherwise `None`."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.update_user_name:
      identifier: database.db.update_user_name
      description:
        overall: "This function is designed to update a user's name in a database. It takes the existing username, which serves as the unique identifier (`_id`), and a new name. The function performs an update operation on the `dbusers` collection, specifically setting the 'name' field for the matching user. It then returns the count of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          username,str,"The unique identifier of the user whose name is to be updated, corresponding to the `_id` field in the database."
          new_name,str,The new name to assign to the specified user.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation. A value of 0 indicates no document was found or updated.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_gemini_key:
      identifier: database.db.update_gemini_key
      description:
        overall: "This function updates a user's Gemini API key in a database. It takes a username and the new Gemini API key as input. The provided API key is first stripped of any leading/trailing whitespace and then encrypted. Finally, it attempts to locate a user document by its username and updates the 'gemini_api_key' field with the encrypted value. The function returns the count of documents that were successfully modified."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose Gemini API key is to be updated.
          gemini_api_key,str,"The new Gemini API key to be stored, which will be encrypted before being saved to the database."
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents modified by the update operation. This is typically 0 or 1, indicating whether the user's key was successfully updated."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_gpt_key:
      identifier: database.db.update_gpt_key
      description:
        overall: This function updates a user's GPT API key in the database. It takes a username and a plain-text GPT API key as input. The provided API key is first stripped of leading/trailing whitespace and then encrypted. The encrypted key is then stored in the 'gpt_api_key' field for the specified user in the 'dbusers' collection.
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose GPT API key is to be updated.
          gpt_api_key,str,The new plain-text GPT API key to be stored for the user.
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents modified by the update operation, typically 0 or 1."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_ollama_url:
      identifier: database.db.update_ollama_url
      description:
        overall: "This function updates the Ollama base URL for a specific user in a database. It takes a username and a new Ollama base URL as input. The provided URL is first stripped of any leading or trailing whitespace before being stored. The function then performs an update operation on the database, targeting the document identified by the username. It returns the count of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          username,str,The unique identifier (username) of the user whose Ollama base URL needs to be updated.
          ollama_base_url,str,The new Ollama base URL to be set for the specified user. Any leading or trailing whitespace will be removed before storage.
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents that were modified by the update operation. This will typically be 1 if the user exists and the URL was updated, or 0 if the user was not found or the URL was already the same."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_opensrc_key:
      identifier: database.db.update_opensrc_key
      description:
        overall: "This function updates a user's Open Source API key in the database. It first encrypts the provided API key after removing leading/trailing whitespace. Then, it uses the `dbusers` collection to find the user by their username and sets the `opensrc_api_key` field to the newly encrypted value. The function returns the count of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose API key is to be updated.
          opensrc_api_key,str,The new Open Source API key to be stored for the user.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified in the database by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.update_opensrc_url:
      identifier: database.db.update_opensrc_url
      description:
        overall: "This function updates a user's open-source base URL in the database. It takes a username and a new URL as input. The function performs an update operation on the `dbusers` collection, setting the `opensrc_base_url` field for the document matching the provided username. The `opensrc_base_url` is stripped of leading and trailing whitespace before being stored. It returns the count of modified documents."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose open-source base URL needs to be updated.
          opensrc_base_url,str,The new open-source base URL to be set for the user. This value will be stripped of whitespace.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_gemini_key:
      identifier: database.db.fetch_gemini_key
      description:
        overall: "The `fetch_gemini_key` function retrieves a user's Gemini API key from a database. It accepts a `username` string to identify the specific user. The function queries the `dbusers` collection, searching for a document where the `_id` matches the provided `username`. It then extracts the `gemini_api_key` field from the found user document. If a user is found and has a Gemini API key, that key is returned; otherwise, the function returns `None`."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose Gemini API key is to be fetched.
        returns[1]{name,type,description}:
          gemini_api_key,str | None,"The Gemini API key associated with the user, or None if the user is not found or the key does not exist."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_ollama_url:
      identifier: database.db.fetch_ollama_url
      description:
        overall: "This function retrieves the Ollama base URL for a specified user from a database. It queries the `dbusers` collection using the provided username as the `_id`. The function specifically projects the `ollama_base_url` field. It returns the extracted URL if a user is found and the URL exists, otherwise it returns `None`."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose Ollama base URL is to be fetched.
        returns[1]{name,type,description}:
          ollama_base_url,str | None,"The Ollama base URL associated with the user, or None if the user is not found or the URL is not set."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_gpt_key:
      identifier: database.db.fetch_gpt_key
      description:
        overall: "This function, `fetch_gpt_key`, is responsible for retrieving a user's GPT API key from a database. It queries the `dbusers` collection, using the provided `username` as the document's `_id` to locate the specific user. The function is designed to project only the `gpt_api_key` field from the found document. It returns the API key as a string if a user is found and has an associated key, otherwise it returns `None`."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose GPT API key is to be fetched.
        returns[1]{name,type,description}:
          gpt_api_key,str | None,"The GPT API key associated with the user, or None if the user or key is not found."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.fetch_opensrc_key:
      identifier: database.db.fetch_opensrc_key
      description:
        overall: "This function, `fetch_opensrc_key`, is designed to retrieve an Open Source API key associated with a specific user from a database. It takes a username as input and queries the `dbusers` collection to find the corresponding user document. If a user is found, it extracts the `opensrc_api_key` from that document. The function returns the API key if present, or `None` otherwise."
        parameters[1]{name,type,description}:
          username,str,The unique identifier (username) for which to retrieve the Open Source API key.
        returns[1]{name,type,description}:
          opensrc_api_key,str | None,"The Open Source API key associated with the provided username, or None if the user or key is not found."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.fetch_opensrc_url:
      identifier: database.db.fetch_opensrc_url
      description:
        overall: "This function is designed to retrieve a user's Open Source base URL from a database. It queries the 'dbusers' collection, searching for a document where the '_id' field matches the provided 'username'. If a user document is found, it extracts the 'opensrc_base_url' field. The function then returns this URL. If no user is found or the 'opensrc_base_url' field is absent, it returns None."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose Open Source base URL is to be fetched.
        returns[1]{name,type,description}:
          opensrc_base_url,str | None,"The Open Source base URL associated with the user, or None if the user is not found or the URL is not set."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.delete_user:
      identifier: database.db.delete_user
      description:
        overall: "This function is designed to remove a user record from a database collection. It takes a username as input and uses it to identify the document to be deleted. The function performs a delete operation on the `dbusers` collection, targeting the document where the `_id` field matches the provided username. It returns the count of documents successfully deleted."
        parameters[1]{name,type,description}:
          username,str,The unique identifier (username) of the user to be deleted from the database.
        returns[1]{name,type,description}:
          deleted_count,int,The number of documents (users) that were successfully deleted from the collection. Typically 0 or 1.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.get_decrypted_api_keys:
      identifier: database.db.get_decrypted_api_keys
      description:
        overall: "This function retrieves a user's API keys and base URLs from a database. It queries the 'dbusers' collection using the provided username. If the user is found, it decrypts specific API keys (Gemini, GPT, and open-source) using a 'decrypt_text' function and retrieves other URLs directly. The function returns a tuple containing all the processed keys and URLs. If the user is not found, it returns a tuple of two None values."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose API keys and URLs are to be retrieved.
        returns[5]{name,type,description}:
          gemini_plain,str | None,"The decrypted Gemini API key, or None if the user is not found."
          ollama_plain,str | None,"The Ollama base URL, or None if the user is not found."
          gpt_plain,str | None,"The decrypted GPT API key, or None if the user is not found."
          opensrc_plain,str | None,"The decrypted open-source API key, or None if the user is not found."
          opensrc_url,str | None,"The open-source base URL, or None if the user is not found."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.insert_chat:
      identifier: database.db.insert_chat
      description:
        overall: "The `insert_chat` function is responsible for creating a new chat entry within a database. It constructs a dictionary representing the chat, assigning a unique ID using `uuid.uuid4()`, the provided `username` and `chat_name`, and the current timestamp via `datetime.now()`. This chat document is then persisted to a MongoDB collection, likely `dbchats`, using the `insert_one` method. The function concludes by returning the unique `inserted_id` generated by the database for the new entry."
        parameters[2]{name,type,description}:
          username,str,The username associated with the new chat entry.
          chat_name,str,The name of the chat to be created.
        returns[1]{name,type,description}:
          inserted_id,str,The unique identifier of the newly created chat entry in the database.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_chats_by_user:
      identifier: database.db.fetch_chats_by_user
      description:
        overall: This function retrieves all chat documents associated with a specific user from a database collection. It queries the 'dbchats' collection for documents where the 'username' field matches the provided input. The retrieved chats are then sorted in ascending order based on their 'created_at' timestamp before being returned as a list.
        parameters[1]{name,type,description}:
          username,str,The username for which to fetch the associated chat documents.
        returns[1]{name,type,description}:
          chats,list,"A list of chat documents (dictionaries) belonging to the specified user, sorted by their creation date."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.check_chat_exists:
      identifier: database.db.check_chat_exists
      description:
        overall: "This function, `check_chat_exists`, is designed to verify the presence of a specific chat within the `dbchats` collection. It takes a username and a chat name as input parameters. The function executes a query to find a document that matches both the provided username and chat name. It returns a boolean value indicating whether such a chat exists in the database."
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to be checked.
          chat_name,str,The name of the chat to be checked for existence.
        returns[1]{name,type,description}:
          exists,bool,"Returns `True` if a chat with the specified username and chat name is found in the `dbchats` collection, otherwise returns `False`."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.rename_chat_fully:
      identifier: database.db.rename_chat_fully
      description:
        overall: "This function renames a chat and all its associated exchanges (messages) in the database. It first updates the chat entry in the 'dbchats' collection by changing the 'chat_name' from 'old_name' to 'new_name' for a specific 'username'. Subsequently, it updates all related exchanges in the 'dbexchanges' collection, also changing their 'chat_name' from 'old_name' to 'new_name' for the same 'username'. The function returns the count of modified chat entries from the initial update operation."
        parameters[3]{name,type,description}:
          username,str,The username associated with the chat to be renamed.
          old_name,str,The current name of the chat.
          new_name,str,The desired new name for the chat.
        returns[1]{name,type,description}:
          modified_count,int,The number of chat entries that were modified by the update_one operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.insert_exchange:
      identifier: database.db.insert_exchange
      description:
        overall: "This function inserts a new exchange record into a database collection. It generates a unique identifier for the record using `uuid.uuid4()` and constructs a dictionary containing the question, answer, feedback, user details, chat name, and various optional performance metrics like helper/main models used, time taken, and token counts. A `created_at` timestamp is added using `datetime.now()`. The function attempts to insert this prepared record into the `dbexchanges` collection. If successful, it returns the new record's ID; otherwise, it catches any exceptions, prints an error, and returns `None`."
        parameters[13]{name,type,description}:
          question,str,The user's question in the exchange.
          answer,str,The generated answer for the question.
          feedback,str,The feedback provided for the exchange.
          username,str,The username associated with the exchange.
          chat_name,str,The name of the chat where the exchange occurred.
          helper_used,str,"Optional: Information about the helper model used, defaults to an empty string."
          main_used,str,"Optional: Information about the main model used, defaults to an empty string."
          total_time,str,"Optional: The total time taken for the exchange, defaults to an empty string."
          helper_time,str,"Optional: The time spent by the helper model, defaults to an empty string."
          main_time,str,"Optional: The time spent by the main model, defaults to an empty string."
          json_tokens,int,"Optional: The number of JSON tokens used, defaults to 0."
          toon_tokens,int,"Optional: The number of Toon tokens used, defaults to 0."
          savings_percent,float,"Optional: The percentage of savings achieved, defaults to 0.0."
        returns[2]{name,type,description}:
          new_id,str,The unique identifier of the newly inserted exchange record if the operation is successful.
          None,None,Returned if an exception occurs during the database insertion.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_exchanges_by_user:
      identifier: database.db.fetch_exchanges_by_user
      description:
        overall: "This function is designed to retrieve all exchange records associated with a specific user from a database. It queries the `dbexchanges` collection using the provided username as a filter. The results are then sorted chronologically by their 'created_at' timestamp in ascending order. Finally, the function returns these sorted exchange records as a list, facilitating their display or further processing."
        parameters[1]{name,type,description}:
          username,str,The unique identifier of the user whose exchange records are to be fetched.
        returns[1]{name,type,description}:
          exchanges,list,"A list of exchange documents (dictionaries) belonging to the specified user, sorted by their creation timestamp."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_exchanges_by_chat:
      identifier: database.db.fetch_exchanges_by_chat
      description:
        overall: This function retrieves a list of exchange documents from a database collection named `dbexchanges`. It filters these documents based on a provided username and a specific chat name. The results are then sorted by their `created_at` timestamp in ascending order and returned as a list.
        parameters[2]{name,type,description}:
          username,str,The username used to filter the exchanges.
          chat_name,str,The name of the chat used to filter the exchanges.
        returns[1]{name,type,description}:
          exchanges,list,"A list of exchange documents matching the specified username and chat name, sorted by creation time."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_exchange_feedback:
      identifier: database.db.update_exchange_feedback
      description:
        overall: "This function is responsible for updating the feedback score associated with a specific exchange record in a database. It accepts an exchange identifier and an integer feedback value. The function performs an update operation on the 'dbexchanges' collection, targeting the document with the matching '_id' and setting its 'feedback' field to the provided integer. It then returns the count of documents that were successfully modified by this operation."
        parameters[2]{name,type,description}:
          exchange_id,Any,The unique identifier for the exchange record that needs its feedback updated. This is used to locate the specific document in the database.
          feedback,int,The integer value representing the new feedback score to be assigned to the specified exchange record.
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents that were modified as a result of the update operation. A value of 1 typically indicates a successful update, while 0 suggests no matching document was found or no change was necessary."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_exchange_feedback_message:
      identifier: database.db.update_exchange_feedback_message
      description:
        overall: This function updates a specific exchange record in a database collection named 'dbexchanges'. It identifies the target record using the provided 'exchange_id' and sets its 'feedback_message' field to the new string value. The function then returns the count of documents that were successfully modified by this operation.
        parameters[2]{name,type,description}:
          exchange_id,Any,The unique identifier for the exchange record to be updated. This is used to match the '_id' field in the database.
          feedback_message,str,The new string value to be set as the feedback message for the specified exchange.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation. This will typically be 0 or 1.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.delete_exchange_by_id:
      identifier: database.db.delete_exchange_by_id
      description:
        overall: "This function is designed to remove a specific exchange record from a database collection. It accepts a unique identifier, `exchange_id`, and uses it to locate and delete a single document within the `dbexchanges` collection. The function then reports the number of documents successfully deleted, which will typically be 1 if a match was found and removed, or 0 otherwise."
        parameters[1]{name,type,description}:
          exchange_id,str,The unique identifier of the exchange record to be deleted from the database.
        returns[1]{name,type,description}:
          deleted_count,int,The number of documents that were deleted (0 or 1).
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.delete_full_chat:
      identifier: database.db.delete_full_chat
      description:
        overall: "This function is designed to completely remove a specific chat and all its associated message exchanges from the database. It operates by first deleting all message exchanges linked to the provided username and chat name, and then proceeds to delete the chat entry itself. This two-step process ensures data consistency between the frontend and backend systems. The function returns the count of chat documents that were successfully deleted."
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to be deleted.
          chat_name,str,The name of the chat to be deleted.
        returns[1]{name,type,description}:
          deleted_count,int,The number of chat documents deleted from the database. This will typically be 0 or 1 for a delete_one operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.clean_names:
      identifier: frontend.frontend.clean_names
      description:
        overall: "This function processes a list of strings, where each string is expected to represent a path or an identifier containing forward slashes. It iterates through the provided list and for each item, it splits the string by the '/' character. The function then extracts and returns only the last segment of the split string, effectively cleaning the names to their base components."
        parameters[1]{name,type,description}:
          model_list,"List[str]","A list of strings, where each string is expected to be a path-like identifier that may contain '/' characters."
        returns[1]{name,type,description}:
          cleaned_names,"List[str]","A new list containing the last segment of each input string after splitting by '/', effectively providing cleaned base names."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.get_filtered_models:
      identifier: frontend.frontend.get_filtered_models
      description:
        overall: "This function filters a given list of models, referred to as `source_list`, based on a specified `category_name`. It retrieves a set of keywords associated with the category from a global `CATEGORY_KEYWORDS` mapping. If the category's keywords include \"STANDARD\", the function returns only those models from the `source_list` that are also present in a predefined `STANDARD_MODELS` list. Otherwise, it iterates through the `source_list` and includes models whose names contain any of the category's keywords, performing a case-insensitive check. If no models match the keywords during this process, the original `source_list` is returned."
        parameters[2]{name,type,description}:
          source_list,list,The list of models to be filtered.
          category_name,str,The name of the category to use for filtering.
        returns[1]{name,type,description}:
          filtered_models,list,"A new list containing models that match the specified category's keywords, or the original `source_list` if no matches are found or if the 'STANDARD' category logic applies."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.save_gemini_cb:
      identifier: frontend.frontend.save_gemini_cb
      description:
        overall: "This function, `save_gemini_cb`, is designed to handle the saving of a new Gemini API key. It retrieves the potential new key from the Streamlit session state. If a non-empty key is found, it updates the user's Gemini key in the database using `db.update_gemini_key`. After successfully updating, it clears the temporary key from the session state and displays a success notification to the user."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.save_ollama_cb:
      identifier: frontend.frontend.save_ollama_cb
      description:
        overall: "This function is designed to save a new Ollama URL. It retrieves the URL from the Streamlit session state, specifically from the 'in_ollama_url' key. If a valid URL is found, it updates the database using the 'db.update_ollama_url' function, passing the current username and the new URL. Upon successful update, it displays a confirmation toast message to the user."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.load_data_from_db:
      identifier: frontend.frontend.load_data_from_db
      description:
        overall: "This function is responsible for loading chat and exchange data from the database into the Streamlit session state for a given user. It first checks if the data for the current user is already loaded to prevent redundant operations. If not, it initializes the session state, fetches predefined chats, then loads and organizes individual exchanges within those chats. The function also includes logic to handle legacy data where exchanges might exist for undefined chats and ensures a default chat is created if no chats are found for the user. It concludes by setting an active chat and marking the user's data as loaded."
        parameters[1]{name,type,description}:
          username,str,The username for whom to load chat and exchange data.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.handle_feedback_change:
      identifier: frontend.frontend.handle_feedback_change
      description:
        overall: "This function updates the feedback value for a given exchange object. It first modifies the 'feedback' key within the 'ex' dictionary-like object with the provided 'val'. Subsequently, it persists this feedback change to the database by calling 'db.update_exchange_feedback' using the exchange's '_id' and the new feedback value. Finally, it triggers a Streamlit rerun to refresh the UI."
        parameters[2]{name,type,description}:
          ex,dict,"A dictionary-like object representing an exchange, expected to contain at least '_id' and 'feedback' keys."
          val,Any,The new feedback value to be assigned to the exchange.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.handle_delete_exchange:
      identifier: frontend.frontend.handle_delete_exchange
      description:
        overall: "This function is responsible for deleting a specific exchange from the database and subsequently updating the application's session state. It first calls a database utility to remove the exchange by its ID. Afterwards, it checks if the associated chat exists in the Streamlit session state and, if the exchange is found within that chat's exchanges, it removes it from the session state. Finally, it triggers a Streamlit rerun to refresh the UI."
        parameters[2]{name,type,description}:
          chat_name,str,The name of the chat from which the exchange should be removed in the Streamlit session state.
          ex,dict,"The exchange object to be deleted, expected to contain an '_id' key for database deletion and to be matched in the session state."
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.handle_delete_chat:
      identifier: frontend.frontend.handle_delete_chat
      description:
        overall: "This function handles the deletion of a specific chat identified by a username and chat name. It first removes the chat from the database using `db.delete_full_chat`. Subsequently, it cleans up the chat from the Streamlit session state. If other chats exist, it sets the active chat to the first one available. If no chats remain after deletion, it creates a new default chat named 'Chat 1', inserts it into the database, and sets it as the active chat in the session state. Finally, it triggers a Streamlit rerun to update the UI."
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to be deleted.
          chat_name,str,The name of the chat to be deleted.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.extract_repo_name:
      identifier: frontend.frontend.extract_repo_name
      description:
        overall: "This function `extract_repo_name` is designed to parse a given text string and identify a repository name within it. It first attempts to find a URL using a regular expression. If a URL is present, it then uses `urlparse` to break down the URL and extract the path component. The last segment of this path is considered the potential repository name, with any \".git\" suffix being removed for cleanliness. The function returns the extracted repository name as a string or None if no URL is found or a repository name cannot be successfully derived."
        parameters[1]{name,type,description}:
          text,str,The input string that may contain a URL from which a repository name needs to be extracted.
        returns[1]{name,type,description}:
          repository_name,str | None,"The extracted repository name as a string, or None if no URL is found or a repository name cannot be determined from the URL path."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.stream_text_generator:
      identifier: frontend.frontend.stream_text_generator
      description:
        overall: "This function acts as a text generator, taking a string as input and yielding its words sequentially. It splits the input text by spaces and iterates through each word. For every word, it yields the word followed by a space, introducing a small delay of 0.01 seconds between each yield operation using `time.sleep`. This creates a streaming effect, delivering text word by word with a brief pause."
        parameters[1]{name,type,description}:
          text,str,The input string that will be split into words and streamed.
        returns[1]{name,type,description}:
          word,str,"A single word from the input text, followed by a space, yielded sequentially."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    frontend.frontend.render_text_with_mermaid:
      identifier: frontend.frontend.render_text_with_mermaid
      description:
        overall: "This function processes a given markdown text, identifying and rendering both standard markdown content and embedded Mermaid diagrams. It splits the input text based on ````mermaid ... ```` blocks. Regular markdown sections are rendered using `st.markdown` or streamed via `st.write_stream` if `should_stream` is true. Mermaid diagram code blocks are attempted to be rendered using `st_mermaid`, with a fallback to `st.code` if an error occurs during Mermaid rendering."
        parameters[2]{name,type,description}:
          markdown_text,str,The input text containing markdown and potentially embedded Mermaid diagram syntax.
          should_stream,bool,A flag indicating whether to stream the markdown output using `st.write_stream` or render it directly with `st.markdown`. Defaults to `False`.
        returns[1]{name,type,description}:
          None,None,This function performs side effects by rendering content to a Streamlit application and does not explicitly return a value.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.render_exchange:
      identifier: frontend.frontend.render_exchange
      description:
        overall: "This function is responsible for rendering a single chat exchange, which includes both the user's question and the assistant's answer, within a Streamlit application. It first displays the user's question. Subsequently, it renders the assistant's response along with an interactive toolbar. The toolbar provides options for users to give feedback (like/dislike), add comments, download the answer, and delete the exchange. In case the assistant's answer indicates an error, a specific error message and a delete option are displayed instead of the full toolbar."
        parameters[2]{name,type,description}:
          ex,dict,"An exchange object containing details such as the user's question, the assistant's answer, a unique identifier ('_id'), feedback status ('feedback'), and an optional feedback message ('feedback_message')."
          current_chat_name,str,"The name of the current chat session, used when handling the deletion of an exchange."
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
  classes:
    backend.AST_Schema.ASTVisitor:
      identifier: backend.AST_Schema.ASTVisitor
      description:
        overall: "The ASTVisitor class extends `ast.NodeVisitor` and is designed to traverse an Abstract Syntax Tree (AST) of a Python source file. Its primary purpose is to extract structured information about imports, functions, and classes defined within the visited source code. It builds a `schema` dictionary that categorizes these elements, including details like identifiers, names, docstrings, and source code segments, facilitating a comprehensive analysis of the code's structure."
        init_method:
          description: "The constructor initializes the ASTVisitor with the raw source code, the file's absolute path, and the project's root directory. It calculates the module's qualified path and sets up an empty dictionary (`self.schema`) to store extracted imports, functions, and classes. It also initializes `_current_class` to `None` for tracking the context of nested definitions."
          parameters[3]{name,type,description}:
            source_code,str,The raw source code of the file being visited.
            file_path,str,The absolute path to the file being visited.
            project_root,str,"The root directory of the project, used to determine the module path."
        methods[5]:
          - identifier: visit_Import
            description:
              overall: "This method processes `ast.Import` nodes, which represent `import module` statements. It iterates through each imported alias and appends the module name to the `imports` list within the `self.schema` dictionary. After processing the current node, it calls `self.generic_visit(node)` to ensure continued traversal of its children."
              parameters[1]{name,type,description}:
                node,ast.Import,The AST node representing an import statement.
              returns[0]:
              usage_context:
                calls: This method calls `self.generic_visit` to continue AST traversal.
                called_by: This method is called by the `ast.NodeVisitor`'s dispatch mechanism when an `ast.Import` node is encountered during AST traversal.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method processes `ast.ImportFrom` nodes, which represent `from module import name` statements. It iterates through each alias in the import statement and appends the fully qualified import name (e.g., `module.alias`) to the `imports` list within `self.schema`. It then calls `self.generic_visit(node)` to continue traversing the AST."
              parameters[1]{name,type,description}:
                node,ast.ImportFrom,The AST node representing a 'from ... import ...' statement.
              returns[0]:
              usage_context:
                calls: This method calls `self.generic_visit` to continue AST traversal.
                called_by: This method is called by the `ast.NodeVisitor`'s dispatch mechanism when an `ast.ImportFrom` node is encountered during AST traversal.
            error: null
          - identifier: visit_ClassDef
            description:
              overall: "This method processes `ast.ClassDef` nodes, representing class definitions. It constructs a dictionary containing detailed information about the class, including its identifier, name, docstring, source code segment, and line numbers. This class information is then appended to the `classes` list in `self.schema`. It temporarily sets `_current_class` to the newly created class info to correctly associate nested function definitions (methods) with their parent class. After visiting children, `_current_class` is reset."
              parameters[1]{name,type,description}:
                node,ast.ClassDef,The AST node representing a class definition.
              returns[0]:
              usage_context:
                calls: "This method calls `ast.get_docstring` and `ast.get_source_segment` to extract class details, and `self.generic_visit` for AST traversal."
                called_by: This method is called by the `ast.NodeVisitor`'s dispatch mechanism when an `ast.ClassDef` node is encountered during AST traversal.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method processes `ast.FunctionDef` nodes, representing function or method definitions. It first checks if a class is currently being visited (indicated by `_current_class`). If so, it creates method-specific information and appends it to the `method_context` of the current class. Otherwise, it creates function-specific information and appends it to the `functions` list in `self.schema`. It extracts details like identifier, name, arguments, docstring, and line numbers. Finally, it calls `self.generic_visit` to continue AST traversal."
              parameters[1]{name,type,description}:
                node,ast.FunctionDef,The AST node representing a function or method definition.
              returns[0]:
              usage_context:
                calls: "This method calls `ast.get_docstring` and `ast.get_source_segment` to extract function/method details, and `self.generic_visit` for AST traversal."
                called_by: This method is called by the `ast.NodeVisitor`'s dispatch mechanism when an `ast.FunctionDef` node is encountered during AST traversal.
            error: null
          - identifier: visit_AsyncFunctionDef
            description:
              overall: "This method processes `ast.AsyncFunctionDef` nodes, which represent asynchronous function definitions. It delegates the processing of these nodes directly to the `visit_FunctionDef` method, treating them similarly to regular function definitions for the purpose of schema generation and data extraction."
              parameters[1]{name,type,description}:
                node,ast.AsyncFunctionDef,The AST node representing an asynchronous function definition.
              returns[0]:
              usage_context:
                calls: This method calls `self.visit_FunctionDef` to handle the actual processing of the asynchronous function node.
                called_by: This method is called by the `ast.NodeVisitor`'s dispatch mechanism when an `ast.AsyncFunctionDef` node is encountered during AST traversal.
            error: null
        usage_context:
          dependencies: "The class depends on the `ast` module for AST traversal and node manipulation, and `path_to_module` for resolving module paths."
          instantiated_by: This class is not explicitly instantiated by other known components within the provided context.
      error: null
    backend.AST_Schema.ASTAnalyzer:
      identifier: backend.AST_Schema.ASTAnalyzer
      description:
        overall: "The ASTAnalyzer class is designed to process and analyze the Abstract Syntax Tree (AST) of a Python codebase within a Git repository. It provides functionality to parse individual Python files, extract their structural components (imports, functions, classes), and then integrate relationship data (like calls and dependencies) into the generated schema. This class serves as a core component for building a comprehensive, interconnected representation of a project's code structure."
        init_method:
          description: "This constructor initializes the ASTAnalyzer class. It does not take any specific parameters beyond 'self' and performs no explicit setup or attribute assignments, effectively creating a stateless instance."
          parameters[0]:
        methods[2]:
          - identifier: merge_relationship_data
            description:
              overall: "This method integrates raw relationship data, such as incoming and outgoing calls, into a comprehensive AST schema. It iterates through files, functions, and classes within the 'full_schema' to enrich their context. For functions, it populates 'calls' and 'called_by' lists. For classes, it adds 'instantiated_by' information and calculates external 'dependencies' based on method calls that are not internal to the class. The method modifies the 'full_schema' in place and then returns the updated schema."
              parameters[2]{name,type,description}:
                full_schema,dict,"A dictionary representing the complete Abstract Syntax Tree (AST) schema of a codebase, structured by files, functions, and classes."
                raw_relationships,dict,"A dictionary containing raw relationship data, typically with \"outgoing\" and \"incoming\" keys mapping identifiers to lists of related entities."
              returns[1]{name,type,description}:
                full_schema,dict,"The modified 'full_schema' dictionary, now enriched with relationship data for functions and classes."
              usage_context:
                calls: "This method primarily interacts with dictionary methods like 'get', 'items', and 'add' to process and update the schema."
                called_by: This method is not explicitly called by any other function or method within the provided context.
            error: null
          - identifier: analyze_repository
            description:
              overall: "This method analyzes a list of file objects from a Git repository to construct a full AST schema. It first determines the common project root from all file paths. It then iterates through each Python file, parses its content into an AST, and uses an ASTVisitor to extract schema nodes (imports, functions, classes). The extracted schema for each file is then added to a 'full_schema' dictionary, which is returned upon completion. Error handling is included for parsing failures."
              parameters[2]{name,type,description}:
                files,list,"A list of file objects, each expected to have 'path' and 'content' attributes."
                repo,GitRepository,"An instance of a GitRepository, though it's not directly used in the provided method body."
              returns[1]{name,type,description}:
                full_schema,dict,A dictionary representing the aggregated AST schema for all processed Python files in the repository.
              usage_context:
                calls: "This method calls 'os.path.commonpath', 'os.path.isfile', 'os.path.dirname', 'ast.parse', 'ASTVisitor' (instantiation), 'ASTVisitor.visit', and 'print'."
                called_by: This method is not explicitly called by any other function or method within the provided context.
            error: null
        usage_context:
          dependencies: "This class depends on the 'ast' module for parsing Python source code, the 'os' module for path manipulation, and 'getRepo.GitRepository' for repository interaction, as well as an 'ASTVisitor' class which is instantiated internally."
          instantiated_by: This class is not explicitly instantiated by any other component within the provided context.
      error: null
    backend.File_Dependency.FileDependencyGraph:
      identifier: backend.File_Dependency.FileDependencyGraph
      description:
        overall: "The FileDependencyGraph class extends the AST NodeVisitor to build a graph of import dependencies within Python files. It traverses the Abstract Syntax Tree of a given file, identifying both standard and 'from ... import ...' statements. Its core functionality involves resolving relative imports to their canonical forms and recording these relationships in an internal dictionary, providing a structured understanding of how files depend on each other within a repository."
        init_method:
          description: Initializes the FileDependencyGraph instance by setting the target filename and the repository root directory. These values are stored as instance attributes for use during the AST traversal and dependency resolution process.
          parameters[2]{name,type,description}:
            filename,str,The path to the Python file whose dependencies are to be analyzed.
            repo_root,Any,"The root directory of the repository, used for resolving file paths and relative imports."
        methods[3]:
          - identifier: _resolve_module_name
            description:
              overall: "This private method is responsible for resolving relative import statements, such as 'from .. import name1, name2'. It calculates the correct module path based on the import level and the current file's location within the repository. It verifies the existence of the target module file or symbol (including those exported via '__init__.py') and returns a list of successfully resolved names. An ImportError is raised if no valid modules or symbols can be found."
              parameters[1]{name,type,description}:
                node,ImportFrom,The AST node representing the 'from ... import ...' statement to be resolved.
              returns[1]{name,type,description}:
                resolved,"list[str]",A list of actual module or symbol names that the relative import resolves to.
              usage_context:
                calls: "This method calls external functions like 'get_all_temp_files', 'Path', 'Path.stem', 'Path.name', 'Path.resolve', 'Path.exists', 'Path.read_text', 'parse', 'walk', 'literal_eval', and 'iskeyword'. It also defines and utilizes two nested helper functions, 'module_file_exists' and 'init_exports_symbol'."
                called_by: This method is called by 'visit_ImportFrom' when processing relative import statements.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method is part of the AST NodeVisitor pattern, designed to process 'Import' and 'ImportFrom' nodes. It iterates through the aliases within the import statement and records each imported module or symbol name as a dependency for the current file in the 'import_dependencies' dictionary. It then calls 'generic_visit' to ensure continued traversal of the AST."
              parameters[2]{name,type,description}:
                node,Import | ImportFrom,The AST node representing either an 'import' or 'from ... import ...' statement.
                base_name,str | None,"An optional base name for the module, typically used when a specific module part has already been resolved (e.g., from a 'from ... import ...' statement)."
              returns[0]:
              usage_context:
                calls: This method calls 'self.generic_visit' to continue the AST traversal.
                called_by: This method is called by 'visit_ImportFrom' and implicitly by the AST NodeVisitor when encountering 'Import' nodes.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method, also part of the AST NodeVisitor, specifically handles 'ImportFrom' nodes. If the import is an absolute import (e.g., 'from a.b.c import d'), it extracts the base module name ('c') and passes it to 'visit_Import'. For relative imports (e.g., 'from .. import name'), it delegates the resolution to '_resolve_module_name'. Upon successful resolution, it records each resolved base name as a dependency via 'visit_Import', and includes error handling for failed relative import resolutions."
              parameters[1]{name,type,description}:
                node,ImportFrom,The AST node representing the 'from ... import ...' statement.
              returns[0]:
              usage_context:
                calls: "This method calls 'str.split', 'self.visit_Import', 'self._resolve_module_name', 'print', and 'self.generic_visit'."
                called_by: This method is implicitly called by the AST NodeVisitor when traversing an 'ImportFrom' node.
            error: null
        usage_context:
          dependencies: "The class depends on the 'ast' module for its core functionality as a NodeVisitor, specifically using 'Assign', 'ClassDef', 'FunctionDef', 'Import', 'ImportFrom', 'Name', 'NodeVisitor', 'literal_eval', 'parse', and 'walk'. It also relies on 'keyword.iskeyword' and 'pathlib.Path' for path manipulation and keyword checking, and an external 'get_all_temp_files' function."
          instantiated_by: The class is not explicitly shown to be instantiated by any other components in the provided context.
      error: null
    backend.HelperLLM.LLMHelper:
      identifier: backend.HelperLLM.LLMHelper
      description:
        overall: "The LLMHelper class provides a centralized interface for interacting with various Large Language Models (LLMs), specifically for generating structured documentation for Python functions and classes. It handles the configuration of different LLM providers (Gemini, OpenAI, Ollama, custom APIs), manages API keys, loads system prompts, and implements batch processing with rate limiting. The class ensures that LLM outputs conform to predefined Pydantic schemas (FunctionAnalysis and ClassAnalysis), facilitating reliable and structured data generation."
        init_method:
          description: "The constructor initializes the LLMHelper by validating the API key, loading system prompts from specified file paths, and configuring the underlying language model based on the `model_name`. It sets up different LLM clients (Google Gemini, OpenAI, Ollama, or custom) and wraps them with structured output capabilities for `FunctionAnalysis` and `ClassAnalysis` Pydantic schemas. It also calls `_configure_batch_settings` to set an appropriate batch size for API calls."
          parameters[5]{name,type,description}:
            api_key,str,"The API key for the chosen LLM service (e.g., Gemini, OpenAI)."
            function_prompt_path,str,File path to the system prompt for function analysis.
            class_prompt_path,str,File path to the system prompt for class analysis.
            model_name,str,"The name of the LLM model to use (default: \"gemini-2.0-flash-lite\")."
            base_url,str | None,Optional base URL for custom LLM endpoints like Ollama or SCADSLLM.
        methods[3]:
          - identifier: _configure_batch_settings
            description:
              overall: "This private method sets the `batch_size` attribute of the LLMHelper instance based on the provided `model_name`. It defines specific batch sizes for various Gemini, Llama, and GPT models, as well as a general batch size for custom or unknown models. The purpose is to optimize API call efficiency and respect rate limits by adjusting the number of concurrent requests. If an unknown model is encountered, it logs a warning and uses a conservative default batch size."
              parameters[1]{name,type,description}:
                model_name,str,The name of the LLM model for which to configure batch settings.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other methods or functions within its source code.
                called_by: This method is called by the `__init__` method of the `LLMHelper` class.
            error: null
          - identifier: generate_for_functions
            description:
              overall: "This method generates and validates documentation for a list of `FunctionAnalysisInput` objects using the configured LLM. It processes inputs in batches, converting each input into a JSON payload and constructing a conversation with the `function_system_prompt`. The method then calls the `function_llm` (which is configured for structured output of `FunctionAnalysis`) in batches, handling potential errors by extending the result list with `None` for failed items. It includes a waiting period between batches to adhere to API rate limits."
              parameters[1]{name,type,description}:
                function_inputs,"List[FunctionAnalysisInput]",A list of input objects containing function details for which documentation is to be generated.
              returns[1]{name,type,description}:
                None,"List[Optional[FunctionAnalysis]]","A list of `FunctionAnalysis` objects, where each object represents the generated and validated documentation for a function, or `None` if an error occurred during generation for that specific function."
              usage_context:
                calls: "This method calls `json.dumps`, `len`, `min`, `logging.info`, `logging.error`, `self.function_llm.batch`, and `time.sleep`."
                called_by: This method is not explicitly called by other methods within the provided `method_context`.
            error: null
          - identifier: generate_for_classes
            description:
              overall: "This method is responsible for generating and validating documentation for a list of `ClassAnalysisInput` objects. Similar to `generate_for_functions`, it processes inputs in batches, creating JSON payloads and conversations with the `class_system_prompt`. It uses the `class_llm` (configured for structured output of `ClassAnalysis`) to perform batched API calls. Error handling ensures that if a batch fails, the corresponding results are filled with `None` to maintain list order. A delay is introduced between batches to respect API rate limits."
              parameters[1]{name,type,description}:
                class_inputs,"List[ClassAnalysisInput]",A list of input objects containing class details for which documentation is to be generated.
              returns[1]{name,type,description}:
                None,"List[Optional[ClassAnalysis]]","A list of `ClassAnalysis` objects, where each object represents the generated and validated documentation for a class, or `None` if an error occurred during generation for that specific class."
              usage_context:
                calls: "This method calls `json.dumps`, `len`, `min`, `logging.info`, `logging.error`, `self.class_llm.batch`, and `time.sleep`."
                called_by: This method is not explicitly called by other methods within the provided `method_context`.
            error: null
        usage_context:
          dependencies: "The class depends on `os`, `json`, `logging`, `time`, components from the `typing` module, `dotenv.load_dotenv`, `langchain_google_genai.ChatGoogleGenerativeAI`, `langchain_ollama.ChatOllama`, `langchain_openai.ChatOpenAI`, components from `langchain.messages`, `pydantic.ValidationError`, and custom schemas like `schemas.types.FunctionAnalysis`, `schemas.types.ClassAnalysis`, `schemas.types.FunctionAnalysisInput`, `schemas.types.FunctionContextInput`, `schemas.types.ClassAnalysisInput`, `schemas.types.ClassContextInput`."
          instantiated_by: The class is not explicitly instantiated by any other components within the provided context.
      error: null
    backend.MainLLM.MainLLM:
      identifier: backend.MainLLM.MainLLM
      description:
        overall: "The MainLLM class serves as a versatile interface for interacting with various Large Language Models (LLMs), abstracting away the specifics of different providers. It handles the initialization of LLM clients (Gemini, OpenAI-compatible, Ollama) based on configuration, loads a system prompt, and provides methods for both synchronous and streaming communication with the chosen LLM. This class centralizes LLM interactions, making it easier to switch between models and manage prompts."
        init_method:
          description: "The constructor initializes the MainLLM instance by setting up the system prompt from a file and configuring the appropriate LLM client based on the `model_name`. It supports Google Generative AI (Gemini/GPT), custom OpenAI-compatible APIs (SCADSLLM_URL), and Ollama, raising errors if required environment variables or files are missing or if the API key is not provided."
          parameters[4]{name,type,description}:
            api_key,str,The API key for the chosen LLM provider.
            prompt_file_path,str,The file path to the system prompt that will be used for LLM interactions.
            model_name,str,"The name of the LLM model to use, defaulting to 'gemini-2.5-pro'. This determines which LLM client is initialized."
            base_url,str,"An optional base URL for custom LLM endpoints, used if `model_name` doesn't match specific providers like Gemini or OpenAI, or if an Ollama model is specified."
        methods[2]:
          - identifier: call_llm
            description:
              overall: "This method performs a synchronous call to the configured LLM. It constructs a list of messages including the system prompt and the user's input, then invokes the LLM and returns the content of the response. Error handling is included to log any issues during the API call, returning None in case of an exception."
              parameters[1]{name,type,description}:
                user_input,str,The user's query or message to send to the LLM for a synchronous response.
              returns[2]{name,type,description}:
                content,str,The textual content of the LLM's response.
                None,None,Returns None if an error occurs during the LLM call.
              usage_context:
                calls: "This method calls `SystemMessage` to create a system message, `HumanMessage` to create a user message, `self.llm.invoke` to send messages to the LLM, and `logging.info` and `logging.error` for logging purposes."
                called_by: This method is not explicitly called by any other functions or methods within the provided context.
            error: null
          - identifier: stream_llm
            description:
              overall: "This method initiates a streaming interaction with the LLM, yielding chunks of the response as they become available. It prepares the messages with the system prompt and user input, then iterates through the LLM's stream, yielding each content chunk. In case of an error, it logs the error and yields an error message string."
              parameters[1]{name,type,description}:
                user_input,str,The user's query or message for the streaming LLM interaction.
              returns[2]{name,type,description}:
                chunk.content,str,Yields individual content chunks from the LLM's streaming response.
                error_message,str,Yields an error message string if an exception occurs during the streaming call.
              usage_context:
                calls: "This method calls `SystemMessage` to create a system message, `HumanMessage` to create a user message, `self.llm.stream` to initiate a streaming call to the LLM, and `logging.info` and `logging.error` for logging purposes."
                called_by: This method is not explicitly called by any other functions or methods within the provided context.
            error: null
        usage_context:
          dependencies: "This class relies on external libraries such as `langchain_google_genai`, `langchain_ollama`, `langchain_openai`, `langchain.messages`, and the `logging` module. It also depends on environment variables like `SCADSLLM_URL` and `OLLAMA_BASE_URL` for certain model configurations."
          instantiated_by: This class is not explicitly instantiated by any other components within the provided context.
      error: null
    backend.basic_info.ProjektInfoExtractor:
      identifier: backend.basic_info.ProjektInfoExtractor
      description:
        overall: "The ProjektInfoExtractor class is designed to extract and consolidate fundamental project information from various common project files like README, pyproject.toml, and requirements.txt. It initializes an internal dictionary structure with placeholder values and then systematically parses these files to populate the fields. The class prioritizes information from pyproject.toml for dependencies and provides fallback mechanisms, such as deriving a project title from the repository URL, ensuring a comprehensive overview of the project."
        init_method:
          description: "The __init__ method initializes the ProjektInfoExtractor instance. It sets a constant `INFO_NICHT_GEFUNDEN` (German for \"Information not found\") and initializes a nested dictionary `self.info` with various project information fields, all pre-filled with this placeholder. This structure ensures that if no information is found during parsing, a default \"not found\" message is present."
          parameters[0]:
        methods[7]:
          - identifier: _clean_content
            description:
              overall: "This private helper method is responsible for sanitizing string content by removing null bytes (\\x00). Null bytes can appear due to encoding errors, especially when a file encoded as UTF-16 is incorrectly read as UTF-8. It returns an empty string if the input content is empty, ensuring that subsequent parsing operations receive clean data."
              parameters[1]{name,type,description}:
                content,str,The string content to be cleaned.
              returns[1]{name,type,description}:
                cleaned_content,str,The content with null bytes removed.
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions."
                called_by: "This method is called by _parse_readme, _parse_toml, and _parse_requirements."
            error: null
          - identifier: _finde_datei
            description:
              overall: "This private helper method searches for a file within a given list of file objects. It takes a list of filename patterns and a list of file objects, each expected to have a 'path' attribute. It performs a case-insensitive comparison of each file's path against the provided patterns, returning the first matching file object or None if no match is found."
              parameters[2]{name,type,description}:
                patterns,"List[str]",A list of filename patterns to search for.
                dateien,"List[Any]","A list of file objects, each expected to have a 'path' attribute."
              returns[1]{name,type,description}:
                found_file,"Optional[Any]","The first file object that matches a pattern, or None if no file is found."
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions."
                called_by: This method is called by extrahiere_info.
            error: null
          - identifier: _extrahiere_sektion_aus_markdown
            description:
              overall: "This private method extracts text content located under a Markdown level 2 heading (e.g., '## Heading'). It takes the Markdown content and a list of keywords. It constructs a regular expression to find any of the keywords in a heading and captures all text following it until the next level 2 heading or the end of the document, returning the stripped text."
              parameters[2]{name,type,description}:
                inhalt,str,The Markdown content to parse.
                keywords,"List[str]",A list of keywords to match against Markdown headings.
              returns[1]{name,type,description}:
                extracted_section,"Optional[str]","The stripped text content under the matched heading, or None if no matching section is found."
              usage_context:
                calls: "This method calls re.escape, re.compile, and re.search."
                called_by: This method is called by _parse_readme.
            error: null
          - identifier: _parse_readme
            description:
              overall: "This private method parses the content of a README file to extract various project details. It first cleans the content using `_clean_content`. It then attempts to extract the project title from a level 1 Markdown heading, a general description, key features, tech stack, current status, installation instructions, and a quick start guide by calling `_extrahiere_sektion_aus_markdown` with different keyword lists. The extracted information updates the `self.info` dictionary."
              parameters[1]{name,type,description}:
                inhalt,str,The raw content of the README file.
              returns[0]:
              usage_context:
                calls: "This method calls _clean_content, _extrahiere_sektion_aus_markdown, and re.search."
                called_by: This method is called by extrahiere_info.
            error: null
          - identifier: _parse_toml
            description:
              overall: "This private method parses the content of a pyproject.toml file. It first cleans the content. If the `tomllib` module is not available, it prints a warning and returns. Otherwise, it attempts to load the TOML content, extract the project name, description, and dependencies from the `[project]` section, and update the `self.info` dictionary. It includes error handling for `tomllib.TOMLDecodeError`."
              parameters[1]{name,type,description}:
                inhalt,str,The raw content of the pyproject.toml file.
              returns[0]:
              usage_context:
                calls: "This method calls _clean_content, tomllib.loads, and data.get."
                called_by: This method is called by extrahiere_info.
            error: null
          - identifier: _parse_requirements
            description:
              overall: "This private method parses the content of a requirements.txt file. It cleans the content and then extracts dependencies by splitting the content into lines, stripping whitespace, and filtering out empty lines or comments. It only updates the `self.info` dictionary's `dependencies` field if it hasn't already been populated by the TOML parser, ensuring `pyproject.toml` takes precedence."
              parameters[1]{name,type,description}:
                inhalt,str,The raw content of the requirements.txt file.
              returns[0]:
              usage_context:
                calls: This method calls _clean_content.
                called_by: This method is called by extrahiere_info.
            error: null
          - identifier: extrahiere_info
            description:
              overall: "This public method orchestrates the entire information extraction process. It first identifies relevant files (README, pyproject.toml, requirements.txt) using `_finde_datei`. It then parses these files in a prioritized order (pyproject.toml, then requirements.txt, then README) to populate the `self.info` dictionary. Finally, it formats the extracted dependencies and attempts to derive a project title from the `repo_url` if no title was found elsewhere, returning the complete `self.info` dictionary."
              parameters[2]{name,type,description}:
                dateien,"List[Any]","A list of file objects, each expected to have 'path' and 'content' attributes."
                repo_url,str,"The URL of the repository, used as a fallback for the project title."
              returns[1]{name,type,description}:
                project_info,"Dict[str, Any]",A dictionary containing all extracted project information.
              usage_context:
                calls: "This method calls _finde_datei, _parse_toml, _parse_requirements, _parse_readme, os.path.basename, and repo_url.removesuffix."
                called_by: This method is not explicitly called by any other method in the provided context.
            error: null
        usage_context:
          dependencies: "The class depends on the 're' module for regular expression operations, the 'os' module for path manipulation, and 'tomllib' for parsing TOML files. It also uses 'typing' for type hints."
          instantiated_by: This class is not explicitly instantiated by any known component.
      error: null
    backend.callgraph.CallGraph:
      identifier: backend.callgraph.CallGraph
      description:
        overall: "The CallGraph class extends `ast.NodeVisitor` to construct a directed call graph from a Python source file. It systematically traverses the Abstract Syntax Tree (AST) to identify and record function definitions, class structures, import statements, and function calls. By maintaining context about the current file, class, and function scope, it accurately resolves fully qualified names for all callable entities, ultimately building a comprehensive graph of inter-function dependencies within the analyzed code."
        init_method:
          description: "The `__init__` method initializes the CallGraph instance by setting up essential attributes required for AST traversal and call graph construction. It stores the filename, initializes `current_function` and `current_class` to track scope, and sets up dictionaries and sets like `local_defs`, `import_mapping`, `function_set`, and `edges` to store collected call graph data. A `networkx.DiGraph` is also initialized to represent the graph structure."
          parameters[1]{name,type,description}:
            filename,str,The path or name of the Python source file being analyzed.
        methods[11]:
          - identifier: _recursive_call
            description:
              overall: "This private helper method recursively traverses an Abstract Syntax Tree (AST) node to extract the components of a dotted name, typically representing a function or attribute call. It handles `ast.Call`, `ast.Name`, and `ast.Attribute` nodes, building a list of string parts that form the fully qualified name. For instance, `obj.method()` would yield `['obj', 'method']`, providing a structured representation of the call target."
              parameters[1]{name,type,description}:
                node,ast.AST,"The AST node to be recursively analyzed, typically an `ast.Call`, `ast.Name`, or `ast.Attribute`."
              returns[1]{name,type,description}:
                parts,"list[str]",A list of string components representing the dotted name of the call target.
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions within its source code."
                called_by: This method is not explicitly called by any other functions or methods based on the provided context.
            error: null
          - identifier: _resolve_all_callee_names
            description:
              overall: "This private method takes a list of potential callee name components and resolves them to their fully qualified names. It first checks `self.local_defs` for local definitions, then `self.import_mapping` for imported entities. If no direct resolution is found, it constructs a full name using the current `filename` and `current_class` context. This ensures that call targets are accurately identified regardless of their origin (local, imported, or within the same file/class)."
              parameters[1]{name,type,description}:
                callee_nodes,"list[list[str]]",A list where each inner list contains string components of a potential callee's name.
              returns[1]{name,type,description}:
                resolved,"list[str]",A list of fully qualified string names for the resolved callees.
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions within its source code."
                called_by: This method is not explicitly called by any other functions or methods based on the provided context.
            error: null
          - identifier: _make_full_name
            description:
              overall: "This private utility method constructs a fully qualified name for a given base name, optionally including a class name. It prepends the `self.filename` and, if provided, the `class_name` to the `basename`, using `::` as a separator. This standardization ensures that all functions and methods within the call graph have unique and traceable identifiers, facilitating accurate graph construction and analysis."
              parameters[2]{name,type,description}:
                basename,str,The base name of the function or method.
                class_name,str | None,"The name of the class if the entity is a method; otherwise, it is None."
              returns[1]{name,type,description}:
                full_name,str,The fully qualified name of the function or method as a string.
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions within its source code."
                called_by: This method is not explicitly called by any other functions or methods based on the provided context.
            error: null
          - identifier: _current_caller
            description:
              overall: "This private helper method determines the identifier of the current calling context. If `self.current_function` is set, it returns that value, indicating an active function scope. Otherwise, it defaults to a global scope identifier, using the `self.filename` if available, or a generic `<global-scope>`. This is crucial for correctly attributing calls to their originating scope within the call graph."
              parameters[0]:
              returns[1]{name,type,description}:
                caller_identifier,str,A string representing the identifier of the current function or global scope.
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions within its source code."
                called_by: This method is not explicitly called by any other functions or methods based on the provided context.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method, part of the `ast.NodeVisitor` pattern, processes `ast.Import` nodes. It iterates through the aliases within the import statement, extracting the original module name and any assigned alias (`asname`). These mappings are then stored in `self.import_mapping`, which is used later to resolve calls to imported modules. After processing, it calls `generic_visit` to continue the AST traversal."
              parameters[1]{name,type,description}:
                node,ast.Import,The AST node representing an `import` statement.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions within its source code."
                called_by: This method is not explicitly called by any other functions or methods based on the provided context.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method handles `ast.ImportFrom` nodes, which represent `from ... import ...` statements. It extracts the base module name and then iterates through the imported names and their aliases. These mappings are stored in `self.import_mapping`, allowing the call graph builder to correctly resolve references to functions or classes imported from specific modules. It ensures proper context for resolving call targets."
              parameters[1]{name,type,description}:
                node,ast.ImportFrom,The AST node representing a `from ... import ...` statement.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions within its source code."
                called_by: This method is not explicitly called by any other functions or methods based on the provided context.
            error: null
          - identifier: visit_ClassDef
            description:
              overall: "This method processes `ast.ClassDef` nodes, which define classes in the source code. It temporarily updates `self.current_class` to the name of the class being visited, ensuring that any methods defined within this class are correctly associated with it. After traversing the class's body using `self.generic_visit(node)`, it restores the `self.current_class` to its previous value, maintaining proper scope context."
              parameters[1]{name,type,description}:
                node,ast.ClassDef,The AST node representing a class definition.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions within its source code."
                called_by: This method is not explicitly called by any other functions or methods based on the provided context.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method processes `ast.FunctionDef` nodes, handling regular function definitions. It constructs the fully qualified name of the function using `_make_full_name` and stores it in `self.local_defs` for later resolution. It updates `self.current_function` to reflect the current scope, adds the function as a node to the `self.graph`, and records it in `self.function_set`. After traversing the function's body, it restores the previous function context."
              parameters[1]{name,type,description}:
                node,ast.FunctionDef,The AST node representing a function definition.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions within its source code."
                called_by: This method is not explicitly called by any other functions or methods based on the provided context.
            error: null
          - identifier: visit_AsyncFunctionDef
            description:
              overall: "This method processes `ast.AsyncFunctionDef` nodes, which represent asynchronous function definitions. It delegates the primary processing logic to the `visit_FunctionDef` method. This approach ensures that asynchronous functions are treated similarly to regular functions for the purpose of call graph construction, allowing for consistent handling of function definitions regardless of their synchronous or asynchronous nature."
              parameters[1]{name,type,description}:
                node,ast.AsyncFunctionDef,The AST node representing an asynchronous function definition.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions within its source code."
                called_by: This method is not explicitly called by any other functions or methods based on the provided context.
            error: null
          - identifier: visit_Call
            description:
              overall: "This method is invoked for every `ast.Call` node encountered during AST traversal, representing a function or method invocation. It first identifies the `caller` using `_current_caller` and then extracts the `callee` name components using `_recursive_call`. These components are then resolved to their fully qualified names using `_resolve_all_callee_names`. Finally, it records the call as an edge in `self.edges` from the identified caller to each resolved callee, building the core of the call graph."
              parameters[1]{name,type,description}:
                node,ast.Call,The AST node representing a function or method call.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions within its source code."
                called_by: This method is not explicitly called by any other functions or methods based on the provided context.
            error: null
          - identifier: visit_If
            description:
              overall: "This method processes `ast.If` nodes, handling conditional statements. It includes special logic to detect and manage the `if __name__ == \"__main__\":` block, which is a common entry point in Python scripts. When this block is identified, `self.current_function` is temporarily set to `<main_block>` to correctly attribute calls within this specific execution context. For all other `if` statements, it simply proceeds with a generic AST traversal."
              parameters[1]{name,type,description}:
                node,ast.If,The AST node representing an `if` statement.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions within its source code."
                called_by: This method is not explicitly called by any other functions or methods based on the provided context.
            error: null
        usage_context:
          dependencies: This class does not explicitly depend on any external components based on the provided context.
          instantiated_by: This class is not explicitly instantiated by any other components based on the provided context.
      error: null
    backend.getRepo.RepoFile:
      identifier: backend.getRepo.RepoFile
      description:
        overall: "The RepoFile class represents a single file within a Git repository, designed for efficient access to its content and metadata. It employs a lazy-loading strategy for the Git blob object, file content, and size, ensuring these resources are only retrieved when explicitly requested. This class provides structured properties to access the file's underlying Git blob, its decoded content, and its size, along with utility methods for basic content analysis and serialization into a dictionary format."
        init_method:
          description: "The constructor initializes a RepoFile object by storing the file's path and the Git Tree object from which it originates. It also sets up internal attributes (`_blob`, `_content`, `_size`) to None, indicating that the file's Git blob, content, and size will be loaded lazily upon first access."
          parameters[2]{name,type,description}:
            file_path,str,The path to the file within the repository.
            commit_tree,git.Tree,The Tree object of the commit from which the file originates.
        methods[6]:
          - identifier: blob
            description:
              overall: "This property provides lazy loading for the Git blob object associated with the file. It checks if the `_blob` attribute is already set; if not, it attempts to retrieve the blob from the `_tree` using the stored `path`. If the file is not found in the commit tree, it raises a `FileNotFoundError` to indicate the issue."
              parameters[0]:
              returns[1]{name,type,description}:
                blob,git.Blob,The Git blob object representing the file.
              usage_context:
                calls: No external functions or methods are explicitly called by this method according to the provided context.
                called_by: No other functions or methods are identified as calling this method in the provided context.
            error: null
          - identifier: content
            description:
              overall: "This property provides lazy loading for the decoded content of the file. It first checks if the `_content` attribute is None. If so, it accesses the `blob` property to get the Git blob, reads its data stream, and decodes it using UTF-8, ignoring any decoding errors. The decoded string content is then stored and returned for subsequent access."
              parameters[0]:
              returns[1]{name,type,description}:
                content,str,The decoded string content of the file.
              usage_context:
                calls: No external functions or methods are explicitly called by this method according to the provided context.
                called_by: No other functions or methods are identified as calling this method in the provided context.
            error: null
          - identifier: size
            description:
              overall: "This property provides lazy loading for the size of the file in bytes. It checks if the `_size` attribute is None. If it is, it accesses the `blob` property to get the Git blob and retrieves its `size` attribute. The file size is then stored internally and returned, avoiding redundant calculations."
              parameters[0]:
              returns[1]{name,type,description}:
                size,int,The size of the file in bytes.
              usage_context:
                calls: No external functions or methods are explicitly called by this method according to the provided context.
                called_by: No other functions or methods are identified as calling this method in the provided context.
            error: null
          - identifier: analyze_word_count
            description:
              overall: "This method serves as an example analysis function, demonstrating how to process the file's content. It calculates and returns the total number of words present in the file's content. It achieves this by first accessing the `content` property to retrieve the file's text, then splitting the content string by whitespace, and finally returning the length of the resulting list of words."
              parameters[0]:
              returns[1]{name,type,description}:
                word_count,int,The total number of words in the file content.
              usage_context:
                calls: No external functions or methods are explicitly called by this method according to the provided context.
                called_by: No other functions or methods are identified as calling this method in the provided context.
            error: null
          - identifier: __repr__
            description:
              overall: "This special method provides a developer-friendly string representation of the RepoFile object. It constructs a concise string that includes the class name and the path of the file. This representation is useful for debugging, logging, and quickly identifying instances of the RepoFile class."
              parameters[0]:
              returns[1]{name,type,description}:
                representation,str,"A string representation of the RepoFile object, including its path."
              usage_context:
                calls: No external functions or methods are explicitly called by this method according to the provided context.
                called_by: No other functions or methods are identified as calling this method in the provided context.
            error: null
          - identifier: to_dict
            description:
              overall: "This method converts the RepoFile object into a dictionary representation, making it suitable for serialization or structured data exchange. It includes essential metadata such as the file's path, its base name, its size, and its type. Optionally, the file's content can be included in the dictionary if the `include_content` parameter is set to True."
              parameters[1]{name,type,description}:
                include_content,bool,A flag indicating whether the file's content should be included in the dictionary. Defaults to False.
              returns[1]{name,type,description}:
                data,dict,A dictionary containing the file's metadata and optionally its content.
              usage_context:
                calls: No external functions or methods are explicitly called by this method according to the provided context.
                called_by: No other functions or methods are identified as calling this method in the provided context.
            error: null
        usage_context:
          dependencies: No external dependencies are explicitly listed in the provided context.
          instantiated_by: No specific instantiation points for this class are identified in the provided context.
      error: null
    backend.getRepo.GitRepository:
      identifier: backend.getRepo.GitRepository
      description:
        overall: "The GitRepository class provides a robust mechanism for interacting with remote Git repositories. It handles the cloning of a specified repository into a temporary local directory, manages the retrieval of all files as RepoFile objects, and can construct a hierarchical file tree. Furthermore, it implements the context manager protocol, ensuring proper cleanup of the temporary directory upon exiting a 'with' block or in case of initialization errors."
        init_method:
          description: "The constructor initializes the GitRepository by cloning a remote Git repository into a temporary local directory. It sets up attributes like the repository URL, the path to the temporary directory, the git.Repo object, and the latest commit and its tree. It handles potential GitCommandError during cloning by cleaning up the temporary directory."
          parameters[1]{name,type,description}:
            repo_url,str,The URL of the Git repository to clone.
        methods[5]:
          - identifier: get_all_files
            description:
              overall: "This method retrieves all file paths from the cloned Git repository using 'git.ls_files()'. It then creates a list of RepoFile objects, one for each file path, associating them with the repository's commit tree. This list of RepoFile objects is stored internally in 'self.files' and returned."
              parameters[0]:
              returns[1]{name,type,description}:
                "null","list[RepoFile]",A list of RepoFile instances representing all files in the repository.
              usage_context:
                calls: This method calls self.repo.git.ls_files() to get file paths and RepoFile to create file objects.
                called_by: This method is called by get_file_tree if self.files is empty.
            error: null
          - identifier: close
            description:
              overall: "This method is responsible for cleaning up the temporary directory where the Git repository was cloned. It checks if 'self.temp_dir' is set, prints a message indicating deletion, and then sets 'self.temp_dir' to None. This prevents further access to the deleted directory."
              parameters[0]:
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call other methods or functions, but it interacts with the file system implicitly by indicating deletion of self.temp_dir."
                called_by: This method is called by the __init__ method in case of a cloning error and by the __exit__ context manager method.
            error: null
          - identifier: __enter__
            description:
              overall: "This special method makes the GitRepository class compatible with the 'with' statement (context manager protocol). When entering a 'with' block, it simply returns the instance of the GitRepository itself, allowing it to be bound to a variable."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",GitRepository,The instance of the GitRepository itself.
              usage_context:
                calls: This method does not call any other methods or functions.
                called_by: This method is implicitly called when a GitRepository instance is used in a 'with' statement.
            error: null
          - identifier: __exit__
            description:
              overall: "This special method is part of the context manager protocol and is automatically called when exiting a 'with' block, regardless of whether an exception occurred. Its primary responsibility is to ensure that the temporary directory created for the repository is cleaned up by calling the 'close' method."
              parameters[3]{name,type,description}:
                exc_type,type | None,"The type of the exception that caused the exit, or None if no exception occurred."
                exc_val,Exception | None,"The exception instance that caused the exit, or None."
                exc_tb,traceback | None,"The traceback object associated with the exception, or None."
              returns[0]:
              usage_context:
                calls: This method calls the self.close() method to clean up resources.
                called_by: This method is implicitly called when exiting a 'with' statement that uses a GitRepository instance.
            error: null
          - identifier: get_file_tree
            description:
              overall: "This method constructs a hierarchical tree representation of the files within the Git repository. If 'self.files' is not already populated, it first calls 'get_all_files' to retrieve them. It then iterates through each RepoFile object, splitting its path to build nested dictionary structures representing directories and files. Each file is appended to its corresponding directory's children list, optionally including file content."
              parameters[1]{name,type,description}:
                include_content,bool,"If True, the content of each file will be included in its dictionary representation."
              returns[1]{name,type,description}:
                "null",dict,A dictionary representing the hierarchical file tree of the repository.
              usage_context:
                calls: This method calls self.get_all_files() to populate the file list and file_obj.to_dict() to get the dictionary representation of each file.
                called_by: The input method_context does not specify any callers for this method.
            error: null
        usage_context:
          dependencies: "The class depends on tempfile for temporary directory management, git.Repo and git.GitCommandError from the git library for Git operations, and logging for informational messages."
          instantiated_by: The input context indicates that this class is not explicitly instantiated by any known components within the provided context.
      error: null
    backend.relationship_analyzer.ProjectAnalyzer:
      identifier: backend.relationship_analyzer.ProjectAnalyzer
      description:
        overall: "The ProjectAnalyzer class is designed to perform static analysis on a Python project to build a comprehensive call graph. It identifies all Python files, collects definitions of classes, functions, and methods, and then resolves the call relationships between these entities. The class provides methods to initiate the analysis, retrieve the raw call graph, and format the relationships into incoming and outgoing structures, offering a structured view of code dependencies."
        init_method:
          description: "This method initializes a new ProjectAnalyzer instance. It sets the project's root directory, initializes internal data structures such as dictionaries for definitions, a defaultdict for the call graph, and a dictionary for storing file ASTs. It also defines a set of directories to be ignored during the analysis process."
          parameters[1]{name,type,description}:
            project_root,str,The absolute path to the root directory of the Python project to be analyzed.
        methods[6]:
          - identifier: analyze
            description:
              overall: "This method orchestrates the entire project analysis process. It first identifies all Python files within the project, then iterates through them to collect definitions of functions, classes, and methods. Subsequently, it iterates through the files again to resolve call relationships between these defined entities. Finally, it clears the stored ASTs to free up memory and returns the populated call graph."
              parameters[0]:
              returns[1]{name,type,description}:
                call_graph,defaultdict(list),"A dictionary-like structure representing the call graph, where keys are callee identifiers and values are lists of caller information."
              usage_context:
                calls: "This method calls `_find_py_files` to locate Python files, `_collect_definitions` to gather entity definitions, and `_resolve_calls` to establish call relationships."
                called_by: This method is not explicitly called by other functions or methods within the provided context.
            error: null
          - identifier: get_raw_relationships
            description:
              overall: "This method processes the internal `call_graph` to generate two dictionaries: `outgoing` and `incoming` relationships. It iterates through the call graph, extracting caller and callee identifiers, and populates sets for outgoing calls from a caller and incoming calls to a callee. The final output provides sorted lists of these relationships, offering a clear view of dependencies."
              parameters[0]:
              returns[1]{name,type,description}:
                relationships,dict,"A dictionary containing two keys, 'outgoing' and 'incoming'. Each key maps to a dictionary where keys are entity identifiers and values are sorted lists of related entity identifiers."
              usage_context:
                calls: "This method utilizes `defaultdict`, `list`, and `sorted` to process and format the call graph data."
                called_by: This method is not explicitly called by other functions or methods within the provided context.
            error: null
          - identifier: _find_py_files
            description:
              overall: "This private helper method is responsible for recursively traversing the project root directory to locate all Python files. It uses `os.walk` to navigate the directory structure, filtering out specified `ignore_dirs` to avoid analyzing irrelevant content. It compiles a list of absolute paths to all discovered Python files."
              parameters[0]:
              returns[1]{name,type,description}:
                py_files,"list[str]","A list of absolute file paths to all Python files found within the project root, excluding ignored directories."
              usage_context:
                calls: This method calls `os.walk` to traverse directories and `os.path.join` to construct file paths.
                called_by: This method is called by `analyze` to get the list of Python files for analysis.
            error: null
          - identifier: _collect_definitions
            description:
              overall: "This private method parses a given Python file to identify and record definitions of classes, functions, and methods. It reads the file, parses its content into an Abstract Syntax Tree (AST), and then walks the AST to find `FunctionDef` and `ClassDef` nodes. For each definition, it constructs a unique path name and stores its file path, line number, and type in the `self.definitions` dictionary, also storing the AST in `self.file_asts`."
              parameters[1]{name,type,description}:
                filepath,str,The absolute path to the Python file being analyzed.
              returns[0]:
              usage_context:
                calls: "This method calls `open`, `ast.parse`, `ast.walk`, `_get_parent`, `path_to_module` (an external utility), `logging.error`, `isinstance`, `ast.FunctionDef`, and `ast.ClassDef` during its execution."
                called_by: This method is called by `analyze` to collect definitions from each Python file.
            error: null
          - identifier: _get_parent
            description:
              overall: "This private helper method traverses an Abstract Syntax Tree (AST) to find the immediate parent node of a given child node. It iterates through all nodes in the tree and checks their children to identify if any child matches the provided node. If a match is found, the parent node is returned."
              parameters[2]{name,type,description}:
                tree,ast.AST,The root of the Abstract Syntax Tree to search within.
                node,ast.AST,The child node for which to find the parent.
              returns[1]{name,type,description}:
                parent_node,ast.AST | None,"The parent AST node of the given `node`, or `None` if no parent is found (e.g., if `node` is the root of the `tree`)."
              usage_context:
                calls: This method calls `ast.walk` to traverse the AST and `ast.iter_child_nodes` to examine children of each node.
                called_by: This method is called by `_collect_definitions` to determine the parent of a function or class definition.
            error: null
          - identifier: _resolve_calls
            description:
              overall: "This private method processes the AST of a given file to identify and resolve function and method calls. It retrieves the pre-parsed AST for the file and then uses a `CallResolverVisitor` (an external class) to traverse the AST and detect calls. The resolved calls, along with their caller information, are then extended into the class's `self.call_graph`."
              parameters[1]{name,type,description}:
                filepath,str,The absolute path to the Python file whose calls are to be resolved.
              returns[0]:
              usage_context:
                calls: "This method calls `self.file_asts.get` to retrieve an AST, instantiates `CallResolverVisitor`, calls `resolver.visit` on the AST, and uses `logging.error` for exception handling."
                called_by: This method is called by `analyze` to resolve calls within each Python file.
            error: null
        usage_context:
          dependencies: "This class depends on `os` for file system operations, `ast` for parsing Python code, `logging` for error reporting, and `collections.defaultdict` for its internal graph representation. It also implicitly depends on `path_to_module` and `CallResolverVisitor` which are not defined in the provided source but are used."
          instantiated_by: This class is not explicitly instantiated by other functions or methods within the provided context.
      error: null
    backend.relationship_analyzer.CallResolverVisitor:
      identifier: backend.relationship_analyzer.CallResolverVisitor
      description:
        overall: "The CallResolverVisitor class is an AST NodeVisitor designed to traverse an Abstract Syntax Tree (AST) of Python code to identify and resolve function and method calls. It maintains a scope of imported names and instantiated object types to accurately determine the fully qualified name of called entities. The visitor collects information about each call, including the caller's identifier, file, line number, and type, storing these relationships for later analysis."
        init_method:
          description: "The constructor initializes the CallResolverVisitor with the current file path, the project's root directory, and a dictionary of known definitions. It sets up internal state variables such as `module_path`, `scope`, `instance_types`, `current_caller_name`, `current_class_name`, and a `defaultdict` to store discovered calls."
          parameters[3]{name,type,description}:
            filepath,str,The path to the Python file being analyzed.
            project_root,str,"The root directory of the project, used to determine module paths."
            definitions,dict,"A dictionary containing known definitions (e.g., functions, classes) within the project, used for validation."
        methods[7]:
          - identifier: visit_ClassDef
            description:
              overall: "This method is invoked when the AST visitor encounters a class definition. It updates the `current_class_name` attribute to reflect the newly entered class scope. After visiting all child nodes within the class, it restores the `current_class_name` to its previous value, ensuring correct scope management during AST traversal."
              parameters[1]{name,type,description}:
                node,ast.ClassDef,The AST node representing the class definition.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other functions or methods according to the provided context.
                called_by: This method is not explicitly called by other functions or methods according to the provided context.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method is called when the AST visitor encounters a function definition (either a global function or a method within a class). It constructs a fully qualified identifier for the function, incorporating the module and class name if applicable. This identifier is then set as `current_caller_name` for subsequent call resolution within the function's body, and the previous caller name is restored upon exiting the function's scope."
              parameters[1]{name,type,description}:
                node,ast.FunctionDef,The AST node representing the function definition.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other functions or methods according to the provided context.
                called_by: This method is not explicitly called by other functions or methods according to the provided context.
            error: null
          - identifier: visit_Call
            description:
              overall: "This method processes function and method call expressions within the AST. It attempts to resolve the fully qualified name of the called entity using the `_resolve_call_qname` helper method. If the callee's qualified name is successfully resolved and exists in the known definitions, it records the call, including the caller's file, line number, full identifier, and type (module, local function, or method). This information is stored in the `self.calls` dictionary."
              parameters[1]{name,type,description}:
                node,ast.Call,The AST node representing a function or method call.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other functions or methods according to the provided context.
                called_by: This method is not explicitly called by other functions or methods according to the provided context.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method handles `import` statements in the source code. It iterates through the imported names and their aliases, storing the mapping from the alias (or original name) to the full module name in the `self.scope` dictionary. This allows the visitor to resolve imported names later during call analysis."
              parameters[1]{name,type,description}:
                node,ast.Import,The AST node representing an import statement.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other functions or methods according to the provided context.
                called_by: This method is not explicitly called by other functions or methods according to the provided context.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method processes `from ... import ...` statements. It determines the full module path for the imported names, considering relative imports (`node.level`). It then stores the mapping from the imported name (or its alias) to its fully qualified path in the `self.scope` dictionary, enabling accurate resolution of imported entities."
              parameters[1]{name,type,description}:
                node,ast.ImportFrom,The AST node representing a 'from ... import ...' statement.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other functions or methods according to the provided context.
                called_by: This method is not explicitly called by other functions or methods according to the provided context.
            error: null
          - identifier: visit_Assign
            description:
              overall: "This method processes assignment statements to identify instances of class instantiation. If an assignment involves a call to a name that is resolved as a known class within the `self.scope` and `self.definitions`, it records the qualified class name associated with the assigned variable. This information is stored in `self.instance_types` to help resolve method calls on these instances later."
              parameters[1]{name,type,description}:
                node,ast.Assign,The AST node representing an assignment statement.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other functions or methods according to the provided context.
                called_by: This method is not explicitly called by other functions or methods according to the provided context.
            error: null
          - identifier: _resolve_call_qname
            description:
              overall: "This private helper method attempts to determine the fully qualified name (QName) of a function or method being called. It handles various scenarios: direct name calls (checking `self.scope` and local module paths), and attribute calls on variables (checking `self.instance_types` for class instances or `self.scope` for module attributes). It returns the resolved QName as a string or `None` if it cannot be resolved."
              parameters[1]{name,type,description}:
                func_node,ast.expr,"The AST node representing the function or method being called (e.g., `ast.Name` or `ast.Attribute`)."
              returns[1]{name,type,description}:
                None,str | None,"The fully qualified name of the called entity if resolved, otherwise None."
              usage_context:
                calls: This method does not explicitly call other functions or methods according to the provided context.
                called_by: This method is not explicitly called by other functions or methods according to the provided context.
            error: null
        usage_context:
          dependencies: The class does not have explicit external dependencies listed in the provided context.
          instantiated_by: The class is not explicitly instantiated by any known components according to the provided context.
      error: null
    schemas.types.ParameterDescription:
      identifier: schemas.types.ParameterDescription
      description:
        overall: "The ParameterDescription class is a Pydantic BaseModel designed to provide a structured representation for a single parameter of a function. It encapsulates essential information such as the parameter's name, its data type, and a descriptive explanation of its purpose. This class serves as a standardized data structure for consistent parameter documentation and analysis within a larger system."
        init_method:
          description: "This class, being a Pydantic BaseModel, has an implicitly generated constructor. It initializes an instance of ParameterDescription by taking 'name', 'type', and 'description' as keyword arguments, validating them against their respective string types, and assigning them as instance attributes."
          parameters[3]{name,type,description}:
            name,str,The name of the function parameter.
            type,str,"The data type of the parameter, typically as a string representation (e.g., 'str', 'int', 'List[str]')."
            description,str,A textual explanation detailing the purpose and usage of the parameter.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in the provided context.
          instantiated_by: The instantiation points for this class are not specified in the provided context.
      error: null
    schemas.types.ReturnDescription:
      identifier: schemas.types.ReturnDescription
      description:
        overall: "The ReturnDescription class is a Pydantic BaseModel designed to structure and validate information about a function's return value. It encapsulates the return value's identifier, its Python type, and a descriptive explanation. This class serves as a standardized data structure for documenting function outputs within a larger system, ensuring consistency and machine-readability."
        init_method:
          description: "This class, being a Pydantic BaseModel, automatically generates an `__init__` method. This constructor initializes an instance of `ReturnDescription` by validating and assigning the `name`, `type`, and `description` fields based on the provided arguments."
          parameters[3]{name,type,description}:
            name,str,The name or identifier of the return value.
            type,str,The Python type hint or a descriptive string of the return value's type.
            description,str,A detailed explanation of what the return value represents.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in the provided context.
          instantiated_by: The specific locations where this class is instantiated are not provided in the current context.
      error: null
    schemas.types.UsageContext:
      identifier: schemas.types.UsageContext
      description:
        overall: "The UsageContext class is a Pydantic BaseModel designed to encapsulate information about the calling context of a function or method. It provides a structured way to store details regarding what a function calls and where it is called from, using two string attributes. This model ensures data validation and serialization for these context-related strings."
        init_method:
          description: "The __init__ method for UsageContext is implicitly generated by Pydantic's BaseModel. It initializes an instance of UsageContext by setting the 'calls' and 'called_by' attributes based on the provided string values, ensuring they conform to the defined types."
          parameters[2]{name,type,description}:
            calls,str,"A string describing the functions, methods, or external entities that the analyzed component calls."
            called_by,str,"A string describing the functions, methods, or external contexts that call the analyzed component."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies.
          instantiated_by: The instantiation points for this class are not explicitly provided in the context.
      error: null
    schemas.types.FunctionDescription:
      identifier: schemas.types.FunctionDescription
      description:
        overall: "The FunctionDescription class is a Pydantic BaseModel designed to encapsulate a comprehensive analysis of a Python function. It serves as a structured data container for describing a function's high-level purpose, its input parameters, its return values, and its operational context within a larger system. This model is crucial for generating detailed documentation or for AI systems that need to understand function behavior."
        init_method:
          description: "This class is a Pydantic BaseModel, so its constructor is automatically generated by Pydantic. It initializes an instance of FunctionDescription by accepting values for its defined fields: overall, parameters, returns, and usage_context."
          parameters[4]{name,type,description}:
            overall,str,A high-level summary of the function's purpose and implementation.
            parameters,"List[ParameterDescription]",A list of objects describing each parameter of the function.
            returns,"List[ReturnDescription]",A list of objects describing the return values of the function.
            usage_context,UsageContext,An object describing where the function is called and what it calls.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies within the provided context.
          instantiated_by: The specific points of instantiation for this class are not provided in the current context.
      error: null
    schemas.types.FunctionAnalysis:
      identifier: schemas.types.FunctionAnalysis
      description:
        overall: "The FunctionAnalysis class is a Pydantic BaseModel designed to structure and validate the comprehensive analysis of a single function. It acts as the primary data model for representing a function's identifier, its detailed description (including purpose, parameters, and return values), and any errors encountered during its analysis. This class is crucial for standardizing the output of automated function analysis within a larger system."
        init_method:
          description: "The FunctionAnalysis class does not explicitly define an __init__ method. As a Pydantic BaseModel, its constructor is implicitly generated, allowing instantiation by providing values for its fields: 'identifier', 'description', and optionally 'error'. These fields are validated upon object creation according to their specified types."
          parameters[3]{name,type,description}:
            identifier,str,The unique name or identifier of the function being analyzed.
            description,FunctionDescription,"A detailed analysis object containing the function's overall purpose, parameters, return values, and usage context."
            error,"Optional[str]","An optional string detailing any errors encountered during the function's analysis, defaulting to None if no errors occurred."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies.
          instantiated_by: The specific points of instantiation for this class are not provided in the current context.
      error: null
    schemas.types.ConstructorDescription:
      identifier: schemas.types.ConstructorDescription
      description:
        overall: "The ConstructorDescription class is a Pydantic BaseModel designed to structure information about the `__init__` method of another class. It serves as a data container, holding a textual summary of the constructor's purpose and a list of its individual parameters. This class is crucial for providing a standardized, machine-readable representation of constructor details within a larger documentation or analysis system."
        init_method:
          description: "This class, being a Pydantic BaseModel, automatically generates an `__init__` method. It is initialized by providing values for its defined fields: a string `description` and a list of `ParameterDescription` objects. This constructor sets up the instance with the provided details about a class's `__init__` method."
          parameters[2]{name,type,description}:
            description,str,A string containing a summary or explanation of the constructor's behavior and purpose.
            parameters,"List[ParameterDescription]","A list of ParameterDescription objects, each detailing a specific parameter of the constructor, including its name, type, and description."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies.
          instantiated_by: The specific points of instantiation for this class are not provided in the current context.
      error: null
    schemas.types.ClassContext:
      identifier: schemas.types.ClassContext
      description:
        overall: The ClassContext class is a Pydantic model designed to encapsulate information about a class's contextual relationships. It specifically stores details regarding the external dependencies that a class relies upon and the locations or entities responsible for its instantiation. This model provides a structured way to represent and manage metadata about a class's integration within a larger system.
        init_method:
          description: "This method initializes an instance of `ClassContext`, setting up its `dependencies` and `instantiated_by` attributes. As a Pydantic model, the constructor handles validation and assignment of these fields based on the provided arguments."
          parameters[2]{name,type,description}:
            dependencies,str,A string describing the external dependencies of a class.
            instantiated_by,str,A string describing where the class is instantiated.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external dependencies in the provided context.
          instantiated_by: This class does not explicitly list any points of instantiation in the provided context.
      error: null
    schemas.types.ClassDescription:
      identifier: schemas.types.ClassDescription
      description:
        overall: "The `ClassDescription` class is a Pydantic model designed to structure and store a comprehensive analysis of a Python class. It serves as a schema for representing various aspects of a class, including its high-level purpose, details about its constructor, a list of its methods with their individual analyses, and its contextual usage within a larger system. This model is crucial for generating structured documentation or for further automated processing of class definitions."
        init_method:
          description: "The `__init__` method for `ClassDescription` is implicitly generated by Pydantic's `BaseModel`. It initializes an instance of `ClassDescription` by validating and assigning values to its defined fields: `overall`, `init_method`, `methods`, and `usage_context`. These fields are expected to conform to their respective type hints, ensuring data integrity upon instantiation."
          parameters[4]{name,type,description}:
            overall,str,A string describing the overall purpose and functionality of the analyzed class.
            init_method,ConstructorDescription,An object containing the detailed analysis of the analyzed class's constructor (`__init__` method).
            methods,"List[FunctionAnalysis]","A list of `FunctionAnalysis` objects, each detailing an individual method within the analyzed class."
            usage_context,ClassContext,An object providing context about the analyzed class's dependencies and where it is instantiated.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in the provided context.
          instantiated_by: This class is not explicitly listed as being instantiated by any other components in the provided context.
      error: null
    schemas.types.ClassAnalysis:
      identifier: schemas.types.ClassAnalysis
      description:
        overall: "The ClassAnalysis class serves as the root Pydantic model for representing a comprehensive analysis of a Python class. It encapsulates the class's unique identifier, a detailed ClassDescription object containing its constructor and method analyses, and an optional error field to indicate any issues during the analysis process. This model is designed to structure the output of an AI code analyst, providing a standardized format for class-level insights."
        init_method:
          description: "This class does not explicitly define an `__init__` method. It inherits from `pydantic.BaseModel`, and its initialization is handled automatically by Pydantic based on the type hints of its fields, allowing instantiation by passing keyword arguments corresponding to its defined fields."
          parameters[0]:
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies within the provided context.
          instantiated_by: The instantiation points for this class are not specified in the provided context.
      error: null
    schemas.types.CallInfo:
      identifier: schemas.types.CallInfo
      description:
        overall: "The CallInfo class is a Pydantic BaseModel designed to represent a specific call event identified by a relationship analyzer. It encapsulates details about where a call originated, including the file, the function or method name, the type of calling entity (e.g., 'method', 'function'), and the exact line number. This model is intended for use in lists that track 'called_by' or 'instantiated_by' relationships within a system."
        init_method:
          description: "As a Pydantic BaseModel, CallInfo's constructor is automatically generated. It initializes an instance of CallInfo by validating and assigning values to its fields: file, function, mode, and line. This allows for easy creation of CallInfo objects with structured data."
          parameters[4]{name,type,description}:
            file,str,The path to the file where the call event occurred.
            function,str,The name of the function or method that made the call.
            mode,str,"The type of the calling entity, such as 'method', 'function', or 'module'."
            line,int,The line number in the file where the call event occurred.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly depend on other components within the provided context.
          instantiated_by: This class is not explicitly shown to be instantiated by any other components within the provided context.
      error: null
    schemas.types.FunctionContextInput:
      identifier: schemas.types.FunctionContextInput
      description:
        overall: "The FunctionContextInput class serves as a Pydantic BaseModel, defining a structured schema for capturing context relevant to function analysis. It is designed to hold information about what a function calls and by whom it is called, facilitating a standardized way to represent this data. This class acts as a data transfer object or a configuration model for function-related context."
        init_method:
          description: "The class utilizes Pydantic's BaseModel for initialization, meaning its attributes `calls` and `called_by` are automatically validated and assigned upon instantiation based on the provided input. There is no explicit `__init__` method defined within the class itself."
          parameters[0]:
        methods[0]:
        usage_context:
          dependencies: "This class does not explicitly declare external functional dependencies within the provided context, beyond its inheritance from Pydantic's BaseModel."
          instantiated_by: The specific points of instantiation for this class are not provided in the current context.
      error: null
    schemas.types.FunctionAnalysisInput:
      identifier: schemas.types.FunctionAnalysisInput
      description:
        overall: "The FunctionAnalysisInput class is a Pydantic BaseModel designed to standardize the input data required for analyzing a Python function. It serves as a data transfer object, ensuring that all necessary components‚Äîsuch as the function's identifier, its source code, relevant imports, and contextual information‚Äîare present and correctly typed before analysis proceeds. This class facilitates robust data validation and clear communication between different parts of a larger AI system focused on code analysis."
        init_method:
          description: "This class does not explicitly define an __init__ method. As a Pydantic BaseModel, its constructor is implicitly generated to accept keyword arguments corresponding to its defined fields: mode, identifier, source_code, imports, and context."
          parameters[5]{name,type,description}:
            mode,"Literal[\"function_analysis\"]","Specifies the analysis mode, which must be 'function_analysis'."
            identifier,str,The unique name or identifier of the function to be analyzed.
            source_code,str,The raw source code of the function.
            imports,"List[str]",A list of import statements relevant to the function.
            context,FunctionContextInput,Additional contextual information required for the analysis.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list external functional dependencies within the provided context.
          instantiated_by: The instantiation points for this class are not specified in the provided context.
      error: null
    schemas.types.MethodContextInput:
      identifier: schemas.types.MethodContextInput
      description:
        overall: "The `MethodContextInput` class is a Pydantic BaseModel designed to provide a structured schema for capturing and validating contextual information about a specific method. It defines fields to store the method's unique identifier, a list of other functions or methods it calls, a list of `CallInfo` objects indicating where it is called from, its arguments, and its optional docstring. This class serves as a data transfer object or a configuration model for method-level analysis within a larger system."
        init_method:
          description: "As a Pydantic BaseModel, the `__init__` method for `MethodContextInput` is automatically generated by Pydantic. It handles the instantiation of a `MethodContextInput` object by accepting keyword arguments corresponding to its defined fields, performing data validation and type coercion during the object's creation."
          parameters[5]{name,type,description}:
            identifier,str,The unique string identifier for the method being described.
            calls,"List[str]","A list of string identifiers for other methods, classes, or functions that this method calls."
            called_by,"List[CallInfo]","A list of `CallInfo` objects, each detailing a location or context from which this method is invoked."
            args,"List[str]",A list of string representations of the arguments passed to this method.
            docstring,"Optional[str]","The docstring content of the method, if available; otherwise, it can be null."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list external functional dependencies within the provided context.
          instantiated_by: The specific locations where this class is instantiated are not provided in the current context.
      error: null
    schemas.types.ClassContextInput:
      identifier: schemas.types.ClassContextInput
      description:
        overall: "The ClassContextInput class is a Pydantic BaseModel designed to encapsulate structured contextual information necessary for analyzing a Python class. It serves as a data container, defining fields for external dependencies, instantiation points, and detailed context for each method within the class. This model provides a standardized format for collecting and organizing metadata about a class's environment and internal structure."
        init_method:
          description: "The class is initialized by Pydantic's BaseModel constructor. It automatically handles the assignment of `dependencies`, `instantiated_by`, and `method_context` based on the provided type hints, ensuring data validation and type enforcement upon instantiation."
          parameters[3]{name,type,description}:
            dependencies,"List[str]",A list of strings representing external dependencies of the class being analyzed.
            instantiated_by,"List[CallInfo]",A list of CallInfo objects indicating where the class is instantiated.
            method_context,"List[MethodContextInput]","A list of MethodContextInput objects, each providing detailed context for a specific method within the class."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external dependencies within the provided context.
          instantiated_by: The instantiation points for this class are not specified in the provided context.
      error: null
    schemas.types.ClassAnalysisInput:
      identifier: schemas.types.ClassAnalysisInput
      description:
        overall: "The ClassAnalysisInput class is a Pydantic BaseModel designed to define the structured input required for generating a ClassAnalysis object. It serves as a data validation and parsing schema, ensuring that all necessary components like the analysis mode, class identifier, source code, import statements, and contextual information are present and correctly typed before proceeding with the analysis. This class acts as a contract for the data payload used in the class analysis workflow."
        init_method:
          description: "This class does not explicitly define an __init__ method. Pydantic's BaseModel automatically generates a constructor based on the defined fields, allowing instantiation by providing values for `mode`, `identifier`, `source_code`, `imports`, and `context`."
          parameters[5]{name,type,description}:
            mode,"Literal[\"class_analysis\"]","Specifies the operation mode, which must be 'class_analysis' to indicate a class analysis request."
            identifier,str,The unique name or identifier of the class being analyzed.
            source_code,str,The raw Python source code of the entire class definition to be analyzed.
            imports,"List[str]",A list of import statements relevant to the source file where the class is defined.
            context,ClassContextInput,"Additional contextual information, such as dependencies and instantiation points, relevant for the class analysis."
        methods[0]:
        usage_context:
          dependencies: "This class does not explicitly list any direct functional dependencies within its definition, relying on Pydantic's BaseModel for its core functionality."
          instantiated_by: "The instantiation points for this class are not provided in the current context, but it is typically instantiated by a system component that prepares data for class analysis."
      error: null