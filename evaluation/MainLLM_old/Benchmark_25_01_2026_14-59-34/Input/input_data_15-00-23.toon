basic_info:
  projekt_uebersicht:
    titel: Repo Onboarding Agent üöÄ
    beschreibung: ```
    aktueller_status: Information not found
    key_features: Information not found
    tech_stack: Information not found
  installation:
    dependencies: "- altair==4.2.2\n- annotated-types==0.7.0\n- anyio==4.11.0\n- attrs==25.4.0\n- bcrypt==5.0.0\n- blinker==1.9.0\n- cachetools==6.2.2\n- captcha==0.7.1\n- certifi==2025.11.12\n- cffi==2.0.0\n- charset-normalizer==3.4.4\n- click==8.3.1\n- colorama==0.4.6\n- contourpy==1.3.3\n- cryptography==46.0.3\n- cycler==0.12.1\n- distro==1.9.0\n- dnspython==2.8.0\n- dotenv==0.9.9\n- entrypoints==0.4\n- extra-streamlit-components==0.1.81\n- filetype==1.2.0\n- fonttools==4.61.0\n- gitdb==4.0.12\n- GitPython==3.1.45\n- google-ai-generativelanguage==0.9.0\n- google-api-core==2.28.1\n- google-auth==2.43.0\n- googleapis-common-protos==1.72.0\n- grpcio==1.76.0\n- grpcio-status==1.76.0\n- h11==0.16.0\n- httpcore==1.0.9\n- httpx==0.28.1\n- idna==3.11\n- Jinja2==3.1.6\n- jiter==0.12.0\n- jsonpatch==1.33\n- jsonpointer==3.0.0\n- jsonschema==4.25.1\n- jsonschema-specifications==2025.9.1\n- kiwisolver==1.4.9\n- langchain==1.0.8\n- langchain-core==1.1.0\n- langchain-google-genai==3.1.0\n- langchain-ollama==1.0.0\n- langchain-openai==1.1.0\n- langgraph==1.0.3\n- langgraph-checkpoint==3.0.1\n- langgraph-prebuilt==1.0.5\n- langgraph-sdk==0.2.9\n- langsmith==0.4.46\n- MarkupSafe==3.0.3\n- matplotlib==3.10.7\n- narwhals==2.12.0\n- networkx==3.6\n- numpy==2.3.5\n- ollama==0.6.1\n- openai==2.8.1\n- orjson==3.11.4\n- ormsgpack==1.12.0\n- packaging==25.0\n- pandas==2.3.3\n- pillow==12.0.0\n- proto-plus==1.26.1\n- protobuf==6.33.1\n- pyarrow==21.0.0\n- pyasn1==0.6.1\n- pyasn1_modules==0.4.2\n- pycparser==2.23\n- pydantic==2.12.4\n- pydantic_core==2.41.5\n- pydeck==0.9.1\n- PyJWT==2.10.1\n- pymongo==4.15.4\n- pyparsing==3.2.5\n- python-dateutil==2.9.0.post0\n- python-dotenv==1.2.1\n- pytz==2025.2\n- PyYAML==6.0.3\n- referencing==0.37.0\n- regex==2025.11.3\n- requests==2.32.5\n- requests-toolbelt==1.0.0\n- rpds-py==0.29.0\n- rsa==4.9.1\n- setuptools==75.9.1\n- six==1.17.0\n- smmap==5.0.2\n- sniffio==1.3.1\n- streamlit==1.51.0\n- streamlit-authenticator==0.4.2\n- streamlit-mermaid==0.3.0\n- tenacity==9.1.2\n- tiktoken==0.12.0\n- toml==0.10.2\n- toolz==1.1.0\n- toon_format @ git+https://github.com/toon-format/toon-python.git@9c4f0c0c24f2a0b0b376315f4b8707f8c9006de6\n- tornado==6.5.2\n- tqdm==4.67.1\n- typing-inspection==0.4.2\n- typing_extensions==4.15.0\n- tzdata==2025.2\n- urllib3==2.5.0\n- watchdog==6.0.0\n- xxhash==3.6.0\n- zstandard==0.25.0\n- nbformat"
    setup_anleitung: Information not found
    quick_start_guide: Information not found
file_tree:
  name: root
  type: directory
  children[16]:
    - path: .env.example
      name: .env.example
      size: 48
      type: file
    - path: .gitignore
      name: .gitignore
      size: 184
      type: file
    - name: SystemPrompts
      type: directory
      children[6]{path,name,size,type}:
        SystemPrompts/SystemPromptClassHelperLLM.txt,SystemPromptClassHelperLLM.txt,6645,file
        SystemPrompts/SystemPromptFunctionHelperLLM.txt,SystemPromptFunctionHelperLLM.txt,5546,file
        SystemPrompts/SystemPromptHelperLLM.txt,SystemPromptHelperLLM.txt,9350,file
        SystemPrompts/SystemPromptMainLLM.txt,SystemPromptMainLLM.txt,8380,file
        SystemPrompts/SystemPromptMainLLMToon.txt,SystemPromptMainLLMToon.txt,8456,file
        SystemPrompts/SystemPromptNotebookLLM.txt,SystemPromptNotebookLLM.txt,6921,file
    - path: analysis_output.json
      name: analysis_output.json
      size: 569396
      type: file
    - name: backend
      type: directory
      children[12]{path,name,size,type}:
        backend/AST_Schema.py,AST_Schema.py,6622,file
        backend/File_Dependency.py,File_Dependency.py,7384,file
        backend/HelperLLM.py,HelperLLM.py,17280,file
        backend/MainLLM.py,MainLLM.py,4281,file
        backend/__init__.py,__init__.py,0,file
        backend/basic_info.py,basic_info.py,7284,file
        backend/callgraph.py,callgraph.py,9020,file
        backend/converter.py,converter.py,3594,file
        backend/getRepo.py,getRepo.py,4739,file
        backend/main.py,main.py,25591,file
        backend/relationship_analyzer.py,relationship_analyzer.py,8279,file
        backend/scads_key_test.py,scads_key_test.py,663,file
    - name: database
      type: directory
      children[1]{path,name,size,type}:
        database/db.py,db.py,8222,file
    - name: frontend
      type: directory
      children[4]:
        - name: .streamlit
          type: directory
          children[1]{path,name,size,type}:
            frontend/.streamlit/config.toml,config.toml,165,file
        - path: frontend/__init__.py
          name: __init__.py
          size: 0
          type: file
        - path: frontend/frontend.py
          name: frontend.py
          size: 31176
          type: file
        - name: gifs
          type: directory
          children[1]{path,name,size,type}:
            frontend/gifs/4j.gif,4j.gif,3306723,file
    - name: notizen
      type: directory
      children[8]:
        - path: notizen/Report Agenda.txt
          name: Report Agenda.txt
          size: 3492
          type: file
        - path: notizen/Zwischenpraesentation Agenda.txt
          name: Zwischenpraesentation Agenda.txt
          size: 809
          type: file
        - path: notizen/doc_bestandteile.md
          name: doc_bestandteile.md
          size: 847
          type: file
        - name: grafiken
          type: directory
          children[4]:
            - name: "1"
              type: directory
              children[5]{path,name,size,type}:
                notizen/grafiken/1/File_Dependency_Graph_Repo.dot,File_Dependency_Graph_Repo.dot,1227,file
                notizen/grafiken/1/global_callgraph.png,global_callgraph.png,940354,file
                notizen/grafiken/1/global_graph.png,global_graph.png,4756519,file
                notizen/grafiken/1/global_graph_2.png,global_graph_2.png,242251,file
                notizen/grafiken/1/repo.dot,repo.dot,13870,file
            - name: "2"
              type: directory
              children[12]{path,name,size,type}:
                notizen/grafiken/2/FDG_repo.dot,FDG_repo.dot,9150,file
                notizen/grafiken/2/fdg_graph.png,fdg_graph.png,425374,file
                notizen/grafiken/2/fdg_graph_2.png,fdg_graph_2.png,4744718,file
                notizen/grafiken/2/filtered_callgraph_flask.png,filtered_callgraph_flask.png,614631,file
                notizen/grafiken/2/filtered_callgraph_repo-agent.png,filtered_callgraph_repo-agent.png,280428,file
                notizen/grafiken/2/filtered_callgraph_repo-agent_3.png,filtered_callgraph_repo-agent_3.png,1685838,file
                notizen/grafiken/2/filtered_repo_callgraph_flask.dot,filtered_repo_callgraph_flask.dot,19984,file
                notizen/grafiken/2/filtered_repo_callgraph_repo-agent-3.dot,filtered_repo_callgraph_repo-agent-3.dot,20120,file
                notizen/grafiken/2/filtered_repo_callgraph_repo-agent.dot,filtered_repo_callgraph_repo-agent.dot,3118,file
                notizen/grafiken/2/global_callgraph.png,global_callgraph.png,608807,file
                notizen/grafiken/2/graph_flask.md,graph_flask.md,17398,file
                notizen/grafiken/2/repo.dot,repo.dot,19984,file
            - name: Flask-Repo
              type: directory
              children[65]{path,name,size,type}:
                notizen/grafiken/Flask-Repo/__init__.dot,__init__.dot,89,file
                notizen/grafiken/Flask-Repo/__main__.dot,__main__.dot,87,file
                notizen/grafiken/Flask-Repo/app.dot,app.dot,78,file
                notizen/grafiken/Flask-Repo/auth.dot,auth.dot,1126,file
                notizen/grafiken/Flask-Repo/blog.dot,blog.dot,939,file
                notizen/grafiken/Flask-Repo/blueprints.dot,blueprints.dot,4601,file
                notizen/grafiken/Flask-Repo/cli.dot,cli.dot,8269,file
                notizen/grafiken/Flask-Repo/conf.dot,conf.dot,489,file
                notizen/grafiken/Flask-Repo/config.dot,config.dot,2130,file
                notizen/grafiken/Flask-Repo/conftest.dot,conftest.dot,1756,file
                notizen/grafiken/Flask-Repo/ctx.dot,ctx.dot,3009,file
                notizen/grafiken/Flask-Repo/db.dot,db.dot,782,file
                notizen/grafiken/Flask-Repo/debughelpers.dot,debughelpers.dot,1903,file
                notizen/grafiken/Flask-Repo/factory.dot,factory.dot,220,file
                notizen/grafiken/Flask-Repo/flask.dot,flask.dot,82,file
                notizen/grafiken/Flask-Repo/globals.dot,globals.dot,370,file
                notizen/grafiken/Flask-Repo/hello.dot,hello.dot,152,file
                notizen/grafiken/Flask-Repo/helpers.dot,helpers.dot,2510,file
                notizen/grafiken/Flask-Repo/importerrorapp.dot,importerrorapp.dot,155,file
                notizen/grafiken/Flask-Repo/logging.dot,logging.dot,564,file
                notizen/grafiken/Flask-Repo/make_celery.dot,make_celery.dot,99,file
                notizen/grafiken/Flask-Repo/multiapp.dot,multiapp.dot,88,file
                notizen/grafiken/Flask-Repo/provider.dot,provider.dot,1769,file
                notizen/grafiken/Flask-Repo/scaffold.dot,scaffold.dot,3700,file
                notizen/grafiken/Flask-Repo/sessions.dot,sessions.dot,4178,file
                notizen/grafiken/Flask-Repo/signals.dot,signals.dot,133,file
                notizen/grafiken/Flask-Repo/tag.dot,tag.dot,3750,file
                notizen/grafiken/Flask-Repo/tasks.dot,tasks.dot,312,file
                notizen/grafiken/Flask-Repo/templating.dot,templating.dot,2277,file
                notizen/grafiken/Flask-Repo/test_appctx.dot,test_appctx.dot,2470,file
                notizen/grafiken/Flask-Repo/test_async.dot,test_async.dot,1919,file
                notizen/grafiken/Flask-Repo/test_auth.dot,test_auth.dot,725,file
                notizen/grafiken/Flask-Repo/test_basic.dot,test_basic.dot,17383,file
                notizen/grafiken/Flask-Repo/test_blog.dot,test_blog.dot,1105,file
                notizen/grafiken/Flask-Repo/test_blueprints.dot,test_blueprints.dot,11515,file
                notizen/grafiken/Flask-Repo/test_cli.dot,test_cli.dot,6435,file
                notizen/grafiken/Flask-Repo/test_config.dot,test_config.dot,2854,file
                notizen/grafiken/Flask-Repo/test_config.png,test_config.png,288693,file
                notizen/grafiken/Flask-Repo/test_converters.dot,test_converters.dot,937,file
                notizen/grafiken/Flask-Repo/test_db.dot,test_db.dot,481,file
                notizen/grafiken/Flask-Repo/test_factory.dot,test_factory.dot,201,file
                notizen/grafiken/Flask-Repo/test_helpers.dot,test_helpers.dot,5857,file
                notizen/grafiken/Flask-Repo/test_instance_config.dot,test_instance_config.dot,1249,file
                notizen/grafiken/Flask-Repo/test_js_example.dot,test_js_example.dot,453,file
                notizen/grafiken/Flask-Repo/test_json.dot,test_json.dot,4021,file
                notizen/grafiken/Flask-Repo/test_json_tag.dot,test_json_tag.dot,1452,file
                notizen/grafiken/Flask-Repo/test_logging.dot,test_logging.dot,1348,file
                notizen/grafiken/Flask-Repo/test_regression.dot,test_regression.dot,763,file
                notizen/grafiken/Flask-Repo/test_reqctx.dot,test_reqctx.dot,3809,file
                notizen/grafiken/Flask-Repo/test_request.dot,test_request.dot,668,file
                notizen/grafiken/Flask-Repo/test_session_interface.dot,test_session_interface.dot,667,file
                notizen/grafiken/Flask-Repo/test_signals.dot,test_signals.dot,1671,file
                notizen/grafiken/Flask-Repo/test_subclassing.dot,test_subclassing.dot,612,file
                notizen/grafiken/Flask-Repo/test_templating.dot,test_templating.dot,5129,file
                notizen/grafiken/Flask-Repo/test_testing.dot,test_testing.dot,4081,file
                notizen/grafiken/Flask-Repo/test_user_error_handler.dot,test_user_error_handler.dot,5984,file
                notizen/grafiken/Flask-Repo/test_views.dot,test_views.dot,2641,file
                notizen/grafiken/Flask-Repo/testing.dot,testing.dot,2785,file
                notizen/grafiken/Flask-Repo/typing.dot,typing.dot,86,file
                notizen/grafiken/Flask-Repo/typing_app_decorators.dot,typing_app_decorators.dot,500,file
                notizen/grafiken/Flask-Repo/typing_error_handler.dot,typing_error_handler.dot,420,file
                notizen/grafiken/Flask-Repo/typing_route.dot,typing_route.dot,1717,file
                notizen/grafiken/Flask-Repo/views.dot,views.dot,1125,file
                notizen/grafiken/Flask-Repo/wrappers.dot,wrappers.dot,911,file
                notizen/grafiken/Flask-Repo/wsgi.dot,wsgi.dot,19,file
            - name: Repo-onboarding
              type: directory
              children[15]{path,name,size,type}:
                notizen/grafiken/Repo-onboarding/AST.dot,AST.dot,1361,file
                notizen/grafiken/Repo-onboarding/Frontend.dot,Frontend.dot,538,file
                notizen/grafiken/Repo-onboarding/HelperLLM.dot,HelperLLM.dot,1999,file
                notizen/grafiken/Repo-onboarding/HelperLLM.png,HelperLLM.png,234861,file
                notizen/grafiken/Repo-onboarding/MainLLM.dot,MainLLM.dot,2547,file
                notizen/grafiken/Repo-onboarding/agent.dot,agent.dot,2107,file
                notizen/grafiken/Repo-onboarding/basic_info.dot,basic_info.dot,1793,file
                notizen/grafiken/Repo-onboarding/callgraph.dot,callgraph.dot,1478,file
                notizen/grafiken/Repo-onboarding/getRepo.dot,getRepo.dot,1431,file
                notizen/grafiken/Repo-onboarding/graph_AST.png,graph_AST.png,132743,file
                notizen/grafiken/Repo-onboarding/graph_AST2.png,graph_AST2.png,12826,file
                notizen/grafiken/Repo-onboarding/graph_AST3.png,graph_AST3.png,136016,file
                notizen/grafiken/Repo-onboarding/main.dot,main.dot,1091,file
                notizen/grafiken/Repo-onboarding/tools.dot,tools.dot,1328,file
                notizen/grafiken/Repo-onboarding/types.dot,types.dot,133,file
        - path: notizen/notizen.md
          name: notizen.md
          size: 4937
          type: file
        - path: notizen/paul_notizen.md
          name: paul_notizen.md
          size: 2033
          type: file
        - path: notizen/praesentation_notizen.md
          name: praesentation_notizen.md
          size: 2738
          type: file
        - path: notizen/technische_notizen.md
          name: technische_notizen.md
          size: 3616
          type: file
    - path: output.json
      name: output.json
      size: 454008
      type: file
    - path: output.toon
      name: output.toon
      size: 341854
      type: file
    - path: readme.md
      name: readme.md
      size: 1905
      type: file
    - path: requirements.txt
      name: requirements.txt
      size: 4286
      type: file
    - name: result
      type: directory
      children[28]{path,name,size,type}:
        result/ast_schema_01_12_2025_11-49-24.json,ast_schema_01_12_2025_11-49-24.json,201215,file
        result/notebook_report_23_12_2025_12-56-24_NotebookLLM_gemini-2.5-flash.md,notebook_report_23_12_2025_12-56-24_NotebookLLM_gemini-2.5-flash.md,15776,file
        result/notebook_report_27_12_2025_15-06-09_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-06-09_NotebookLLM_gemini-2.5-flash.md,14804,file
        result/notebook_report_27_12_2025_15-09-29_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-09-29_NotebookLLM_gemini-2.5-flash.md,17598,file
        result/notebook_report_27_12_2025_15-26-34_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-26-34_NotebookLLM_gemini-2.5-flash.md,20458,file
        result/notebook_report_27_12_2025_15-33-06_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-33-06_NotebookLLM_gemini-2.5-flash.md,15268,file
        result/notebook_report_29_12_2025_15-03-21_NotebookLLM_gemini-2.5-flash.md,notebook_report_29_12_2025_15-03-21_NotebookLLM_gemini-2.5-flash.md,16111,file
        result/report_01_12_2025_12-26-46_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_12-26-46_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,79523,file
        result/report_01_12_2025_12-55-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_12-55-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,143883,file
        result/report_01_12_2025_13-37-30_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_13-37-30_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,141924,file
        result/report_01_12_2025_14-15-04_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_14-15-04_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,129176,file
        result/report_01_12_2025_14-42-38_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_14-42-38_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,38933,file
        result/report_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,138015,file
        result/report_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.md,report_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.md,127747,file
        result/report_03_12_2025_22-46-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_03_12_2025_22-46-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,80305,file
        result/report_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,163921,file
        result/report_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.md,report_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.md,128520,file
        result/report_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.md,report_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.md,7245,file
        result/report_14_11_2025_14-52-36.md,report_14_11_2025_14-52-36.md,16393,file
        result/report_14_11_2025_15-21-53.md,report_14_11_2025_15-21-53.md,108585,file
        result/report_14_11_2025_15-26-24.md,report_14_11_2025_15-26-24.md,11659,file
        result/report_21_11_2025_15-43-30.md,report_21_11_2025_15-43-30.md,57940,file
        result/report_21_11_2025_16-06-12.md,report_21_11_2025_16-06-12.md,76423,file
        result/report_22_11_2025_14-01-50_Helper_llama3_Main_geminipro.md,report_22_11_2025_14-01-50_Helper_llama3_Main_geminipro.md,39595,file
        result/report_22_11_2025_14-39-55_Helper_llama3_MainLLM_llama3.md,report_22_11_2025_14-39-55_Helper_llama3_MainLLM_llama3.md,2013,file
        result/result_2025-11-11_12-30-53.md,result_2025-11-11_12-30-53.md,1232,file
        result/result_2025-11-11_12-43-51.md,result_2025-11-11_12-43-51.md,33683,file
        result/result_2025-11-11_12-45-37.md,result_2025-11-11_12-45-37.md,1193,file
    - name: schemas
      type: directory
      children[1]{path,name,size,type}:
        schemas/types.py,types.py,7559,file
    - name: statistics
      type: directory
      children[5]{path,name,size,type}:
        statistics/savings_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.png,savings_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.png,23279,file
        statistics/savings_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.png,savings_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.png,23650,file
        statistics/savings_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.png,savings_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.png,23805,file
        statistics/savings_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.png,savings_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.png,22698,file
        statistics/savings_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.png,savings_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.png,22539,file
    - path: test.json
      name: test.json
      size: 241669
      type: file
ast_schema:
  files:
    "backend/AST_Schema.py":
      ast_nodes:
        imports[3]: ast,os,getRepo.GitRepository
        functions[1]:
          - mode: function_analysis
            identifier: backend.AST_Schema.path_to_module
            name: path_to_module
            args[2]: filepath,project_root
            docstring: Wandelt einen Dateipfad in einen Python-Modulpfad um.
            source_code: "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    \n    module_path = rel_path.replace(os.path.sep, '.')\n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n    return module_path"
            start_line: 5
            end_line: 18
            context:
              calls[0]:
              called_by[0]:
        classes[2]:
          - mode: class_analysis
            identifier: backend.AST_Schema.ASTVisitor
            name: ASTVisitor
            docstring: null
            source_code: "class ASTVisitor(ast.NodeVisitor):\n    def __init__(self, source_code: str, file_path: str, project_root: str):\n        self.source_code = source_code\n        self.file_path = file_path\n        self.project_root = project_root\n        self.module_path = path_to_module(self.file_path, self.project_root)\n        self.schema = {\"imports\": [], \"functions\": [], \"classes\": []}\n        self._current_class = None\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(f\"{node.module}.{alias.name}\")\n        self.generic_visit(node)\n\n    def visit_ClassDef(self, node):\n        class_identifier = f\"{self.module_path}.{node.name}\"\n        class_info = {\n            \"mode\": \"class_analysis\",\n            \"identifier\": class_identifier,\n            \"name\": node.name,\n            \"docstring\": ast.get_docstring(node),\n            \"source_code\": ast.get_source_segment(self.source_code, node),\n            \"start_line\": node.lineno,\n            \"end_line\": node.end_lineno,\n            \"context\": {\n                \"dependencies\": [],\n                \"instantiated_by\": [],\n                \"method_context\": []\n            },   \n        }\n        self.schema[\"classes\"].append(class_info)\n        self._current_class = class_info \n        self.generic_visit(node)\n        self._current_class = None\n\n    def visit_FunctionDef(self, node):\n        if self._current_class:\n            method_identifier = f\"{self._current_class['identifier']}.{node.name}\"\n            method_context_info = {\n                \"identifier\": method_identifier,\n                \"name\": node.name,\n                \"calls\": [],\n                \"called_by\": [],\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n            }\n            self._current_class[\"context\"][\"method_context\"].append(method_context_info)\n        else:\n            func_identifier = f\"{self.module_path}.{node.name}\"\n            func_info = {\n                \"mode\": \"function_analysis\",\n                \"identifier\": func_identifier,\n                \"name\": node.name,\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"source_code\": ast.get_source_segment(self.source_code, node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n                \"context\": {\n                    \"calls\": [],\n                    \"called_by\": []\n                }\n            }\n            self.schema[\"functions\"].append(func_info)\n        self.generic_visit(node)\n    \n    def visit_AsyncFunctionDef(self, node):\n        self.visit_FunctionDef(node)"
            start_line: 20
            end_line: 94
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[6]:
                - identifier: backend.AST_Schema.ASTVisitor.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[4]: self,source_code,file_path,project_root
                  docstring: null
                  start_line: 21
                  end_line: 27
                - identifier: backend.AST_Schema.ASTVisitor.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 29
                  end_line: 32
                - identifier: backend.AST_Schema.ASTVisitor.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 34
                  end_line: 37
                - identifier: backend.AST_Schema.ASTVisitor.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 39
                  end_line: 58
                - identifier: backend.AST_Schema.ASTVisitor.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 60
                  end_line: 91
                - identifier: backend.AST_Schema.ASTVisitor.visit_AsyncFunctionDef
                  name: visit_AsyncFunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 93
                  end_line: 94
          - mode: class_analysis
            identifier: backend.AST_Schema.ASTAnalyzer
            name: ASTAnalyzer
            docstring: null
            source_code: "class ASTAnalyzer:\n\n    def __init__(self):\n        pass\n\n    def merge_relationship_data(self, full_schema: dict, raw_relationships: dict) -> dict:\n\n        outgoing_calls = raw_relationships.get(\"outgoing\", {})\n        incoming_calls = raw_relationships.get(\"incoming\", {})\n\n        for file_path, file_data in full_schema.get(\"files\", {}).items():\n            ast_nodes = file_data.get(\"ast_nodes\", {})\n            \n            for func in ast_nodes.get(\"functions\", []):\n                func_id = func.get(\"identifier\")\n                if func_id:\n                    func[\"context\"][\"calls\"] = outgoing_calls.get(func_id, [])\n                    func[\"context\"][\"called_by\"] = incoming_calls.get(func_id, [])\n\n            for cls in ast_nodes.get(\"classes\", []):\n                cls_id = cls.get(\"identifier\")\n                \n                if cls_id:\n                    cls[\"context\"][\"instantiated_by\"] = incoming_calls.get(cls_id, [])\n\n                class_dependencies = set()\n                \n                for method in cls[\"context\"].get(\"method_context\", []):\n                    method_id = method.get(\"identifier\")\n                    if method_id:\n                        m_calls = outgoing_calls.get(method_id, [])\n                        method[\"calls\"] = m_calls\n                        method[\"called_by\"] = incoming_calls.get(method_id, [])\n                        \n                        for callee in m_calls:\n                            if cls_id and not callee.startswith(f\"{cls_id}.\"):\n                                class_dependencies.add(callee)\n                \n                cls[\"context\"][\"dependencies\"] = sorted(list(class_dependencies))\n        \n        return full_schema\n\n    def analyze_repository(self, files: list, repo: GitRepository) -> dict:\n        full_schema = {\n            \"files\": {}\n        }\n\n        all_paths = [file_obj.path for file_obj in files]\n        if not all_paths:\n            return full_schema\n        \n        project_root = os.path.commonpath(all_paths)\n        if os.path.isfile(project_root):\n            project_root = os.path.dirname(project_root)\n\n        for file_obj in files:\n            if not file_obj.path.endswith('.py'):\n                continue\n            \n            file_content = file_obj.content\n            if not file_content.strip():\n                continue\n\n            try:\n                tree = ast.parse(file_content)\n                visitor = ASTVisitor(\n                    source_code=file_content, \n                    file_path=file_obj.path, \n                    project_root=project_root\n                )\n                visitor.visit(tree)\n                file_schema_nodes = visitor.schema\n\n                if file_schema_nodes[\"imports\"] or file_schema_nodes[\"functions\"] or file_schema_nodes[\"classes\"]:\n                    full_schema[\"files\"][file_obj.path] = {\n                        \"ast_nodes\": file_schema_nodes\n                    }\n\n            except (SyntaxError, ValueError) as e:\n                print(f\"Warnung: Konnte Datei '{file_obj.path}' nicht parsen. Fehler: {e}\")\n\n        return full_schema"
            start_line: 97
            end_line: 178
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[3]:
                - identifier: backend.AST_Schema.ASTAnalyzer.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 99
                  end_line: 100
                - identifier: backend.AST_Schema.ASTAnalyzer.merge_relationship_data
                  name: merge_relationship_data
                  calls[0]:
                  called_by[0]:
                  args[3]: self,full_schema,raw_relationships
                  docstring: null
                  start_line: 102
                  end_line: 137
                - identifier: backend.AST_Schema.ASTAnalyzer.analyze_repository
                  name: analyze_repository
                  calls[0]:
                  called_by[0]:
                  args[3]: self,files,repo
                  docstring: null
                  start_line: 139
                  end_line: 178
    "backend/File_Dependency.py":
      ast_nodes:
        imports[17]: networkx,os,ast.Assign,ast.AST,ast.ClassDef,ast.FunctionDef,ast.Import,ast.ImportFrom,ast.Name,ast.NodeVisitor,ast.literal_eval,ast.parse,ast.walk,keyword.iskeyword,pathlib.Path,getRepo.GitRepository,callgraph.make_safe_dot
        functions[3]:
          - mode: function_analysis
            identifier: backend.File_Dependency.build_file_dependency_graph
            name: build_file_dependency_graph
            args[3]: filename,tree,repo_root
            docstring: null
            source_code: "def build_file_dependency_graph(filename: str, tree: AST, repo_root: str) -> nx.DiGraph:\n    graph = nx.DiGraph()\n\n    tree_visitor = FileDependencyGraph(filename, repo_root)\n    tree_visitor.visit(tree)\n\n    for caller, callees in tree_visitor.import_dependencies.items():\n        graph.add_node(caller)\n        graph.add_nodes_from(callees)\n        for callee in callees:\n            graph.add_edge(caller, callee)\n    \n    return graph"
            start_line: 153
            end_line: 165
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.File_Dependency.build_repository_graph
            name: build_repository_graph
            args[1]: repository
            docstring: null
            source_code: "def build_repository_graph(repository: GitRepository) -> nx.DiGraph:\n    all_files = repository.get_all_files()\n    repo_root = repository.temp_dir\n    global_graph = nx.DiGraph()\n\n    for file in all_files: \n        if not file.path.endswith(\".py\"):\n            continue\n        filename = str(os.path.basename(file.path)).removesuffix(\".py\")\n        tree = parse(file.content)\n        graph = build_file_dependency_graph(filename, tree, repo_root)\n        \n        for node in graph.nodes:\n            global_graph.add_node(node)\n\n        for caller, callee in graph.edges:\n            if callee:\n                global_graph.add_node(callee)\n                global_graph.add_edge(caller, callee)\n    \n    return global_graph"
            start_line: 167
            end_line: 187
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.File_Dependency.get_all_temp_files
            name: get_all_temp_files
            args[1]: directory
            docstring: null
            source_code: "def get_all_temp_files(directory: str) -> list[Path]:\n    root_path = Path(directory).resolve()\n    all_files = [file.relative_to(root_path) for file in root_path.rglob(\"*.py\")]\n\n    return all_files"
            start_line: 189
            end_line: 193
            context:
              calls[0]:
              called_by[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.File_Dependency.FileDependencyGraph
            name: FileDependencyGraph
            docstring: null
            source_code: "class FileDependencyGraph(NodeVisitor):\n\n    import_dependencies: dict[str, set[str]] = {}\n    def __init__(self, filename: str, repo_root):\n        \"\"\"\n        Initialisiert den File Dependency Graphen\n\n        Args:\n\n        \"\"\"\n        self.filename = filename\n        self.repo_root = repo_root\n\n    def _resolve_module_name(self, node: ImportFrom) -> list[str]:\n        \"\"\"\n        L√∂st relative Imports der Form `from .. import name1, name2` auf.\n        Liefert die Liste der tats√§chlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\n        Wirft ImportError, wenn nichts aufgel√∂st werden konnte.\n        \"\"\"\n        level_depth = node.level\n        names = [alias.name for alias in node.names]  \n        all_files = get_all_temp_files(self.repo_root)  \n\n        candidates = [p for p in all_files if p.stem == Path(self.filename).stem or p.name == f\"{self.filename}.py\"]\n\n        if not candidates:\n            raise ImportError(f\"Kann aktuelle Datei '{self.filename}' im Repo nicht finden.\")\n\n\n        candidates.sort(key=lambda p: len(p.parts))\n        current_rel_path = candidates[0]\n        base_dir = current_rel_path.parent\n        if level_depth <= 0:\n            raise ImportError(\"Erwarteter relativer Import (level >= 1).\")\n        for _ in range(level_depth - 1):\n            if base_dir == base_dir.parent and len(base_dir.parts) == 0:\n                raise ImportError(f\"Relative Import-Ebene ({level_depth}) zu gro√ü f√ºr Datei '{current_rel_path}'.\")\n            base_dir = base_dir.parent\n\n        repo_root_path = Path(self.repo_root).resolve()\n        resolved: list[str] = []\n\n        def module_file_exists(rel_base: Path, name: str) -> bool:\n            file_path = repo_root_path / rel_base / f\"{name}.py\"\n            pkg_init = repo_root_path / rel_base / name / \"__init__.py\"\n            return file_path.exists() or pkg_init.exists()\n\n        def init_exports_symbol(rel_base: Path, symbol: str) -> bool:\n            \"\"\"\n            Pr√ºft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\n            oder als Name (funktion/klasse/assign) definiert ist.\n            \"\"\"\n            init_path = repo_root_path / rel_base / \"__init__.py\"\n            if not init_path.exists():\n                return False\n            try:\n                src = init_path.read_text(encoding=\"utf-8\")\n                mod = parse(src, filename=str(init_path))\n            except Exception:\n                return False\n\n            for node_ in walk(mod):\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == \"__all__\":\n                            try:\n                                value = literal_eval(node_.value)\n                                if isinstance(value, (list, tuple)) and symbol in value:\n                                    return True\n                            except Exception:\n                                pass\n                if isinstance(node_, (FunctionDef, ClassDef)) and node_.name == symbol:\n                    return True\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == symbol:\n                            return True\n            return False\n\n        for name in names:\n            if not name.isidentifier() or iskeyword(name):\n                continue\n\n            if module_file_exists(base_dir, name):\n                resolved.append(name)\n                continue\n            if init_exports_symbol(base_dir, name):\n                resolved.append(name)\n                continue\n\n        resolved = sorted(set(resolved))\n\n        if not resolved:\n            raise ImportError(\n                f\"Kein passendes Modul/Symbol f√ºr relative Import-Aufl√∂sung gefunden \"\n                f\"(level={level_depth}, names={names}, base_dir={base_dir})\"\n            )\n\n        return resolved\n    \n    def visit_Import(self, node: Import | ImportFrom, base_name: str | None = None):\n        for alias in node.names:\n        \n            if self.filename not in self.import_dependencies:\n                self.import_dependencies[self.filename] = set()\n\n            if base_name:\n                self.import_dependencies[self.filename].add(base_name)\n            else:\n                self.import_dependencies[self.filename].add(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node: ImportFrom):\n        \"\"\"\n        Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\n        also c, und dieser wird als callee f√ºr den caller, das File, gesetzt.\n        \"\"\"\n        module_name = node.module\n        if module_name:\n            module_base = module_name.split(\".\")[-1]\n            self.visit_Import(node, module_base)\n        else:\n            try:\n                resolved = self._resolve_module_name(node)\n                for base in resolved:\n                    self.visit_Import(node, base)\n            except ImportError as e:\n                print(f\"Aufl√∂sung eines relativen Imports fehlgeschlagen: {e}\")\n\n        self.generic_visit(node)"
            start_line: 22
            end_line: 151
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[6]:
                - identifier: backend.File_Dependency.FileDependencyGraph.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[3]: self,filename,repo_root
                  docstring: "Initialisiert den File Dependency Graphen\n\nArgs:"
                  start_line: 25
                  end_line: 33
                - identifier: backend.File_Dependency.FileDependencyGraph._resolve_module_name
                  name: _resolve_module_name
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: "L√∂st relative Imports der Form `from .. import name1, name2` auf.\nLiefert die Liste der tats√§chlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\nWirft ImportError, wenn nichts aufgel√∂st werden konnte."
                  start_line: 35
                  end_line: 120
                - identifier: backend.File_Dependency.FileDependencyGraph.module_file_exists
                  name: module_file_exists
                  calls[0]:
                  called_by[0]:
                  args[2]: rel_base,name
                  docstring: null
                  start_line: 64
                  end_line: 67
                - identifier: backend.File_Dependency.FileDependencyGraph.init_exports_symbol
                  name: init_exports_symbol
                  calls[0]:
                  called_by[0]:
                  args[2]: rel_base,symbol
                  docstring: "Pr√ºft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\noder als Name (funktion/klasse/assign) definiert ist."
                  start_line: 69
                  end_line: 99
                - identifier: backend.File_Dependency.FileDependencyGraph.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[3]: self,node,base_name
                  docstring: null
                  start_line: 122
                  end_line: 132
                - identifier: backend.File_Dependency.FileDependencyGraph.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: "Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\nalso c, und dieser wird als callee f√ºr den caller, das File, gesetzt."
                  start_line: 134
                  end_line: 151
    "backend/HelperLLM.py":
      ast_nodes:
        imports[23]: os,json,logging,time,typing.List,typing.Dict,typing.Any,typing.Optional,typing.Union,dotenv.load_dotenv,langchain_google_genai.ChatGoogleGenerativeAI,langchain_ollama.ChatOllama,langchain_openai.ChatOpenAI,langchain.messages.HumanMessage,langchain.messages.SystemMessage,langchain.messages.AIMessage,pydantic.ValidationError,schemas.types.FunctionAnalysis,schemas.types.ClassAnalysis,schemas.types.FunctionAnalysisInput,schemas.types.FunctionContextInput,schemas.types.ClassAnalysisInput,schemas.types.ClassContextInput
        functions[1]:
          - mode: function_analysis
            identifier: backend.HelperLLM.main_orchestrator
            name: main_orchestrator
            args[0]:
            docstring: "Dummy Data and processing loop for testing the LLMHelper class.\nThis version is syntactically correct and logically matches the Pydantic models."
            source_code: "def main_orchestrator():\n    \"\"\"\n    Dummy Data and processing loop for testing the LLMHelper class.\n    This version is syntactically correct and logically matches the Pydantic models.\n    \"\"\"\n    \n    # --- Step 1: Define the pre-computed analysis for each method ---\n    \n    # Example Input 1: For the 'add_item' function\n    add_item_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"source_code\": \"\"\"def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\\\"\\\"\\\"\n        if not isinstance(quantity, int) or quantity <= 0:\n            raise ValueError(\"Quantity must be a positive integer.\")\n    \n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n        print(f\"Added {quantity} of {item_name}. New total: {self.inventory[item_name]}\")\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"process_shipment\", \"restock_api_endpoint\"]\n        }\n    })\n\n    # Example Input 2: For the 'check_stock' function\n    check_stock_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"source_code\": \"\"\"def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Retrieves the current stock quantity for a given item.\\\"\\\"\\\"\n        stock_level = self.inventory.get(item_name, 0)\n        return stock_level\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"fulfill_order\", \"ui_display_handler\"]\n        }\n    })\n\n    # Example Input 3: For the 'generate_report' function\n    generate_report_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"source_code\": \"\"\"def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a timestamped string summary of the current inventory state.\\\"\\\"\\\"\n        timestamp = datetime.now()\n        report_header = \"Inventory Report as of: {ts}\\\\n\".format(ts=timestamp)\n        report_lines = [report_header]\n        \n        for item, quantity in self.inventory.items():\n            line = \"- {name}: {qty}\".format(name=item, qty=quantity)\n            report_lines.append(line)\n            \n        return \"\\\\n\".join(report_lines)\"\"\",\n        \"imports\": [\"from datetime import datetime\"],\n        \"context\": {\n            \"calls\": [\"datetime.now\", \"str.format\"],\n            \"called_by\": [\"daily_cron_job\", \"admin_dashboard_export\"]\n        }\n    })\n\n    add_item_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"description\": {\n            \"overall\": \"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to add.\"},\n                {\"name\": \"quantity\", \"type\": \"int\", \"description\": \"The number of units to add. Must be a positive integer.\"}\n            ],\n            \"returns\": [],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"process_shipment and restock_api_endpoint\" }\n        },\n        \"error\": None\n    })\n\n    check_stock_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"description\": {\n            \"overall\": \"Retrieves the current stock quantity for a given item.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to check.\"}\n            ],\n            \"returns\": [\n                {\"name\": \"stock_level\", \"type\": \"int\", \"description\": \"The quantity of the item in stock. Returns 0 if the item is not found.\"}\n            ],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"fulfill_order and ui_display_handler\" }\n        },\n        \"error\": None\n    })\n\n    generate_report_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"description\": {\n            \"overall\": \"Generates a timestamped string summary of the current inventory state.\",\n            \"parameters\": [ {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"} ],\n            \"returns\": [\n                {\"name\": \"report\", \"type\": \"str\", \"description\": \"A formatted string detailing all items and their quantities.\"}\n            ],\n            \"usage_context\": { \"calls\": \"datetime.now and str.format\", \"called_by\": \"daily_cron_job and admin_dashboard_export\" }\n        },\n        \"error\": None\n    })\n\n    class_input = ClassAnalysisInput(\n        mode=\"class_analysis\",\n        identifier=\"InventoryManager\",\n        # NOTE: The indentation of this string is now fixed. It starts at column 0.\n        source_code=\"\"\"class InventoryManager:\n    \\\"\\\"\\\"Manages stock levels for products in a warehouse.\\\"\\\"\\\"\n    def __init__(self, warehouse_id: str):\n        self.warehouse_id = warehouse_id\n        self.inventory = {}  # item_name: quantity\n\n    def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds an item to the inventory.\\\"\\\"\\\"\n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n\n    def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Checks the stock of a specific item.\\\"\\\"\\\"\n        return self.inventory.get(item_name, 0)\n\n    def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a summary report of the inventory.\\\"\\\"\\\"\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        report_lines = [f\"Inventory Report for {self.warehouse_id} at {timestamp}\"]\n        if not self.inventory:\n            report_lines.append(\"Inventory is empty.\")\n        else:\n            for item, quantity in self.inventory.items():\n                report_lines.append(f\"- {item}: {quantity}\")\n        return \"\\\\n\".join(report_lines)\n\"\"\",\n        imports=[\"from datetime import datetime\"],\n        context=ClassContextInput(\n            dependencies=[\"datetime\"],\n            instantiated_by=[\"main_app_startup\", \"warehouse_worker_script\"],\n            methods_analysis=[\n                add_item_analysis,\n                check_stock_analysis,\n                generate_report_analysis\n            ]\n        )\n    )\n\n    \n    # The helper methods expect a LIST of inputs, so we wrap our single object in a list.\n    input = [add_item_input, check_stock_input, generate_report_input]\n    analysis = [add_item_analysis, check_stock_analysis, generate_report_analysis] \n\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    llm_helper = LLMHelper(api_key=GEMINI_API_KEY, function_prompt_path=function_prompt_file, class_prompt_path=class_prompt_file)\n\n    # This will be the final JSON containing all documentation\n    final_documentation = {}\n\n    # The analysis of the methods is already provided in the context.\n    logging.info(\"\\n--- Generating documentation for classes ---\")\n    \n    # The `generate_for_classes` method returns a list of results.\n    analysis_results = llm_helper.generate_for_functions(input)\n\n    # --- Step 4: Process the results ---\n    \n    # We loop through the list of results (even though there's only one in this case).\n    for doc in analysis_results:\n        if doc:\n            logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n            if \"classes\" not in final_documentation:\n                final_documentation[\"classes\"] = {}\n            # Use .model_dump() to convert the Pydantic object back to a dict for JSON serialization\n            final_documentation[\"classes\"][doc.identifier] = doc.model_dump() \n        else:\n            logging.warning(f\"Failed to generate doc for a class\")\n\n    # --- Step 5: Display the final aggregated result ---\n    logging.info(\"\\n--- Final Generated Documentation ---\")\n    print(json.dumps(final_documentation, indent=2))"
            start_line: 227
            end_line: 413
            context:
              calls[0]:
              called_by[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.HelperLLM.LLMHelper
            name: LLMHelper
            docstring: "A class to interact with Google Gemini for generating code snippet documentation.\nIt centralizes API interaction, error handling, and validates I/O using Pydantic."
            source_code: "class LLMHelper:\n    \"\"\"\n    A class to interact with Google Gemini for generating code snippet documentation.\n    It centralizes API interaction, error handling, and validates I/O using Pydantic.\n    \"\"\"\n    def __init__(self, api_key: str, function_prompt_path: str, class_prompt_path: str, model_name: str = \"gemini-2.0-flash-lite\", base_url: str = None):\n        if not api_key:\n            raise ValueError(\"Gemini API Key must be set.\")\n        \n        try:\n            with open(function_prompt_path, 'r', encoding='utf-8') as f:\n                self.function_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Function system prompt file not found at: {function_prompt_path}\")\n            raise\n\n        # Handle the second file\n        try:\n            with open(class_prompt_path, 'r', encoding='utf-8') as f:\n                self.class_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Class system prompt file not found at: {class_prompt_path}\")\n            raise\n        \n        # Batch-Size config\n        self.model_name = model_name\n        self._configure_batch_settings(model_name)\n\n\n        if model_name.startswith(\"gemini-\"):\n            base_llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=0.3, \n            )\n        \n        elif model_name.startswith(\"gpt-\") and \"openGPT\" not in model_name:\n            base_llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=0.3,\n            )\n\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            if not SCADSLLM_URL:\n                raise ValueError(f\"SCADSLLM_URL environment variable is required for model {model_name}\")\n            \n            logging.info(f\"Connecting to Custom API at {SCADSLLM_URL} for model {model_name}\")\n            \n            base_llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                base_url=SCADSLLM_URL,\n                temperature=0.3,\n            )\n\n        else:\n            target_url = base_url if base_url else OLLAMA_BASE_URL\n            logging.info(f\"Using Ollama at {target_url} for model {model_name}\")\n            base_llm = ChatOllama(\n                model=model_name,\n                temperature=0.3,\n                base_url=target_url,\n            )\n\n        self.function_llm = base_llm.with_structured_output(FunctionAnalysis, method=\"json_schema\")\n        self.class_llm = base_llm.with_structured_output(ClassAnalysis, method=\"json_schema\")\n\n        self.raw_llm = base_llm\n\n        logging.info(f\"LLMHelper initialized with model '{model_name}'. Batch Size: {self.batch_size}\")\n\n    def _configure_batch_settings(self, model_name: str):\n\n        if model_name == \"gemini-2.0-flash-lite\":\n            self.batch_size = 30\n        \n        elif model_name == \"gemini-flash-latest\":\n            self.batch_size = 10\n\n        elif model_name == \"gemini-2.5-flash\":\n            self.batch_size = 10\n            \n        elif model_name == \"gemini-2.5-pro\":\n            self.batch_size = 2\n        \n        elif model_name == \"gemini-2.0-flash\":\n            self.batch_size = 15\n\n        elif model_name == \"gemini-2.5-flash-lite\":\n            self.batch_size = 15\n\n        elif model_name == \"llama3\":\n            self.batch_size = 50\n\n        elif model_name == \"gpt-5.1\":\n            self.batch_size = 500\n\n        elif model_name == \"gpt-5-mini\":\n            self.batch_size = 500\n\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            self.batch_size = 500\n            \n        else:\n            logging.warning(f\"Unknown model '{model_name}', using conservative defaults.\")\n            self.batch_size = 2\n\n    def generate_for_functions(self, function_inputs: List[FunctionAnalysisInput]) -> List[Optional[FunctionAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of functions.\"\"\"\n\n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 62\n\n        if not function_inputs:\n            return []\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(function_input.model_dump(), indent=2) \n            for function_input in function_inputs\n        ]\n       \n        conversations = [[SystemMessage(content=self.function_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_functions = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n        \n            try:\n                batch_results = self.function_llm.batch(batch_conversations, config={\"max_concurrency\": self.batch_size})\n                all_validated_functions.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                all_validated_functions.extend([None] * len(batch_conversations))\n            \n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_functions\n        \n            \n    \n\n    def generate_for_classes(self, class_inputs: List[ClassAnalysisInput]) -> List[Optional[ClassAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of classes.\"\"\"\n        if not class_inputs:\n            return []\n        \n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 62\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(class_input.model_dump(), indent=2)\n            for class_input in class_inputs\n        ]\n\n        conversations = [[SystemMessage(content=self.class_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_classes = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n\n            try:\n                batch_results = self.class_llm.batch(batch_conversations , config={\"max_concurrency\": self.batch_size})\n                all_validated_classes.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                # Falls ein Fehler auftritt, f√ºllen wir die Liste mit None auf, um die Reihenfolge zu wahren\n                all_validated_classes.extend([None] * len(batch_conversations))\n\n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_classes"
            start_line: 31
            end_line: 221
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[4]:
                - identifier: backend.HelperLLM.LLMHelper.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[6]: self,api_key,function_prompt_path,class_prompt_path,model_name,base_url
                  docstring: null
                  start_line: 36
                  end_line: 101
                - identifier: backend.HelperLLM.LLMHelper._configure_batch_settings
                  name: _configure_batch_settings
                  calls[0]:
                  called_by[0]:
                  args[2]: self,model_name
                  docstring: null
                  start_line: 103
                  end_line: 137
                - identifier: backend.HelperLLM.LLMHelper.generate_for_functions
                  name: generate_for_functions
                  calls[0]:
                  called_by[0]:
                  args[2]: self,function_inputs
                  docstring: Generates and validates documentation for a batch of functions.
                  start_line: 139
                  end_line: 178
                - identifier: backend.HelperLLM.LLMHelper.generate_for_classes
                  name: generate_for_classes
                  calls[0]:
                  called_by[0]:
                  args[2]: self,class_inputs
                  docstring: Generates and validates documentation for a batch of classes.
                  start_line: 183
                  end_line: 221
    "backend/MainLLM.py":
      ast_nodes:
        imports[9]: os,logging,sys,dotenv.load_dotenv,langchain_google_genai.ChatGoogleGenerativeAI,langchain_ollama.ChatOllama,langchain_openai.ChatOpenAI,langchain.messages.HumanMessage,langchain.messages.SystemMessage
        functions[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.MainLLM.MainLLM
            name: MainLLM
            docstring: Hauptklasse f√ºr die Interaktion mit dem LLM.
            source_code: "class MainLLM:\n    \"\"\"\n    Hauptklasse f√ºr die Interaktion mit dem LLM.\n    \"\"\"\n    def __init__(self, api_key: str, prompt_file_path: str, model_name: str = \"gemini-2.5-pro\", base_url: str = None):\n        if not api_key:\n            raise ValueError(\"Gemini API Key must be set.\")\n        \n        try:\n            with open(prompt_file_path, 'r', encoding='utf-8') as f:\n                self.system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"System prompt file not found at: {prompt_file_path}\")\n            raise\n        \n        self.model_name = model_name\n\n        if model_name.startswith(\"gemini-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n\n        elif model_name.startswith(\"gpt-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            if not SCADSLLM_URL:\n                raise ValueError(f\"SCADSLLM_URL environment variable is required for model {model_name}\")\n            \n            logging.info(f\"Connecting to Custom API at {SCADSLLM_URL} for model {model_name}\")\n            \n            self.llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                base_url=SCADSLLM_URL,\n                temperature=1.0,\n            )\n\n        else:\n            target_url = base_url if base_url else OLLAMA_BASE_URL\n            self.llm = ChatOllama(\n                model=model_name,\n                temperature=1.0,\n                base_url=target_url,\n            )\n\n        logging.info(f\"Main LLM initialized with model '{model_name}'.\")\n    \n    def call_llm(self, user_input: str):\n\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with HelperLLM input...\")\n\n        try:\n            response = self.llm.invoke(messages)\n            logging.info(\"LLM call successful.\")\n            return response.content\n        except Exception as e:\n            logging.error(f\"Error during LLM call: {e}\")\n            return None\n\n    def stream_llm(self, user_input: str):\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with 'stream'...\")\n\n        try:\n            stream_iterator = self.llm.stream(messages)\n            \n            for chunk in stream_iterator:\n                yield chunk.content\n        except Exception as e:\n            error_message = f\"\\n--- Error during LLM stream call: {e} ---\"\n            logging.error(error_message)\n            yield error_message"
            start_line: 22
            end_line: 106
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[3]:
                - identifier: backend.MainLLM.MainLLM.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[5]: self,api_key,prompt_file_path,model_name,base_url
                  docstring: null
                  start_line: 26
                  end_line: 73
                - identifier: backend.MainLLM.MainLLM.call_llm
                  name: call_llm
                  calls[0]:
                  called_by[0]:
                  args[2]: self,user_input
                  docstring: null
                  start_line: 75
                  end_line: 89
                - identifier: backend.MainLLM.MainLLM.stream_llm
                  name: stream_llm
                  calls[0]:
                  called_by[0]:
                  args[2]: self,user_input
                  docstring: null
                  start_line: 91
                  end_line: 106
    "backend/basic_info.py":
      ast_nodes:
        imports[7]: re,os,tomllib,typing.List,typing.Dict,typing.Any,typing.Optional
        functions[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.basic_info.ProjektInfoExtractor
            name: ProjektInfoExtractor
            docstring: "Extrahiert grundlegende Projektinformationen aus g√§ngigen Projektdateien\nwie README, pyproject.toml und requirements.txt."
            source_code: "class ProjektInfoExtractor:\n    \"\"\"\n    Extrahiert grundlegende Projektinformationen aus g√§ngigen Projektdateien\n    wie README, pyproject.toml und requirements.txt.\n    \"\"\"\n    def __init__(self):\n        self.INFO_NICHT_GEFUNDEN = \"Information not found\"\n        # Initialisiert die Struktur mit Platzhaltern\n        self.info = {\n            \"projekt_uebersicht\": {\n                \"titel\": self.INFO_NICHT_GEFUNDEN,\n                \"beschreibung\": self.INFO_NICHT_GEFUNDEN,\n                \"aktueller_status\": self.INFO_NICHT_GEFUNDEN,\n                \"key_features\": self.INFO_NICHT_GEFUNDEN,\n                \"tech_stack\": self.INFO_NICHT_GEFUNDEN,\n            },\n            \"installation\": {\n                \"dependencies\": self.INFO_NICHT_GEFUNDEN,\n                \"setup_anleitung\": self.INFO_NICHT_GEFUNDEN,\n                \"quick_start_guide\": self.INFO_NICHT_GEFUNDEN,\n            }\n        }\n\n    def _clean_content(self, content: str) -> str:\n        \"\"\"Entfernt Null-Bytes, die durch Encoding-Fehler (UTF-16 als UTF-8 gelesen) entstehen.\"\"\"\n        if not content:\n            return \"\"\n        return content.replace('\\x00', '')\n\n    def _finde_datei(self, patterns: List[str], dateien: List[Any]) -> Optional[Any]:\n        \"\"\"Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht.\"\"\"\n        for datei in dateien:\n            for pattern in patterns:\n                if datei.path.lower().endswith(pattern.lower()):\n                    return datei\n        return None\n\n    def _extrahiere_sektion_aus_markdown(self, inhalt: str, keywords: List[str]) -> Optional[str]:\n        \"\"\"\n        Extrahiert den Text unter einer Markdown-√úberschrift (##).\n        \"\"\"\n        # Erstellt ein Regex-Pattern, das auf jedes der Schl√ºsselw√∂rter reagiert\n        keyword_pattern = \"|\".join(re.escape(k) for k in keywords)\n        \n        # Sucht nach \"## Schl√ºsselwort\" und erfasst alles bis zur n√§chsten \"##\" oder dem Dateiende\n        pattern = re.compile(\n            rf\"##\\s*({keyword_pattern})\\s*\\n(.*?)(?=\\n##|\\Z)\",\n            re.IGNORECASE | re.DOTALL\n        )\n        match = pattern.search(inhalt)\n        if match:\n            return match.group(2).strip()\n        return None\n\n    def _parse_readme(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer README-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        if self.info[\"projekt_uebersicht\"][\"titel\"] == self.INFO_NICHT_GEFUNDEN:\n            title_match = re.search(r\"^\\s*#\\s*(.*)\", inhalt)\n            if title_match:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = title_match.group(1).strip()\n\n        # Beschreibung (Fallback)\n        if self.info[\"projekt_uebersicht\"][\"beschreibung\"] == self.INFO_NICHT_GEFUNDEN:\n            desc_match = re.search(r\"^\\s*#\\s*.*\\n+([^#\\n].*)\", inhalt, re.DOTALL)\n            if desc_match:\n                 self.info[\"projekt_uebersicht\"][\"beschreibung\"] = desc_match.group(1).strip().split('\\n\\n')[0]\n\n        # Key Features\n        features = self._extrahiere_sektion_aus_markdown(inhalt, [\"Features\", \"Key Features\", \"Merkmale\"])\n        if features:\n            self.info[\"projekt_uebersicht\"][\"key_features\"] = features\n            \n        # Tech Stack\n        tech_stack = self._extrahiere_sektion_aus_markdown(inhalt, [\"Tech Stack\", \"Technology\", \"Technologien\"])\n        if tech_stack:\n            self.info[\"projekt_uebersicht\"][\"tech_stack\"] = tech_stack\n            \n        # Status\n        status = self._extrahiere_sektion_aus_markdown(inhalt, [\"Status\", \"Current Status\"])\n        if status:\n            self.info[\"projekt_uebersicht\"][\"aktueller_status\"] = status\n\n        # Setup-Anleitung\n        setup = self._extrahiere_sektion_aus_markdown(inhalt, [\"Installation\", \"Setup\", \"Getting Started\"])\n        if setup:\n            self.info[\"installation\"][\"setup_anleitung\"] = setup\n            \n        # Quick Start\n        quick_start = self._extrahiere_sektion_aus_markdown(inhalt, [\"Quick Start\", \"Schnellstart\"])\n        if quick_start:\n            self.info[\"installation\"][\"quick_start_guide\"] = quick_start\n\n    def _parse_toml(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer pyproject.toml-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        if not tomllib:\n            print(\"Warnung: 'tomli' ist nicht installiert. pyproject.toml kann nicht analysiert werden.\")\n            return\n            \n        try:\n            data = tomllib.loads(inhalt)\n            project_data = data.get(\"project\", {})\n            \n            if \"name\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = project_data[\"name\"]\n            if \"description\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"beschreibung\"] = project_data[\"description\"]\n            if \"dependencies\" in project_data:\n                self.info[\"installation\"][\"dependencies\"] = project_data[\"dependencies\"]\n        except tomllib.TOMLDecodeError as e:\n            print(f\"Warnung: Fehler beim Parsen der pyproject.toml: {e}\")\n\n    def _parse_requirements(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer requirements.txt-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        # Nur f√ºllen, wenn noch keine Dependencies aus toml gefunden wurden\n        if self.info[\"installation\"][\"dependencies\"] == self.INFO_NICHT_GEFUNDEN:\n            lines = inhalt.splitlines()\n            dependencies = [\n                line.strip() for line in lines \n                if line.strip() and not line.strip().startswith('#')\n            ]\n            if dependencies:\n                self.info[\"installation\"][\"dependencies\"] = dependencies\n\n    def extrahiere_info(self, dateien: List[Any], repo_url: str) -> Dict[str, Any]:\n        \"\"\"\n        Orchestriert die Extraktion von Informationen.\n        \"\"\"\n        # Relevante Dateien finden\n        readme_datei = self._finde_datei([\"readme.md\", \"readme.rst\", \"readme.txt\", \"readme\"], dateien)\n        toml_datei = self._finde_datei([\"pyproject.toml\"], dateien)\n        req_datei = self._finde_datei([\"requirements.txt\"], dateien)\n\n        # Dateien parsen (mit Priorisierung)\n        if toml_datei:\n            self._parse_toml(toml_datei.content)\n\n        if req_datei:\n            self._parse_requirements(req_datei.content)\n            \n        if readme_datei:\n            self._parse_readme(readme_datei.content)\n            \n        # Finale Formatierung der Dependencies\n        deps = self.info[\"installation\"][\"dependencies\"]\n        if isinstance(deps, list):\n            if not deps:\n                self.info[\"installation\"][\"dependencies\"] = self.INFO_NICHT_GEFUNDEN\n            else:\n                self.info[\"installation\"][\"dependencies\"] = \"\\n\".join(f\"- {dep}\" for dep in deps)\n        \n        # Titel aus URL ableiten, falls nichts gefunden wurde (oder als Zusatzinfo)\n        if repo_url:\n            repo_name = os.path.basename(repo_url.removesuffix('.git'))\n            # Wenn noch kein Titel da ist oder er generisch war\n            if self.info[\"projekt_uebersicht\"][\"titel\"] == self.INFO_NICHT_GEFUNDEN:\n                 self.info[\"projekt_uebersicht\"][\"titel\"] = f\"{repo_name} documentation\"\n\n        return self.info"
            start_line: 9
            end_line: 172
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[8]:
                - identifier: backend.basic_info.ProjektInfoExtractor.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 14
                  end_line: 30
                - identifier: backend.basic_info.ProjektInfoExtractor._clean_content
                  name: _clean_content
                  calls[0]:
                  called_by[0]:
                  args[2]: self,content
                  docstring: "Entfernt Null-Bytes, die durch Encoding-Fehler (UTF-16 als UTF-8 gelesen) entstehen."
                  start_line: 32
                  end_line: 36
                - identifier: backend.basic_info.ProjektInfoExtractor._finde_datei
                  name: _finde_datei
                  calls[0]:
                  called_by[0]:
                  args[3]: self,patterns,dateien
                  docstring: "Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht."
                  start_line: 38
                  end_line: 44
                - identifier: backend.basic_info.ProjektInfoExtractor._extrahiere_sektion_aus_markdown
                  name: _extrahiere_sektion_aus_markdown
                  calls[0]:
                  called_by[0]:
                  args[3]: self,inhalt,keywords
                  docstring: Extrahiert den Text unter einer Markdown-√úberschrift (##).
                  start_line: 46
                  end_line: 61
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_readme
                  name: _parse_readme
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer README-Datei.
                  start_line: 63
                  end_line: 101
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_toml
                  name: _parse_toml
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer pyproject.toml-Datei.
                  start_line: 103
                  end_line: 122
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_requirements
                  name: _parse_requirements
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer requirements.txt-Datei.
                  start_line: 124
                  end_line: 136
                - identifier: backend.basic_info.ProjektInfoExtractor.extrahiere_info
                  name: extrahiere_info
                  calls[0]:
                  called_by[0]:
                  args[3]: self,dateien,repo_url
                  docstring: Orchestriert die Extraktion von Informationen.
                  start_line: 138
                  end_line: 172
    "backend/callgraph.py":
      ast_nodes:
        imports[9]: ast,networkx,os,pathlib.Path,typing.Dict,getRepo.GitRepository,getRepo.GitRepository,basic_info.ProjektInfoExtractor,os
        functions[2]:
          - mode: function_analysis
            identifier: backend.callgraph.make_safe_dot
            name: make_safe_dot
            args[2]: graph,out_path
            docstring: null
            source_code: "def make_safe_dot(graph: nx.DiGraph, out_path: str):\n    mapping = {}\n    H = graph.copy()\n    for i, n in enumerate(list(graph.nodes())):\n        safe = f\"n{i}\"           \n        mapping[n] = safe\n    H = nx.relabel_nodes(H, mapping)\n    for orig, safe in mapping.items():\n        H.nodes[safe][\"label\"] = orig\n    nx.drawing.nx_pydot.write_dot(H, out_path)"
            start_line: 173
            end_line: 182
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.callgraph.build_filtered_callgraph
            name: build_filtered_callgraph
            args[1]: repo
            docstring: Baut den globalen Call-Graphen und filtert ihn auf selbst geschriebene Funktionen.
            source_code: "def build_filtered_callgraph(repo: GitRepository) -> nx.DiGraph:\n    \"\"\"\n    Baut den globalen Call-Graphen und filtert ihn auf selbst geschriebene Funktionen.\n    \"\"\"\n    all_file_objects = repo.get_all_files()\n    own_functions = set()\n    file_trees = {}\n\n    for file in all_file_objects:\n        if not file.path.endswith(\".py\"):\n            continue\n        filename = Path(file.path).stem\n        tree = ast.parse(file.content)\n        file_trees[filename] = tree\n        visitor = CallGraph(filename)\n        visitor.visit(tree)\n        own_functions.update(visitor.function_set)\n\n    # Globalen Call-Graph bauen\n    global_graph = nx.DiGraph()\n    for filename, tree in file_trees.items():\n        visitor = CallGraph(filename)\n        visitor.visit(tree)\n        for caller, callees in visitor.edges.items():\n            if caller not in own_functions:\n                continue \n            for callee in callees:\n                if callee in own_functions:\n                    global_graph.add_edge(caller, callee)\n                    global_graph.add_node(caller)\n                    global_graph.add_node(callee)\n    return global_graph"
            start_line: 185
            end_line: 216
            context:
              calls[0]:
              called_by[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.callgraph.CallGraph
            name: CallGraph
            docstring: null
            source_code: "class CallGraph(ast.NodeVisitor):\n    def __init__(self, filename: str):\n        self.filename = filename\n        self.current_function = None\n        self.current_class = None\n        \n        self.local_defs: dict[str, str] = {}\n        self.graph = nx.DiGraph()\n        self.import_mapping: dict[str, str] = {}\n        self.function_set: set[str] = set()\n        self.edges: Dict[str, set[str]] = {}\n\n    def _recursive_call(self, node):\n        \"\"\"\n        Liefert eine Liste der Namenskomponenten als dotted string z.B. ['pkg', 'mod', 'Class', 'method']\n        \"\"\"\n        if isinstance(node, ast.Call):\n            return self._recursive_call(node.func)\n        if isinstance(node, ast.Name):\n            return [node.id]\n        if isinstance(node, ast.Attribute):\n            parts = self._recursive_call(node.value)\n            parts.append(node.attr)\n            return parts\n        return []\n\n    def _resolve_all_callee_names(self, callee_nodes: list[list[str]]) -> list[str]:\n        \"\"\"\n        callee_nodes ist jetzt eine Liste von Listen (Name-Steps).\n        Wir pr√ºfen zuerst lokale Definitionen, dann import_mapping.\n        \"\"\"\n        resolved = []\n        for parts in callee_nodes:\n            if not parts:\n                continue\n            simple = parts[-1]\n            dotted = \".\".join(parts[-2:]) if len(parts) >= 2 else simple\n\n            if simple in self.local_defs:\n                resolved.append(self.local_defs[simple])\n                continue\n            if dotted in self.local_defs:\n                resolved.append(self.local_defs[dotted])\n                continue\n\n            first = parts[0]\n            if first in self.import_mapping:\n                mod = self.import_mapping[first]\n                rest = \"::\".join(parts[1:]) if len(parts) > 1 else simple\n                resolved.append(f\"{mod}::{rest}\")\n                continue\n\n            if self.current_class:\n                resolved.append(f\"{self.filename}::{parts[0]}::{simple}\" if len(parts)==1 else f\"{self.filename}::{'::'.join(parts)}\")\n            else:\n                resolved.append(f\"{self.filename}::{ '::'.join(parts)}\")\n        return resolved\n\n    def _make_full_name(self, basename: str, class_name: str | None = None) -> str:\n        if class_name:\n            return f\"{self.filename}::{class_name}::{basename}\"\n        return f\"{self.filename}::{basename}\"\n    \n    def _current_caller(self) -> str:\n        if self.current_function:\n            return self.current_function\n        return f\"<{self.filename}>\" if self.filename else \"<global-scope>\"\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            module_name = alias.name\n            module_asname = alias.asname if alias.asname else module_name\n            self.import_mapping[module_asname] = module_name\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        module_name = node.module.split(\".\")[-1] if node.module else \"\"\n        for alias in node.names:\n            self.import_mapping[alias.asname or alias.name] = f\"{module_name}\" if module_name else alias.name\n\n    def visit_ClassDef(self, node: ast.ClassDef):\n        prev_class = self.current_class\n        self.current_class = node.name\n        self.generic_visit(node)\n        self.current_class = prev_class\n        \n    def visit_FunctionDef(self, node):\n        prev_function = self.current_function\n        full_name = self._make_full_name(node.name, self.current_class)\n        self.local_defs[node.name] = full_name\n        if self.current_class:\n            self.local_defs[f\"{self.current_class}.{node.name}\"] = full_name\n\n        self.current_function = full_name\n        self.graph.add_node(self.current_function)\n        self.generic_visit(node)\n        self.function_set.add(self.current_function)\n        self.current_function = prev_function\n\n    def visit_AsyncFunctionDef(self, node):\n        self.visit_FunctionDef(node)\n\n    def visit_Call(self, node):\n        caller = self._current_caller()\n        parts = self._recursive_call(node)\n        resolved_callees = self._resolve_all_callee_names([parts])\n\n        if caller not in self.edges:\n            self.edges[caller] = set()\n\n        for callee in resolved_callees:\n            if callee:\n                self.edges[caller].add(callee)\n        self.generic_visit(node)\n\n    def visit_If(self, node):\n        if (isinstance(node.test, ast.Compare) and\n            isinstance(node.test.left, ast.Name) and \n            node.test.left.id == \"__name__\"):\n            caller_backup = self.current_function\n            self.current_function = \"<main_block>\"\n            self.generic_visit(node)\n            self.current_function = caller_backup\n        else:\n            self.generic_visit(node)"
            start_line: 10
            end_line: 134
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[12]:
                - identifier: backend.callgraph.CallGraph.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,filename
                  docstring: null
                  start_line: 11
                  end_line: 20
                - identifier: backend.callgraph.CallGraph._recursive_call
                  name: _recursive_call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: "Liefert eine Liste der Namenskomponenten als dotted string z.B. ['pkg', 'mod', 'Class', 'method']"
                  start_line: 22
                  end_line: 34
                - identifier: backend.callgraph.CallGraph._resolve_all_callee_names
                  name: _resolve_all_callee_names
                  calls[0]:
                  called_by[0]:
                  args[2]: self,callee_nodes
                  docstring: "callee_nodes ist jetzt eine Liste von Listen (Name-Steps).\nWir pr√ºfen zuerst lokale Definitionen, dann import_mapping."
                  start_line: 36
                  end_line: 66
                - identifier: backend.callgraph.CallGraph._make_full_name
                  name: _make_full_name
                  calls[0]:
                  called_by[0]:
                  args[3]: self,basename,class_name
                  docstring: null
                  start_line: 68
                  end_line: 71
                - identifier: backend.callgraph.CallGraph._current_caller
                  name: _current_caller
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 73
                  end_line: 76
                - identifier: backend.callgraph.CallGraph.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 78
                  end_line: 83
                - identifier: backend.callgraph.CallGraph.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 85
                  end_line: 88
                - identifier: backend.callgraph.CallGraph.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 90
                  end_line: 94
                - identifier: backend.callgraph.CallGraph.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 96
                  end_line: 107
                - identifier: backend.callgraph.CallGraph.visit_AsyncFunctionDef
                  name: visit_AsyncFunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 109
                  end_line: 110
                - identifier: backend.callgraph.CallGraph.visit_Call
                  name: visit_Call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 112
                  end_line: 123
                - identifier: backend.callgraph.CallGraph.visit_If
                  name: visit_If
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 125
                  end_line: 134
    "backend/converter.py":
      ast_nodes:
        imports[4]: logging,nbformat,base64,nbformat.reader.NotJSONError
        functions[5]:
          - mode: function_analysis
            identifier: backend.converter.wrap_cdata
            name: wrap_cdata
            args[1]: content
            docstring: Wraps content in CDATA tags.
            source_code: "def wrap_cdata(content):\n    \"\"\"Wraps content in CDATA tags.\"\"\"\n    return f\"<![CDATA[\\n{content}\\n]]>\""
            start_line: 8
            end_line: 10
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.converter.extract_output_content
            name: extract_output_content
            args[2]: outputs,image_list
            docstring: "Extracts text and handles images by decoding Base64 to bytes.\nReturns: A list of text strings or placeholders."
            source_code: "def extract_output_content(outputs, image_list):\n    \"\"\"\n    Extracts text and handles images by decoding Base64 to bytes.\n    Returns: A list of text strings or placeholders.\n    \"\"\"\n    extracted_xml_snippets = []\n    \n    for output in outputs:\n\n        if output.output_type in ('display_data', 'execute_result') and hasattr(output, 'data'):\n            data = output.data\n            \n            # Helper to process image\n            def process_image(mime_type):\n                if mime_type in data:\n                    try:\n                        b64_str = data[mime_type]\n                        b64_str = b64_str.replace('\\n', '')\n\n                        image_index = len(image_list)\n                        image_list.append({\n                            \"mime_type\": mime_type,\n                            \"data\": b64_str\n                        })\n\n                        return f'\\n<IMAGE_PLACEHOLDER index=\"{image_index}\" mime=\"{mime_type}\"/>\\n'\n                    except Exception as e:\n                        return f\"<ERROR>Could not decode image: {e}</ERROR>\"\n                return None\n\n            # Ignore jpeg if png is found\n            img_xml = process_image('image/png')\n            if not img_xml:\n                img_xml = process_image('image/jpeg')\n            \n            if img_xml:\n                extracted_xml_snippets.append(img_xml)\n            elif 'text/plain' in data:\n                extracted_xml_snippets.append(data['text/plain'])\n\n        elif output.output_type == 'stream':\n            extracted_xml_snippets.append(output.text)\n            \n        elif output.output_type == 'error':\n            extracted_xml_snippets.append(f\"{output.ename}: {output.evalue}\")\n\n    return extracted_xml_snippets"
            start_line: 12
            end_line: 58
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.converter.process_image
            name: process_image
            args[1]: mime_type
            docstring: null
            source_code: "def process_image(mime_type):\n                if mime_type in data:\n                    try:\n                        b64_str = data[mime_type]\n                        b64_str = b64_str.replace('\\n', '')\n\n                        image_index = len(image_list)\n                        image_list.append({\n                            \"mime_type\": mime_type,\n                            \"data\": b64_str\n                        })\n\n                        return f'\\n<IMAGE_PLACEHOLDER index=\"{image_index}\" mime=\"{mime_type}\"/>\\n'\n                    except Exception as e:\n                        return f\"<ERROR>Could not decode image: {e}</ERROR>\"\n                return None"
            start_line: 25
            end_line: 40
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.converter.convert_notebook_to_xml
            name: convert_notebook_to_xml
            args[1]: file_content
            docstring: null
            source_code: "def convert_notebook_to_xml(file_content):\n    try:\n        nb = nbformat.reads(file_content, as_version=4)\n    except NotJSONError:\n        return \"<ERROR>Could not parse file as JSON/Notebook</ERROR>\", []\n\n    xml_parts = []\n    extracted_images = [] \n\n    for cell in nb.cells:\n        if cell.cell_type == 'markdown':\n            cell_xml = f'<CELL type=\"markdown\">\\n{cell.source}\\n</CELL>'\n            xml_parts.append(cell_xml)\n\n        elif cell.cell_type == 'code':\n            source_code = wrap_cdata(cell.source)\n            xml_parts.append(f'<CELL type=\"code\">\\n{source_code}\\n</CELL>')\n\n            if hasattr(cell, 'outputs') and cell.outputs:\n                snippets = extract_output_content(cell.outputs, extracted_images)\n                \n                content = \"\\n\".join(snippets)\n                \n                if content.strip():\n                    wrapped_output = wrap_cdata(content)\n                    xml_parts.append(f'<CELL type=\"output\">\\n{wrapped_output}\\n</CELL>')\n\n    return \"\\n\\n\".join(xml_parts), extracted_images"
            start_line: 60
            end_line: 87
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.converter.process_repo_notebooks
            name: process_repo_notebooks
            args[1]: repo_files
            docstring: null
            source_code: "def process_repo_notebooks(repo_files):\n    notebook_files = [f for f in repo_files if f.path.endswith('.ipynb')]\n    logging.info(f\"Found {len(notebook_files)} notebooks.\")\n        \n    results = {}\n\n    for nb_file in notebook_files:\n        logging.info(f\"Processing: {nb_file.path}\")\n\n        xml_output, images = convert_notebook_to_xml(nb_file.content)\n        results[nb_file.path] = {\n            \"xml\": xml_output,\n            \"images\": images\n        }\n            \n    return results"
            start_line: 90
            end_line: 105
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "backend/getRepo.py":
      ast_nodes:
        imports[5]: tempfile,git.Repo,git.GitCommandError,logging,os
        functions[0]:
        classes[2]:
          - mode: class_analysis
            identifier: backend.getRepo.RepoFile
            name: RepoFile
            docstring: "Repr√§sentiert eine einzelne Datei in einem Git-Repository.\n\nDer Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tats√§chlichem Zugriff."
            source_code: "class RepoFile:\n    \"\"\"\n    Repr√§sentiert eine einzelne Datei in einem Git-Repository.\n    \n    Der Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tats√§chlichem Zugriff.\n    \"\"\"\n    def __init__(self, file_path, commit_tree):\n        \"\"\"\n        Initialisiert das RepoFile-Objekt.\n\n        Args:\n            file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n            commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt.\n        \"\"\"\n        self.path = file_path\n        self._tree = commit_tree\n        \n\n        self._blob = None\n        self._content = None\n        self._size = None\n\n    @property\n    def blob(self):\n        \"\"\"Lazy-l√§dt das Git-Blob-Objekt.\"\"\"\n        if self._blob is None:\n            try:\n                self._blob = self._tree[self.path]\n            except KeyError:\n                raise FileNotFoundError(f\"Datei '{self.path}' konnte im Commit-Tree nicht gefunden werden.\")\n        return self._blob\n\n    @property\n    def content(self):\n        \"\"\"Lazy-l√§dt und gibt den dekodierten Inhalt der Datei zur√ºck.\"\"\"\n        if self._content is None:\n            self._content = self.blob.data_stream.read().decode('utf-8', errors='ignore')\n        return self._content\n\n    @property\n    def size(self):\n        \"\"\"Lazy-l√§dt und gibt die Gr√∂√üe der Datei in Bytes zur√ºck.\"\"\"\n        if self._size is None:\n            self._size = self.blob.size\n        return self._size\n        \n    def analyze_word_count(self):\n        \"\"\"\n        Eine Beispiel-Analyse-Methode. Z√§hlt die W√∂rter im Dateiinhalt.\n        \"\"\"\n        return len(self.content.split())\n\n    def __repr__(self):\n        \"\"\"Gibt eine n√ºtzliche String-Repr√§sentation des Objekts zur√ºck.\"\"\"\n        return f\"<RepoFile(path='{self.path}')>\"\n    \n    def to_dict(self, include_content=False):\n        data = {\n            \"path\": self.path,\n            \"name\": os.path.basename(self.path),\n            \"size\": self.size,\n            \"type\": \"file\"\n        }\n        if include_content:\n            data[\"content\"] = self.content\n        return data"
            start_line: 7
            end_line: 72
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[7]:
                - identifier: backend.getRepo.RepoFile.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[3]: self,file_path,commit_tree
                  docstring: "Initialisiert das RepoFile-Objekt.\n\nArgs:\n    file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n    commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt."
                  start_line: 13
                  end_line: 27
                - identifier: backend.getRepo.RepoFile.blob
                  name: blob
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt das Git-Blob-Objekt.
                  start_line: 30
                  end_line: 37
                - identifier: backend.getRepo.RepoFile.content
                  name: content
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt und gibt den dekodierten Inhalt der Datei zur√ºck.
                  start_line: 40
                  end_line: 44
                - identifier: backend.getRepo.RepoFile.size
                  name: size
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt und gibt die Gr√∂√üe der Datei in Bytes zur√ºck.
                  start_line: 47
                  end_line: 51
                - identifier: backend.getRepo.RepoFile.analyze_word_count
                  name: analyze_word_count
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Eine Beispiel-Analyse-Methode. Z√§hlt die W√∂rter im Dateiinhalt.
                  start_line: 53
                  end_line: 57
                - identifier: backend.getRepo.RepoFile.__repr__
                  name: __repr__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Gibt eine n√ºtzliche String-Repr√§sentation des Objekts zur√ºck.
                  start_line: 59
                  end_line: 61
                - identifier: backend.getRepo.RepoFile.to_dict
                  name: to_dict
                  calls[0]:
                  called_by[0]:
                  args[2]: self,include_content
                  docstring: null
                  start_line: 63
                  end_line: 72
          - mode: class_analysis
            identifier: backend.getRepo.GitRepository
            name: GitRepository
            docstring: "Verwaltet ein Git-Repository, einschlie√ülich Klonen in ein tempor√§res\nVerzeichnis und Bereitstellung von RepoFile-Objekten."
            source_code: "class GitRepository:\n    \"\"\"\n    Verwaltet ein Git-Repository, einschlie√ülich Klonen in ein tempor√§res\n    Verzeichnis und Bereitstellung von RepoFile-Objekten.\n    \"\"\"\n    def __init__(self, repo_url):\n        self.repo_url = repo_url\n        self.temp_dir = tempfile.mkdtemp()\n        self.repo = None\n\n        self.files = []\n        \n        try:\n            logging.info(f\"Clone Repository {self.repo_url}...\")\n            self.repo = Repo.clone_from(self.repo_url, self.temp_dir)\n            self.latest_commit = self.repo.head.commit\n            self.commit_tree = self.latest_commit.tree\n            logging.info(\"Cloning successful.\")\n        except GitCommandError as e:\n            self.close()\n            raise RuntimeError(f\"Error cloning repository: {e}\") from e\n\n    def get_all_files(self):\n        \"\"\"\n        Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zur√ºck.\n\n        Returns:\n            list[RepoFile]: Eine Liste von RepoFile-Instanzen.\n        \"\"\"\n        file_paths = self.repo.git.ls_files().split('\\n')\n        self.files = [RepoFile(path, self.commit_tree) for path in file_paths if path]\n        return self.files\n\n    def close(self):\n        \"\"\"L√∂scht das tempor√§re Verzeichnis und dessen Inhalt.\"\"\"\n        if self.temp_dir:\n            print(f\"\\nL√∂sche tempor√§res Verzeichnis: {self.temp_dir}\")\n            self.temp_dir = None\n            \n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def get_file_tree(self, include_content=False):\n\n        if not self.files:\n            self.get_all_files()\n\n        tree = {\"name\": \"root\", \"type\": \"directory\", \"children\": []}\n\n        for file_obj in self.files:\n            parts = file_obj.path.split('/')\n            current_level = tree[\"children\"]\n            \n            # Iteriere durch die Ordnerstruktur\n            for part in parts[:-1]:\n                # Suche, ob Ordner schon existiert\n                found = next((item for item in current_level if item[\"name\"] == part and item[\"type\"] == \"directory\"), None)\n                if not found:\n                    new_dir = {\"name\": part, \"type\": \"directory\", \"children\": []}\n                    current_level.append(new_dir)\n                    current_level = new_dir[\"children\"]\n                else:\n                    current_level = found[\"children\"]\n            \n            # Datei am Ende hinzuf√ºgen\n            current_level.append(file_obj.to_dict(include_content=include_content))\n\n        return tree"
            start_line: 78
            end_line: 148
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[6]:
                - identifier: backend.getRepo.GitRepository.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,repo_url
                  docstring: null
                  start_line: 83
                  end_line: 98
                - identifier: backend.getRepo.GitRepository.get_all_files
                  name: get_all_files
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: "Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zur√ºck.\n\nReturns:\n    list[RepoFile]: Eine Liste von RepoFile-Instanzen."
                  start_line: 100
                  end_line: 109
                - identifier: backend.getRepo.GitRepository.close
                  name: close
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: L√∂scht das tempor√§re Verzeichnis und dessen Inhalt.
                  start_line: 111
                  end_line: 115
                - identifier: backend.getRepo.GitRepository.__enter__
                  name: __enter__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 117
                  end_line: 118
                - identifier: backend.getRepo.GitRepository.__exit__
                  name: __exit__
                  calls[0]:
                  called_by[0]:
                  args[4]: self,exc_type,exc_val,exc_tb
                  docstring: null
                  start_line: 120
                  end_line: 121
                - identifier: backend.getRepo.GitRepository.get_file_tree
                  name: get_file_tree
                  calls[0]:
                  called_by[0]:
                  args[2]: self,include_content
                  docstring: null
                  start_line: 123
                  end_line: 148
    "backend/main.py":
      ast_nodes:
        imports[28]: json,math,logging,os,re,time,math,datetime.datetime,matplotlib.pyplot,datetime.datetime,pathlib.Path,dotenv.load_dotenv,getRepo.GitRepository,AST_Schema.ASTAnalyzer,MainLLM.MainLLM,basic_info.ProjektInfoExtractor,HelperLLM.LLMHelper,relationship_analyzer.ProjectAnalyzer,schemas.types.FunctionContextInput,schemas.types.FunctionAnalysisInput,schemas.types.ClassContextInput,schemas.types.ClassAnalysisInput,schemas.types.MethodContextInput,toon_format.encode,toon_format.count_tokens,toon_format.estimate_savings,toon_format.compare_formats,converter.process_repo_notebooks
        functions[7]:
          - mode: function_analysis
            identifier: backend.main.create_savings_chart
            name: create_savings_chart
            args[4]: json_tokens,toon_tokens,savings_percent,output_path
            docstring: Erstellt ein Balkendiagramm f√ºr den Token-Vergleich und speichert es.
            source_code: "def create_savings_chart(json_tokens, toon_tokens, savings_percent, output_path):\n    \"\"\"Erstellt ein Balkendiagramm f√ºr den Token-Vergleich und speichert es.\"\"\"\n    labels = ['JSON', 'TOON']\n    values = [json_tokens, toon_tokens]\n    colors = ['#ff9999', '#66b3ff']\n\n    plt.figure(figsize=(8, 6))\n    bars = plt.bar(labels, values, color=colors, width=0.5)\n\n    # Titel und Beschriftungen\n    plt.title(f'Token-Vergleich: {savings_percent:.2f}% Einsparung', fontsize=14)\n    plt.ylabel('Anzahl Token')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n\n    # Werte √ºber den Balken anzeigen\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width() / 2.0, height, \n                 f'{int(height):,}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n\n    # Speichern\n    plt.savefig(output_path)\n    plt.close()"
            start_line: 33
            end_line: 55
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.calculate_net_time
            name: calculate_net_time
            args[5]: start_time,end_time,total_items,batch_size,model_name
            docstring: Berechnet die Dauer abz√ºglich der Sleep-Zeiten f√ºr Rate-Limits.
            source_code: "def calculate_net_time(start_time, end_time, total_items, batch_size, model_name):\n    \"\"\"Berechnet die Dauer abz√ºglich der Sleep-Zeiten f√ºr Rate-Limits.\"\"\"\n    total_duration = end_time - start_time\n    \n    if not model_name.startswith(\"gemini-\"):\n        return total_duration\n\n    if total_items == 0:\n        return 0\n\n    num_batches = math.ceil(total_items / batch_size)\n    sleep_count = max(0, num_batches - 1)\n    total_sleep_time = sleep_count * 61\n    \n    net_time = total_duration - total_sleep_time\n    return max(0, net_time)"
            start_line: 57
            end_line: 72
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.main_workflow
            name: main_workflow
            args[4]: input,api_keys,model_names,status_callback
            docstring: null
            source_code: "def main_workflow(input, api_keys: dict, model_names: dict, status_callback=None):\n    \n    def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)\n\n    update_status(\"üîç Analysiere Input...\")\n    \n    user_input = input\n    \n    # API Key & Ollama Base URL aus Frontend holen\n    gemini_api_key = api_keys.get(\"gemini\")\n    openai_api_key = api_keys.get(\"gpt\")\n    scadsllm_api_key = api_keys.get(\"scadsllm\")\n    scadsllm_base_url = api_keys.get(\"scadsllm_base_url\")\n    ollama_base_url = api_keys.get(\"ollama\")\n    base_url = None\n\n    if model_names[\"helper\"].startswith(\"gpt-\"):\n        helper_api_key = openai_api_key\n    elif model_names[\"helper\"].startswith(\"gemini-\"):\n        helper_api_key = gemini_api_key\n    elif \"/\" in model_names[\"helper\"] or model_names[\"helper\"].startswith(\"alias-\") or any(x in model_names[\"helper\"] for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        helper_api_key = scadsllm_api_key\n        base_url = scadsllm_base_url\n    else:\n        helper_api_key = None\n        base_url = ollama_base_url\n    if model_names[\"main\"].startswith(\"gpt-\"):\n        main_api_key = openai_api_key\n    elif model_names[\"main\"].startswith(\"gemini-\"):\n        main_api_key = gemini_api_key\n    elif \"/\" in model_names[\"main\"] or model_names[\"main\"].startswith(\"alias-\") or any(x in model_names[\"main\"] for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        main_api_key = scadsllm_api_key\n        base_url = scadsllm_base_url\n    else:\n        main_api_key = None\n        base_url = ollama_base_url\n\n    # Standardeinstellungen f√ºr Modelle\n    helper_model = model_names.get(\"helper\", \"gpt-5-mini\")\n    main_model = model_names.get(\"main\", \"gpt-5.1\")\n\n    # Error Handling f√ºr fehlende API Keys\n    if not gemini_api_key and \"gemini\" in helper_model:\n        raise ValueError(\"Gemini API Key was not provided in api_keys dictionary.\")\n\n    # URL Extraktion\n    repo_url = None\n    url_pattern = r\"https?://(?:www\\.)?github\\.com/[^\\s]+\"\n    match = re.search(url_pattern, user_input)\n\n    if match:\n        repo_url = match.group(0)\n        logging.info(f\"Extracted repository URL: {repo_url}\")\n    else:\n        raise ValueError(\"Could not find a valid URL in the provided input.\")\n    \n    # Repo klonen und Dateien extrahieren\n    update_status(f\"‚¨áÔ∏è Klone Repository: {repo_url} ...\")\n    \n    repo_files = []\n    local_repo_path = \"\" \n\n    try: \n\n        with GitRepository(repo_url) as repo:\n            repo_files = repo.get_all_files()\n            local_repo_path = repo.temp_dir\n\n            logging.info(f\"Total files retrieved: {len(repo_files)}\")\n\n    except Exception as e:\n        logging.error(f\"Error cloning repository: {e}\")\n        raise \n\n\n    # Extrahiere Basic Infos\n    update_status(\"‚ÑπÔ∏è Extrahiere Basis-Informationen...\")\n    try:\n        info_extractor = ProjektInfoExtractor()\n        basic_project_info = info_extractor.extrahiere_info(dateien=repo_files, repo_url=repo_url)\n        logging.info(\"Basic project info extracted\")\n    except Exception as e:\n        logging.error(f\"Error extracting basic project info: {e}\")\n        basic_project_info = \"Could not extract basic info.\"\n\n    # Erstelle Repository Dateibaum\n    update_status(\"üå≤ Erstelle Repository Dateibaum...\")\n    try:\n        repo_file_tree = repo.get_file_tree()\n        logging.info(\"Repository file tree constructed\")\n    except Exception as e:\n        logging.error(f\"Error constructing repository file tree: {e}\")\n        repo_file_tree = \"Could not create file tree.\"\n\n    # Relationship Analyse durchf√ºhren\n    update_status(\"üîó Analysiere Beziehungen (Calls & Instanziierungen)...\")\n    try:\n        rel_analyzer = ProjectAnalyzer(project_root=local_repo_path)\n        rel_analyzer.analyze()\n        \n        raw_relationships = rel_analyzer.get_raw_relationships()\n        \n        logging.info(f\"Relationships analyzed.\")\n    except Exception as e:\n        logging.error(f\"Error in relationship analyzer: {e}\")\n        raw_relationships = {\"outgoing\": {}, \"incoming\": {}}\n\n    # Erstelle AST Schema\n    update_status(\"üå≥ Erstelle Abstract Syntax Tree (AST)...\")\n    try:        \n        ast_analyzer = ASTAnalyzer()   \n        ast_schema = ast_analyzer.analyze_repository(files=repo_files, repo=repo)\n        logging.info(\"AST schema created\")\n    except Exception as e:\n        logging.error(f\"Error retrieving repository files: {e}\")\n        raise\n\n    # Anreichern des AST Schemas mit Relationship Daten\n    update_status(\"‚ûï Reiche AST mit Beziehungsdaten an...\")            \n    try:   \n        ast_schema = ast_analyzer.merge_relationship_data(ast_schema, raw_relationships)\n        logging.info(\"AST schema created and enriched\")\n\n    except Exception as e:\n        logging.error(f\"Error processing repository: {e}\")\n        raise\n\n    # Vorbereitung der HelperLLM Eingaben\n    update_status(\"‚öôÔ∏è Bereite Daten f√ºr Helper LLM vor...\")\n    \n    helper_llm_function_input = []\n    helper_llm_class_input = []\n\n    try:\n        for filename, file_data in ast_schema['files'].items():\n            ast_nodes = file_data.get('ast_nodes', {})\n            imports = ast_nodes.get('imports', [])\n            functions = ast_nodes.get('functions', [])\n            classes = ast_nodes.get('classes', [])\n\n            for function in functions:\n                context = function.get('context', {})\n                \n                raw_called_by = context.get('called_by', [])\n                clean_called_by = [cb for cb in raw_called_by if isinstance(cb, dict)]\n\n                function_context = FunctionContextInput(\n                    calls = context.get('calls', []),\n                    called_by = clean_called_by \n                )\n                \n                function_input = FunctionAnalysisInput(\n                    mode = function.get('mode', 'function_analysis'),\n                    identifier = function.get('identifier'),\n                    source_code = function.get('source_code'),\n                    imports = imports,\n                    context = function_context\n                )\n                \n                helper_llm_function_input.append(function_input)\n\n            for _class in classes:\n                context = _class.get('context', {})\n                \n                method_context_inputs = []\n                for method in context.get('method_context', []):\n                    \n                    raw_method_called_by = method.get('called_by', [])\n                    clean_method_called_by = [cb for cb in raw_method_called_by if isinstance(cb, dict)]\n\n                    method_context_inputs.append(\n                        MethodContextInput(\n                            identifier=method.get('identifier'),\n                            calls=method.get('calls', []),\n                            called_by=clean_method_called_by, \n                            args=method.get('args', []),\n                            docstring=method.get('docstring')\n                        )\n                    )\n\n                raw_instantiated_by = context.get('instantiated_by', [])\n                clean_instantiated_by = [ib for ib in raw_instantiated_by if isinstance(ib, dict)]\n\n                class_context = ClassContextInput(\n                    dependencies = context.get('dependencies', []),\n                    instantiated_by = clean_instantiated_by, \n                    method_context = method_context_inputs\n                )\n\n                class_input = ClassAnalysisInput(\n                    mode = _class.get('mode', 'class_analysis'),\n                    identifier =_class.get('identifier'),\n                    source_code = _class.get('source_code'), \n                    imports = imports, \n                    context = class_context\n                )\n                \n                helper_llm_class_input.append(class_input)\n\n    except Exception as e:\n        logging.error(f\"Error preparing inputs for Helper LLM: {e}\")\n        raise\n    \n    # Initialisiere HelperLLM\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    \n    llm_helper = LLMHelper(\n        api_key=helper_api_key, \n        function_prompt_path=function_prompt_file, \n        class_prompt_path=class_prompt_file,\n        model_name=helper_model,\n        base_url=base_url,\n    )\n    \n    if ollama_base_url:\n        os.environ[\"OLLAMA_BASE_URL\"] = ollama_base_url\n\n    # Initialisiere Ergebniscontainer\n    analysis_results = {}\n    function_analysis_results = []\n    class_analysis_results = []\n\n    # Call HelperLLM f√ºr Funktionen\n    update_status(f\"ü§ñ Helper LLM: Analysiere {len(helper_llm_function_input)} Funktionen ({helper_model})...\")\n\n    try:\n        net_time_func = 0\n        if len(helper_llm_function_input) > 0:\n\n            logging.info(\"\\n--- Generating documentation for Functions ---\")\n            t_start_func = time.time()    \n            function_analysis_results = llm_helper.generate_for_functions(helper_llm_function_input)    \n            t_end_func = time.time()\n            net_time_func = calculate_net_time(t_start_func, t_end_func, len(helper_llm_function_input), llm_helper.batch_size, helper_model)\n\n        if len(function_analysis_results) != 0:\n            for doc in function_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"functions\" not in analysis_results:\n                        analysis_results[\"functions\"] = {}\n                    analysis_results[\"functions\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(f\"Failed to generate doc for a function\") \n    except Exception as e:\n        logging.error(f\"Error during Helper LLM function analysis: {e}\")\n        raise\n    \n\n    # Call HelperLLM f√ºr Klassen\n    try:\n        net_time_class = 0\n        if len(helper_llm_class_input) > 0:\n            # Rate Limit Sleep f√ºr Gemini Modelle\n            if llm_helper.model_name.startswith(\"gemini-\") & (len(helper_llm_function_input) > 0):\n                update_status(\"üí§ Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n                time.sleep(65)\n            \n            update_status(f\"ü§ñ Helper LLM: Analysiere {len(helper_llm_class_input)} Klassen ({helper_model})...\")\n            \n            logging.info(\"\\n--- Generating documentation for Classes ---\")\n            t_start_class = time.time()\n            class_analysis_results = llm_helper.generate_for_classes(helper_llm_class_input)\n            t_end_class = time.time()\n            net_time_class = calculate_net_time(t_start_class, t_end_class, len(helper_llm_class_input), llm_helper.batch_size, helper_model)\n\n        if len(class_analysis_results) != 0:\n            for doc in class_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"classes\" not in analysis_results:\n                        analysis_results[\"classes\"] = {}\n                    analysis_results[\"classes\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(f\"Failed to generate doc for a class\")\n    except Exception as e:\n        logging.error(f\"Error during Helper LLM class analysis: {e}\")\n        raise\n\n    total_helper_time = net_time_func + net_time_class\n\n    # MainLLM Input Vorbereitung\n    main_llm_input = {\n        \"basic_info\": basic_project_info,\n        \"file_tree\": repo_file_tree,\n        \"ast_schema\": ast_schema,\n        \"analysis_results\": analysis_results\n    }\n\n    # Speichern als JSON (Optional)\n    main_llm_input_json = json.dumps(main_llm_input, indent=2)\n    # with open(\"output.json\", \"w\", encoding=\"utf-8\") as f:\n    #     f.write(main_llm_input_json)\n    #     logging.info(\"JSON-Datei wurde gespeichert.\")\n\n    # Konvertiere Input in Toon Format\n    main_llm_input_toon = encode(main_llm_input)\n\n    #Speichern in TOON Format (Optional)\n    # with open(\"output.toon\", \"w\", encoding=\"utf-8\") as f:\n    #     f.write(main_llm_input_toon)\n    #     logging.info(\"output.toon erfolgreich gespeichert.\")\n    \n    # Token Evaluation\n    savings_data = None\n    try:\n        logging.info(\"--- Evaluierung der Token-Ersparnis ---\")\n        savings_data = estimate_savings(main_llm_input)\n        logging.info(f\"JSON Tokens: {savings_data['json_tokens']}\")\n        logging.info(f\"TOON Tokens: {savings_data['toon_tokens']}\")\n        logging.info(f\"Ersparnis:   {savings_data['savings_percent']:.2f}%\")\n        \n    except Exception as e:\n        logging.warning(f\"Token evaluation could not be performed: {e}\")    \n\n\n\n    prompt_file_mainllm = \"SystemPrompts/SystemPromptMainLLM.txt\"\n    prompt_file_mainllm_toon = \"SystemPrompts/SystemPromptMainLLMToon.txt\"\n    # MainLLM Ausf√ºhrung\n    main_llm = MainLLM(\n        api_key=main_api_key, \n        prompt_file_path=prompt_file_mainllm_toon,\n        model_name=main_model,\n        base_url=base_url,\n    )\n\n\n    # RPM Limit Sleep f√ºr Gemini Modelle\n    if llm_helper.model_name == main_llm.model_name and main_llm.model_name.startswith(\"gemini-\"):\n        time.sleep(65)\n        update_status(\"üí§ Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n\n    # Call MainLLM f√ºr finalen Report\n    update_status(f\"üß† Main LLM: Generiere finalen Report ({main_model})...\")\n    try:\n        total_main_time = 0\n        logging.info(\"\\n--- Generating Final Report ---\")\n        t_start_main = time.time()\n        final_report = main_llm.call_llm(main_llm_input_toon)\n        t_end_main = time.time()\n        total_main_time = t_end_main - t_start_main\n    except Exception as e:\n        logging.error(f\"Error during Main LLM final report generation: {e}\")\n        raise\n\n    \n    # --- Speichern der Ergebnisse ---\n    output_dir = \"result\"\n    stats_dir = \"Statistics\" # Neuer Ordner\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(stats_dir, exist_ok=True) # Stelle sicher, dass Statistics Ordner existiert\n\n    total_active_time = total_helper_time + total_main_time\n    \n    # Dateiname f√ºr Report\n    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M-%S\")\n    report_filename = f\"report_{timestamp}_Helper_{llm_helper.model_name}_MainLLM_{main_llm.model_name}.md\"\n    report_filepath = os.path.join(output_dir, report_filename)\n    \n    if final_report:\n        with open(report_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(final_report)\n        logging.info(f\"Final report saved to '{report_filepath}'.\")\n        \n        if savings_data:\n            try:\n                savings_filename = report_filename.replace(\"report_\", \"savings_\").replace(\".md\", \".png\")\n                savings_filepath = os.path.join(stats_dir, savings_filename)\n                \n                logging.info(f\"Erstelle Token-Chart: {savings_filepath}\")\n                create_savings_chart(\n                    json_tokens=savings_data['json_tokens'], \n                    toon_tokens=savings_data['toon_tokens'], \n                    savings_percent=savings_data['savings_percent'],\n                    output_path=savings_filepath\n                )\n            except Exception as chart_error:\n                logging.error(f\"Konnte Diagramm nicht erstellen: {chart_error}\")\n\n    else:\n        final_report = \"Error: Report generation failed or returned empty.\"\n\n    metrics = {\n        \"helper_time\": round(total_helper_time, 2),\n        \"main_time\": round(total_main_time, 2),\n        \"total_time\": round(total_active_time, 2),\n        \"helper_model\": helper_model,\n        \"main_model\": main_model,\n        \"json_tokens\": savings_data['json_tokens'] if savings_data else None,\n        \"toon_tokens\": savings_data['toon_tokens'] if savings_data else None,\n        \"savings_percent\": round(savings_data['savings_percent'], 2) if savings_data else None\n    \n    }\n\n    return {\n        \"report\": final_report,\n        \"metrics\": metrics\n    }"
            start_line: 75
            end_line: 477
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.update_status
            name: update_status
            args[1]: msg
            docstring: null
            source_code: "def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)"
            start_line: 77
            end_line: 80
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.notebook_workflow
            name: notebook_workflow
            args[4]: input,api_keys,model,status_callback
            docstring: null
            source_code: "def notebook_workflow(input, api_keys, model, status_callback=None):\n    t_start = time.time()\n    final_report = \"keine Notebooks gefunden\"\n    def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)\n\n    def gemini_payload(basic_info, nb_path, xml_content, images):\n        \n        intro_json = json.dumps({\n            \"basic_info\": basic_info,\n            \"current_notebook_path\": nb_path\n        }, indent=2)\n        \n        payload_content = []\n\n        payload_content.append({\n            \"type\": \"text\",\n            \"text\": f\"Context Information:\\n{intro_json}\\n\\nNotebook XML Structure:\\n\"\n        })\n\n        # Regex to find: <IMAGE_PLACEHOLDER index=\"0\" mime=\"image/png\"/>\n        pattern = r'(<IMAGE_PLACEHOLDER index=\"(\\d+)\" mime=\"([^\"]+)\"/>)'\n        last_pos = 0\n        \n        for match in re.finditer(pattern, xml_content):\n            text_segment = xml_content[last_pos:match.start()]\n            if text_segment.strip():\n                payload_content.append({\n                    \"type\": \"text\",\n                    \"text\": text_segment\n                })\n                \n            image_index = int(match.group(2))\n            mime_type = match.group(3)\n            \n            if image_index < len(images):\n                img_data = images[image_index]\n                b64_string = img_data['data']\n                \n                payload_content.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:{mime_type};base64,{b64_string}\"\n                    }\n                })\n                \n            last_pos = match.end()\n\n        # Add any remaining text after the last image\n        remaining_text = xml_content[last_pos:]\n        if remaining_text.strip():\n            payload_content.append({\n                \"type\": \"text\",\n                \"text\": remaining_text\n            })\n\n        return payload_content\n\n\n    base_url = None\n\n    if model.startswith(\"gpt-\"):\n        input_api_key = api_keys.get(\"gpt\")\n    elif model.startswith(\"gemini-\"):\n        input_api_key = api_keys.get(\"gemini\")\n    elif \"/\" in model or model.startswith(\"alias-\") or any(x in model for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        input_api_key = api_keys.get(\"scadsllm\")\n        base_url = api_keys.get(\"scadsllm_base_url\")\n    else:\n        input_api_key = None\n        base_url = api_keys.get(\"ollama\")\n\n    update_status(\"üîç Analysiere Input...\")\n\n    # URL Extraktion\n    repo_url = None\n    url_pattern = r\"https?://(?:www\\.)?github\\.com/[^\\s]+\"\n    match = re.search(url_pattern, input)\n\n    if match:\n        repo_url = match.group(0)\n        logging.info(f\"Extracted repository URL: {repo_url}\")\n    else:\n        raise ValueError(\"Could not find a valid URL in the provided input.\")\n    \n    update_status(f\"‚¨áÔ∏è Klone Repository: {repo_url} ...\")\n\n    try: \n        with GitRepository(repo_url) as repo:\n            repo_files = repo.get_all_files()\n    except Exception as e:\n        logging.error(f\"Error cloning repository: {e}\")\n        raise\n    \n    update_status(f\"üîó Bereite Input vor...\")\n\n    # convert to XML\n    processed_data = process_repo_notebooks(repo_files)\n\n    \n    # Extrahiere Basic Infos\n    update_status(\"‚ÑπÔ∏è Extrahiere Basis-Informationen...\")\n    try:\n        info_extractor = ProjektInfoExtractor()\n        basic_project_info = info_extractor.extrahiere_info(dateien=repo_files, repo_url=repo_url)\n        logging.info(\"Basic project info extracted\")\n    except Exception as e:\n        logging.error(f\"Error extracting basic project info: {e}\")\n        basic_project_info = \"Could not extract basic info.\"\n\n\n\n    prompt_file_notebook_llm = \"SystemPrompts/SystemPromptNotebookLLM.txt\"\n    notebook_llm = MainLLM(\n        api_key=input_api_key, \n        prompt_file_path=prompt_file_notebook_llm,\n        model_name=model,\n        base_url=base_url,\n    )\n\n    notebook_reports = []\n    total_notebooks = len(processed_data)\n    \n    logging.info(f\"Starting sequential processing of {total_notebooks} notebooks...\")\n\n    # Iterate over each notebook file individually\n    for index, (nb_path, nb_data) in enumerate(processed_data.items(), 1):\n        \n        update_status(f\"üß† Generiere Report ({index}/{total_notebooks}): {os.path.basename(nb_path)}\")\n        \n        nb_xml = nb_data['xml']\n        nb_images = nb_data['images']\n\n        llm_payload = gemini_payload(basic_project_info, nb_path, nb_xml, nb_images)\n\n        try:\n            single_report = notebook_llm.call_llm(llm_payload)\n            if single_report and isinstance(single_report, str):\n                notebook_reports.append(single_report)\n            else:\n                logging.warning(f\"LLM lieferte kein Ergebnis f√ºr {nb_path}\")\n                notebook_reports.append(f\"## Analyse f√ºr {os.path.basename(nb_path)}\\nFehler: Das Modell lieferte keine Antwort.\")\n            \n        except Exception as e:\n            logging.error(f\"Error generating report for {nb_path}: {e}\")\n            notebook_reports.append(f\"# Error processing {nb_path}\\n\\nError: {str(e)}\\n\\n---\\n\")\n\n    # Concatenate all reports\n    final_report = \"\\n\\n<br>\\n<br>\\n<br>\\n\\n\".join([str(r) for r in notebook_reports if r is not None])\n\n    \n    # --- Speichern der Ergebnisse ---\n    output_dir = \"result\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Dateiname f√ºr Report\n    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M-%S\")\n    report_filename = f\"notebook_report_{timestamp}_NotebookLLM_{notebook_llm.model_name}.md\"\n    report_filepath = os.path.join(output_dir, report_filename)\n    \n    if final_report:\n        with open(report_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(final_report)\n        logging.info(f\"Final report saved to '{report_filepath}'.\")\n\n    # Am Ende der Funktion nach dem Speichern:\n    total_time = time.time() - t_start\n    \n    # Metriken f√ºr das Frontend bauen\n    metrics = {\n        \"helper_time\": \"0\", # Notebook-Workflow hat meist keinen separaten Helper\n        \"main_time\": round(total_time, 2),\n        \"total_time\": round(total_time, 2),\n        \"helper_model\": \"None\",\n        \"main_model\": model,\n        \"json_tokens\": 0,\n        \"toon_tokens\": 0,\n        \"savings_percent\": 0\n    }\n\n    return {\n        \"report\": final_report,\n        \"metrics\": metrics\n    }"
            start_line: 483
            end_line: 668
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.update_status
            name: update_status
            args[1]: msg
            docstring: null
            source_code: "def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)"
            start_line: 486
            end_line: 489
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.gemini_payload
            name: gemini_payload
            args[4]: basic_info,nb_path,xml_content,images
            docstring: null
            source_code: "def gemini_payload(basic_info, nb_path, xml_content, images):\n        \n        intro_json = json.dumps({\n            \"basic_info\": basic_info,\n            \"current_notebook_path\": nb_path\n        }, indent=2)\n        \n        payload_content = []\n\n        payload_content.append({\n            \"type\": \"text\",\n            \"text\": f\"Context Information:\\n{intro_json}\\n\\nNotebook XML Structure:\\n\"\n        })\n\n        # Regex to find: <IMAGE_PLACEHOLDER index=\"0\" mime=\"image/png\"/>\n        pattern = r'(<IMAGE_PLACEHOLDER index=\"(\\d+)\" mime=\"([^\"]+)\"/>)'\n        last_pos = 0\n        \n        for match in re.finditer(pattern, xml_content):\n            text_segment = xml_content[last_pos:match.start()]\n            if text_segment.strip():\n                payload_content.append({\n                    \"type\": \"text\",\n                    \"text\": text_segment\n                })\n                \n            image_index = int(match.group(2))\n            mime_type = match.group(3)\n            \n            if image_index < len(images):\n                img_data = images[image_index]\n                b64_string = img_data['data']\n                \n                payload_content.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:{mime_type};base64,{b64_string}\"\n                    }\n                })\n                \n            last_pos = match.end()\n\n        # Add any remaining text after the last image\n        remaining_text = xml_content[last_pos:]\n        if remaining_text.strip():\n            payload_content.append({\n                \"type\": \"text\",\n                \"text\": remaining_text\n            })\n\n        return payload_content"
            start_line: 491
            end_line: 541
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "backend/relationship_analyzer.py":
      ast_nodes:
        imports[4]: ast,os,logging,collections.defaultdict
        functions[1]:
          - mode: function_analysis
            identifier: backend.relationship_analyzer.path_to_module
            name: path_to_module
            args[2]: filepath,project_root
            docstring: Wandelt einen Dateipfad in einen Python-Modulpfad um.
            source_code: "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    module_path = rel_path.replace(os.path.sep, '.')\n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n    return module_path"
            start_line: 6
            end_line: 18
            context:
              calls[0]:
              called_by[0]:
        classes[2]:
          - mode: class_analysis
            identifier: backend.relationship_analyzer.ProjectAnalyzer
            name: ProjectAnalyzer
            docstring: null
            source_code: "class ProjectAnalyzer:\n    \n    def __init__(self, project_root):\n        self.project_root = os.path.abspath(project_root)\n        self.definitions = {}\n        self.call_graph = defaultdict(list)\n        self.file_asts = {} \n        self.ignore_dirs = {'.git', '.venv', 'venv', '__pycache__', 'node_modules', 'dist', 'build', 'docs'}\n\n    def analyze(self):\n        py_files = self._find_py_files()\n        \n        for filepath in py_files:\n            self._collect_definitions(filepath)\n            \n        for filepath in py_files:\n            self._resolve_calls(filepath)\n            \n        self.file_asts.clear()\n        \n        return self.call_graph\n\n    def get_raw_relationships(self):\n\n        outgoing = defaultdict(set)\n        incoming = defaultdict(set)\n\n        for callee_id, callers_info_list in self.call_graph.items():\n            for info in callers_info_list:\n                caller_id = info['caller']\n                \n                if caller_id and callee_id:\n                    outgoing[caller_id].add(callee_id)\n                    incoming[callee_id].add(caller_id)\n        \n        return {\n            \"outgoing\": {k: sorted(list(v)) for k, v in outgoing.items()},\n            \"incoming\": {k: sorted(list(v)) for k, v in incoming.items()}\n        }\n\n    def _find_py_files(self):\n        py_files = []\n        for root, dirs, files in os.walk(self.project_root):\n            dirs[:] = [d for d in dirs if d not in self.ignore_dirs]\n            for file in files:\n                if file.endswith(\".py\"):\n                    py_files.append(os.path.join(root, file))\n        return py_files\n\n    def _collect_definitions(self, filepath):\n        try:\n            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n                source = f.read()\n                tree = ast.parse(source, filename=filepath)\n                \n            self.file_asts[filepath] = tree\n            module_path = path_to_module(filepath, self.project_root)\n            \n            for node in ast.walk(tree):\n                if isinstance(node, ast.FunctionDef):\n                    parent = self._get_parent(tree, node)\n                    if isinstance(parent, ast.ClassDef):\n                        path_name = f\"{module_path}.{parent.name}.{node.name}\"\n                        def_type = 'method'\n                    else:\n                        path_name = f\"{module_path}.{node.name}\"\n                        def_type = 'function'\n                    self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': def_type}\n                elif isinstance(node, ast.ClassDef):\n                    path_name = f\"{module_path}.{node.name}\"\n                    self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': 'class'}\n        except Exception as e:\n            logging.error(f\"Error collecting definitions in {filepath}: {e}\")\n            self.file_asts[filepath] = None\n\n    def _get_parent(self, tree, node):\n        for parent in ast.walk(tree):\n            for child in ast.iter_child_nodes(parent):\n                if child is node:\n                    return parent\n        return None\n\n    def _resolve_calls(self, filepath):\n        tree = self.file_asts.get(filepath)\n        if not tree:\n            return\n\n        try:\n            resolver = CallResolverVisitor(filepath, self.project_root, self.definitions)\n            resolver.visit(tree)\n            \n            for callee_pathname, caller_info_list in resolver.calls.items():\n                self.call_graph[callee_pathname].extend(caller_info_list)\n        except Exception as e:\n            logging.error(f\"Error resolving calls in {filepath}: {e}\")"
            start_line: 20
            end_line: 114
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[7]:
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,project_root
                  docstring: null
                  start_line: 22
                  end_line: 27
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.analyze
                  name: analyze
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 29
                  end_line: 40
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.get_raw_relationships
                  name: get_raw_relationships
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 42
                  end_line: 58
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._find_py_files
                  name: _find_py_files
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 60
                  end_line: 67
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._collect_definitions
                  name: _collect_definitions
                  calls[0]:
                  called_by[0]:
                  args[2]: self,filepath
                  docstring: null
                  start_line: 69
                  end_line: 93
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._get_parent
                  name: _get_parent
                  calls[0]:
                  called_by[0]:
                  args[3]: self,tree,node
                  docstring: null
                  start_line: 95
                  end_line: 100
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._resolve_calls
                  name: _resolve_calls
                  calls[0]:
                  called_by[0]:
                  args[2]: self,filepath
                  docstring: null
                  start_line: 102
                  end_line: 114
          - mode: class_analysis
            identifier: backend.relationship_analyzer.CallResolverVisitor
            name: CallResolverVisitor
            docstring: null
            source_code: "class CallResolverVisitor(ast.NodeVisitor):\n    def __init__(self, filepath, project_root, definitions):\n        self.filepath = filepath\n        self.module_path = path_to_module(filepath, project_root)\n        self.definitions = definitions\n        self.scope = {}\n        self.instance_types = {}\n        self.current_caller_name = self.module_path\n        self.current_class_name = None\n        self.calls = defaultdict(list)\n\n    def visit_ClassDef(self, node):\n        old_class_name = self.current_class_name\n        self.current_class_name = node.name\n        self.generic_visit(node)\n        self.current_class_name = old_class_name\n\n    def visit_FunctionDef(self, node):\n        old_caller = self.current_caller_name\n        \n        if self.current_class_name:\n            full_identifier = f\"{self.module_path}.{self.current_class_name}.{node.name}\"\n        else:\n            full_identifier = f\"{self.module_path}.{node.name}\"\n            \n        self.current_caller_name = full_identifier\n        self.generic_visit(node)\n        self.current_caller_name = old_caller\n\n    def visit_Call(self, node):\n        callee_pathname = self._resolve_call_qname(node.func)\n        if callee_pathname and callee_pathname in self.definitions:\n            \n            if self.current_caller_name == self.module_path:\n                caller_type = 'module'\n            elif '.<locals>.' in self.current_caller_name:\n                caller_type = 'local_function'\n            elif self.current_class_name:\n                caller_type = 'method'\n            else:\n                caller_type = 'function'\n            \n            caller_info = {\n                'file': os.path.basename(self.filepath),\n                'line': node.lineno,\n                'caller': self.current_caller_name,\n                'caller_type': caller_type \n            }\n            self.calls[callee_pathname].append(caller_info)\n        self.generic_visit(node)\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.scope[alias.asname or alias.name] = alias.name\n        self.generic_visit(node)\n        \n    def visit_ImportFrom(self, node):\n        module = node.module or ''\n        for alias in node.names:\n            name = alias.asname or alias.name\n            if node.level > 0:\n                base = self.module_path.split('.')\n                prefix = '.'.join(base[:-node.level])\n                full_module_path = f\"{prefix}.{module}\" if module else prefix\n            else:\n                full_module_path = module\n            self.scope[name] = f\"{full_module_path}.{alias.name}\"\n        self.generic_visit(node)\n\n    def visit_Assign(self, node):\n        if isinstance(node.value, ast.Call) and isinstance(node.value.func, ast.Name):\n            class_name = node.value.func.id\n            if class_name in self.scope:\n                qualified_class_name = self.scope[class_name]\n                if qualified_class_name in self.definitions:\n                    for target in node.targets:\n                        if isinstance(target, ast.Name):\n                            self.instance_types[target.id] = qualified_class_name\n        self.generic_visit(node)\n\n    def _resolve_call_qname(self, func_node):\n        if isinstance(func_node, ast.Name):\n            name = func_node.id\n            if name in self.scope:\n                return self.scope[name]\n            local_pathname = f\"{self.module_path}.{name}\"\n            if local_pathname in self.definitions:\n                return local_pathname\n        elif isinstance(func_node, ast.Attribute) and isinstance(func_node.value, ast.Name):\n            var_name = func_node.value.id\n            method_name = func_node.attr\n            if var_name in self.instance_types:\n                class_pathname = self.instance_types[var_name]\n                return f\"{class_pathname}.{method_name}\"\n            if var_name in self.scope:\n                module_pathname = self.scope[var_name]\n                return f\"{module_pathname}.{method_name}\"\n        return None"
            start_line: 117
            end_line: 214
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[8]:
                - identifier: backend.relationship_analyzer.CallResolverVisitor.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[4]: self,filepath,project_root,definitions
                  docstring: null
                  start_line: 118
                  end_line: 126
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 128
                  end_line: 132
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 134
                  end_line: 144
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Call
                  name: visit_Call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 146
                  end_line: 166
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 168
                  end_line: 171
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 173
                  end_line: 184
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Assign
                  name: visit_Assign
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 186
                  end_line: 195
                - identifier: backend.relationship_analyzer.CallResolverVisitor._resolve_call_qname
                  name: _resolve_call_qname
                  calls[0]:
                  called_by[0]:
                  args[2]: self,func_node
                  docstring: null
                  start_line: 197
                  end_line: 214
    "backend/scads_key_test.py":
      ast_nodes:
        imports[3]: os,dotenv.load_dotenv,openai.OpenAI
        functions[0]:
        classes[0]:
    "database/db.py":
      ast_nodes:
        imports[7]: datetime.datetime,pymongo.MongoClient,dotenv.load_dotenv,streamlit_authenticator,cryptography.fernet.Fernet,uuid,os
        functions[29]:
          - mode: function_analysis
            identifier: database.db.encrypt_text
            name: encrypt_text
            args[1]: text
            docstring: null
            source_code: "def encrypt_text(text: str) -> str:\n    if not text or not cipher_suite: return text\n    return cipher_suite.encrypt(text.strip().encode()).decode()"
            start_line: 28
            end_line: 30
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.decrypt_text
            name: decrypt_text
            args[1]: text
            docstring: null
            source_code: "def decrypt_text(text: str) -> str:\n    if not text or not cipher_suite: return text\n    try:\n        return cipher_suite.decrypt(text.strip().encode()).decode()\n    except Exception:\n        return text"
            start_line: 32
            end_line: 37
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.insert_user
            name: insert_user
            args[3]: username,name,password
            docstring: null
            source_code: "def insert_user(username: str, name: str, password: str):\n    user = {\n        \"_id\": username,\n        \"name\": name, \n        \"hashed_password\": stauth.Hasher.hash(password),\n        \"gemini_api_key\": \"\",\n        \"ollama_base_url\": \"\",\n        \"gpt_api_key\": \"\"\n    }\n    result = dbusers.insert_one(user)\n    return result.inserted_id"
            start_line: 43
            end_line: 53
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_all_users
            name: fetch_all_users
            args[0]:
            docstring: null
            source_code: "def fetch_all_users():\n    return list(dbusers.find())"
            start_line: 55
            end_line: 56
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_user
            name: fetch_user
            args[1]: username
            docstring: null
            source_code: "def fetch_user(username: str):\n    return dbusers.find_one({\"_id\": username})"
            start_line: 58
            end_line: 59
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_user_name
            name: update_user_name
            args[2]: username,new_name
            docstring: null
            source_code: "def update_user_name(username: str, new_name: str):\n    # Achtung: _id kann in Mongo nicht einfach so ge√§ndert werden. \n    # Hier wird nur das Name-Feld geupdated.\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"name\": new_name}})\n    return result.modified_count"
            start_line: 61
            end_line: 65
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_gemini_key
            name: update_gemini_key
            args[2]: username,gemini_api_key
            docstring: null
            source_code: "def update_gemini_key(username: str, gemini_api_key: str):\n    encrypted_key = encrypt_text(gemini_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"gemini_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 67
            end_line: 70
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_gpt_key
            name: update_gpt_key
            args[2]: username,gpt_api_key
            docstring: null
            source_code: "def update_gpt_key(username: str, gpt_api_key: str):\n    encrypted_key = encrypt_text(gpt_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"gpt_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 72
            end_line: 75
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_ollama_url
            name: update_ollama_url
            args[2]: username,ollama_base_url
            docstring: null
            source_code: "def update_ollama_url(username: str, ollama_base_url: str):\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"ollama_base_url\": ollama_base_url.strip()}})\n    return result.modified_count"
            start_line: 77
            end_line: 79
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_opensrc_key
            name: update_opensrc_key
            args[2]: username,opensrc_api_key
            docstring: null
            source_code: "def update_opensrc_key(username: str, opensrc_api_key: str):\n    encrypted_key = encrypt_text(opensrc_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"opensrc_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 81
            end_line: 84
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_opensrc_url
            name: update_opensrc_url
            args[2]: username,opensrc_base_url
            docstring: null
            source_code: "def update_opensrc_url(username: str, opensrc_base_url: str):\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"opensrc_base_url\": opensrc_base_url.strip()}})\n    return result.modified_count"
            start_line: 86
            end_line: 88
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_gemini_key
            name: fetch_gemini_key
            args[1]: username
            docstring: null
            source_code: "def fetch_gemini_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"gemini_api_key\": 1, \"_id\": 0})\n    return user.get(\"gemini_api_key\") if user else None"
            start_line: 90
            end_line: 92
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_ollama_url
            name: fetch_ollama_url
            args[1]: username
            docstring: null
            source_code: "def fetch_ollama_url(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"ollama_base_url\": 1, \"_id\": 0})\n    return user.get(\"ollama_base_url\") if user else None"
            start_line: 94
            end_line: 96
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_gpt_key
            name: fetch_gpt_key
            args[1]: username
            docstring: null
            source_code: "def fetch_gpt_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"gpt_api_key\": 1, \"_id\": 0})\n    return user.get(\"gpt_api_key\") if user else None"
            start_line: 98
            end_line: 100
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_opensrc_key
            name: fetch_opensrc_key
            args[1]: username
            docstring: null
            source_code: "def fetch_opensrc_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"opensrc_api_key\": 1, \"_id\": 0})\n    return user.get(\"opensrc_api_key\") if user else None"
            start_line: 102
            end_line: 104
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_opensrc_url
            name: fetch_opensrc_url
            args[1]: username
            docstring: null
            source_code: "def fetch_opensrc_url(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"opensrc_base_url\": 1, \"_id\": 0})\n    return user.get(\"opensrc_base_url\") if user else None"
            start_line: 106
            end_line: 108
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.delete_user
            name: delete_user
            args[1]: username
            docstring: null
            source_code: "def delete_user(username: str):\n    return dbusers.delete_one({\"_id\": username}).deleted_count"
            start_line: 110
            end_line: 111
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.get_decrypted_api_keys
            name: get_decrypted_api_keys
            args[1]: username
            docstring: null
            source_code: "def get_decrypted_api_keys(username: str):\n    user = dbusers.find_one({\"_id\": username})\n    if not user: return None, None\n    gemini_plain = decrypt_text(user.get(\"gemini_api_key\", \"\"))\n    ollama_plain = user.get(\"ollama_base_url\", \"\")\n    gpt_plain = decrypt_text(user.get(\"gpt_api_key\", \"\"))\n    opensrc_plain = decrypt_text(user.get(\"opensrc_api_key\", \"\"))\n    opensrc_url = user.get(\"opensrc_base_url\", \"\")\n    return gemini_plain, ollama_plain, gpt_plain, opensrc_plain, opensrc_url"
            start_line: 113
            end_line: 121
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.insert_chat
            name: insert_chat
            args[2]: username,chat_name
            docstring: Erstellt einen neuen Chat-Eintrag.
            source_code: "def insert_chat(username: str, chat_name: str):\n    \"\"\"Erstellt einen neuen Chat-Eintrag.\"\"\"\n    chat = {\n        \"_id\": str(uuid.uuid4()),\n        \"username\": username,\n        \"chat_name\": chat_name,\n        \"created_at\": datetime.now()\n    }\n    result = dbchats.insert_one(chat)\n    return result.inserted_id"
            start_line: 127
            end_line: 136
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_chats_by_user
            name: fetch_chats_by_user
            args[1]: username
            docstring: Holt alle definierten Chats eines Users.
            source_code: "def fetch_chats_by_user(username: str):\n    \"\"\"Holt alle definierten Chats eines Users.\"\"\"\n    # Sortieren nach Erstellung macht Sinn\n    chats = list(dbchats.find({\"username\": username}).sort(\"created_at\", 1))\n    return chats"
            start_line: 138
            end_line: 142
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.check_chat_exists
            name: check_chat_exists
            args[2]: username,chat_name
            docstring: null
            source_code: "def check_chat_exists(username: str, chat_name: str):\n    return dbchats.find_one({\"username\": username, \"chat_name\": chat_name}) is not None"
            start_line: 144
            end_line: 145
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.rename_chat_fully
            name: rename_chat_fully
            args[3]: username,old_name,new_name
            docstring: Benennt einen Chat und alle zugeh√∂rigen Exchanges um.
            source_code: "def rename_chat_fully(username: str, old_name: str, new_name: str):\n    \"\"\"\n    Benennt einen Chat und alle zugeh√∂rigen Exchanges um.\n    \"\"\"\n    # 1. Chat-Eintrag umbenennen\n    result = dbchats.update_one(\n        {\"username\": username, \"chat_name\": old_name}, \n        {\"$set\": {\"chat_name\": new_name}}\n    )\n    \n    # 2. Alle Messages (Exchanges) umh√§ngen\n    dbexchanges.update_many(\n        {\"username\": username, \"chat_name\": old_name},\n        {\"$set\": {\"chat_name\": new_name}}\n    )\n    return result.modified_count"
            start_line: 148
            end_line: 163
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.insert_exchange
            name: insert_exchange
            args[13]: question,answer,feedback,username,chat_name,helper_used,main_used,total_time,helper_time,main_time,json_tokens,toon_tokens,savings_percent
            docstring: null
            source_code: "def insert_exchange(question: str, answer: str, feedback: str, username: str, chat_name: str, \n                    helper_used: str=\"\", main_used: str=\"\", total_time: str=\"\", helper_time: str=\"\", main_time: str=\"\",\n                    json_tokens=0, toon_tokens=0, savings_percent=0.0):\n    new_id = str(uuid.uuid4())\n    exchange = {\n        \"_id\": new_id,\n        \"question\": question,\n        \"answer\": answer,\n        \"feedback\": feedback,\n        \"feedback_message\": \"\",\n        \"chat_name\": chat_name,\n        \"username\": username,\n        \"helper_used\": helper_used,\n        \"main_used\": main_used,\n        \"total_time\": total_time,\n        \"helper_time\": helper_time,\n        \"main_time\": main_time,\n        \"json_tokens\": json_tokens,\n        \"toon_tokens\": toon_tokens,\n        \"savings_percent\": savings_percent,\n        \"created_at\": datetime.now()\n    }\n    try:\n        dbexchanges.insert_one(exchange)\n        return new_id\n    except Exception as e:\n        print(f\"DB Error: {e}\")\n        return None"
            start_line: 169
            end_line: 196
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_exchanges_by_user
            name: fetch_exchanges_by_user
            args[1]: username
            docstring: null
            source_code: "def fetch_exchanges_by_user(username: str):\n    # Sortieren nach Zeitstempel wichtig f√ºr die Anzeige\n    exchanges = list(dbexchanges.find({\"username\": username}).sort(\"created_at\", 1))\n    return exchanges"
            start_line: 198
            end_line: 201
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_exchanges_by_chat
            name: fetch_exchanges_by_chat
            args[2]: username,chat_name
            docstring: null
            source_code: "def fetch_exchanges_by_chat(username: str, chat_name: str):\n    exchanges = list(dbexchanges.find({\"username\": username, \"chat_name\": chat_name}).sort(\"created_at\", 1))\n    return exchanges"
            start_line: 203
            end_line: 205
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_exchange_feedback
            name: update_exchange_feedback
            args[2]: exchange_id,feedback
            docstring: null
            source_code: "def update_exchange_feedback(exchange_id, feedback: int):\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback\": feedback}})\n    return result.modified_count"
            start_line: 207
            end_line: 209
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_exchange_feedback_message
            name: update_exchange_feedback_message
            args[2]: exchange_id,feedback_message
            docstring: null
            source_code: "def update_exchange_feedback_message(exchange_id, feedback_message: str):\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback_message\": feedback_message}})\n    return result.modified_count"
            start_line: 211
            end_line: 213
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.delete_exchange_by_id
            name: delete_exchange_by_id
            args[1]: exchange_id
            docstring: null
            source_code: "def delete_exchange_by_id(exchange_id: str):\n    result = dbexchanges.delete_one({\"_id\": exchange_id})\n    return result.deleted_count"
            start_line: 215
            end_line: 217
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.delete_full_chat
            name: delete_full_chat
            args[2]: username,chat_name
            docstring: "L√∂scht den Chat UND alle zugeh√∂rigen Exchanges.\nDas sorgt f√ºr Konsistenz zwischen Frontend und Backend."
            source_code: "def delete_full_chat(username: str, chat_name: str):\n    \"\"\"\n    L√∂scht den Chat UND alle zugeh√∂rigen Exchanges.\n    Das sorgt f√ºr Konsistenz zwischen Frontend und Backend.\n    \"\"\"\n    # 1. Alle Nachrichten in diesem Chat l√∂schen\n    del_exchanges = dbexchanges.delete_many({\"username\": username, \"chat_name\": chat_name})\n    \n    # 2. Den Chat selbst aus der Chat-Liste l√∂schen\n    del_chat = dbchats.delete_one({\"username\": username, \"chat_name\": chat_name})\n    \n    return del_chat.deleted_count"
            start_line: 221
            end_line: 232
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "frontend/frontend.py":
      ast_nodes:
        imports[16]: numpy,datetime.datetime,time,pymongo.MongoClient,dotenv.load_dotenv,os,sys,urllib.parse.urlparse,logging,traceback,re,streamlit_mermaid.st_mermaid,backend.main,database.db,streamlit,streamlit_authenticator
        functions[12]:
          - mode: function_analysis
            identifier: frontend.frontend.clean_names
            name: clean_names
            args[1]: model_list
            docstring: null
            source_code: "def clean_names(model_list):\n    return [m.split(\"/\")[-1] for m in model_list]"
            start_line: 54
            end_line: 55
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.get_filtered_models
            name: get_filtered_models
            args[2]: source_list,category_name
            docstring: Filtert eine Liste basierend auf der gew√§hlten Kategorie.
            source_code: "def get_filtered_models(source_list, category_name):\n    \"\"\"Filtert eine Liste basierend auf der gew√§hlten Kategorie.\"\"\"\n    keywords = CATEGORY_KEYWORDS.get(category_name, [\"\"])\n    \n    if \"STANDARD\" in keywords:\n        # Nur Modelle zur√ºckgeben, die auch in der Standard-Liste sind\n        return [m for m in source_list if m in STANDARD_MODELS]\n    \n    filtered = []\n    for model in source_list:\n        # Pr√ºfen ob ein Keyword im Namen steckt\n        if any(k in model.lower() for k in keywords):\n            filtered.append(model)\n            \n    return filtered if filtered else source_list"
            start_line: 86
            end_line: 100
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.save_gemini_cb
            name: save_gemini_cb
            args[0]:
            docstring: null
            source_code: "def save_gemini_cb():\n    new_key = st.session_state.get(\"in_gemini_key\", \"\")\n    if new_key:\n        db.update_gemini_key(st.session_state[\"username\"], new_key)\n        st.session_state[\"in_gemini_key\"] = \"\"\n        st.toast(\"Gemini Key erfolgreich gespeichert! ‚úÖ\")"
            start_line: 143
            end_line: 148
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.save_ollama_cb
            name: save_ollama_cb
            args[0]:
            docstring: null
            source_code: "def save_ollama_cb():\n    new_url = st.session_state.get(\"in_ollama_url\", \"\")\n    if new_url:\n        db.update_ollama_url(st.session_state[\"username\"], new_url)\n        st.toast(\"Ollama URL gespeichert! ‚úÖ\")"
            start_line: 150
            end_line: 154
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.load_data_from_db
            name: load_data_from_db
            args[1]: username
            docstring: L√§dt Chats und Exchanges konsistent aus der DB.
            source_code: "def load_data_from_db(username: str):\n    \"\"\"L√§dt Chats und Exchanges konsistent aus der DB.\"\"\"\n    \n    # Nur laden, wenn neuer User oder noch nicht geladen\n    if \"loaded_user\" not in st.session_state or st.session_state.loaded_user != username:\n        st.session_state.chats = {}\n        \n        # 1. Erst die definierten Chats laden (damit auch leere Chats da sind)\n        db_chats = db.fetch_chats_by_user(username)\n        for c in db_chats:\n            c_name = c.get(\"chat_name\")\n            if c_name:\n                st.session_state.chats[c_name] = {\"exchanges\": []}\n\n        # 2. Dann die Exchanges laden und einsortieren\n        db_exchanges = db.fetch_exchanges_by_user(username)\n        for ex in db_exchanges:\n            c_name = ex.get(\"chat_name\", \"Unbenannt\")\n            \n            # Falls Exchanges existieren f√ºr Chats, die nicht in dbchats sind (Legacy Support)\n            if c_name not in st.session_state.chats:\n                st.session_state.chats[c_name] = {\"exchanges\": []}\n            \n            if \"feedback\" not in ex or ex[\"feedback\"] is None:\n                ex[\"feedback\"] = np.nan\n            \n            st.session_state.chats[c_name][\"exchanges\"].append(ex)\n\n        # 3. Default Chat erstellen, falls gar nichts existiert\n        if not st.session_state.chats:\n            initial_name = \"Chat 1\"\n            # Konsistent in DB anlegen\n            db.insert_chat(username, initial_name)\n            st.session_state.chats[initial_name] = {\"exchanges\": []}\n            st.session_state.active_chat = initial_name\n        else:\n            # Active Chat setzen, falls n√∂tig\n            if \"active_chat\" not in st.session_state or st.session_state.active_chat not in st.session_state.chats:\n                # Nimm den ersten verf√ºgbaren Chat\n                st.session_state.active_chat = list(st.session_state.chats.keys())[0]\n        \n        st.session_state.loaded_user = username"
            start_line: 160
            end_line: 201
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.handle_feedback_change
            name: handle_feedback_change
            args[2]: ex,val
            docstring: null
            source_code: "def handle_feedback_change(ex, val):\n    ex[\"feedback\"] = val\n    db.update_exchange_feedback(ex[\"_id\"], val)\n    st.rerun()"
            start_line: 207
            end_line: 210
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.handle_delete_exchange
            name: handle_delete_exchange
            args[2]: chat_name,ex
            docstring: null
            source_code: "def handle_delete_exchange(chat_name, ex):\n    db.delete_exchange_by_id(ex[\"_id\"])\n    if chat_name in st.session_state.chats:\n        if ex in st.session_state.chats[chat_name][\"exchanges\"]:\n            st.session_state.chats[chat_name][\"exchanges\"].remove(ex)\n    st.rerun()"
            start_line: 212
            end_line: 217
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.handle_delete_chat
            name: handle_delete_chat
            args[2]: username,chat_name
            docstring: null
            source_code: "def handle_delete_chat(username, chat_name):\n    # KONSISTENZ: Ruft jetzt delete_full_chat in DB auf\n    db.delete_full_chat(username, chat_name)\n    \n    # State bereinigen\n    if chat_name in st.session_state.chats:\n        del st.session_state.chats[chat_name]\n    \n    # Active Chat neu setzen\n    if len(st.session_state.chats) > 0:\n        st.session_state.active_chat = list(st.session_state.chats.keys())[0]\n    else:\n        # Wenn alles weg ist, neuen leeren Chat anlegen\n        new_name = \"Chat 1\"\n        db.insert_chat(username, new_name)\n        st.session_state.chats[new_name] = {\"exchanges\": []}\n        st.session_state.active_chat = new_name\n        \n    st.rerun()"
            start_line: 219
            end_line: 237
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.extract_repo_name
            name: extract_repo_name
            args[1]: text
            docstring: null
            source_code: "def extract_repo_name(text):\n    url_match = re.search(r'(https?://[^\\s]+)', text)\n    if url_match:\n        url = url_match.group(0)\n        parsed = urlparse(url)\n        path = parsed.path.strip(\"/\")\n        if path:\n            repo_name = path.split(\"/\")[-1]\n            if repo_name.endswith(\".git\"):\n                repo_name = repo_name[:-4]\n            return repo_name\n    return None"
            start_line: 243
            end_line: 254
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.stream_text_generator
            name: stream_text_generator
            args[1]: text
            docstring: null
            source_code: "def stream_text_generator(text):\n    for word in text.split(\" \"):\n        yield word + \" \"\n        time.sleep(0.01)"
            start_line: 256
            end_line: 259
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.render_text_with_mermaid
            name: render_text_with_mermaid
            args[2]: markdown_text,should_stream
            docstring: null
            source_code: "def render_text_with_mermaid(markdown_text, should_stream=False):\n    if not markdown_text:\n        return\n\n    parts = re.split(r\"```mermaid\\s+(.*?)\\s+```\", markdown_text, flags=re.DOTALL)\n\n    for i, part in enumerate(parts):\n        if i % 2 == 0:\n            if part.strip():\n                if should_stream:\n                    st.write_stream(stream_text_generator(part))\n                else:\n                    st.markdown(part)\n        else:\n            try:\n                st_mermaid(part, key=f\"mermaid_{hash(part)}_{i}\")\n            except Exception:\n                st.code(part, language=\"mermaid\")"
            start_line: 261
            end_line: 278
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.render_exchange
            name: render_exchange
            args[2]: ex,current_chat_name
            docstring: null
            source_code: "def render_exchange(ex, current_chat_name):\n    st.chat_message(\"user\").write(ex[\"question\"])\n\n    with st.chat_message(\"assistant\"):\n        answer_text = ex.get(\"answer\", \"\")\n        is_error = answer_text.startswith(\"Fehler\") or answer_text.startswith(\"Error\")\n\n        # --- 1. TOOLBAR CONTAINER ---\n        # Nutzt die neuen Parameter horizontal=True und alignment\n        # border=True rahmt die Toolbar ein\n        # horizontal_alignment=\"right\" schiebt alles nach rechts\n        with st.container(horizontal=True, horizontal_alignment=\"right\"):\n            \n            if not is_error:\n                # 1. Status Text (wird jetzt links von den Buttons angezeigt, aber rechtsb√ºndig im Container)\n                st.write(\"hier die Antwort:\")\n                if ex.get(\"feedback\") == 1:\n                    st.caption(\"‚úÖ Hilfreich\")\n                elif ex.get(\"feedback\") == 0:\n                    st.caption(\"‚ùå Nicht hilfreich\")\n                \n                # 2. Die Buttons (einfach untereinander im Code, erscheinen nebeneinander im UI)\n                \n                # Like\n                type_primary_up = ex.get(\"feedback\") == 1\n                if st.button(\"üëç\", key=f\"up_{ex['_id']}\", type=\"primary\" if type_primary_up else \"secondary\", help=\"Hilfreich\"):\n                    handle_feedback_change(ex, 1)\n\n                # Dislike\n                type_primary_down = ex.get(\"feedback\") == 0\n                if st.button(\"üëé\", key=f\"down_{ex['_id']}\", type=\"primary\" if type_primary_down else \"secondary\", help=\"Nicht hilfreich\"):\n                    handle_feedback_change(ex, 0)\n\n                # Comment Popover\n                with st.popover(\"üí¨\", help=\"Notiz hinzuf√ºgen\"):\n                    msg = st.text_area(\"Notiz:\", value=ex.get(\"feedback_message\", \"\"), key=f\"txt_{ex['_id']}\")\n                    if st.button(\"Speichern\", key=f\"save_{ex['_id']}\"):\n                        ex[\"feedback_message\"] = msg\n                        db.update_exchange_feedback_message(ex[\"_id\"], msg)\n                        st.success(\"Gespeichert!\")\n                        time.sleep(0.5)\n                        st.rerun()\n\n                # Download\n                st.download_button(\n                    label=\"üì•\",\n                    data=ex[\"answer\"],\n                    file_name=f\"response_{ex['_id']}.md\",\n                    mime=\"text/markdown\",\n                    key=f\"dl_{ex['_id']}\",\n                    help=\"Als Markdown herunterladen\"\n                )\n\n                # Delete\n                if st.button(\"üóëÔ∏è\", key=f\"del_{ex['_id']}\", help=\"Nachricht l√∂schen\", type=\"secondary\"):\n                    handle_delete_exchange(current_chat_name, ex)\n\n            else:\n                # Fehlerfall\n                st.error(\"‚ö†Ô∏è Fehler\")\n                if st.button(\"üóëÔ∏è L√∂schen\", key=f\"del_err_{ex['_id']}\"):\n                    handle_delete_exchange(current_chat_name, ex)\n\n        # --- 2. CONTENT (Antwort) ---\n        with st.container(height=500, border=True):\n             render_text_with_mermaid(ex[\"answer\"], should_stream=False)"
            start_line: 284
            end_line: 349
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "schemas/types.py":
      ast_nodes:
        imports[5]: typing.List,typing.Optional,typing.Literal,pydantic.BaseModel,pydantic.ValidationError
        functions[0]:
        classes[15]:
          - mode: class_analysis
            identifier: schemas.types.ParameterDescription
            name: ParameterDescription
            docstring: Describes a single parameter of a function.
            source_code: "class ParameterDescription(BaseModel):\n    \"\"\"Describes a single parameter of a function.\"\"\"\n    name: str\n    type: str\n    description: str"
            start_line: 6
            end_line: 10
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ReturnDescription
            name: ReturnDescription
            docstring: Describes the return value of a function.
            source_code: "class ReturnDescription(BaseModel):\n    \"\"\"Describes the return value of a function.\"\"\"\n    name: str\n    type: str\n    description: str"
            start_line: 12
            end_line: 16
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.UsageContext
            name: UsageContext
            docstring: Describes the calling context of a function.
            source_code: "class UsageContext(BaseModel):\n    \"\"\"Describes the calling context of a function.\"\"\"\n    calls: str\n    called_by: str"
            start_line: 18
            end_line: 21
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionDescription
            name: FunctionDescription
            docstring: Contains the detailed analysis of a function's purpose and signature.
            source_code: "class FunctionDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a function's purpose and signature.\"\"\"\n    overall: str\n    parameters: List[ParameterDescription]\n    returns: List[ReturnDescription]\n    usage_context: UsageContext"
            start_line: 23
            end_line: 28
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionAnalysis
            name: FunctionAnalysis
            docstring: The main model representing the entire JSON schema for a function.
            source_code: "class FunctionAnalysis(BaseModel):\n    \"\"\"The main model representing the entire JSON schema for a function.\"\"\"\n    identifier: str\n    description: FunctionDescription\n    error: Optional[str] = None"
            start_line: 30
            end_line: 34
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ConstructorDescription
            name: ConstructorDescription
            docstring: Describes the __init__ method of a class.
            source_code: "class ConstructorDescription(BaseModel):\n    \"\"\"Describes the __init__ method of a class.\"\"\"\n    description: str\n    parameters: List[ParameterDescription]"
            start_line: 39
            end_line: 42
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassContext
            name: ClassContext
            docstring: Describes the class's external dependencies and primary points of instantiation.
            source_code: "class ClassContext(BaseModel):\n    \"\"\"Describes the class's external dependencies and primary points of instantiation.\"\"\"\n    dependencies: str\n    instantiated_by: str"
            start_line: 44
            end_line: 47
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassDescription
            name: ClassDescription
            docstring: "Contains the detailed analysis of a class's purpose, constructor, and methods."
            source_code: "class ClassDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a class's purpose, constructor, and methods.\"\"\"\n    overall: str\n    init_method: ConstructorDescription\n    methods: List[FunctionAnalysis]\n    usage_context: ClassContext"
            start_line: 49
            end_line: 54
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassAnalysis
            name: ClassAnalysis
            docstring: The main model for the entire JSON schema for a class.
            source_code: "class ClassAnalysis(BaseModel):\n    \"\"\"The main model for the entire JSON schema for a class.\"\"\"\n    identifier: str\n    description: ClassDescription\n    error: Optional[str] = None"
            start_line: 56
            end_line: 60
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.CallInfo
            name: CallInfo
            docstring: "Represents a specific call event from the relationship analyzer.\nUsed in 'called_by' and 'instantiated_by' lists."
            source_code: "class CallInfo(BaseModel):\n    \"\"\"\n    Represents a specific call event from the relationship analyzer.\n    Used in 'called_by' and 'instantiated_by' lists.\n    \"\"\"\n    file: str\n    function: str  # Name des Aufrufers\n    mode: str      # z.B. 'method', 'function', 'module'\n    line: int"
            start_line: 65
            end_line: 73
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionContextInput
            name: FunctionContextInput
            docstring: Structured context for analyzing a function.
            source_code: "class FunctionContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a function.\"\"\"\n    calls: List[str]\n    called_by: List[CallInfo]"
            start_line: 78
            end_line: 81
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionAnalysisInput
            name: FunctionAnalysisInput
            docstring: The required input to generate a FunctionAnalysis object.
            source_code: "class FunctionAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a FunctionAnalysis object.\"\"\"\n    mode: Literal[\"function_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: FunctionContextInput"
            start_line: 83
            end_line: 89
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.MethodContextInput
            name: MethodContextInput
            docstring: Structured context for a classes methods
            source_code: "class MethodContextInput(BaseModel):\n    \"\"\"Structured context for a classes methods\"\"\"\n    identifier: str\n    calls: List[str]\n    called_by: List[CallInfo]\n    args: List[str]\n    docstring: Optional[str]"
            start_line: 94
            end_line: 100
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassContextInput
            name: ClassContextInput
            docstring: Structured context for analyzing a class.
            source_code: "class ClassContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a class.\"\"\"\n    dependencies: List[str]\n    instantiated_by: List[CallInfo]\n    method_context: List[MethodContextInput]"
            start_line: 102
            end_line: 106
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassAnalysisInput
            name: ClassAnalysisInput
            docstring: The required input to generate a ClassAnalysis object.
            source_code: "class ClassAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a ClassAnalysis object.\"\"\"\n    mode: Literal[\"class_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: ClassContextInput"
            start_line: 108
            end_line: 114
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
analysis_results:
  functions:
    backend.AST_Schema.path_to_module:
      identifier: backend.AST_Schema.path_to_module
      description:
        overall: "This function converts a given file system path into a Python module path string. It first attempts to determine the relative path from a specified project root, falling back to the basename if a ValueError occurs. It then removes the '.py' extension if present, replaces system path separators with dots, and finally removes the '.__init__' suffix if the resulting module path represents an initialization file. The function returns the formatted module path."
        parameters[2]{name,type,description}:
          filepath,str,The absolute or relative path to the Python file to be converted.
          project_root,str,"The root directory of the project, used as a base to calculate the relative module path."
        returns[1]{name,type,description}:
          module_path,str,"The converted Python module path string, e.g., 'my_package.my_module'."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.File_Dependency.build_file_dependency_graph:
      identifier: backend.File_Dependency.build_file_dependency_graph
      description:
        overall: "This function constructs a directed graph representing file-level import dependencies within a given Python file's Abstract Syntax Tree (AST). It initializes a NetworkX directed graph and then uses a `FileDependencyGraph` visitor to traverse the AST, collecting import relationships. Finally, it populates the graph with nodes for callers and callees, adding edges to represent these dependencies, and returns the completed graph."
        parameters[3]{name,type,description}:
          filename,str,The path to the Python file being analyzed for dependencies.
          tree,AST,The Abstract Syntax Tree (AST) of the Python file.
          repo_root,str,"The root directory of the repository, used for resolving relative paths."
        returns[1]{name,type,description}:
          graph,nx.DiGraph,A NetworkX directed graph where nodes represent files and edges represent import dependencies.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.File_Dependency.build_repository_graph:
      identifier: backend.File_Dependency.build_repository_graph
      description:
        overall: "This function constructs a directed graph representing the dependencies between Python files within a given Git repository. It iterates through all Python files in the repository, parses each file's content into an Abstract Syntax Tree (AST), and then uses a helper function, `build_file_dependency_graph`, to generate a dependency graph for that individual file. The nodes and edges from each file's graph are then aggregated into a single, comprehensive global directed graph, which is ultimately returned."
        parameters[1]{name,type,description}:
          repository,GitRepository,The GitRepository object representing the repository whose Python file dependencies are to be analyzed.
        returns[1]{name,type,description}:
          global_graph,nx.DiGraph,A NetworkX directed graph where nodes represent Python files (or components within them) and edges represent dependencies between them across the entire repository.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.File_Dependency.get_all_temp_files:
      identifier: backend.File_Dependency.get_all_temp_files
      description:
        overall: "This function `get_all_temp_files` is designed to locate and list all Python files within a specified directory and its subdirectories. It takes a string representing a directory path as input. The function first converts the input directory string into an absolute `pathlib.Path` object. It then recursively searches for all files ending with \".py\" within this root path. Finally, it returns a list of these Python files, with each file's path expressed relative to the initial root directory."
        parameters[1]{name,type,description}:
          directory,str,The string path to the root directory from which to start searching for Python files.
        returns[1]{name,type,description}:
          all_files,"list[Path]","A list of `pathlib.Path` objects, where each path represents a Python file found within the specified directory, relative to that directory."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.HelperLLM.main_orchestrator:
      identifier: backend.HelperLLM.main_orchestrator
      description:
        overall: "The `main_orchestrator` function serves as a testing and orchestration loop for the `LLMHelper` class. It initializes predefined `FunctionAnalysisInput`, `FunctionAnalysis`, and `ClassAnalysisInput` objects to simulate data. An `LLMHelper` instance is created, which then processes these inputs to generate documentation for the functions. The generated documentation is aggregated into a `final_documentation` dictionary and subsequently printed."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    backend.callgraph.make_safe_dot:
      identifier: backend.callgraph.make_safe_dot
      description:
        overall: "This function takes a NetworkX directed graph and an output file path, then generates a 'safe' DOT file representation of the graph. It achieves this by creating a copy of the input graph and relabeling its nodes with simple, sequential identifiers (e.g., \"n0\", \"n1\"). The original node names are preserved by storing them as 'label' attributes for the newly named nodes. Finally, the modified graph, with its safe node names and original labels, is written to the specified output path as a DOT file."
        parameters[2]{name,type,description}:
          graph,nx.DiGraph,The NetworkX directed graph to be processed and written to a DOT file.
          out_path,str,The file path where the safe DOT representation of the graph will be saved.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.callgraph.build_filtered_callgraph:
      identifier: backend.callgraph.build_filtered_callgraph
      description:
        overall: "This function `build_filtered_callgraph` is responsible for constructing a directed call graph for Python files within a given Git repository. It operates by first identifying all user-defined functions across the repository's Python files, parsing each file into an Abstract Syntax Tree (AST) to extract these definitions. Subsequently, it iterates through these parsed files again to detect call relationships between functions. The function then builds a `networkx.DiGraph`, adding an edge only if both the calling and called functions are part of the previously identified set of user-defined functions, effectively filtering out external library calls. The resulting graph represents the internal call structure of the repository's own codebase."
        parameters[1]{name,type,description}:
          repo,GitRepository,The Git repository object from which to extract file information and build the call graph.
        returns[1]{name,type,description}:
          global_graph,nx.DiGraph,A directed graph representing the call relationships between user-defined functions within the repository.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    backend.converter.wrap_cdata:
      identifier: backend.converter.wrap_cdata
      description:
        overall: "This function takes a string `content` as input and encloses it within XML CDATA tags. It constructs a new string that begins with \"<![CDATA[\\n\", followed by the provided content, and ends with \"\\n]]>\". The primary purpose is to escape content for XML, ensuring it is parsed as character data rather than markup. It returns the resulting CDATA-wrapped string."
        parameters[1]{name,type,description}:
          content,str,The string content to be wrapped within CDATA tags.
        returns[1]{name,type,description}:
          wrapped_content,str,"A string containing the original content enclosed within CDATA tags, including leading and trailing newlines within the CDATA block."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.extract_output_content:
      identifier: backend.converter.extract_output_content
      description:
        overall: "This function processes a list of execution outputs, typically from a notebook, to extract relevant content. It iterates through each output, identifying its type to determine how to handle its data. For image data (PNG or JPEG), it decodes the Base64 string, stores it in a provided `image_list`, and generates an XML-like placeholder string. For text-based outputs, it extracts the plain text or formats error messages. The function aggregates these processed contents into a list of strings, which it then returns."
        parameters[2]{name,type,description}:
          outputs,list,"A list of output objects, likely from a notebook execution, each containing data or text based on its output type."
          image_list,list,A list passed by reference to store extracted image data. Each image is stored as a dictionary containing its mime type and base64 encoded data.
        returns[1]{name,type,description}:
          extracted_xml_snippets,"list[str]","A list of strings, where each string is either plain text, an error message, or an XML-like placeholder for an image."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.process_image:
      identifier: backend.converter.process_image
      description:
        overall: "This function `process_image` is designed to handle base64 encoded image data based on a provided MIME type. It checks for the presence of the `mime_type` within an external `data` object. If found, it retrieves the corresponding base64 string, removes newline characters, and appends a dictionary containing the MIME type and processed data to an external `image_list`. The function then returns a formatted placeholder string. If an error occurs during processing, an error message string is returned. If the `mime_type` is not found in `data`, the function returns `None`."
        parameters[1]{name,type,description}:
          mime_type,str,The MIME type of the image to be processed. This type is used as a key to access the image's base64 data from an external `data` object.
        returns[3]{name,type,description}:
          image_placeholder_string,str,"A formatted string containing an image placeholder with its index and MIME type, indicating successful processing and storage in `image_list`."
          error_message_string,str,An error message string if an exception occurs during the decoding or processing of the image data.
          None,None,Returns None if the provided `mime_type` is not found as a key in the external `data` object.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.convert_notebook_to_xml:
      identifier: backend.converter.convert_notebook_to_xml
      description:
        overall: "This function converts the content of a Jupyter notebook, provided as a string, into a custom XML format. It first attempts to parse the input string as a Jupyter notebook; if this fails due to a JSON parsing error, it returns a specific error message. The function then iterates through each cell, converting markdown cells and code cells (including their outputs) into corresponding XML tags. It also handles the extraction of images from code cell outputs. Finally, it returns the concatenated XML string and a list of any extracted images."
        parameters[1]{name,type,description}:
          file_content,str,The string content of a Jupyter notebook file to be converted.
        returns[2]{name,type,description}:
          xml_output,str,"A string containing the XML representation of the notebook, or an error message if parsing fails."
          extracted_images,list,"A list of extracted image data (e.g., base64 strings) from the notebook outputs."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.process_repo_notebooks:
      identifier: backend.converter.process_repo_notebooks
      description:
        overall: "This function processes a collection of repository files to identify and convert Jupyter notebooks. It filters the input list to find files with a '.ipynb' extension. For each identified notebook, it extracts its content and uses an external utility, 'convert_notebook_to_xml', to generate an XML representation and associated image data. The function then aggregates these results into a dictionary, mapping each notebook's path to its converted XML and images."
        parameters[1]{name,type,description}:
          repo_files,"List[Any]","An iterable collection of file objects, where each object is expected to have a 'path' attribute (string) and a 'content' attribute (representing the notebook's raw data)."
        returns[1]{name,type,description}:
          results,"Dict[str, Dict[str, Any]]",A dictionary where keys are the string paths of the processed notebooks. Each value is another dictionary containing 'xml' (the XML string output from conversion) and 'images' (a list of image data extracted from the notebook).
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.create_savings_chart:
      identifier: backend.main.create_savings_chart
      description:
        overall: "This function generates a bar chart comparing JSON and TOON token counts, displaying a savings percentage in the title. It visualizes the token values above the bars, adds appropriate labels and a grid, and then saves the generated chart to a specified file path before closing the plot. The chart uses distinct colors for JSON and TOON bars to enhance readability."
        parameters[4]{name,type,description}:
          json_tokens,int,The number of tokens for the JSON format.
          toon_tokens,int,The number of tokens for the TOON format.
          savings_percent,float,The percentage of token savings to display in the chart title.
          output_path,str,The file path where the generated chart image will be saved.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.calculate_net_time:
      identifier: backend.main.calculate_net_time
      description:
        overall: "This function calculates the net processing time by subtracting estimated sleep durations from the total elapsed time. It specifically applies this sleep time adjustment only if the `model_name` starts with \"gemini-\". The sleep time is determined by the number of batches processed, with each batch incurring an estimated 61-second sleep. If the model is not a 'gemini-' model, the total duration is returned directly. If no items are processed, the net time is 0."
        parameters[5]{name,type,description}:
          start_time,float,The starting timestamp or duration component from which to calculate total duration.
          end_time,float,The ending timestamp or duration component used with `start_time` to calculate total duration.
          total_items,int,"The total number of items processed, used to determine the number of batches."
          batch_size,int,"The number of items processed per batch, used to calculate the total number of batches."
          model_name,str,"The name of the model; if it starts with 'gemini-', sleep time adjustments are applied."
        returns[1]{name,type,description}:
          net_time,float,"The calculated net processing time, adjusted for estimated sleep durations if the model is 'gemini-', or the total duration otherwise. Returns 0 if `total_items` is 0 or if the calculated net time is negative."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.main_workflow:
      identifier: backend.main.main_workflow
      description:
        overall: "This function orchestrates a comprehensive workflow for analyzing a GitHub repository. It begins by extracting API keys and model configurations, then proceeds to clone the specified repository. The workflow includes extracting basic project information, constructing a file tree, performing relationship analysis between code components, and building an Abstract Syntax Tree (AST) schema. This AST schema is then enriched with the extracted relationship data. Finally, it prepares and dispatches tasks to a Helper LLM for detailed function and class analysis, aggregates these results, and uses a Main LLM to generate a final report, which is then saved along with performance metrics."
        parameters[4]{name,type,description}:
          input,str,"The initial user input, which is expected to contain a GitHub repository URL for analysis."
          api_keys,dict,"A dictionary containing various API keys (e.g., 'gemini', 'gpt', 'scadsllm') and base URLs required for LLM interactions."
          model_names,dict,A dictionary specifying the names of the 'helper' and 'main' LLM models to be utilized in the workflow.
          status_callback,Callable,"An optional callback function used to provide status updates during the workflow execution, typically for UI feedback. Defaults to None."
        returns[2]{name,type,description}:
          report,str,The final comprehensive report generated by the Main LLM based on the repository analysis.
          metrics,dict,"A dictionary containing performance metrics, including execution times for helper and main LLMs, total active time, model names used, and token savings data if available."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.update_status:
      identifier: backend.main.update_status
      description:
        overall: "This function, `update_status`, is designed to handle and log status messages. It takes a message string as input. The function first checks if a `status_callback` function is available; if so, it invokes this callback with the provided message. Regardless of the callback's presence, it then logs the message at the INFO level using the `logging` module."
        parameters[1]{name,type,description}:
          msg,str,The status message to be processed and logged.
        returns[1]{name,type,description}:
          None,None,This function does not explicitly return any value; it performs side effects by calling a callback and logging.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    backend.main.notebook_workflow:
      identifier: backend.main.notebook_workflow
      description:
        overall: "This function orchestrates a comprehensive workflow for analyzing GitHub repository notebooks using a Language Model (LLM). It begins by extracting a GitHub repository URL from the input, cloning the repository, and then processing its notebooks. It extracts basic project information and, for each notebook, constructs a detailed payload including XML structure and embedded images. This payload is then sent to a configured LLM (based on the specified model and API keys) to generate individual reports. Finally, all generated reports are concatenated, saved to a markdown file, and execution metrics are returned."
        parameters[4]{name,type,description}:
          input,str,"The input string, expected to contain a GitHub repository URL for analysis."
          api_keys,dict,"A dictionary containing various API keys required for different LLM providers (e.g., 'gpt', 'gemini', 'scadsllm', 'ollama')."
          model,str,"The name of the LLM model to be used for processing the notebooks (e.g., 'gpt-4', 'gemini-pro')."
          status_callback,callable,An optional callback function that receives status messages during the workflow execution for real-time updates.
        returns[2]{name,type,description}:
          report,str,The final concatenated markdown report generated from the analysis of all notebooks.
          metrics,dict,"A dictionary containing performance metrics of the workflow execution, including total time and model information."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.gemini_payload:
      identifier: backend.main.gemini_payload
      description:
        overall: "This function constructs a multimodal content payload, designed for a Gemini-style AI model. It takes basic project information, a notebook path, XML content, and a list of images as input. The function first serializes the basic information and notebook path into a JSON string. It then iteratively processes the provided XML content, identifying and replacing `<IMAGE_PLACEHOLDER>` tags with corresponding base64 encoded image data from the `images` list. The final output is a structured list of dictionaries, alternating between text segments from the XML and image URLs, suitable for a multimodal API."
        parameters[4]{name,type,description}:
          basic_info,dict,"A dictionary or object containing basic information about the project or context, which will be serialized into JSON."
          nb_path,str,"The file path of the current notebook, which will be included in the context JSON."
          xml_content,str,"The XML content of the notebook, which may contain `<IMAGE_PLACEHOLDER>` tags to be replaced with actual image data."
          images,"list[dict]","A list of image data objects. Each object is expected to contain a 'data' key with a base64 encoded image string, corresponding to image placeholders in the XML content."
        returns[1]{name,type,description}:
          payload_content,"list[dict]","A list of dictionaries, where each dictionary represents a content block for a multimodal AI payload. Blocks can be of type 'text' or 'image_url', containing the processed text and image data."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.relationship_analyzer.path_to_module:
      identifier: backend.relationship_analyzer.path_to_module
      description:
        overall: "This function converts a given file path into a Python module path string. It calculates the relative path of the file with respect to a specified project root, handling cases where `os.path.relpath` might raise a `ValueError` by falling back to the base filename. The function then removes the '.py' extension if present and replaces directory separators with dots. Finally, it specifically handles and removes '.__init__' suffixes to correctly represent package modules."
        parameters[2]{name,type,description}:
          filepath,str,The absolute or relative path to the Python file.
          project_root,str,"The root directory of the project, used to determine the relative path of the file."
        returns[1]{name,type,description}:
          module_path,str,The converted Python module path string.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.encrypt_text:
      identifier: database.db.encrypt_text
      description:
        overall: "This function `encrypt_text` is designed to encrypt a given string using a `cipher_suite` object. It first checks if the input `text` is empty or if the `cipher_suite` is not initialized; in such cases, it returns the original text without encryption. Otherwise, it prepares the text by stripping leading/trailing whitespace and encoding it to bytes before passing it to the `cipher_suite` for encryption. Finally, the encrypted bytes are decoded back into a string and returned."
        parameters[1]{name,type,description}:
          text,str,"The string value to be encrypted. If empty or `cipher_suite` is not available, the original text is returned."
        returns[1]{name,type,description}:
          encrypted_text_or_original,str,"The encrypted version of the input string, or the original string if the input was empty or `cipher_suite` was not initialized."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.decrypt_text:
      identifier: database.db.decrypt_text
      description:
        overall: "This function attempts to decrypt a given string using an external `cipher_suite` object. It first checks if the input `text` or `cipher_suite` is empty, returning the original text if either condition is true. If decryption is attempted, the text is stripped of whitespace, encoded to bytes, decrypted, and then decoded back into a string. In case of any error during the decryption process, the original, unencrypted text is returned."
        parameters[1]{name,type,description}:
          text,str,The string to be decrypted.
        returns[1]{name,type,description}:
          decrypted_text,str,"The decrypted string if successful, or the original string if decryption fails or conditions are not met."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.insert_user:
      identifier: database.db.insert_user
      description:
        overall: "This function is responsible for inserting a new user record into the database. It takes a username, name, and password as input. It constructs a user dictionary, hashes the provided password using `stauth.Hasher.hash`, and initializes API key fields as empty strings. Finally, it inserts this user document into the `dbusers` collection."
        parameters[3]{name,type,description}:
          username,str,"The unique identifier for the user, which will be stored as the `_id` in the database."
          name,str,The full name of the user.
          password,str,"The plain-text password for the user, which will be hashed before storage."
        returns[1]{name,type,description}:
          inserted_id,Any,The `_id` of the newly inserted user document in the database.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.fetch_all_users:
      identifier: database.db.fetch_all_users
      description:
        overall: "The `fetch_all_users` function is responsible for retrieving all user records from a database collection. It executes a `find()` operation on the `dbusers` object, which is presumed to be a database collection or cursor. The results obtained from this operation are then converted into a standard Python list, which is subsequently returned by the function."
        parameters[0]:
        returns[1]{name,type,description}:
          users,list,A list of user documents (dictionaries) retrieved from the database.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_user:
      identifier: database.db.fetch_user
      description:
        overall: This function is designed to retrieve a single user record from a database collection. It accepts a username as input and uses it to query the `dbusers` collection. The function specifically searches for a document where the `_id` field matches the provided username. It then returns the first matching document found or `None` if no such user exists.
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user to be fetched from the database.
        returns[1]{name,type,description}:
          user_document,dict | None,"A dictionary representing the user's document if found, otherwise `None`."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_user_name:
      identifier: database.db.update_user_name
      description:
        overall: "This function is responsible for updating a user's name in a database. It takes the current username (which serves as the document's `_id`) and a new name as input. The function then performs an `update_one` operation on the `dbusers` collection, targeting the document with the matching `_id` and setting its 'name' field to the `new_name`. It returns the count of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          username,str,The unique identifier of the user whose name is to be updated. This is used as the `_id` field in the database.
          new_name,str,The new name to assign to the specified user.
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents that were modified by the update operation. A value of 1 indicates success if the user was found and updated, 0 otherwise."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_gemini_key:
      identifier: database.db.update_gemini_key
      description:
        overall: "This function is responsible for updating a user's Gemini API key within a database. It takes a username and the new API key as input. The provided API key is first processed by removing any leading or trailing whitespace, then it is encrypted. Finally, the function performs an update operation on the 'dbusers' collection, setting the 'gemini_api_key' field for the specified user with the newly encrypted key."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose Gemini API key needs to be updated.
          gemini_api_key,str,"The new Gemini API key to be stored, which will be encrypted before being saved to the database."
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents that were modified by the update operation, typically 0 or 1."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_gpt_key:
      identifier: database.db.update_gpt_key
      description:
        overall: "This function updates a user's GPT API key in the database. It first encrypts the provided API key, ensuring any leading or trailing whitespace is removed. The encrypted key is then stored in the `dbusers` collection for the specified username. The function returns the number of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          username,str,The username of the user whose GPT API key is to be updated.
          gpt_api_key,str,The new GPT API key to be encrypted and stored.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_ollama_url:
      identifier: database.db.update_ollama_url
      description:
        overall: "This function updates the Ollama base URL for a specific user in the database. It locates the user document using the provided username as the unique identifier. The `ollama_base_url` field within that document is then updated to the new value, ensuring any leading or trailing whitespace is removed. The function returns an integer indicating how many documents were modified by this operation."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose Ollama URL is to be updated.
          ollama_base_url,str,"The new base URL for Ollama, which will be stored after stripping leading/trailing whitespace."
        returns[1]{name,type,description}:
          modified_count,int,The number of documents modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_opensrc_key:
      identifier: database.db.update_opensrc_key
      description:
        overall: "This function updates the 'opensrc_api_key' for a specified user in the database. It first encrypts the provided API key, ensuring any leading or trailing whitespace is removed. The encrypted key is then stored in the 'opensrc_api_key' field for the user identified by their username. The function returns the count of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          username,str,The username of the user whose API key is to be updated.
          opensrc_api_key,str,The new Open Source API key to be stored for the user. It will be stripped of whitespace and encrypted before storage.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.update_opensrc_url:
      identifier: database.db.update_opensrc_url
      description:
        overall: "This function updates the 'opensrc_base_url' field for a specific user in a database. It identifies the user by their 'username', which is mapped to the '_id' field. The provided 'opensrc_base_url' is stripped of leading/trailing whitespace before being stored. The function returns the count of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose 'opensrc_base_url' is to be updated. This is used as the '_id' in the database query.
          opensrc_base_url,str,The new base URL for the open-source repository. This string will have leading/trailing whitespace removed before being stored in the database.
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents that were modified by the update operation. A value of 1 indicates success if the user existed and the URL was different, 0 if the user existed but the URL was the same, or if the user was not found."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_gemini_key:
      identifier: database.db.fetch_gemini_key
      description:
        overall: "This function retrieves the Gemini API key associated with a given username from a database. It queries the 'dbusers' collection, searching for a document where the '_id' matches the provided username. If a user document is found, it extracts the 'gemini_api_key' field. The function returns this key if present, or None if the user is not found or the key is missing."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose Gemini API key is to be fetched.
        returns[1]{name,type,description}:
          gemini_api_key,str | None,"The Gemini API key for the specified user, or None if the user is not found or the key does not exist."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.fetch_ollama_url:
      identifier: database.db.fetch_ollama_url
      description:
        overall: "This function retrieves the Ollama base URL associated with a specific username from a database. It queries the `dbusers` collection, searching for a document where the `_id` matches the provided username. The query is configured to return only the `ollama_base_url` field. If a user document is found, the function extracts and returns the `ollama_base_url` value. If no user is found, it returns `None`."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose Ollama base URL is to be fetched.
        returns[1]{name,type,description}:
          ollama_base_url,str | None,"The Ollama base URL for the specified user, or None if the user is not found in the database."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_gpt_key:
      identifier: database.db.fetch_gpt_key
      description:
        overall: "This function is designed to retrieve a user's GPT API key from a database. It queries the `dbusers` collection, searching for a document where the `_id` field matches the provided `username`. The query is optimized to only return the `gpt_api_key` field. If a matching user document is found, the function extracts and returns the 'gpt_api_key' value. If no user is found, or if the key is not present, it returns None."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose GPT API key needs to be fetched.
        returns[1]{name,type,description}:
          gpt_api_key,str | None,"The GPT API key string if found for the specified user, otherwise None."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.fetch_opensrc_key:
      identifier: database.db.fetch_opensrc_key
      description:
        overall: "This function, `fetch_opensrc_key`, is designed to retrieve an Open Source API key associated with a specific username from a database. It queries a collection named `dbusers` to locate a document matching the provided `username` by its `_id`. The query specifically projects the `opensrc_api_key` field. If a user document is found, the function returns the value of the `opensrc_api_key`; otherwise, it returns `None`."
        parameters[1]{name,type,description}:
          username,str,The username for which to retrieve the Open Source API key.
        returns[1]{name,type,description}:
          opensrc_api_key,str | None,"The Open Source API key associated with the given username, or None if the user or key is not found."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_opensrc_url:
      identifier: database.db.fetch_opensrc_url
      description:
        overall: "This function retrieves the Open Source base URL for a specified user from a database collection. It queries the 'dbusers' collection, searching for a document where the '_id' field matches the provided username. The function is designed to efficiently fetch only the 'opensrc_base_url' field. If a user document is found, the associated URL is returned; otherwise, it returns None."
        parameters[1]{name,type,description}:
          username,str,The unique identifier (username) of the user whose Open Source base URL is to be retrieved.
        returns[1]{name,type,description}:
          opensrc_base_url,str | None,"The Open Source base URL associated with the user, or None if the user is not found in the database."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.delete_user:
      identifier: database.db.delete_user
      description:
        overall: "This function is designed to remove a user record from the database. It takes a username as input and uses it to locate and delete a single document within the 'dbusers' collection. The function then returns an integer indicating the number of documents that were successfully deleted, which will typically be 0 or 1."
        parameters[1]{name,type,description}:
          username,str,The unique identifier of the user to be deleted from the database.
        returns[1]{name,type,description}:
          deleted_count,int,The number of documents deleted by the operation (0 or 1).
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.get_decrypted_api_keys:
      identifier: database.db.get_decrypted_api_keys
      description:
        overall: "This function retrieves a user's API keys and base URLs from a database. It queries the 'dbusers' collection using the provided username. If the user is found, it decrypts specific API keys (Gemini, GPT, open-source) and retrieves base URLs (Ollama, open-source). If the user is not found, it returns a tuple of two None values."
        parameters[1]{name,type,description}:
          username,str,The username used to identify and retrieve the user's data from the database.
        returns[5]{name,type,description}:
          gemini_plain,str | None,"The decrypted Gemini API key, or None if the user is not found."
          ollama_plain,str | None,"The Ollama base URL, or None if the user is not found."
          gpt_plain,str | None,"The decrypted GPT API key, or None if the user is not found."
          opensrc_plain,str | None,"The decrypted open-source API key, or None if the user is not found."
          opensrc_url,str | None,"The open-source base URL, or None if the user is not found."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.insert_chat:
      identifier: database.db.insert_chat
      description:
        overall: "The `insert_chat` function is responsible for creating a new chat entry in a database. It constructs a dictionary containing a unique identifier generated by `uuid.uuid4()`, the provided username, the chat name, and the current timestamp. This dictionary is then inserted into the `dbchats` collection using `insert_one()`. The function ultimately returns the unique ID of the newly inserted chat document."
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat entry.
          chat_name,str,The name of the chat to be created.
        returns[1]{name,type,description}:
          inserted_id,str,The unique identifier (MongoDB _id) of the newly created chat entry.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_chats_by_user:
      identifier: database.db.fetch_chats_by_user
      description:
        overall: "This function, `fetch_chats_by_user`, is designed to retrieve all chat records associated with a specific user from a database. It takes a username as input and queries the `dbchats` collection. The retrieved chat documents are then sorted by their `created_at` timestamp in ascending order. Finally, the function returns these sorted chat documents as a list."
        parameters[1]{name,type,description}:
          username,str,The username for which to fetch chat records.
        returns[1]{name,type,description}:
          chats,"list[dict]","A list of chat documents (dictionaries) associated with the specified username, sorted by creation date."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.check_chat_exists:
      identifier: database.db.check_chat_exists
      description:
        overall: This function verifies the existence of a specific chat entry within the 'dbchats' collection. It queries the collection using both the provided username and chat name as criteria. The function returns a boolean value indicating whether a matching chat record was found in the database.
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to be checked.
          chat_name,str,The name of the chat to check for existence.
        returns[1]{name,type,description}:
          chat_exists,bool,"True if a chat matching the username and chat name exists, False otherwise."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.rename_chat_fully:
      identifier: database.db.rename_chat_fully
      description:
        overall: "This function renames a chat and all its associated exchanges (messages) in the database. It performs two distinct update operations: first, it updates the chat entry in the 'dbchats' collection, changing its 'chat_name' from the old name to the new name. Second, it updates all related exchanges in the 'dbexchanges' collection, ensuring their 'chat_name' also reflects the new name. The function returns the count of modified chat entries from the initial update operation."
        parameters[3]{name,type,description}:
          username,str,The username associated with the chat to be renamed.
          old_name,str,The current name of the chat.
          new_name,str,The desired new name for the chat.
        returns[1]{name,type,description}:
          modified_count,int,The number of chat entries that were modified in the 'dbchats' collection.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.insert_exchange:
      identifier: database.db.insert_exchange
      description:
        overall: "This function, `insert_exchange`, is responsible for creating and inserting a new exchange record into a database collection. It generates a unique identifier for the new record using `uuid.uuid4()`. A dictionary is then constructed containing various details about the exchange, including user input, feedback, user information, and performance metrics, along with a timestamp. The function attempts to insert this prepared dictionary into the `dbexchanges` collection. Upon successful insertion, it returns the generated unique ID; otherwise, it catches any exceptions, prints an error, and returns `None`."
        parameters[13]{name,type,description}:
          question,str,The question string of the exchange.
          answer,str,The answer string provided in the exchange.
          feedback,str,The feedback string for the exchange.
          username,str,The username associated with the exchange.
          chat_name,str,The name of the chat where the exchange occurred.
          helper_used,str,"Optional. Indicates which helper component was used, defaulting to an empty string."
          main_used,str,"Optional. Indicates which main component was used, defaulting to an empty string."
          total_time,str,"Optional. The total time taken for the exchange, defaulting to an empty string."
          helper_time,str,"Optional. The time taken by the helper component, defaulting to an empty string."
          main_time,str,"Optional. The time taken by the main component, defaulting to an empty string."
          json_tokens,int,"Optional. The number of JSON tokens used, defaulting to 0."
          toon_tokens,int,"Optional. The number of 'toon' tokens used, defaulting to 0."
          savings_percent,float,"Optional. The percentage of savings, defaulting to 0.0."
        returns[2]{name,type,description}:
          new_id,str,The unique identifier of the newly inserted exchange record upon successful insertion.
          None,None,Returns None if an error occurs during the database insertion.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_exchanges_by_user:
      identifier: database.db.fetch_exchanges_by_user
      description:
        overall: "This function, `fetch_exchanges_by_user`, is designed to retrieve exchange records from a database based on a provided username. It queries a collection, presumably `dbexchanges`, for all entries where the 'username' field matches the input. The retrieved records are then sorted in ascending order by their 'created_at' timestamp. Finally, the function converts these sorted records into a list and returns it."
        parameters[1]{name,type,description}:
          username,str,The username for which to fetch exchange records.
        returns[1]{name,type,description}:
          exchanges,list,"A list of exchange records associated with the specified username, sorted by creation timestamp."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_exchanges_by_chat:
      identifier: database.db.fetch_exchanges_by_chat
      description:
        overall: This function retrieves a list of exchange records from a database collection named `dbexchanges`. It filters these records based on a provided username and chat name. The results are then sorted by their creation timestamp in ascending order before being returned as a list.
        parameters[2]{name,type,description}:
          username,str,The username used to filter the exchange records.
          chat_name,str,The name of the chat used to filter the exchange records.
        returns[1]{name,type,description}:
          exchanges,list,"A list of exchange documents that match the specified username and chat name, sorted by creation time."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.update_exchange_feedback:
      identifier: database.db.update_exchange_feedback
      description:
        overall: "This function is responsible for updating the feedback score associated with a specific exchange record in a database. It takes an exchange identifier and an integer feedback value as input. The function performs an update operation on the `dbexchanges` collection, locating the document by its `_id` and setting its `feedback` field to the provided value. It then returns the count of documents that were successfully modified."
        parameters[2]{name,type,description}:
          exchange_id,Any,The unique identifier for the exchange record to be updated. Its specific type is not hinted but is expected to match the database's `_id` type.
          feedback,int,The integer value representing the feedback to be stored for the exchange.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.update_exchange_feedback_message:
      identifier: database.db.update_exchange_feedback_message
      description:
        overall: This function is designed to update a specific exchange record within a database collection. It accepts an identifier for the exchange and a new feedback message. The function locates the document corresponding to the provided exchange ID and updates its 'feedback_message' field with the new string. It then returns the count of documents that were successfully modified by this operation.
        parameters[2]{name,type,description}:
          exchange_id,Any,The unique identifier for the exchange record that needs to be updated.
          feedback_message,str,The new feedback message to be stored for the specified exchange.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified as a result of the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.delete_exchange_by_id:
      identifier: database.db.delete_exchange_by_id
      description:
        overall: This function is responsible for deleting a single exchange record from the database. It takes an exchange identifier as input and uses it to locate and remove the corresponding document from the `dbexchanges` collection. The function then reports the number of documents that were successfully deleted.
        parameters[1]{name,type,description}:
          exchange_id,str,The unique identifier of the exchange record to be deleted.
        returns[1]{name,type,description}:
          deleted_count,int,"The number of documents deleted. Typically 1 if found and deleted, or 0 if not found."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.delete_full_chat:
      identifier: database.db.delete_full_chat
      description:
        overall: "This function is designed to completely remove a chat and all its associated message exchanges from the database. It first deletes all exchanges linked to the given username and chat name using `dbexchanges.delete_many`. Subsequently, it removes the chat entry itself using `dbchats.delete_one`. The function aims to maintain data consistency between frontend and backend by ensuring all related data is purged."
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to be deleted.
          chat_name,str,The specific name of the chat to be deleted.
        returns[1]{name,type,description}:
          deleted_count,int,"The number of chat documents deleted, typically 0 or 1, representing the chat itself."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.clean_names:
      identifier: frontend.frontend.clean_names
      description:
        overall: "This function takes a list of strings, where each string is expected to contain path-like components separated by a forward slash ('/'). It iterates through this list, and for each string, it splits the string by the '/' delimiter and extracts the last element. The function then returns a new list composed of these extracted last segments, effectively simplifying the input names by removing any preceding path information."
        parameters[1]{name,type,description}:
          model_list,"list[str]","A list of strings, where each string may represent a path or an identifier containing '/' characters. Each string will be processed to extract its final segment."
        returns[1]{name,type,description}:
          cleaned_names,"list[str]",A new list containing the last segment of each string from the input 'model_list' after splitting by '/'.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.get_filtered_models:
      identifier: frontend.frontend.get_filtered_models
      description:
        overall: "This function filters a provided list of models (`source_list`) based on a specified `category_name`. It retrieves associated keywords from an external `CATEGORY_KEYWORDS` mapping. If the \"STANDARD\" keyword is present for the category, it returns only models that are also found in `STANDARD_MODELS`. Otherwise, it iterates through the `source_list` and includes models whose names contain any of the category's keywords. If no models match the keywords, the original `source_list` is returned."
        parameters[2]{name,type,description}:
          source_list,list,The list of models to be filtered.
          category_name,str,The name of the category used to determine filtering keywords.
        returns[1]{name,type,description}:
          filtered_models,list,"A list of models filtered according to the specified category, or the original `source_list` if no filtering criteria are met or no matches are found."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.save_gemini_cb:
      identifier: frontend.frontend.save_gemini_cb
      description:
        overall: "This function serves as a callback to save a Gemini API key. It retrieves a potential new key from the Streamlit session state. If a new key is found, it updates the user's Gemini key in the database using the `db.update_gemini_key` function. After saving, it clears the key from the session state and displays a success toast notification to the user."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.save_ollama_cb:
      identifier: frontend.frontend.save_ollama_cb
      description:
        overall: "This function, `save_ollama_cb`, is designed to save a user-provided Ollama URL. It first attempts to retrieve a potential new Ollama URL from the Streamlit session state, specifically from the key 'in_ollama_url'. If a non-empty URL is found, the function proceeds to update this URL in the database. The update operation uses the `db.update_ollama_url` method, associating the new URL with the current user's username, which is also retrieved from the session state. Upon successful update, a confirmation toast message is displayed to the user via `st.toast`."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.load_data_from_db:
      identifier: frontend.frontend.load_data_from_db
      description:
        overall: "The `load_data_from_db` function is responsible for loading chat and exchange data for a specific user from the database into the Streamlit session state. It first checks if the data for the given username is already loaded to avoid redundant operations. If not loaded, it initializes the session state, then fetches predefined chats and subsequently all exchanges associated with the user. Exchanges are then organized into their respective chats, handling cases where chats might not have been explicitly defined but have associated exchanges. Finally, it ensures that at least one default chat exists if none were loaded and sets an active chat in the session state."
        parameters[1]{name,type,description}:
          username,str,The username for whom to load chat and exchange data from the database.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.handle_feedback_change:
      identifier: frontend.frontend.handle_feedback_change
      description:
        overall: "This function, `handle_feedback_change`, is designed to update a feedback value associated with an 'exchange' object and persist this change to a database. It takes an exchange object, `ex`, and a new feedback value, `val`, as input. The function first updates the 'feedback' key within the `ex` dictionary with the provided `val`. Subsequently, it invokes `db.update_exchange_feedback` to store this updated feedback value in the database, using the `_id` from the `ex` object. Finally, it triggers a re-execution of the Streamlit application using `st.rerun()`, likely to refresh the user interface to reflect the change."
        parameters[2]{name,type,description}:
          ex,dict,The exchange object or dictionary whose 'feedback' field needs to be updated. It is expected to contain 'feedback' and '_id' keys.
          val,Any,The new feedback value to be assigned to the 'feedback' field of the exchange object and stored in the database.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.handle_delete_exchange:
      identifier: frontend.frontend.handle_delete_exchange
      description:
        overall: "This function handles the deletion of a specific exchange. It first removes the exchange from the database using its ID. Subsequently, it checks if the associated chat exists in the Streamlit session state and, if the exchange is present within that chat's exchanges list, it removes it from the session state. Finally, it triggers a Streamlit rerun to update the UI."
        parameters[2]{name,type,description}:
          chat_name,str,The name of the chat to which the exchange belongs.
          ex,dict,"The exchange object, expected to be a dictionary containing an '_id' field for database deletion."
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.handle_delete_chat:
      identifier: frontend.frontend.handle_delete_chat
      description:
        overall: "This function handles the deletion of a specific chat identified by a username and chat name. It first removes the chat from the database using `db.delete_full_chat`. Subsequently, it updates the Streamlit session state by removing the chat from `st.session_state.chats`. If other chats exist, the first remaining chat becomes the active chat. If no chats are left, a new default chat named \"Chat 1\" is created, inserted into the database, and set as the active chat. Finally, it triggers a Streamlit rerun to refresh the UI."
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to be deleted.
          chat_name,str,The name of the chat to be deleted.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    frontend.frontend.extract_repo_name:
      identifier: frontend.frontend.extract_repo_name
      description:
        overall: "This function is designed to extract a repository name from a given text string. It first attempts to find a URL within the text using a regular expression. If a URL is identified, it then parses this URL to isolate the path component. The last segment of the path is considered the potential repository name, with a specific check to remove a '.git' suffix if present. If no URL is found or a repository name cannot be successfully extracted, the function returns None."
        parameters[1]{name,type,description}:
          text,str,The input string that may contain a URL from which to extract a repository name.
        returns[1]{name,type,description}:
          repo_name,str | None,"The extracted repository name as a string, or None if no valid repository name could be found within the provided text."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.stream_text_generator:
      identifier: frontend.frontend.stream_text_generator
      description:
        overall: "This function acts as a generator that takes a string of text and yields its words one by one, introducing a small delay between each word. It splits the input text by spaces and then iterates through the resulting words. For each word, it yields the word concatenated with a space, followed by a brief pause using `time.sleep(0.01)`."
        parameters[1]{name,type,description}:
          text,str,The input string of text to be processed and streamed word by word.
        returns[1]{name,type,description}:
          word_with_space,str,"Each word from the input text, followed by a space, yielded sequentially."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.render_text_with_mermaid:
      identifier: frontend.frontend.render_text_with_mermaid
      description:
        overall: "This function processes markdown text, specifically designed to render content that may include Mermaid diagrams. It first checks if the input markdown text is empty, returning early if so. The function then splits the markdown text into parts, separating regular markdown content from blocks identified as Mermaid diagrams. Regular markdown parts are rendered using Streamlit's `st.markdown` or `st.write_stream` if streaming is enabled. Mermaid diagram parts are attempted to be rendered using `st_mermaid`, with a fallback to `st.code` if an error occurs during Mermaid rendering."
        parameters[2]{name,type,description}:
          markdown_text,str,"The input text, potentially containing embedded Mermaid diagram syntax."
          should_stream,bool,A flag indicating whether regular markdown text parts should be streamed or rendered directly. Defaults to False.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.render_exchange:
      identifier: frontend.frontend.render_exchange
      description:
        overall: "The render_exchange function is responsible for displaying a single chat exchange, comprising a user's question and an assistant's answer, within a Streamlit application. It first renders the user's question. Subsequently, it displays the assistant's answer, which includes a dynamic toolbar. This toolbar provides functionalities such as feedback (like/dislike), adding comments via a popover, downloading the answer as Markdown, and deleting the exchange. The function also incorporates error handling to display a specific error message if the assistant's answer indicates a problem."
        parameters[2]{name,type,description}:
          ex,dict,"A dictionary representing a single chat exchange, expected to contain keys like 'question', 'answer', 'feedback', 'feedback_message', and '_id'."
          current_chat_name,str,"The name of the current chat session, used primarily when handling the deletion of an exchange."
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
  classes:
    backend.AST_Schema.ASTVisitor:
      identifier: backend.AST_Schema.ASTVisitor
      description:
        overall: "The ASTVisitor class extends ast.NodeVisitor to traverse an Abstract Syntax Tree (AST) of Python source code. Its primary purpose is to extract structured information about imports, functions, and classes defined within a given source file, storing this data in a 'schema' dictionary. It differentiates between top-level functions and methods nested within classes, providing a mechanism to collect detailed metadata for each."
        init_method:
          description: "The __init__ method initializes an ASTVisitor instance by storing the provided source code, file path, and project root. It calculates the module path using an external utility function and sets up an empty schema dictionary to store discovered imports, functions, and classes. It also initializes '_current_class' to None to track the current class being visited during AST traversal."
          parameters[3]{name,type,description}:
            source_code,str,The raw source code of the file being visited.
            file_path,str,The absolute path to the file being visited.
            project_root,str,"The root directory of the project, used to determine relative module paths."
        methods[5]:
          - identifier: visit_Import
            description:
              overall: "This method processes 'ast.Import' nodes, which represent 'import module' statements. It iterates through the imported module names (aliases) and appends each module name to the 'imports' list within the 'self.schema' dictionary. After processing the import, it calls 'self.generic_visit' to continue traversing the AST to child nodes."
              parameters[1]{name,type,description}:
                node,ast.Import,The AST node representing an import statement.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions according to the provided context."
                called_by: This method is not explicitly called by other functions or methods according to the provided context.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method handles 'ast.ImportFrom' nodes, corresponding to 'from module import name' statements. It constructs fully qualified import names (e.g., 'module.name') for each alias and adds them to the 'imports' list in 'self.schema'. It then delegates to 'self.generic_visit' to continue the AST traversal for any child nodes."
              parameters[1]{name,type,description}:
                node,ast.ImportFrom,The AST node representing a 'from ... import ...' statement.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions according to the provided context."
                called_by: This method is not explicitly called by other functions or methods according to the provided context.
            error: null
          - identifier: visit_ClassDef
            description:
              overall: "This method processes 'ast.ClassDef' nodes, representing class definitions. It constructs a 'class_info' dictionary containing details like the class identifier, name, docstring, source code segment, and line numbers. This 'class_info' is appended to the 'classes' list in 'self.schema'. It also sets 'self._current_class' to the newly created 'class_info' before recursively visiting child nodes, and then resets 'self._current_class' to 'None' after the class's children have been visited."
              parameters[1]{name,type,description}:
                node,ast.ClassDef,The AST node representing a class definition.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions according to the provided context."
                called_by: This method is not explicitly called by other functions or methods according to the provided context.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method handles 'ast.FunctionDef' nodes, which represent regular function definitions. It distinguishes between methods within a class and top-level functions. If '_current_class' is set, it extracts method-specific information (identifier, name, arguments, docstring, line numbers) and appends it to the 'method_context' of the '_current_class'. Otherwise, it creates a 'func_info' dictionary for a top-level function and appends it to the 'functions' list in 'self.schema'. Finally, it calls 'self.generic_visit' to continue traversal."
              parameters[1]{name,type,description}:
                node,ast.FunctionDef,The AST node representing a function definition.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions according to the provided context."
                called_by: This method is not explicitly called by other functions or methods according to the provided context.
            error: null
          - identifier: visit_AsyncFunctionDef
            description:
              overall: "This method processes 'ast.AsyncFunctionDef' nodes, which represent asynchronous function definitions. It delegates its processing directly to the 'visit_FunctionDef' method, treating async functions similarly to regular functions for the purpose of schema generation. This ensures that asynchronous functions are captured and processed in the same manner as synchronous ones."
              parameters[1]{name,type,description}:
                node,ast.AsyncFunctionDef,The AST node representing an asynchronous function definition.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions according to the provided context."
                called_by: This method is not explicitly called by other functions or methods according to the provided context.
            error: null
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in its context.
          instantiated_by: This class is not explicitly listed as being instantiated by any other components in its context.
      error: null
    backend.AST_Schema.ASTAnalyzer:
      identifier: backend.AST_Schema.ASTAnalyzer
      description:
        overall: "The ASTAnalyzer class is responsible for analyzing Python source code to construct a detailed Abstract Syntax Tree (AST) schema of a repository. It can parse individual files, identify functions and classes, and then integrate inter-component relationship data, such as function calls and class instantiations, into this structured representation. This class serves as a core component for understanding the structural and dependency landscape of a codebase."
        init_method:
          description: "This constructor initializes the ASTAnalyzer class. It does not take any specific parameters beyond `self` and performs no explicit setup or attribute assignments, effectively creating an instance with no initial state."
          parameters[0]:
        methods[2]:
          - identifier: merge_relationship_data
            description:
              overall: "This method integrates raw relationship data, specifically incoming and outgoing calls, into a structured AST schema. It iterates through the files, functions, and classes within the provided `full_schema`, populating their respective context fields with call and instantiation information. For classes, it also identifies and lists external dependencies based on the methods' outgoing calls, ensuring a comprehensive view of inter-component interactions."
              parameters[2]{name,type,description}:
                full_schema,dict,"The comprehensive schema of the AST nodes, including files, functions, and classes, to be updated."
                raw_relationships,dict,A dictionary containing raw 'outgoing' and 'incoming' call relationships to be merged.
              returns[1]{name,type,description}:
                full_schema,dict,The updated full_schema dictionary with integrated relationship data.
              usage_context:
                calls: This method does not explicitly call other functions or methods based on the provided context.
                called_by: This method is not explicitly called by other functions or methods based on the provided context.
            error: null
          - identifier: analyze_repository
            description:
              overall: "This method processes a list of file objects from a repository to build a comprehensive AST schema. It filters for Python files, reads their content, and uses the `ast` module and an `ASTVisitor` to parse the code and extract AST nodes. The extracted schema for each valid Python file is then added to a `full_schema` dictionary, while gracefully handling potential syntax or value errors during parsing."
              parameters[2]{name,type,description}:
                files,list,"A list of file objects, each expected to have 'path' and 'content' attributes."
                repo,GitRepository,"An object representing the Git repository, used for context but not directly accessed in the provided snippet."
              returns[1]{name,type,description}:
                full_schema,dict,"A dictionary representing the AST schema of the analyzed repository, structured by files."
              usage_context:
                calls: This method does not explicitly call other functions or methods based on the provided context.
                called_by: This method is not explicitly called by other functions or methods based on the provided context.
            error: null
        usage_context:
          dependencies: The class does not have explicit external dependencies listed in the provided context.
          instantiated_by: The class is not explicitly instantiated by other components based on the provided context.
      error: null
    backend.File_Dependency.FileDependencyGraph:
      identifier: backend.File_Dependency.FileDependencyGraph
      description:
        overall: "The FileDependencyGraph class extends ast.NodeVisitor to construct a graph of import dependencies for a given Python file within a repository. It is initialized with the target filename and the repo_root to facilitate accurate path resolution. The class overrides visit_Import and visit_ImportFrom to capture and record both direct and from ... import ... style dependencies. A key private method, _resolve_module_name, handles the complexities of relative imports by dynamically checking for module and symbol existence, ensuring a robust dependency mapping. The collected dependencies are stored in the import_dependencies dictionary."
        init_method:
          description: This constructor initializes the FileDependencyGraph instance by setting the filename of the file currently being analyzed and the repo_root directory. These values are stored as instance attributes to be used by other methods for path resolution and dependency tracking.
          parameters[2]{name,type,description}:
            filename,str,The path to the file currently being analyzed for dependencies.
            repo_root,str,"The root directory of the repository, used for resolving file paths."
        methods[3]:
          - identifier: _resolve_module_name
            description:
              overall: "This private method is responsible for resolving relative import statements, such as from .. import name. It calculates the correct base directory based on the import level and the current file's location within the repository. It then verifies the existence of the imported modules or symbols by checking for corresponding .py files, __init__.py files, or explicit exports within __init__.py's __all__ attribute. If no valid resolution is found, an ImportError is raised."
              parameters[1]{name,type,description}:
                node,ImportFrom,The AST ImportFrom node representing the relative import statement to be resolved.
              returns[1]{name,type,description}:
                resolved,"list[str]",A list of successfully resolved module or symbol names.
              usage_context:
                calls: This method does not explicitly call other functions or methods according to the provided context.
                called_by: This method is not explicitly called by other functions or methods according to the provided context.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method processes both Import and ImportFrom AST nodes to record dependencies. It iterates through the aliases within the import node and adds the imported module or symbol name to the import_dependencies dictionary, keyed by the current self.filename. An optional base_name parameter allows for specific module names to be recorded, particularly useful for from ... import ... scenarios. It then continues the AST traversal."
              parameters[2]{name,type,description}:
                node,Import | ImportFrom,The AST node representing either an Import or ImportFrom statement.
                base_name,str | None,"An optional string specifying the base module name to record, overriding the alias name if provided."
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other functions or methods according to the provided context.
                called_by: This method is not explicitly called by other functions or methods according to the provided context.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method is an AST visitor specifically for ImportFrom nodes. It differentiates between absolute and relative imports. For absolute imports, it extracts the base module name and delegates to visit_Import. For relative imports, it utilizes the _resolve_module_name helper to determine the actual modules or symbols being imported. Each resolved name is then passed to visit_Import to record the dependency. It includes basic error handling for failed relative import resolutions."
              parameters[1]{name,type,description}:
                node,ImportFrom,The AST ImportFrom node representing the import statement.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other functions or methods according to the provided context.
                called_by: This method is not explicitly called by other functions or methods according to the provided context.
            error: null
        usage_context:
          dependencies: The class does not explicitly list any external dependencies in the provided context.
          instantiated_by: The class is not explicitly instantiated by any other components according to the provided context.
      error: null
    backend.HelperLLM.LLMHelper:
      identifier: backend.HelperLLM.LLMHelper
      description:
        overall: "The LLMHelper class serves as a robust interface for interacting with various Large Language Models (LLMs) to generate structured documentation for Python functions and classes. It centralizes the logic for LLM API interaction, including dynamic model selection (supporting Gemini, OpenAI, Ollama, and custom endpoints), API key management, and the loading of specific system prompts for different analysis tasks. The class is designed for efficient batch processing, incorporating rate limiting and comprehensive error handling to ensure reliable and consistent documentation generation."
        init_method:
          description: "The constructor initializes the LLMHelper instance by setting up the necessary API key, loading system prompts from specified file paths for both function and class analysis, and configuring the underlying LLM client. It dynamically selects and initializes the appropriate LangChain chat model (e.g., ChatGoogleGenerativeAI, ChatOpenAI, ChatOllama) based on the provided model name and base URL, then wraps these models with structured output capabilities using Pydantic schemas. It also calls a private method to configure batch processing settings specific to the chosen model."
          parameters[5]{name,type,description}:
            api_key,str,The API key required for authenticating with the chosen Large Language Model service.
            function_prompt_path,str,The file path to the system prompt specifically designed for guiding the LLM in function analysis.
            class_prompt_path,str,The file path to the system prompt specifically designed for guiding the LLM in class analysis.
            model_name,str,"The identifier for the LLM model to be used, defaulting to 'gemini-2.0-flash-lite'. This determines which LLM client is initialized."
            base_url,str,"An optional base URL for custom LLM endpoints, particularly useful for self-hosted models or specific API gateways. Defaults to None."
        methods[3]:
          - identifier: _configure_batch_settings
            description:
              overall: "This private method is responsible for setting the `batch_size` attribute of the LLMHelper instance based on the provided `model_name`. It contains a series of conditional statements that assign specific batch sizes optimized for different LLM models, such as various Gemini versions, Llama3, and GPT models. For unknown or custom models, it applies a conservative default batch size. This configuration is crucial for managing API rate limits and optimizing the performance of batch requests to the LLMs."
              parameters[1]{name,type,description}:
                model_name,str,The name of the LLM model for which the batch processing settings need to be configured.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call other methods or functions directly within its body, besides standard Python operations and logging."
                called_by: This method is called by the `__init__` method of the `LLMHelper` class during object initialization to set up batch processing parameters.
            error: null
          - identifier: generate_for_functions
            description:
              overall: "This method orchestrates the generation and validation of documentation for a batch of Python functions using the configured LLM. It takes a list of `FunctionAnalysisInput` objects, serializes each into a JSON payload, and constructs a list of conversation messages including the function-specific system prompt. The method then processes these conversations in batches, sending them to the `function_llm` while adhering to predefined batch sizes and waiting times to respect API rate limits. It includes error handling to gracefully manage failures during batch calls, ensuring that the output list maintains its structure by inserting `None` for failed analyses."
              parameters[1]{name,type,description}:
                function_inputs,"List[FunctionAnalysisInput]","A list of input objects, each containing the necessary data for the LLM to analyze and generate documentation for a single function."
              returns[1]{name,type,description}:
                None,"List[Optional[FunctionAnalysis]]","A list where each element is either a successfully generated and validated `FunctionAnalysis` object or `None` if an error occurred during its generation, maintaining the order of the input."
              usage_context:
                calls: "This method calls `json.dumps` for serialization, `SystemMessage` and `HumanMessage` to format LLM prompts, `self.function_llm.batch` to execute batch LLM calls, `logging.info` and `logging.error` for operational feedback, and `time.sleep` to implement rate limiting delays."
                called_by: The input context does not specify explicit callers for this method within the provided scope.
            error: null
          - identifier: generate_for_classes
            description:
              overall: "This method facilitates the generation and validation of documentation for a batch of Python classes using the configured LLM. It accepts a list of `ClassAnalysisInput` objects, converts them into JSON payloads, and prepares them as conversations with the class-specific system prompt. The method then iteratively sends these conversations to the `class_llm` in batches, incorporating a waiting period between batches to comply with API rate limits. It also implements robust error handling, ensuring that if an LLM call fails for a batch, the corresponding entries in the output list are filled with `None` to preserve the overall structure and order."
              parameters[1]{name,type,description}:
                class_inputs,"List[ClassAnalysisInput]","A list of input objects, each containing the necessary data for the LLM to analyze and generate documentation for a single class."
              returns[1]{name,type,description}:
                None,"List[Optional[ClassAnalysis]]","A list where each element is either a successfully generated and validated `ClassAnalysis` object or `None` if an error occurred during its generation, maintaining the order of the input."
              usage_context:
                calls: "This method calls `json.dumps` for serialization, `SystemMessage` and `HumanMessage` to format LLM prompts, `self.class_llm.batch` to execute batch LLM calls, `logging.info` and `logging.error` for operational feedback, and `time.sleep` to implement rate limiting delays."
                called_by: The input context does not specify explicit callers for this method within the provided scope.
            error: null
        usage_context:
          dependencies: "The class depends on `logging` for output, `json` for serialization, `time` for rate limiting, `langchain_google_genai.ChatGoogleGenerativeAI`, `langchain_ollama.ChatOllama`, and `langchain_openai.ChatOpenAI` for integrating with various LLM providers. It also relies on Pydantic schemas such as `FunctionAnalysis`, `ClassAnalysis`, `FunctionAnalysisInput`, and `ClassAnalysisInput` for defining structured input and output data."
          instantiated_by: The specific instantiation points for this class are not provided in the given context.
      error: null
    backend.MainLLM.MainLLM:
      identifier: backend.MainLLM.MainLLM
      description:
        overall: "The MainLLM class provides a consolidated interface for interacting with various large language models, including Google Gemini, OpenAI-compatible APIs (such as custom SCADSLLM endpoints or generic GPT models), and Ollama. It handles the loading of a system prompt from a specified file and dynamically initializes the appropriate LLM client based on the provided model name. The class offers both synchronous and streaming methods for generating responses, abstracting the underlying LLM provider details."
        init_method:
          description: "This constructor initializes the MainLLM class by loading a system prompt from a file and configuring the appropriate Language Model (LLM) client based on the `model_name`. It supports Google Gemini, OpenAI (including custom API endpoints), and Ollama models, ensuring necessary API keys and base URLs are provided, and raises a ValueError if the API key is missing or a FileNotFoundError if the prompt file is not found."
          parameters[4]{name,type,description}:
            api_key,str,The API key required for authenticating with the chosen LLM provider.
            prompt_file_path,str,"The file path to the system prompt text, which is loaded during initialization."
            model_name,str,"The name of the LLM model to use, defaulting to 'gemini-2.5-pro'. This parameter dictates which specific LLM client (e.g., ChatGoogleGenerativeAI, ChatOpenAI, ChatOllama) is instantiated."
            base_url,str | None,"An optional base URL for custom LLM endpoints, primarily used for Ollama or other self-hosted models, or when connecting to a custom OpenAI-compatible API."
        methods[2]:
          - identifier: call_llm
            description:
              overall: "This method sends a user's input to the configured LLM and returns the complete generated response content. It constructs a list of messages, including the pre-loaded system prompt and the user's query, and then synchronously invokes the LLM client. The method includes error handling to catch and log exceptions that may occur during the LLM call, returning None in such cases."
              parameters[1]{name,type,description}:
                user_input,str,The user's query or message to be sent to the LLM for a synchronous response.
              returns[1]{name,type,description}:
                content,str | None,"The generated text content from the LLM, or None if an error occurred during the call."
              usage_context:
                calls: This method is not explicitly calling any other functions or methods within the provided context.
                called_by: This method is not explicitly called by any other functions or methods within the provided context.
            error: null
          - identifier: stream_llm
            description:
              overall: "This method provides a streaming interface to the LLM, yielding chunks of the response content as they become available. It prepares the message payload, including the system prompt and user input, and then utilizes the LLM client's streaming capability. The method incorporates error handling, yielding an error message string if an exception occurs during the streaming process."
              parameters[1]{name,type,description}:
                user_input,str,The user's query or message to be sent to the LLM for a streaming response.
              returns[1]{name,type,description}:
                chunk.content,"Generator[str, None, None]","A generator that yields string chunks of the LLM's response content, or an error message string if an exception occurs during streaming."
              usage_context:
                calls: This method is not explicitly calling any other functions or methods within the provided context.
                called_by: This method is not explicitly called by any other functions or methods within the provided context.
            error: null
        usage_context:
          dependencies: This class does not have any explicit external dependencies listed in the provided context.
          instantiated_by: This class is not explicitly instantiated by any components listed in the provided context.
      error: null
    backend.basic_info.ProjektInfoExtractor:
      identifier: backend.basic_info.ProjektInfoExtractor
      description:
        overall: "The ProjektInfoExtractor class is designed to systematically extract and consolidate fundamental project information from common project configuration files such as README.md, pyproject.toml, and requirements.txt. It provides a structured approach to gather details like project title, description, features, tech stack, installation instructions, and dependencies. The class prioritizes information sources, ensuring that more structured data (like TOML) overrides less structured data (like README) where applicable, and offers fallbacks like deriving a title from the repository URL."
        init_method:
          description: "The constructor initializes the ProjektInfoExtractor instance by setting a default string for 'Information not found' and creating a nested dictionary `self.info`. This dictionary is pre-filled with placeholders for various project details, ensuring a consistent structure for storing extracted information."
          parameters[0]:
        methods[7]:
          - identifier: _clean_content
            description:
              overall: "This private helper method processes a given string content to remove null bytes ('\\x00'). Null bytes can appear due to encoding errors, such as reading a UTF-16 encoded file as UTF-8. The method ensures the content is clean before further parsing, returning an empty string if the input content is falsy."
              parameters[1]{name,type,description}:
                content,str,The string content to be cleaned.
              returns[1]{name,type,description}:
                "",str,"The cleaned string content, with null bytes removed, or an empty string if the input was empty."
              usage_context:
                calls: This method does not make any external calls.
                called_by: "This method is called by _parse_readme, _parse_toml, and _parse_requirements."
            error: null
          - identifier: _finde_datei
            description:
              overall: "This private helper method searches through a list of file objects to find one whose path matches any of the provided patterns. The search is case-insensitive to ensure flexibility in file naming. It iterates through each file and then each pattern, returning the first matching file object found or None if no match is present."
              parameters[2]{name,type,description}:
                patterns,"List[str]","A list of string patterns to match against file paths, e.g., ['readme.md']."
                dateien,"List[Any]","A list of file-like objects, each expected to have a 'path' attribute."
              returns[1]{name,type,description}:
                "","Optional[Any]","The first file object that matches a pattern, or None if no match is found."
              usage_context:
                calls: This method does not make any external calls.
                called_by: This method is called by extrahiere_info.
            error: null
          - identifier: _extrahiere_sektion_aus_markdown
            description:
              overall: "This private method extracts text content located under a Markdown level 2 heading (##). It constructs a regular expression pattern to match any of the provided keywords within a '##' heading. The method then captures all content following that heading until the next '##' heading or the end of the document, returning the stripped text."
              parameters[2]{name,type,description}:
                inhalt,str,The Markdown content string to search within.
                keywords,"List[str]",A list of keywords to match against Markdown '##' headings.
              returns[1]{name,type,description}:
                "","Optional[str]","The extracted section content as a string, or None if no matching section is found."
              usage_context:
                calls: "This method calls re.escape, re.compile, and re.search."
                called_by: This method is called by _parse_readme.
            error: null
          - identifier: _parse_readme
            description:
              overall: "This private method parses the content of a README file to extract various project details and update the `self.info` dictionary. It first cleans the content, then uses regular expressions to find the project title and a fallback description. It also leverages `_extrahiere_sektion_aus_markdown` to find specific sections like 'Features', 'Tech Stack', 'Status', 'Installation', and 'Quick Start' within the README."
              parameters[1]{name,type,description}:
                inhalt,str,The string content of the README file.
              returns[0]:
              usage_context:
                calls: "This method calls self._clean_content, re.search, and self._extrahiere_sektion_aus_markdown."
                called_by: This method is called by extrahiere_info.
            error: null
          - identifier: _parse_toml
            description:
              overall: "This private method parses the content of a pyproject.toml file to extract project name, description, and dependencies. It first cleans the content and checks if the `tomllib` module is available. If `tomllib` is present, it attempts to load and parse the TOML content, updating the `self.info` dictionary with relevant data from the `[project]` section. It includes error handling for `TOMLDecodeError`."
              parameters[1]{name,type,description}:
                inhalt,str,The string content of the pyproject.toml file.
              returns[0]:
              usage_context:
                calls: "This method calls self._clean_content, tomllib.loads, and data.get."
                called_by: This method is called by extrahiere_info.
            error: null
          - identifier: _parse_requirements
            description:
              overall: "This private method parses the content of a requirements.txt file to extract project dependencies. It first cleans the content. It only updates the `self.info` dictionary's `dependencies` field if it hasn't already been populated from a pyproject.toml file. It processes each line, filtering out empty lines and comments, and stores the cleaned dependency strings."
              parameters[1]{name,type,description}:
                inhalt,str,The string content of the requirements.txt file.
              returns[0]:
              usage_context:
                calls: This method calls self._clean_content.
                called_by: This method is called by extrahiere_info.
            error: null
          - identifier: extrahiere_info
            description:
              overall: "This is the main public method that orchestrates the extraction of project information from various files. It first identifies relevant files (README, pyproject.toml, requirements.txt) using `_finde_datei`. It then parses these files in a prioritized order (TOML, then requirements, then README) to populate the `self.info` dictionary. Finally, it formats the extracted dependencies and attempts to derive a project title from the repository URL if no title was found elsewhere, returning the complete `self.info` dictionary."
              parameters[2]{name,type,description}:
                dateien,"List[Any]","A list of file-like objects, each containing 'path' and 'content' attributes."
                repo_url,str,"The URL of the repository, used as a fallback for the project title."
              returns[1]{name,type,description}:
                "","Dict[str, Any]",A dictionary containing all extracted project information.
              usage_context:
                calls: "This method calls self._finde_datei, self._parse_toml, self._parse_requirements, self._parse_readme, os.path.basename, and repo_url.removesuffix."
                called_by: This method is not explicitly called by other methods within the provided context.
            error: null
        usage_context:
          dependencies: "The class depends on the 're' module for regular expression operations, the 'os' module for path manipulation, and optionally 'tomllib' for parsing TOML files."
          instantiated_by: This class is not explicitly instantiated by any known components within the provided context.
      error: null
    backend.callgraph.CallGraph:
      identifier: backend.callgraph.CallGraph
      description:
        overall: "The CallGraph class is an AST NodeVisitor designed to construct a call graph for a given Python source file. It traverses the Abstract Syntax Tree (AST) of a file, identifying function definitions, class definitions, import statements, and function calls. The class maintains state about the current file, function, and class to correctly resolve fully qualified names for functions and methods, ultimately building a directed graph of calls."
        init_method:
          description: "The constructor initializes the CallGraph instance with the filename being analyzed and sets up various internal data structures. It prepares attributes to track the current function and class context, store local definitions, manage import mappings, and hold the NetworkX directed graph and a set of function identifiers."
          parameters[1]{name,type,description}:
            filename,str,"The path to the source file being analyzed, used for creating fully qualified names."
        methods[11]:
          - identifier: _recursive_call
            description:
              overall: "This private helper method recursively extracts the components of a name from an AST node, typically representing a function call or an attribute access. It handles `ast.Call`, `ast.Name`, and `ast.Attribute` nodes to build a list of string components that form the dotted path of the name. This is crucial for resolving the full name of a called entity."
              parameters[1]{name,type,description}:
                node,ast.AST,"The AST node to be processed, which can be a Call, Name, or Attribute node."
              returns[1]{name,type,description}:
                parts,"list[str]","A list of string components representing the dotted name, e.g., ['pkg', 'mod', 'Class', 'method']."
              usage_context:
                calls: This method recursively calls itself to traverse the AST node structure.
                called_by: This method is called by the `_resolve_all_callee_names` and `visit_Call` methods.
            error: null
          - identifier: _resolve_all_callee_names
            description:
              overall: "This private method takes a list of potential callee name components and resolves them into fully qualified names. It prioritizes resolution by checking local definitions first, then import mappings, and finally constructs a name based on the current class and filename if no explicit mapping is found. This ensures that calls are correctly linked to their definitions within the context of the file."
              parameters[1]{name,type,description}:
                callee_nodes,"list[list[str]]","A list of lists, where each inner list contains string components of a potential callee's name."
              returns[1]{name,type,description}:
                resolved,"list[str]",A list of fully qualified names (strings) for the resolved callees.
              usage_context:
                calls: "This method accesses instance attributes such as `self.local_defs`, `self.import_mapping`, `self.current_class`, and `self.filename` to resolve callee names."
                called_by: This method is called by the `visit_Call` method.
            error: null
          - identifier: _make_full_name
            description:
              overall: "This private utility method constructs a fully qualified name for a function or method. It combines the filename, an optional class name, and the base name of the entity. This standardization is essential for uniquely identifying functions and methods across the project and within the call graph."
              parameters[2]{name,type,description}:
                basename,str,The base name of the function or method.
                class_name,str | None,"The name of the enclosing class, if the entity is a method. Defaults to None."
              returns[1]{name,type,description}:
                full_name,str,The fully qualified name of the function or method.
              usage_context:
                calls: This method accesses the `self.filename` attribute to construct the full name.
                called_by: This method is called by the `visit_FunctionDef` method.
            error: null
          - identifier: _current_caller
            description:
              overall: "This private method determines the identifier of the current calling context. If a function is currently being visited, its full name is returned. Otherwise, it defaults to a generic scope identifier based on the filename or a global scope placeholder. This helps in attributing calls to their correct source."
              parameters[0]:
              returns[1]{name,type,description}:
                caller_identifier,str,"A string representing the identifier of the current caller, either a function's full name or a scope indicator."
              usage_context:
                calls: This method accesses the `self.current_function` and `self.filename` attributes to determine the caller.
                called_by: This method is called by the `visit_Call` method.
            error: null
          - identifier: visit_Import
            description:
              overall: This method is an AST visitor for `ast.Import` nodes. It processes top-level import statements to build a mapping from the imported module's alias (or its original name) to its actual module name. This mapping is crucial for resolving fully qualified names of imported functions or classes later in the analysis.
              parameters[1]{name,type,description}:
                node,ast.Import,The AST node representing an 'import' statement.
              returns[0]:
              usage_context:
                calls: This method calls `self.generic_visit(node)` to ensure traversal of child nodes.
                called_by: This method is implicitly called by the `ast.NodeVisitor` mechanism when an `ast.Import` node is encountered.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method is an AST visitor for `ast.ImportFrom` nodes. It processes 'from ... import ...' statements, extracting the module name and mapping aliases (or original names) to the imported module. This helps in resolving names that are imported directly from specific modules, contributing to the overall import resolution logic."
              parameters[1]{name,type,description}:
                node,ast.ImportFrom,The AST node representing a 'from ... import ...' statement.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other methods but processes `node.module` and `node.names` attributes.
                called_by: This method is implicitly called by the `ast.NodeVisitor` mechanism when an `ast.ImportFrom` node is encountered.
            error: null
          - identifier: visit_ClassDef
            description:
              overall: "This method is an AST visitor for `ast.ClassDef` nodes. It manages the `current_class` context during AST traversal, setting it to the name of the class being visited and restoring the previous context upon exiting the class definition. This ensures that methods defined within a class are correctly associated with their enclosing class."
              parameters[1]{name,type,description}:
                node,ast.ClassDef,The AST node representing a class definition.
              returns[0]:
              usage_context:
                calls: This method calls `self.generic_visit(node)` to continue the AST traversal within the class definition.
                called_by: This method is implicitly called by the `ast.NodeVisitor` mechanism when an `ast.ClassDef` node is encountered.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method is an AST visitor for `ast.FunctionDef` nodes. It records function definitions, generating a fully qualified name for the function (including class context if applicable) and storing it in `local_defs`. It also adds the function as a node to the NetworkX graph and updates the `current_function` context for subsequent call analysis. After visiting the function's body, it restores the previous function context."
              parameters[1]{name,type,description}:
                node,ast.FunctionDef,The AST node representing a function definition.
              returns[0]:
              usage_context:
                calls: "This method calls `self._make_full_name` to construct the function's identifier, `self.graph.add_node` to add it to the call graph, and `self.generic_visit(node)` for recursive traversal."
                called_by: "This method is implicitly called by the `ast.NodeVisitor` mechanism when an `ast.FunctionDef` node is encountered, and explicitly by `visit_AsyncFunctionDef`."
            error: null
          - identifier: visit_AsyncFunctionDef
            description:
              overall: This method is an AST visitor for `ast.AsyncFunctionDef` nodes. It handles asynchronous function definitions by simply delegating the processing to the `visit_FunctionDef` method. This ensures that both synchronous and asynchronous functions are treated similarly for call graph construction purposes.
              parameters[1]{name,type,description}:
                node,ast.AsyncFunctionDef,The AST node representing an asynchronous function definition.
              returns[0]:
              usage_context:
                calls: This method calls `self.visit_FunctionDef(node)` to process the asynchronous function definition.
                called_by: This method is implicitly called by the `ast.NodeVisitor` mechanism when an `ast.AsyncFunctionDef` node is encountered.
            error: null
          - identifier: visit_Call
            description:
              overall: "This method is an AST visitor for `ast.Call` nodes. It identifies function calls within the code. It first determines the current caller, then extracts and resolves the callee's full name using helper methods. Finally, it records an edge in the `self.edges` dictionary from the caller to the resolved callee, effectively building the call graph."
              parameters[1]{name,type,description}:
                node,ast.Call,The AST node representing a function call.
              returns[0]:
              usage_context:
                calls: "This method calls `self._current_caller`, `self._recursive_call`, `self._resolve_all_callee_names`, and `self.generic_visit(node)` to process the call node."
                called_by: This method is implicitly called by the `ast.NodeVisitor` mechanism when an `ast.Call` node is encountered.
            error: null
          - identifier: visit_If
            description:
              overall: "This method is an AST visitor for `ast.If` nodes. It specifically handles the common `if __name__ == \"__main__\"` block by temporarily setting the `current_function` to a special `<main_block>` identifier. This allows calls within the main execution block to be attributed correctly in the call graph, distinguishing them from calls within defined functions."
              parameters[1]{name,type,description}:
                node,ast.If,The AST node representing an 'if' statement.
              returns[0]:
              usage_context:
                calls: This method calls `self.generic_visit(node)` to continue the AST traversal within the 'if' statement's body.
                called_by: This method is implicitly called by the `ast.NodeVisitor` mechanism when an `ast.If` node is encountered.
            error: null
        usage_context:
          dependencies: "This class depends on `ast` for AST traversal, `networkx` for graph manipulation, and `typing.Dict` for type hinting. Other imports like `os`, `pathlib.Path`, `getRepo.GitRepository`, and `basic_info.ProjektInfoExtractor` are present in the file but not directly used within the class's methods."
          instantiated_by: The instantiation points for this class are not specified in the provided context.
      error: null
    backend.getRepo.RepoFile:
      identifier: backend.getRepo.RepoFile
      description:
        overall: "The RepoFile class represents a single file within a Git repository, designed for efficient, lazy loading of its content and metadata. It encapsulates the file's path and its associated Git tree object, providing properties to access the file's Git blob, decoded content, and size only when they are first requested. Additionally, it includes utility methods for basic content analysis and converting the file's data into a dictionary format."
        init_method:
          description: "The __init__ method initializes a RepoFile instance by storing the file's path and the Git commit_tree object. It sets up internal attributes such as `_blob`, `_content`, and `_size` to None, indicating that these properties will be loaded lazily upon their first access."
          parameters[2]{name,type,description}:
            file_path,str,The path to the file within the repository.
            commit_tree,git.Tree,The Tree object of the commit from which the file originates.
        methods[6]:
          - identifier: blob
            description:
              overall: "This property provides lazy loading of the Git blob object associated with the file. It checks if the `_blob` attribute is already loaded; if not, it attempts to retrieve the blob from the `_tree` using the file path. If the file is not found in the commit tree, a `FileNotFoundError` is raised."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",git.Blob,The Git blob object representing the file.
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions."
                called_by: This method is not explicitly called by any other functions or methods.
            error: null
          - identifier: content
            description:
              overall: "This property provides lazy loading of the file's content. It first checks if the `_content` attribute is already loaded. If not, it accesses the file's blob via the `blob` property, reads its data stream, decodes it using UTF-8 (ignoring errors), and stores the result before returning it."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",str,The decoded content of the file as a string.
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions."
                called_by: This method is not explicitly called by any other functions or methods.
            error: null
          - identifier: size
            description:
              overall: "This property provides lazy loading of the file's size in bytes. It checks if the `_size` attribute is already loaded. If not, it retrieves the size from the file's blob object, accessed via the `blob` property, and stores it before returning."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",int,The size of the file in bytes.
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions."
                called_by: This method is not explicitly called by any other functions or methods.
            error: null
          - identifier: analyze_word_count
            description:
              overall: "This method serves as an example analysis, calculating the number of words present in the file's content. It accesses the file's content via the `content` property, splits the string into words based on whitespace, and returns the count of these words."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",int,The total number of words in the file content.
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions."
                called_by: This method is not explicitly called by any other functions or methods.
            error: null
          - identifier: __repr__
            description:
              overall: "This method provides a useful string representation of the RepoFile object, primarily showing its path. This representation is helpful for debugging and logging purposes, offering a concise summary of the object's identity."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",str,"A string representation of the RepoFile object, including its path."
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions."
                called_by: This method is not explicitly called by any other functions or methods.
            error: null
          - identifier: to_dict
            description:
              overall: "This method converts the RepoFile object into a dictionary representation, providing structured access to its metadata. It includes the file's path, name (basename), size, and type. Optionally, if `include_content` is set to True, the file's content is also added to the dictionary."
              parameters[1]{name,type,description}:
                include_content,bool,"If True, the file's content will be included in the dictionary. Defaults to False."
              returns[1]{name,type,description}:
                data,dict,A dictionary containing file metadata and optionally its content.
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions."
                called_by: This method is not explicitly called by any other functions or methods.
            error: null
        usage_context:
          dependencies: This class does not explicitly depend on any external components based on the provided context.
          instantiated_by: This class is not explicitly instantiated by any other components based on the provided context.
      error: null
    backend.getRepo.GitRepository:
      identifier: backend.getRepo.GitRepository
      description:
        overall: "The GitRepository class provides a robust abstraction for managing a Git repository. It handles the cloning of a remote repository into a temporary local directory, ensures proper cleanup of this directory using a context manager, and offers functionalities to retrieve and structure the repository's files. It can list all files as RepoFile objects and represent the repository's structure as a hierarchical dictionary tree, optionally including file content."
        init_method:
          description: "The constructor initializes a GitRepository instance by cloning the specified remote Git repository into a newly created temporary directory. It sets up essential attributes like the repository URL, the path to the temporary directory, the GitPython Repo object, and the latest commit details. Error handling is included to manage cloning failures."
          parameters[1]{name,type,description}:
            repo_url,str,The URL of the Git repository to be cloned.
        methods[5]:
          - identifier: get_all_files
            description:
              overall: "This method is responsible for retrieving a comprehensive list of all files within the cloned Git repository. It leverages the underlying Git command `ls-files` to obtain file paths, then processes each path to instantiate a RepoFile object. These RepoFile instances are stored internally and returned as a list."
              parameters[0]:
              returns[1]{name,type,description}:
                files,"list[RepoFile]","A list of RepoFile instances, each representing a file found in the repository."
              usage_context:
                calls: This method calls self.repo.git.ls_files() to list files and instantiates RepoFile objects.
                called_by: This method is called by get_file_tree.
            error: null
          - identifier: close
            description:
              overall: "This method handles the cleanup process for the GitRepository instance. Its primary function is to delete the temporary directory and all its contents, where the Git repository was initially cloned. It performs a check to ensure the temporary directory path exists before attempting removal and then nullifies the path."
              parameters[0]:
              returns[0]:
              usage_context:
                calls: This method calls print() for logging purposes.
                called_by: This method is called by __init__ (in case of cloning error) and __exit__.
            error: null
          - identifier: __enter__
            description:
              overall: "This special method enables the GitRepository class to function as a context manager. When the object is used in a 'with' statement, this method is implicitly called, and it simply returns the instance of the object itself, making it available for use within the context block."
              parameters[0]:
              returns[1]{name,type,description}:
                self,GitRepository,"The instance of the GitRepository class, allowing it to be used within a 'with' statement."
              usage_context:
                calls: This method does not explicitly call other functions or methods.
                called_by: This method is implicitly called when the GitRepository object is used in a 'with' statement.
            error: null
          - identifier: __exit__
            description:
              overall: "This special method is part of the context manager protocol, ensuring proper resource management. It is automatically invoked when exiting a 'with' statement, regardless of whether an exception occurred. Its primary role is to call the 'close' method, guaranteeing that the temporary repository directory is cleaned up."
              parameters[3]{name,type,description}:
                exc_type,type | None,"The type of the exception that caused the context to be exited, or None if no exception occurred."
                exc_val,Exception | None,"The exception instance, or None."
                exc_tb,TracebackType | None,"The traceback object, or None."
              returns[0]:
              usage_context:
                calls: This method calls self.close() to clean up resources.
                called_by: This method is implicitly called when exiting a 'with' statement where the GitRepository object is used.
            error: null
          - identifier: get_file_tree
            description:
              overall: "This method generates a hierarchical dictionary representation of the repository's file structure. It first ensures that all files are loaded by calling `get_all_files()` if `self.files` is empty. It then iterates through the `RepoFile` objects, parsing their paths to construct a nested dictionary where directories and files are organized, optionally including the content of each file."
              parameters[1]{name,type,description}:
                include_content,bool,A boolean flag indicating whether the content of each file should be included in its dictionary representation. Defaults to False.
              returns[1]{name,type,description}:
                tree,dict,"A dictionary representing the file tree, with 'name', 'type', and 'children' keys for directories, and file-specific data for files."
              usage_context:
                calls: This method calls self.get_all_files() if self.files is empty and file_obj.to_dict() for each file.
                called_by: This method is not explicitly called by other methods in the provided class definition.
            error: null
        usage_context:
          dependencies: "This class depends on tempfile for temporary directory management, git.Repo and git.GitCommandError from the GitPython library for Git operations, logging for informational messages, and implicitly on a RepoFile class for file representation."
          instantiated_by: This class is not explicitly instantiated by any known components based on the provided context.
      error: null
    backend.relationship_analyzer.ProjectAnalyzer:
      identifier: backend.relationship_analyzer.ProjectAnalyzer
      description:
        overall: "The ProjectAnalyzer class is designed to perform a comprehensive static analysis of a Python project to build a detailed call graph. It identifies all Python files within a specified root directory, collects definitions for functions, methods, and classes, and then resolves the calls between these defined entities. The class provides methods to initiate the analysis process and to retrieve the raw relationships in a structured format, effectively mapping out the functional dependencies within the codebase."
        init_method:
          description: "The constructor initializes the ProjectAnalyzer instance by setting up the project's root directory and various internal data structures. It creates dictionaries to store definitions, a defaultdict for the call graph, a dictionary for file ASTs, and a set of directories to ignore during file traversal. This setup prepares the analyzer for subsequent processing steps."
          parameters[1]{name,type,description}:
            project_root,string,The absolute path to the root directory of the project to be analyzed.
        methods[6]:
          - identifier: analyze
            description:
              overall: "This method orchestrates the entire project analysis workflow. It first identifies all Python files in the project, then iterates through them to collect all function, method, and class definitions. Following definition collection, it performs a second iteration to resolve calls within each file, populating the internal call graph. Finally, it clears the stored file ASTs to free up memory and returns the completed call graph."
              parameters[0]:
              returns[1]{name,type,description}:
                call_graph,"defaultdict[list]","A dictionary-like object mapping callee identifiers to a list of caller information, representing the resolved call relationships."
              usage_context:
                calls: "This method calls internal methods such as _find_py_files to locate files, _collect_definitions to gather definitions, and _resolve_calls to identify function and method invocations."
                called_by: This method is not explicitly called by other methods within the provided context.
            error: null
          - identifier: get_raw_relationships
            description:
              overall: "This method processes the internal call graph to generate a structured representation of incoming and outgoing relationships. It iterates through the collected call graph, populating two defaultdicts: one for outgoing calls from a caller to a callee, and another for incoming calls to a callee from a caller. The final output is a dictionary containing these relationships, with the lists of related entities sorted for consistency."
              parameters[0]:
              returns[1]{name,type,description}:
                relationships,dict,"A dictionary containing 'outgoing' and 'incoming' keys, each mapping to a dictionary where keys are entity identifiers and values are sorted lists of related entity identifiers."
              usage_context:
                calls: "This method primarily uses built-in Python functions like list and sorted, along with defaultdict for data structuring."
                called_by: This method is not explicitly called by other methods within the provided context.
            error: null
          - identifier: _find_py_files
            description:
              overall: "This private helper method is responsible for recursively traversing the project directory to locate all Python source files. It utilizes `os.walk` to navigate the file system, filtering out directories specified in `self.ignore_dirs` to avoid analyzing irrelevant paths. For each directory, it checks files for the '.py' extension and compiles a list of their absolute paths."
              parameters[0]:
              returns[1]{name,type,description}:
                py_files,"list[str]","A list of absolute file paths to all Python files found within the project root, excluding ignored directories."
              usage_context:
                calls: "This method calls functions from the 'os' module, specifically os.walk and os.path.join, to navigate the file system and construct file paths."
                called_by: This method is called by the 'analyze' method to get the list of files to process.
            error: null
          - identifier: _collect_definitions
            description:
              overall: "This private method parses a given Python file to identify and store definitions of functions, methods, and classes. It reads the file's source code, parses it into an Abstract Syntax Tree (AST), and stores this AST for later use. By walking the AST, it identifies `FunctionDef` and `ClassDef` nodes, determines their fully qualified path names and types (function, method, or class), and stores this information in `self.definitions`. Error handling is included to log issues during file processing."
              parameters[1]{name,type,description}:
                filepath,str,The absolute path to the Python file to be analyzed for definitions.
              returns[0]:
              usage_context:
                calls: "This method calls built-in functions like open, external functions like ast.parse and path_to_module, and the internal _get_parent method. It also uses ast.walk, ast.FunctionDef, ast.ClassDef, and logging.error."
                called_by: This method is called by the 'analyze' method for each Python file found in the project.
            error: null
          - identifier: _get_parent
            description:
              overall: "This private helper method traverses an Abstract Syntax Tree (AST) to find the immediate parent node of a specified child node. It iterates through all nodes in the provided AST and then checks each node's children. If a child matches the target node, the current node is returned as its parent. If no parent is found after checking all nodes, it returns None."
              parameters[2]{name,type,description}:
                tree,ast.AST,The root of the Abstract Syntax Tree to search within.
                node,ast.AST,The child node for which to find the parent.
              returns[1]{name,type,description}:
                parent_node,ast.AST | None,"The parent AST node of the given child node, or None if no parent is found."
              usage_context:
                calls: "This method utilizes functions from the 'ast' module, specifically ast.walk and ast.iter_child_nodes, to navigate the AST structure."
                called_by: This method is called by the '_collect_definitions' method to determine the parent of a function or class definition.
            error: null
          - identifier: _resolve_calls
            description:
              overall: "This private method is responsible for resolving function and method calls within a specific Python file's Abstract Syntax Tree (AST). It retrieves the pre-parsed AST for the given filepath and, if available, initializes a `CallResolverVisitor` with the file's context and known definitions. The visitor then traverses the AST to identify calls, and the collected call information is used to extend the `self.call_graph`. Errors encountered during call resolution are logged."
              parameters[1]{name,type,description}:
                filepath,str,The absolute path to the Python file whose calls are to be resolved.
              returns[0]:
              usage_context:
                calls: This method interacts with the 'CallResolverVisitor' class (an external dependency) and its 'visit' method. It also uses 'self.file_asts.get' and 'logging.error'.
                called_by: This method is called by the 'analyze' method for each Python file to resolve calls within it.
            error: null
        usage_context:
          dependencies: "The class depends on external modules such as 'os' for file system operations, 'ast' for parsing Python code into Abstract Syntax Trees, 'logging' for error reporting, and 'collections.defaultdict' for managing dynamic dictionaries. It also implicitly relies on 'path_to_module' and 'CallResolverVisitor' which are not defined within the provided source but are used."
          instantiated_by: This class is not explicitly instantiated by any other components within the provided context.
      error: null
    backend.relationship_analyzer.CallResolverVisitor:
      identifier: backend.relationship_analyzer.CallResolverVisitor
      description:
        overall: "The CallResolverVisitor class is an AST NodeVisitor designed to traverse the Abstract Syntax Tree of a Python file to identify and resolve function and method calls. It maintains state about the current module, class, and function being visited, along with import scopes and instance types, to accurately determine the fully qualified name of each called entity. The primary goal is to collect a comprehensive record of which functions or methods call others within the analyzed source code."
        init_method:
          description: "The constructor initializes the visitor with the file path, project root, and a dictionary of known definitions. It sets up internal state variables such as `module_path`, `scope` for imports, `instance_types` for tracking object types, and `calls` (a defaultdict) to store the resolved call relationships."
          parameters[3]{name,type,description}:
            filepath,str,The path to the Python file being analyzed.
            project_root,str,"The root directory of the project, used to determine the module path."
            definitions,dict,A dictionary containing known fully qualified definitions within the project.
        methods[7]:
          - identifier: visit_ClassDef
            description:
              overall: "This method is invoked when the AST visitor encounters a class definition. It updates the `current_class_name` attribute to reflect the class currently being processed, allowing nested methods to correctly form their fully qualified names. After visiting the class's children, it restores the previous `current_class_name` to maintain correct scope."
              parameters[1]{name,type,description}:
                node,ast.ClassDef,The AST node representing the class definition.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions according to the provided context."
                called_by: This method is not explicitly called by any other components listed in the provided context.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method is triggered upon encountering a function or method definition. It constructs the full qualified identifier for the current function/method, considering if it's nested within a class, and updates `current_caller_name`. This ensures that any calls made within this function are correctly attributed to it. The previous caller name is restored after visiting the function's body."
              parameters[1]{name,type,description}:
                node,ast.FunctionDef,The AST node representing the function or method definition.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions according to the provided context."
                called_by: This method is not explicitly called by any other components listed in the provided context.
            error: null
          - identifier: visit_Call
            description:
              overall: "This method processes function or method call expressions. It attempts to resolve the fully qualified name of the callee using `_resolve_call_qname`. If the callee is successfully resolved and found within the known definitions, it records the call, including the caller's file, line number, full identifier, and type (module, local function, method, or function), into the `self.calls` dictionary."
              parameters[1]{name,type,description}:
                node,ast.Call,The AST node representing a function or method call.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions according to the provided context."
                called_by: This method is not explicitly called by any other components listed in the provided context.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method handles `import` statements, such as `import module_name` or `import module_name as alias`. It populates the `self.scope` dictionary, mapping the imported name (or its alias) to its original module name. This scope information is crucial for resolving qualified names of calls made to imported modules."
              parameters[1]{name,type,description}:
                node,ast.Import,The AST node representing an import statement.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions according to the provided context."
                called_by: This method is not explicitly called by any other components listed in the provided context.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method processes `from ... import ...` statements. It determines the full module path for the imported names, considering relative imports (`node.level`). It then adds these fully qualified names to `self.scope`, mapping the imported name (or alias) to its complete path. This enables accurate resolution of calls to specific functions or classes imported from modules."
              parameters[1]{name,type,description}:
                node,ast.ImportFrom,The AST node representing a 'from ... import ...' statement.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions according to the provided context."
                called_by: This method is not explicitly called by any other components listed in the provided context.
            error: null
          - identifier: visit_Assign
            description:
              overall: "This method analyzes assignment statements to identify instances of class instantiation. If an assignment's value is a call to a class (e.g., `obj = MyClass()`), it resolves the qualified name of that class and stores it in `self.instance_types`, mapping the assigned variable name to its class's qualified name. This helps in resolving method calls on these instances later."
              parameters[1]{name,type,description}:
                node,ast.Assign,The AST node representing an assignment statement.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions according to the provided context."
                called_by: This method is not explicitly called by any other components listed in the provided context.
            error: null
          - identifier: _resolve_call_qname
            description:
              overall: "This helper method attempts to resolve the fully qualified name (QName) of a function or method based on its AST node. It first checks if the name is in the current `self.scope` (for imports), then if it's a local function/method within the current module. For attribute calls (e.g., `obj.method`), it uses `self.instance_types` to find the class of the object or `self.scope` for module-level attributes, constructing the full QName. If resolution fails, it returns `None`."
              parameters[1]{name,type,description}:
                func_node,ast.expr,"The AST node representing the function or method being called (e.g., `ast.Name` or `ast.Attribute`)."
              returns[1]{name,type,description}:
                "null",str | None,"The fully qualified name of the called entity, or `None` if it cannot be resolved."
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions according to the provided context."
                called_by: This method is not explicitly called by any other components listed in the provided context.
            error: null
        usage_context:
          dependencies: This class does not have explicit external functional dependencies listed in the input context.
          instantiated_by: This class is not explicitly instantiated by any other components listed in the input context.
      error: "null"
    schemas.types.ParameterDescription:
      identifier: schemas.types.ParameterDescription
      description:
        overall: "The `ParameterDescription` class is a Pydantic BaseModel designed to provide a structured representation for a single parameter of a function. It encapsulates essential information about a parameter, including its name, data type, and a descriptive explanation. This class serves as a data schema for documenting function parameters, ensuring consistency and ease of processing."
        init_method:
          description: "As a Pydantic BaseModel, the `__init__` method for `ParameterDescription` is implicitly generated. It handles the initialization and validation of the instance attributes based on the provided arguments, ensuring that `name`, `type`, and `description` are all strings."
          parameters[3]{name,type,description}:
            name,str,The name of the parameter.
            type,str,The data type of the parameter.
            description,str,A textual description of the parameter's purpose or usage.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external dependencies.
          instantiated_by: This class is not explicitly listed as being instantiated by any other components.
      error: null
    schemas.types.ReturnDescription:
      identifier: schemas.types.ReturnDescription
      description:
        overall: "The ReturnDescription class is a Pydantic BaseModel designed to standardize the description of a function's return value. It serves as a data structure to hold metadata about a return value, including its name, Python type, and a textual description. This class provides a clear and consistent schema for documenting function outputs."
        init_method:
          description: "As a Pydantic BaseModel, the `__init__` method is automatically generated to initialize instances of ReturnDescription. It accepts and validates three string parameters: `name`, `type`, and `description`, setting them as instance attributes."
          parameters[3]{name,type,description}:
            name,str,The name or identifier of the return value.
            type,str,The Python type hint or a descriptive string of the return value's type.
            description,str,A detailed explanation of what the return value represents or its purpose.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list external functional dependencies in the provided context.
          instantiated_by: The specific locations where this class is instantiated are not provided in the current context.
      error: null
    schemas.types.UsageContext:
      identifier: schemas.types.UsageContext
      description:
        overall: "The UsageContext class is a Pydantic BaseModel designed to encapsulate information about the calling context of a function or method. It provides a structured way to represent both the external entities that call a specific function and the internal entities that the function itself calls. This model is used to provide a clear, machine-readable summary of interaction points within a system."
        init_method:
          description: "This class, inheriting from Pydantic's BaseModel, automatically generates an `__init__` method. It initializes instances with two string attributes: `calls` and `called_by`, which describe the calling context of an entity."
          parameters[2]{name,type,description}:
            calls,str,"A string summarizing the functions, methods, or classes that the entity under analysis calls."
            called_by,str,"A string summarizing the functions, methods, or classes that call the entity under analysis."
        methods[0]:
        usage_context:
          dependencies: "This class does not explicitly list any external functional dependencies beyond its base class, Pydantic's BaseModel."
          instantiated_by: The instantiation points for this class are not explicitly provided in the current context.
      error: null
    schemas.types.FunctionDescription:
      identifier: schemas.types.FunctionDescription
      description:
        overall: "The FunctionDescription class is a Pydantic BaseModel designed to encapsulate a comprehensive analysis of a Python function. It structures information about a function's high-level purpose, its input parameters, its expected return values, and its operational context within a larger system. This model provides a standardized format for representing detailed function metadata."
        init_method:
          description: "The constructor for FunctionDescription is implicitly generated by Pydantic's BaseModel. It initializes an instance by validating and assigning values to its defined fields: 'overall', 'parameters', 'returns', and 'usage_context'. This ensures that all function analysis data conforms to the specified types upon object creation."
          parameters[4]{name,type,description}:
            overall,str,A high-level summary describing the function's purpose and its implementation details.
            parameters,"List[ParameterDescription]","A list of ParameterDescription objects, each detailing an input parameter of the function."
            returns,"List[ReturnDescription]","A list of ReturnDescription objects, each describing a possible return value of the function."
            usage_context,UsageContext,An object containing information about where the function is called and what other functions or methods it calls.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies.
          instantiated_by: The specific instantiation points for this class are not provided in the context.
      error: null
    schemas.types.FunctionAnalysis:
      identifier: schemas.types.FunctionAnalysis
      description:
        overall: "The FunctionAnalysis class is a Pydantic BaseModel designed to encapsulate a comprehensive analysis of a Python function. It serves as a structured data container, holding the function's unique identifier, a detailed description object, and an optional error message. This class is fundamental for representing the output of a function analysis process in a standardized, machine-readable format."
        init_method:
          description: "The FunctionAnalysis class, being a Pydantic BaseModel, automatically generates its `__init__` method. This constructor facilitates the creation of instances by accepting keyword arguments corresponding to its defined fields: `identifier`, `description`, and `error`. Pydantic handles validation and assignment of these values upon instantiation."
          parameters[3]{name,type,description}:
            identifier,str,A unique string identifying the function being analyzed.
            description,FunctionDescription,"An object containing the detailed description and analysis of the function, including its purpose, parameters, returns, and usage context."
            error,"Optional[str]",An optional string providing an error message if the function analysis encountered issues. Defaults to None.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies.
          instantiated_by: The specific points of instantiation for this class are not provided in the context.
      error: null
    schemas.types.ConstructorDescription:
      identifier: schemas.types.ConstructorDescription
      description:
        overall: "The `ConstructorDescription` class is a Pydantic BaseModel designed to encapsulate structured information about the `__init__` method of a Python class. It serves as a schema for representing the constructor's high-level description and a detailed list of its parameters, facilitating standardized data exchange and validation for constructor metadata."
        init_method:
          description: The `__init__` method for `ConstructorDescription` is implicitly generated by Pydantic. It initializes an instance of this model by validating and assigning the provided `description` string and a list of `ParameterDescription` objects to its corresponding attributes.
          parameters[2]{name,type,description}:
            description,str,A string providing a high-level summary or explanation of the constructor's purpose and behavior.
            parameters,"List[ParameterDescription]","A list of `ParameterDescription` objects, each detailing a specific parameter accepted by the constructor, including its name, type, and individual description."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies.
          instantiated_by: The specific points of instantiation for this class are not provided in the context.
      error: null
    schemas.types.ClassContext:
      identifier: schemas.types.ClassContext
      description:
        overall: "The ClassContext class is a Pydantic BaseModel designed to encapsulate metadata about a Python class's operational context. It specifically tracks external dependencies and the locations where the class is instantiated. This model serves as a structured data container for contextual information, facilitating a clear understanding of a class's integration and usage within a larger system."
        init_method:
          description: This class does not explicitly define an `__init__` method. Pydantic's `BaseModel` automatically generates a constructor that initializes `dependencies` and `instantiated_by` based on the provided arguments during instantiation.
          parameters[2]{name,type,description}:
            dependencies,str,A string summarizing the external dependencies of the class being described.
            instantiated_by,str,A string summarizing the primary points or locations where the class being described is instantiated.
        methods[0]:
        usage_context:
          dependencies: "This class does not explicitly declare external functional dependencies within its own definition, as it primarily serves as a data structure."
          instantiated_by: "The instantiation points for this class are not specified in the provided context, suggesting it could be instantiated in various parts of the system where class context information is needed."
      error: null
    schemas.types.ClassDescription:
      identifier: schemas.types.ClassDescription
      description:
        overall: "The ClassDescription class is a Pydantic BaseModel designed to encapsulate a comprehensive analysis of another Python class. It serves as a structured data container, holding an overall summary, detailed information about the class's constructor, a list of analyses for each of its methods, and context regarding its dependencies and instantiation points. Its primary role is to provide a standardized, machine-readable format for describing the structure and functionality of a Python class within a larger system."
        init_method:
          description: "As a Pydantic BaseModel, the __init__ method for ClassDescription is implicitly generated by Pydantic. It initializes instances of ClassDescription by accepting values for its defined fields: overall, init_method, methods, and usage_context. This constructor ensures that instances are created with all necessary analytical components, adhering to the specified types."
          parameters[4]{name,type,description}:
            overall,str,A high-level summary describing the purpose and role of the analyzed class.
            init_method,ConstructorDescription,An object containing detailed analysis of the analyzed class's constructor (__init__ method).
            methods,"List[FunctionAnalysis]","A list of FunctionAnalysis objects, each detailing an individual method within the analyzed class."
            usage_context,ClassContext,An object providing context about the analyzed class's external dependencies and where it is instantiated.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external dependencies.
          instantiated_by: Information about where this class is instantiated is not available.
      error: null
    schemas.types.ClassAnalysis:
      identifier: schemas.types.ClassAnalysis
      description:
        overall: "The ClassAnalysis class serves as the top-level data model for representing a comprehensive analysis of a Python class. It is a Pydantic BaseModel, ensuring data validation and structured storage. This class encapsulates the class's unique identifier, a detailed ClassDescription object containing constructor and method analyses, and an optional field to report any errors encountered during the analysis process."
        init_method:
          description: "This class, being a Pydantic BaseModel, does not explicitly define an __init__ method. Initialization is handled automatically by Pydantic, which validates and assigns values to its fields based on the provided arguments, ensuring type correctness and default value assignment."
          parameters[3]{name,type,description}:
            identifier,str,A unique string identifier for the class being analyzed.
            description,ClassDescription,"An object containing the detailed analysis of the class, including its constructor, methods, and overall purpose."
            error,"Optional[str]","An optional string detailing any errors encountered during the class analysis process, defaulting to None."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies.
          instantiated_by: The instantiation points for this class are not explicitly provided in the context.
      error: null
    schemas.types.CallInfo:
      identifier: schemas.types.CallInfo
      description:
        overall: "The CallInfo class is a Pydantic BaseModel designed to structure and represent a specific call event identified by a relationship analyzer. It encapsulates key details about a function or method call, including the file path, the name of the calling function, the mode of the call (e.g., method, function, module), and the line number where the call occurs. This model is intended for use in lists tracking where a component is 'called by' or 'instantiated by'."
        init_method:
          description: "This class, being a Pydantic BaseModel, automatically generates its `__init__` method. The constructor initializes an instance of `CallInfo` by accepting values for its defined fields: `file`, `function`, `mode`, and `line`. Pydantic handles validation and assignment of these attributes upon instantiation."
          parameters[4]{name,type,description}:
            file,str,The path to the file where the call event occurred.
            function,str,The name of the function or method that made the call.
            mode,str,"The type of call, e.g., 'method', 'function', 'module'."
            line,int,The line number in the file where the call event occurred.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly declare any external functional dependencies within its definition.
          instantiated_by: The provided context does not specify any explicit locations where this class is instantiated.
      error: null
    schemas.types.FunctionContextInput:
      identifier: schemas.types.FunctionContextInput
      description:
        overall: "The FunctionContextInput class is a Pydantic BaseModel designed to encapsulate the contextual information required for analyzing a function. It provides a structured schema to define a function's outbound calls and its inbound callers. This class is primarily used for data validation and serialization, ensuring that function context data adheres to a predefined format."
        init_method:
          description: "This class, being a Pydantic BaseModel, automatically generates an __init__ method. This constructor initializes instances of FunctionContextInput by accepting values for `calls` and `called_by`, ensuring they conform to their specified types and facilitating data validation."
          parameters[2]{name,type,description}:
            calls,"List[str]","A list of strings, where each string represents the identifier of another function, method, or class that this function calls."
            called_by,"List[CallInfo]","A list of CallInfo objects, each detailing a specific instance or location where this function is called from."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in the provided context.
          instantiated_by: The provided context does not specify where this class is instantiated.
      error: null
    schemas.types.FunctionAnalysisInput:
      identifier: schemas.types.FunctionAnalysisInput
      description:
        overall: "The FunctionAnalysisInput class is a Pydantic BaseModel designed to define the structured input required for generating a FunctionAnalysis object. It acts as a data schema, ensuring that all necessary components like the operation mode, function identifier, source code, relevant imports, and contextual information are present and correctly typed before a function analysis can proceed. This class is crucial for standardizing the input payload for an AI system performing code analysis."
        init_method:
          description: "The `__init__` method for FunctionAnalysisInput is implicitly generated by Pydantic's BaseModel. It handles the instantiation of the class by validating and assigning the provided arguments to its corresponding attributes: `mode`, `identifier`, `source_code`, `imports`, and `context`. This ensures that all input data conforms to the defined types and constraints."
          parameters[5]{name,type,description}:
            mode,"Literal[\"function_analysis\"]","Specifies the operational mode, which is fixed to 'function_analysis' for this input schema."
            identifier,str,The unique name or identifier of the function that is to be analyzed.
            source_code,str,The raw Python source code of the function targeted for analysis.
            imports,"List[str]",A list of import statements that are relevant to the function's execution context.
            context,FunctionContextInput,"Additional contextual information, such as dependencies or call relationships, relevant to the function."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list external functional dependencies within the provided context.
          instantiated_by: The instantiation points for this class are not specified in the provided context.
      error: null
    schemas.types.MethodContextInput:
      identifier: schemas.types.MethodContextInput
      description:
        overall: "The `MethodContextInput` class is a Pydantic BaseModel designed to structure and store contextual information about a method. It serves as a data container, defining fields such as the method's identifier, the functions it calls, the functions that call it, its arguments, and its docstring. This class provides a standardized format for representing method-level metadata within a larger system."
        init_method:
          description: "The `__init__` method, implicitly generated by Pydantic's BaseModel, initializes an instance of `MethodContextInput` by validating and assigning values to its defined fields: `identifier`, `calls`, `called_by`, `args`, and `docstring`. It ensures that the provided data conforms to the specified types for each attribute."
          parameters[5]{name,type,description}:
            identifier,str,A unique string identifier for the method.
            calls,"List[str]","A list of string identifiers for other methods, classes, or functions that this method calls."
            called_by,"List[CallInfo]",A list of `CallInfo` objects representing other functions or methods that call this method.
            args,"List[str]",A list of strings representing the arguments of the method.
            docstring,"Optional[str]",An optional string containing the method's docstring.
        methods[0]:
        usage_context:
          dependencies: "This class does not explicitly list any external functional dependencies beyond its base class, `pydantic.BaseModel`."
          instantiated_by: There is no explicit information provided about where this class is instantiated within the system.
      error: null
    schemas.types.ClassContextInput:
      identifier: schemas.types.ClassContextInput
      description:
        overall: "The `ClassContextInput` class is a Pydantic BaseModel designed to encapsulate structured contextual information necessary for analyzing a Python class. It serves as a data schema to organize details such as external dependencies, where the class is instantiated, and specific context for each of its methods. This model facilitates a comprehensive, structured input for further automated analysis processes."
        init_method:
          description: "The `__init__` method for `ClassContextInput` is implicitly generated by Pydantic's BaseModel. It initializes the instance attributes `dependencies`, `instantiated_by`, and `method_context` based on the arguments provided during object creation, ensuring type validation and data integrity according to the defined schema."
          parameters[3]{name,type,description}:
            dependencies,"List[str]",A list of strings representing external dependencies that the class relies upon.
            instantiated_by,"List[CallInfo]",A list of `CallInfo` objects detailing the locations or components that instantiate this class.
            method_context,"List[MethodContextInput]","A list of `MethodContextInput` objects, each providing specific contextual information for a method within the class being analyzed."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly declare external functional dependencies within the provided context.
          instantiated_by: This class is not explicitly shown to be instantiated by other components within the provided context.
      error: null
    ClassAnalysisInput:
      identifier: ClassAnalysisInput
      description:
        overall: "The ClassAnalysisInput class is a Pydantic BaseModel designed to define the structured input required for generating a ClassAnalysis object. It specifies the necessary components, such as the mode of operation, the class identifier, its source code, import statements, and additional contextual information, ensuring data integrity and type validation for the analysis process."
        init_method:
          description: "This class, being a Pydantic BaseModel, is implicitly initialized by accepting keyword arguments corresponding to its defined fields. Pydantic handles the validation, parsing, and assignment of these fields to instance attributes upon instantiation."
          parameters[5]{name,type,description}:
            mode,"Literal[\"class_analysis\"]","Specifies the operational mode, which is fixed to 'class_analysis' for this input type."
            identifier,str,The unique name or identifier of the class being analyzed.
            source_code,str,The complete raw source code of the class definition.
            imports,"List[str]",A list of import statements relevant to the class or its containing module.
            context,ClassContextInput,"Additional contextual information pertinent to the class, such as its dependencies or where it is instantiated."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies within its context.
          instantiated_by: This class is not explicitly noted as being instantiated by any other specific components within the provided context.
      error: null