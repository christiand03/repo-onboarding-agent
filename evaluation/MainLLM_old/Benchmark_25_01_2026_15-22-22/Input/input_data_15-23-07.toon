basic_info:
  projekt_uebersicht:
    titel: Repo Onboarding Agent üöÄ
    beschreibung: ```
    aktueller_status: Information not found
    key_features: Information not found
    tech_stack: Information not found
  installation:
    dependencies: "- altair==4.2.2\n- annotated-types==0.7.0\n- anyio==4.11.0\n- attrs==25.4.0\n- bcrypt==5.0.0\n- blinker==1.9.0\n- cachetools==6.2.2\n- captcha==0.7.1\n- certifi==2025.11.12\n- cffi==2.0.0\n- charset-normalizer==3.4.4\n- click==8.3.1\n- colorama==0.4.6\n- contourpy==1.3.3\n- cryptography==46.0.3\n- cycler==0.12.1\n- distro==1.9.0\n- dnspython==2.8.0\n- dotenv==0.9.9\n- entrypoints==0.4\n- extra-streamlit-components==0.1.81\n- filetype==1.2.0\n- fonttools==4.61.0\n- gitdb==4.0.12\n- GitPython==3.1.45\n- google-ai-generativelanguage==0.9.0\n- google-api-core==2.28.1\n- google-auth==2.43.0\n- googleapis-common-protos==1.72.0\n- grpcio==1.76.0\n- grpcio-status==1.76.0\n- h11==0.16.0\n- httpcore==1.0.9\n- httpx==0.28.1\n- idna==3.11\n- Jinja2==3.1.6\n- jiter==0.12.0\n- jsonpatch==1.33\n- jsonpointer==3.0.0\n- jsonschema==4.25.1\n- jsonschema-specifications==2025.9.1\n- kiwisolver==1.4.9\n- langchain==1.0.8\n- langchain-core==1.1.0\n- langchain-google-genai==3.1.0\n- langchain-ollama==1.0.0\n- langchain-openai==1.1.0\n- langgraph==1.0.3\n- langgraph-checkpoint==3.0.1\n- langgraph-prebuilt==1.0.5\n- langgraph-sdk==0.2.9\n- langsmith==0.4.46\n- MarkupSafe==3.0.3\n- matplotlib==3.10.7\n- narwhals==2.12.0\n- networkx==3.6\n- numpy==2.3.5\n- ollama==0.6.1\n- openai==2.8.1\n- orjson==3.11.4\n- ormsgpack==1.12.0\n- packaging==25.0\n- pandas==2.3.3\n- pillow==12.0.0\n- proto-plus==1.26.1\n- protobuf==6.33.1\n- pyarrow==21.0.0\n- pyasn1==0.6.1\n- pyasn1_modules==0.4.2\n- pycparser==2.23\n- pydantic==2.12.4\n- pydantic_core==2.41.5\n- pydeck==0.9.1\n- PyJWT==2.10.1\n- pymongo==4.15.4\n- pyparsing==3.2.5\n- python-dateutil==2.9.0.post0\n- python-dotenv==1.2.1\n- pytz==2025.2\n- PyYAML==6.0.3\n- referencing==0.37.0\n- regex==2025.11.3\n- requests==2.32.5\n- requests-toolbelt==1.0.0\n- rpds-py==0.29.0\n- rsa==4.9.1\n- setuptools==75.9.1\n- six==1.17.0\n- smmap==5.0.2\n- sniffio==1.3.1\n- streamlit==1.51.0\n- streamlit-authenticator==0.4.2\n- streamlit-mermaid==0.3.0\n- tenacity==9.1.2\n- tiktoken==0.12.0\n- toml==0.10.2\n- toolz==1.1.0\n- toon_format @ git+https://github.com/toon-format/toon-python.git@9c4f0c0c24f2a0b0b376315f4b8707f8c9006de6\n- tornado==6.5.2\n- tqdm==4.67.1\n- typing-inspection==0.4.2\n- typing_extensions==4.15.0\n- tzdata==2025.2\n- urllib3==2.5.0\n- watchdog==6.0.0\n- xxhash==3.6.0\n- zstandard==0.25.0\n- nbformat"
    setup_anleitung: Information not found
    quick_start_guide: Information not found
file_tree:
  name: root
  type: directory
  children[16]:
    - path: .env.example
      name: .env.example
      size: 48
      type: file
    - path: .gitignore
      name: .gitignore
      size: 184
      type: file
    - name: SystemPrompts
      type: directory
      children[6]{path,name,size,type}:
        SystemPrompts/SystemPromptClassHelperLLM.txt,SystemPromptClassHelperLLM.txt,6645,file
        SystemPrompts/SystemPromptFunctionHelperLLM.txt,SystemPromptFunctionHelperLLM.txt,5546,file
        SystemPrompts/SystemPromptHelperLLM.txt,SystemPromptHelperLLM.txt,9350,file
        SystemPrompts/SystemPromptMainLLM.txt,SystemPromptMainLLM.txt,8380,file
        SystemPrompts/SystemPromptMainLLMToon.txt,SystemPromptMainLLMToon.txt,8456,file
        SystemPrompts/SystemPromptNotebookLLM.txt,SystemPromptNotebookLLM.txt,6921,file
    - path: analysis_output.json
      name: analysis_output.json
      size: 569396
      type: file
    - name: backend
      type: directory
      children[12]{path,name,size,type}:
        backend/AST_Schema.py,AST_Schema.py,6622,file
        backend/File_Dependency.py,File_Dependency.py,7384,file
        backend/HelperLLM.py,HelperLLM.py,17280,file
        backend/MainLLM.py,MainLLM.py,4281,file
        backend/__init__.py,__init__.py,0,file
        backend/basic_info.py,basic_info.py,7284,file
        backend/callgraph.py,callgraph.py,9020,file
        backend/converter.py,converter.py,3594,file
        backend/getRepo.py,getRepo.py,4739,file
        backend/main.py,main.py,25591,file
        backend/relationship_analyzer.py,relationship_analyzer.py,8279,file
        backend/scads_key_test.py,scads_key_test.py,663,file
    - name: database
      type: directory
      children[1]{path,name,size,type}:
        database/db.py,db.py,8222,file
    - name: frontend
      type: directory
      children[4]:
        - name: .streamlit
          type: directory
          children[1]{path,name,size,type}:
            frontend/.streamlit/config.toml,config.toml,165,file
        - path: frontend/__init__.py
          name: __init__.py
          size: 0
          type: file
        - path: frontend/frontend.py
          name: frontend.py
          size: 31176
          type: file
        - name: gifs
          type: directory
          children[1]{path,name,size,type}:
            frontend/gifs/4j.gif,4j.gif,3306723,file
    - name: notizen
      type: directory
      children[8]:
        - path: notizen/Report Agenda.txt
          name: Report Agenda.txt
          size: 3492
          type: file
        - path: notizen/Zwischenpraesentation Agenda.txt
          name: Zwischenpraesentation Agenda.txt
          size: 809
          type: file
        - path: notizen/doc_bestandteile.md
          name: doc_bestandteile.md
          size: 847
          type: file
        - name: grafiken
          type: directory
          children[4]:
            - name: "1"
              type: directory
              children[5]{path,name,size,type}:
                notizen/grafiken/1/File_Dependency_Graph_Repo.dot,File_Dependency_Graph_Repo.dot,1227,file
                notizen/grafiken/1/global_callgraph.png,global_callgraph.png,940354,file
                notizen/grafiken/1/global_graph.png,global_graph.png,4756519,file
                notizen/grafiken/1/global_graph_2.png,global_graph_2.png,242251,file
                notizen/grafiken/1/repo.dot,repo.dot,13870,file
            - name: "2"
              type: directory
              children[12]{path,name,size,type}:
                notizen/grafiken/2/FDG_repo.dot,FDG_repo.dot,9150,file
                notizen/grafiken/2/fdg_graph.png,fdg_graph.png,425374,file
                notizen/grafiken/2/fdg_graph_2.png,fdg_graph_2.png,4744718,file
                notizen/grafiken/2/filtered_callgraph_flask.png,filtered_callgraph_flask.png,614631,file
                notizen/grafiken/2/filtered_callgraph_repo-agent.png,filtered_callgraph_repo-agent.png,280428,file
                notizen/grafiken/2/filtered_callgraph_repo-agent_3.png,filtered_callgraph_repo-agent_3.png,1685838,file
                notizen/grafiken/2/filtered_repo_callgraph_flask.dot,filtered_repo_callgraph_flask.dot,19984,file
                notizen/grafiken/2/filtered_repo_callgraph_repo-agent-3.dot,filtered_repo_callgraph_repo-agent-3.dot,20120,file
                notizen/grafiken/2/filtered_repo_callgraph_repo-agent.dot,filtered_repo_callgraph_repo-agent.dot,3118,file
                notizen/grafiken/2/global_callgraph.png,global_callgraph.png,608807,file
                notizen/grafiken/2/graph_flask.md,graph_flask.md,17398,file
                notizen/grafiken/2/repo.dot,repo.dot,19984,file
            - name: Flask-Repo
              type: directory
              children[65]{path,name,size,type}:
                notizen/grafiken/Flask-Repo/__init__.dot,__init__.dot,89,file
                notizen/grafiken/Flask-Repo/__main__.dot,__main__.dot,87,file
                notizen/grafiken/Flask-Repo/app.dot,app.dot,78,file
                notizen/grafiken/Flask-Repo/auth.dot,auth.dot,1126,file
                notizen/grafiken/Flask-Repo/blog.dot,blog.dot,939,file
                notizen/grafiken/Flask-Repo/blueprints.dot,blueprints.dot,4601,file
                notizen/grafiken/Flask-Repo/cli.dot,cli.dot,8269,file
                notizen/grafiken/Flask-Repo/conf.dot,conf.dot,489,file
                notizen/grafiken/Flask-Repo/config.dot,config.dot,2130,file
                notizen/grafiken/Flask-Repo/conftest.dot,conftest.dot,1756,file
                notizen/grafiken/Flask-Repo/ctx.dot,ctx.dot,3009,file
                notizen/grafiken/Flask-Repo/db.dot,db.dot,782,file
                notizen/grafiken/Flask-Repo/debughelpers.dot,debughelpers.dot,1903,file
                notizen/grafiken/Flask-Repo/factory.dot,factory.dot,220,file
                notizen/grafiken/Flask-Repo/flask.dot,flask.dot,82,file
                notizen/grafiken/Flask-Repo/globals.dot,globals.dot,370,file
                notizen/grafiken/Flask-Repo/hello.dot,hello.dot,152,file
                notizen/grafiken/Flask-Repo/helpers.dot,helpers.dot,2510,file
                notizen/grafiken/Flask-Repo/importerrorapp.dot,importerrorapp.dot,155,file
                notizen/grafiken/Flask-Repo/logging.dot,logging.dot,564,file
                notizen/grafiken/Flask-Repo/make_celery.dot,make_celery.dot,99,file
                notizen/grafiken/Flask-Repo/multiapp.dot,multiapp.dot,88,file
                notizen/grafiken/Flask-Repo/provider.dot,provider.dot,1769,file
                notizen/grafiken/Flask-Repo/scaffold.dot,scaffold.dot,3700,file
                notizen/grafiken/Flask-Repo/sessions.dot,sessions.dot,4178,file
                notizen/grafiken/Flask-Repo/signals.dot,signals.dot,133,file
                notizen/grafiken/Flask-Repo/tag.dot,tag.dot,3750,file
                notizen/grafiken/Flask-Repo/tasks.dot,tasks.dot,312,file
                notizen/grafiken/Flask-Repo/templating.dot,templating.dot,2277,file
                notizen/grafiken/Flask-Repo/test_appctx.dot,test_appctx.dot,2470,file
                notizen/grafiken/Flask-Repo/test_async.dot,test_async.dot,1919,file
                notizen/grafiken/Flask-Repo/test_auth.dot,test_auth.dot,725,file
                notizen/grafiken/Flask-Repo/test_basic.dot,test_basic.dot,17383,file
                notizen/grafiken/Flask-Repo/test_blog.dot,test_blog.dot,1105,file
                notizen/grafiken/Flask-Repo/test_blueprints.dot,test_blueprints.dot,11515,file
                notizen/grafiken/Flask-Repo/test_cli.dot,test_cli.dot,6435,file
                notizen/grafiken/Flask-Repo/test_config.dot,test_config.dot,2854,file
                notizen/grafiken/Flask-Repo/test_config.png,test_config.png,288693,file
                notizen/grafiken/Flask-Repo/test_converters.dot,test_converters.dot,937,file
                notizen/grafiken/Flask-Repo/test_db.dot,test_db.dot,481,file
                notizen/grafiken/Flask-Repo/test_factory.dot,test_factory.dot,201,file
                notizen/grafiken/Flask-Repo/test_helpers.dot,test_helpers.dot,5857,file
                notizen/grafiken/Flask-Repo/test_instance_config.dot,test_instance_config.dot,1249,file
                notizen/grafiken/Flask-Repo/test_js_example.dot,test_js_example.dot,453,file
                notizen/grafiken/Flask-Repo/test_json.dot,test_json.dot,4021,file
                notizen/grafiken/Flask-Repo/test_json_tag.dot,test_json_tag.dot,1452,file
                notizen/grafiken/Flask-Repo/test_logging.dot,test_logging.dot,1348,file
                notizen/grafiken/Flask-Repo/test_regression.dot,test_regression.dot,763,file
                notizen/grafiken/Flask-Repo/test_reqctx.dot,test_reqctx.dot,3809,file
                notizen/grafiken/Flask-Repo/test_request.dot,test_request.dot,668,file
                notizen/grafiken/Flask-Repo/test_session_interface.dot,test_session_interface.dot,667,file
                notizen/grafiken/Flask-Repo/test_signals.dot,test_signals.dot,1671,file
                notizen/grafiken/Flask-Repo/test_subclassing.dot,test_subclassing.dot,612,file
                notizen/grafiken/Flask-Repo/test_templating.dot,test_templating.dot,5129,file
                notizen/grafiken/Flask-Repo/test_testing.dot,test_testing.dot,4081,file
                notizen/grafiken/Flask-Repo/test_user_error_handler.dot,test_user_error_handler.dot,5984,file
                notizen/grafiken/Flask-Repo/test_views.dot,test_views.dot,2641,file
                notizen/grafiken/Flask-Repo/testing.dot,testing.dot,2785,file
                notizen/grafiken/Flask-Repo/typing.dot,typing.dot,86,file
                notizen/grafiken/Flask-Repo/typing_app_decorators.dot,typing_app_decorators.dot,500,file
                notizen/grafiken/Flask-Repo/typing_error_handler.dot,typing_error_handler.dot,420,file
                notizen/grafiken/Flask-Repo/typing_route.dot,typing_route.dot,1717,file
                notizen/grafiken/Flask-Repo/views.dot,views.dot,1125,file
                notizen/grafiken/Flask-Repo/wrappers.dot,wrappers.dot,911,file
                notizen/grafiken/Flask-Repo/wsgi.dot,wsgi.dot,19,file
            - name: Repo-onboarding
              type: directory
              children[15]{path,name,size,type}:
                notizen/grafiken/Repo-onboarding/AST.dot,AST.dot,1361,file
                notizen/grafiken/Repo-onboarding/Frontend.dot,Frontend.dot,538,file
                notizen/grafiken/Repo-onboarding/HelperLLM.dot,HelperLLM.dot,1999,file
                notizen/grafiken/Repo-onboarding/HelperLLM.png,HelperLLM.png,234861,file
                notizen/grafiken/Repo-onboarding/MainLLM.dot,MainLLM.dot,2547,file
                notizen/grafiken/Repo-onboarding/agent.dot,agent.dot,2107,file
                notizen/grafiken/Repo-onboarding/basic_info.dot,basic_info.dot,1793,file
                notizen/grafiken/Repo-onboarding/callgraph.dot,callgraph.dot,1478,file
                notizen/grafiken/Repo-onboarding/getRepo.dot,getRepo.dot,1431,file
                notizen/grafiken/Repo-onboarding/graph_AST.png,graph_AST.png,132743,file
                notizen/grafiken/Repo-onboarding/graph_AST2.png,graph_AST2.png,12826,file
                notizen/grafiken/Repo-onboarding/graph_AST3.png,graph_AST3.png,136016,file
                notizen/grafiken/Repo-onboarding/main.dot,main.dot,1091,file
                notizen/grafiken/Repo-onboarding/tools.dot,tools.dot,1328,file
                notizen/grafiken/Repo-onboarding/types.dot,types.dot,133,file
        - path: notizen/notizen.md
          name: notizen.md
          size: 4937
          type: file
        - path: notizen/paul_notizen.md
          name: paul_notizen.md
          size: 2033
          type: file
        - path: notizen/praesentation_notizen.md
          name: praesentation_notizen.md
          size: 2738
          type: file
        - path: notizen/technische_notizen.md
          name: technische_notizen.md
          size: 3616
          type: file
    - path: output.json
      name: output.json
      size: 454008
      type: file
    - path: output.toon
      name: output.toon
      size: 341854
      type: file
    - path: readme.md
      name: readme.md
      size: 1905
      type: file
    - path: requirements.txt
      name: requirements.txt
      size: 4286
      type: file
    - name: result
      type: directory
      children[28]{path,name,size,type}:
        result/ast_schema_01_12_2025_11-49-24.json,ast_schema_01_12_2025_11-49-24.json,201215,file
        result/notebook_report_23_12_2025_12-56-24_NotebookLLM_gemini-2.5-flash.md,notebook_report_23_12_2025_12-56-24_NotebookLLM_gemini-2.5-flash.md,15776,file
        result/notebook_report_27_12_2025_15-06-09_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-06-09_NotebookLLM_gemini-2.5-flash.md,14804,file
        result/notebook_report_27_12_2025_15-09-29_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-09-29_NotebookLLM_gemini-2.5-flash.md,17598,file
        result/notebook_report_27_12_2025_15-26-34_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-26-34_NotebookLLM_gemini-2.5-flash.md,20458,file
        result/notebook_report_27_12_2025_15-33-06_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-33-06_NotebookLLM_gemini-2.5-flash.md,15268,file
        result/notebook_report_29_12_2025_15-03-21_NotebookLLM_gemini-2.5-flash.md,notebook_report_29_12_2025_15-03-21_NotebookLLM_gemini-2.5-flash.md,16111,file
        result/report_01_12_2025_12-26-46_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_12-26-46_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,79523,file
        result/report_01_12_2025_12-55-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_12-55-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,143883,file
        result/report_01_12_2025_13-37-30_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_13-37-30_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,141924,file
        result/report_01_12_2025_14-15-04_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_14-15-04_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,129176,file
        result/report_01_12_2025_14-42-38_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_14-42-38_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,38933,file
        result/report_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,138015,file
        result/report_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.md,report_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.md,127747,file
        result/report_03_12_2025_22-46-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_03_12_2025_22-46-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,80305,file
        result/report_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,163921,file
        result/report_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.md,report_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.md,128520,file
        result/report_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.md,report_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.md,7245,file
        result/report_14_11_2025_14-52-36.md,report_14_11_2025_14-52-36.md,16393,file
        result/report_14_11_2025_15-21-53.md,report_14_11_2025_15-21-53.md,108585,file
        result/report_14_11_2025_15-26-24.md,report_14_11_2025_15-26-24.md,11659,file
        result/report_21_11_2025_15-43-30.md,report_21_11_2025_15-43-30.md,57940,file
        result/report_21_11_2025_16-06-12.md,report_21_11_2025_16-06-12.md,76423,file
        result/report_22_11_2025_14-01-50_Helper_llama3_Main_geminipro.md,report_22_11_2025_14-01-50_Helper_llama3_Main_geminipro.md,39595,file
        result/report_22_11_2025_14-39-55_Helper_llama3_MainLLM_llama3.md,report_22_11_2025_14-39-55_Helper_llama3_MainLLM_llama3.md,2013,file
        result/result_2025-11-11_12-30-53.md,result_2025-11-11_12-30-53.md,1232,file
        result/result_2025-11-11_12-43-51.md,result_2025-11-11_12-43-51.md,33683,file
        result/result_2025-11-11_12-45-37.md,result_2025-11-11_12-45-37.md,1193,file
    - name: schemas
      type: directory
      children[1]{path,name,size,type}:
        schemas/types.py,types.py,7559,file
    - name: statistics
      type: directory
      children[5]{path,name,size,type}:
        statistics/savings_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.png,savings_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.png,23279,file
        statistics/savings_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.png,savings_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.png,23650,file
        statistics/savings_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.png,savings_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.png,23805,file
        statistics/savings_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.png,savings_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.png,22698,file
        statistics/savings_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.png,savings_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.png,22539,file
    - path: test.json
      name: test.json
      size: 241669
      type: file
ast_schema:
  files:
    "backend/AST_Schema.py":
      ast_nodes:
        imports[3]: ast,os,getRepo.GitRepository
        functions[1]:
          - mode: function_analysis
            identifier: backend.AST_Schema.path_to_module
            name: path_to_module
            args[2]: filepath,project_root
            docstring: Wandelt einen Dateipfad in einen Python-Modulpfad um.
            source_code: "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    \n    module_path = rel_path.replace(os.path.sep, '.')\n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n    return module_path"
            start_line: 5
            end_line: 18
            context:
              calls[0]:
              called_by[0]:
        classes[2]:
          - mode: class_analysis
            identifier: backend.AST_Schema.ASTVisitor
            name: ASTVisitor
            docstring: null
            source_code: "class ASTVisitor(ast.NodeVisitor):\n    def __init__(self, source_code: str, file_path: str, project_root: str):\n        self.source_code = source_code\n        self.file_path = file_path\n        self.project_root = project_root\n        self.module_path = path_to_module(self.file_path, self.project_root)\n        self.schema = {\"imports\": [], \"functions\": [], \"classes\": []}\n        self._current_class = None\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(f\"{node.module}.{alias.name}\")\n        self.generic_visit(node)\n\n    def visit_ClassDef(self, node):\n        class_identifier = f\"{self.module_path}.{node.name}\"\n        class_info = {\n            \"mode\": \"class_analysis\",\n            \"identifier\": class_identifier,\n            \"name\": node.name,\n            \"docstring\": ast.get_docstring(node),\n            \"source_code\": ast.get_source_segment(self.source_code, node),\n            \"start_line\": node.lineno,\n            \"end_line\": node.end_lineno,\n            \"context\": {\n                \"dependencies\": [],\n                \"instantiated_by\": [],\n                \"method_context\": []\n            },   \n        }\n        self.schema[\"classes\"].append(class_info)\n        self._current_class = class_info \n        self.generic_visit(node)\n        self._current_class = None\n\n    def visit_FunctionDef(self, node):\n        if self._current_class:\n            method_identifier = f\"{self._current_class['identifier']}.{node.name}\"\n            method_context_info = {\n                \"identifier\": method_identifier,\n                \"name\": node.name,\n                \"calls\": [],\n                \"called_by\": [],\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n            }\n            self._current_class[\"context\"][\"method_context\"].append(method_context_info)\n        else:\n            func_identifier = f\"{self.module_path}.{node.name}\"\n            func_info = {\n                \"mode\": \"function_analysis\",\n                \"identifier\": func_identifier,\n                \"name\": node.name,\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"source_code\": ast.get_source_segment(self.source_code, node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n                \"context\": {\n                    \"calls\": [],\n                    \"called_by\": []\n                }\n            }\n            self.schema[\"functions\"].append(func_info)\n        self.generic_visit(node)\n    \n    def visit_AsyncFunctionDef(self, node):\n        self.visit_FunctionDef(node)"
            start_line: 20
            end_line: 94
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[6]:
                - identifier: backend.AST_Schema.ASTVisitor.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[4]: self,source_code,file_path,project_root
                  docstring: null
                  start_line: 21
                  end_line: 27
                - identifier: backend.AST_Schema.ASTVisitor.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 29
                  end_line: 32
                - identifier: backend.AST_Schema.ASTVisitor.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 34
                  end_line: 37
                - identifier: backend.AST_Schema.ASTVisitor.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 39
                  end_line: 58
                - identifier: backend.AST_Schema.ASTVisitor.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 60
                  end_line: 91
                - identifier: backend.AST_Schema.ASTVisitor.visit_AsyncFunctionDef
                  name: visit_AsyncFunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 93
                  end_line: 94
          - mode: class_analysis
            identifier: backend.AST_Schema.ASTAnalyzer
            name: ASTAnalyzer
            docstring: null
            source_code: "class ASTAnalyzer:\n\n    def __init__(self):\n        pass\n\n    def merge_relationship_data(self, full_schema: dict, raw_relationships: dict) -> dict:\n\n        outgoing_calls = raw_relationships.get(\"outgoing\", {})\n        incoming_calls = raw_relationships.get(\"incoming\", {})\n\n        for file_path, file_data in full_schema.get(\"files\", {}).items():\n            ast_nodes = file_data.get(\"ast_nodes\", {})\n            \n            for func in ast_nodes.get(\"functions\", []):\n                func_id = func.get(\"identifier\")\n                if func_id:\n                    func[\"context\"][\"calls\"] = outgoing_calls.get(func_id, [])\n                    func[\"context\"][\"called_by\"] = incoming_calls.get(func_id, [])\n\n            for cls in ast_nodes.get(\"classes\", []):\n                cls_id = cls.get(\"identifier\")\n                \n                if cls_id:\n                    cls[\"context\"][\"instantiated_by\"] = incoming_calls.get(cls_id, [])\n\n                class_dependencies = set()\n                \n                for method in cls[\"context\"].get(\"method_context\", []):\n                    method_id = method.get(\"identifier\")\n                    if method_id:\n                        m_calls = outgoing_calls.get(method_id, [])\n                        method[\"calls\"] = m_calls\n                        method[\"called_by\"] = incoming_calls.get(method_id, [])\n                        \n                        for callee in m_calls:\n                            if cls_id and not callee.startswith(f\"{cls_id}.\"):\n                                class_dependencies.add(callee)\n                \n                cls[\"context\"][\"dependencies\"] = sorted(list(class_dependencies))\n        \n        return full_schema\n\n    def analyze_repository(self, files: list, repo: GitRepository) -> dict:\n        full_schema = {\n            \"files\": {}\n        }\n\n        all_paths = [file_obj.path for file_obj in files]\n        if not all_paths:\n            return full_schema\n        \n        project_root = os.path.commonpath(all_paths)\n        if os.path.isfile(project_root):\n            project_root = os.path.dirname(project_root)\n\n        for file_obj in files:\n            if not file_obj.path.endswith('.py'):\n                continue\n            \n            file_content = file_obj.content\n            if not file_content.strip():\n                continue\n\n            try:\n                tree = ast.parse(file_content)\n                visitor = ASTVisitor(\n                    source_code=file_content, \n                    file_path=file_obj.path, \n                    project_root=project_root\n                )\n                visitor.visit(tree)\n                file_schema_nodes = visitor.schema\n\n                if file_schema_nodes[\"imports\"] or file_schema_nodes[\"functions\"] or file_schema_nodes[\"classes\"]:\n                    full_schema[\"files\"][file_obj.path] = {\n                        \"ast_nodes\": file_schema_nodes\n                    }\n\n            except (SyntaxError, ValueError) as e:\n                print(f\"Warnung: Konnte Datei '{file_obj.path}' nicht parsen. Fehler: {e}\")\n\n        return full_schema"
            start_line: 97
            end_line: 178
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[3]:
                - identifier: backend.AST_Schema.ASTAnalyzer.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 99
                  end_line: 100
                - identifier: backend.AST_Schema.ASTAnalyzer.merge_relationship_data
                  name: merge_relationship_data
                  calls[0]:
                  called_by[0]:
                  args[3]: self,full_schema,raw_relationships
                  docstring: null
                  start_line: 102
                  end_line: 137
                - identifier: backend.AST_Schema.ASTAnalyzer.analyze_repository
                  name: analyze_repository
                  calls[0]:
                  called_by[0]:
                  args[3]: self,files,repo
                  docstring: null
                  start_line: 139
                  end_line: 178
    "backend/File_Dependency.py":
      ast_nodes:
        imports[17]: networkx,os,ast.Assign,ast.AST,ast.ClassDef,ast.FunctionDef,ast.Import,ast.ImportFrom,ast.Name,ast.NodeVisitor,ast.literal_eval,ast.parse,ast.walk,keyword.iskeyword,pathlib.Path,getRepo.GitRepository,callgraph.make_safe_dot
        functions[3]:
          - mode: function_analysis
            identifier: backend.File_Dependency.build_file_dependency_graph
            name: build_file_dependency_graph
            args[3]: filename,tree,repo_root
            docstring: null
            source_code: "def build_file_dependency_graph(filename: str, tree: AST, repo_root: str) -> nx.DiGraph:\n    graph = nx.DiGraph()\n\n    tree_visitor = FileDependencyGraph(filename, repo_root)\n    tree_visitor.visit(tree)\n\n    for caller, callees in tree_visitor.import_dependencies.items():\n        graph.add_node(caller)\n        graph.add_nodes_from(callees)\n        for callee in callees:\n            graph.add_edge(caller, callee)\n    \n    return graph"
            start_line: 153
            end_line: 165
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.File_Dependency.build_repository_graph
            name: build_repository_graph
            args[1]: repository
            docstring: null
            source_code: "def build_repository_graph(repository: GitRepository) -> nx.DiGraph:\n    all_files = repository.get_all_files()\n    repo_root = repository.temp_dir\n    global_graph = nx.DiGraph()\n\n    for file in all_files: \n        if not file.path.endswith(\".py\"):\n            continue\n        filename = str(os.path.basename(file.path)).removesuffix(\".py\")\n        tree = parse(file.content)\n        graph = build_file_dependency_graph(filename, tree, repo_root)\n        \n        for node in graph.nodes:\n            global_graph.add_node(node)\n\n        for caller, callee in graph.edges:\n            if callee:\n                global_graph.add_node(callee)\n                global_graph.add_edge(caller, callee)\n    \n    return global_graph"
            start_line: 167
            end_line: 187
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.File_Dependency.get_all_temp_files
            name: get_all_temp_files
            args[1]: directory
            docstring: null
            source_code: "def get_all_temp_files(directory: str) -> list[Path]:\n    root_path = Path(directory).resolve()\n    all_files = [file.relative_to(root_path) for file in root_path.rglob(\"*.py\")]\n\n    return all_files"
            start_line: 189
            end_line: 193
            context:
              calls[0]:
              called_by[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.File_Dependency.FileDependencyGraph
            name: FileDependencyGraph
            docstring: null
            source_code: "class FileDependencyGraph(NodeVisitor):\n\n    import_dependencies: dict[str, set[str]] = {}\n    def __init__(self, filename: str, repo_root):\n        \"\"\"\n        Initialisiert den File Dependency Graphen\n\n        Args:\n\n        \"\"\"\n        self.filename = filename\n        self.repo_root = repo_root\n\n    def _resolve_module_name(self, node: ImportFrom) -> list[str]:\n        \"\"\"\n        L√∂st relative Imports der Form `from .. import name1, name2` auf.\n        Liefert die Liste der tats√§chlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\n        Wirft ImportError, wenn nichts aufgel√∂st werden konnte.\n        \"\"\"\n        level_depth = node.level\n        names = [alias.name for alias in node.names]  \n        all_files = get_all_temp_files(self.repo_root)  \n\n        candidates = [p for p in all_files if p.stem == Path(self.filename).stem or p.name == f\"{self.filename}.py\"]\n\n        if not candidates:\n            raise ImportError(f\"Kann aktuelle Datei '{self.filename}' im Repo nicht finden.\")\n\n\n        candidates.sort(key=lambda p: len(p.parts))\n        current_rel_path = candidates[0]\n        base_dir = current_rel_path.parent\n        if level_depth <= 0:\n            raise ImportError(\"Erwarteter relativer Import (level >= 1).\")\n        for _ in range(level_depth - 1):\n            if base_dir == base_dir.parent and len(base_dir.parts) == 0:\n                raise ImportError(f\"Relative Import-Ebene ({level_depth}) zu gro√ü f√ºr Datei '{current_rel_path}'.\")\n            base_dir = base_dir.parent\n\n        repo_root_path = Path(self.repo_root).resolve()\n        resolved: list[str] = []\n\n        def module_file_exists(rel_base: Path, name: str) -> bool:\n            file_path = repo_root_path / rel_base / f\"{name}.py\"\n            pkg_init = repo_root_path / rel_base / name / \"__init__.py\"\n            return file_path.exists() or pkg_init.exists()\n\n        def init_exports_symbol(rel_base: Path, symbol: str) -> bool:\n            \"\"\"\n            Pr√ºft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\n            oder als Name (funktion/klasse/assign) definiert ist.\n            \"\"\"\n            init_path = repo_root_path / rel_base / \"__init__.py\"\n            if not init_path.exists():\n                return False\n            try:\n                src = init_path.read_text(encoding=\"utf-8\")\n                mod = parse(src, filename=str(init_path))\n            except Exception:\n                return False\n\n            for node_ in walk(mod):\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == \"__all__\":\n                            try:\n                                value = literal_eval(node_.value)\n                                if isinstance(value, (list, tuple)) and symbol in value:\n                                    return True\n                            except Exception:\n                                pass\n                if isinstance(node_, (FunctionDef, ClassDef)) and node_.name == symbol:\n                    return True\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == symbol:\n                            return True\n            return False\n\n        for name in names:\n            if not name.isidentifier() or iskeyword(name):\n                continue\n\n            if module_file_exists(base_dir, name):\n                resolved.append(name)\n                continue\n            if init_exports_symbol(base_dir, name):\n                resolved.append(name)\n                continue\n\n        resolved = sorted(set(resolved))\n\n        if not resolved:\n            raise ImportError(\n                f\"Kein passendes Modul/Symbol f√ºr relative Import-Aufl√∂sung gefunden \"\n                f\"(level={level_depth}, names={names}, base_dir={base_dir})\"\n            )\n\n        return resolved\n    \n    def visit_Import(self, node: Import | ImportFrom, base_name: str | None = None):\n        for alias in node.names:\n        \n            if self.filename not in self.import_dependencies:\n                self.import_dependencies[self.filename] = set()\n\n            if base_name:\n                self.import_dependencies[self.filename].add(base_name)\n            else:\n                self.import_dependencies[self.filename].add(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node: ImportFrom):\n        \"\"\"\n        Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\n        also c, und dieser wird als callee f√ºr den caller, das File, gesetzt.\n        \"\"\"\n        module_name = node.module\n        if module_name:\n            module_base = module_name.split(\".\")[-1]\n            self.visit_Import(node, module_base)\n        else:\n            try:\n                resolved = self._resolve_module_name(node)\n                for base in resolved:\n                    self.visit_Import(node, base)\n            except ImportError as e:\n                print(f\"Aufl√∂sung eines relativen Imports fehlgeschlagen: {e}\")\n\n        self.generic_visit(node)"
            start_line: 22
            end_line: 151
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[6]:
                - identifier: backend.File_Dependency.FileDependencyGraph.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[3]: self,filename,repo_root
                  docstring: "Initialisiert den File Dependency Graphen\n\nArgs:"
                  start_line: 25
                  end_line: 33
                - identifier: backend.File_Dependency.FileDependencyGraph._resolve_module_name
                  name: _resolve_module_name
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: "L√∂st relative Imports der Form `from .. import name1, name2` auf.\nLiefert die Liste der tats√§chlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\nWirft ImportError, wenn nichts aufgel√∂st werden konnte."
                  start_line: 35
                  end_line: 120
                - identifier: backend.File_Dependency.FileDependencyGraph.module_file_exists
                  name: module_file_exists
                  calls[0]:
                  called_by[0]:
                  args[2]: rel_base,name
                  docstring: null
                  start_line: 64
                  end_line: 67
                - identifier: backend.File_Dependency.FileDependencyGraph.init_exports_symbol
                  name: init_exports_symbol
                  calls[0]:
                  called_by[0]:
                  args[2]: rel_base,symbol
                  docstring: "Pr√ºft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\noder als Name (funktion/klasse/assign) definiert ist."
                  start_line: 69
                  end_line: 99
                - identifier: backend.File_Dependency.FileDependencyGraph.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[3]: self,node,base_name
                  docstring: null
                  start_line: 122
                  end_line: 132
                - identifier: backend.File_Dependency.FileDependencyGraph.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: "Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\nalso c, und dieser wird als callee f√ºr den caller, das File, gesetzt."
                  start_line: 134
                  end_line: 151
    "backend/HelperLLM.py":
      ast_nodes:
        imports[23]: os,json,logging,time,typing.List,typing.Dict,typing.Any,typing.Optional,typing.Union,dotenv.load_dotenv,langchain_google_genai.ChatGoogleGenerativeAI,langchain_ollama.ChatOllama,langchain_openai.ChatOpenAI,langchain.messages.HumanMessage,langchain.messages.SystemMessage,langchain.messages.AIMessage,pydantic.ValidationError,schemas.types.FunctionAnalysis,schemas.types.ClassAnalysis,schemas.types.FunctionAnalysisInput,schemas.types.FunctionContextInput,schemas.types.ClassAnalysisInput,schemas.types.ClassContextInput
        functions[1]:
          - mode: function_analysis
            identifier: backend.HelperLLM.main_orchestrator
            name: main_orchestrator
            args[0]:
            docstring: "Dummy Data and processing loop for testing the LLMHelper class.\nThis version is syntactically correct and logically matches the Pydantic models."
            source_code: "def main_orchestrator():\n    \"\"\"\n    Dummy Data and processing loop for testing the LLMHelper class.\n    This version is syntactically correct and logically matches the Pydantic models.\n    \"\"\"\n    \n    # --- Step 1: Define the pre-computed analysis for each method ---\n    \n    # Example Input 1: For the 'add_item' function\n    add_item_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"source_code\": \"\"\"def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\\\"\\\"\\\"\n        if not isinstance(quantity, int) or quantity <= 0:\n            raise ValueError(\"Quantity must be a positive integer.\")\n    \n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n        print(f\"Added {quantity} of {item_name}. New total: {self.inventory[item_name]}\")\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"process_shipment\", \"restock_api_endpoint\"]\n        }\n    })\n\n    # Example Input 2: For the 'check_stock' function\n    check_stock_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"source_code\": \"\"\"def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Retrieves the current stock quantity for a given item.\\\"\\\"\\\"\n        stock_level = self.inventory.get(item_name, 0)\n        return stock_level\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"fulfill_order\", \"ui_display_handler\"]\n        }\n    })\n\n    # Example Input 3: For the 'generate_report' function\n    generate_report_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"source_code\": \"\"\"def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a timestamped string summary of the current inventory state.\\\"\\\"\\\"\n        timestamp = datetime.now()\n        report_header = \"Inventory Report as of: {ts}\\\\n\".format(ts=timestamp)\n        report_lines = [report_header]\n        \n        for item, quantity in self.inventory.items():\n            line = \"- {name}: {qty}\".format(name=item, qty=quantity)\n            report_lines.append(line)\n            \n        return \"\\\\n\".join(report_lines)\"\"\",\n        \"imports\": [\"from datetime import datetime\"],\n        \"context\": {\n            \"calls\": [\"datetime.now\", \"str.format\"],\n            \"called_by\": [\"daily_cron_job\", \"admin_dashboard_export\"]\n        }\n    })\n\n    add_item_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"description\": {\n            \"overall\": \"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to add.\"},\n                {\"name\": \"quantity\", \"type\": \"int\", \"description\": \"The number of units to add. Must be a positive integer.\"}\n            ],\n            \"returns\": [],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"process_shipment and restock_api_endpoint\" }\n        },\n        \"error\": None\n    })\n\n    check_stock_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"description\": {\n            \"overall\": \"Retrieves the current stock quantity for a given item.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to check.\"}\n            ],\n            \"returns\": [\n                {\"name\": \"stock_level\", \"type\": \"int\", \"description\": \"The quantity of the item in stock. Returns 0 if the item is not found.\"}\n            ],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"fulfill_order and ui_display_handler\" }\n        },\n        \"error\": None\n    })\n\n    generate_report_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"description\": {\n            \"overall\": \"Generates a timestamped string summary of the current inventory state.\",\n            \"parameters\": [ {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"} ],\n            \"returns\": [\n                {\"name\": \"report\", \"type\": \"str\", \"description\": \"A formatted string detailing all items and their quantities.\"}\n            ],\n            \"usage_context\": { \"calls\": \"datetime.now and str.format\", \"called_by\": \"daily_cron_job and admin_dashboard_export\" }\n        },\n        \"error\": None\n    })\n\n    class_input = ClassAnalysisInput(\n        mode=\"class_analysis\",\n        identifier=\"InventoryManager\",\n        # NOTE: The indentation of this string is now fixed. It starts at column 0.\n        source_code=\"\"\"class InventoryManager:\n    \\\"\\\"\\\"Manages stock levels for products in a warehouse.\\\"\\\"\\\"\n    def __init__(self, warehouse_id: str):\n        self.warehouse_id = warehouse_id\n        self.inventory = {}  # item_name: quantity\n\n    def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds an item to the inventory.\\\"\\\"\\\"\n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n\n    def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Checks the stock of a specific item.\\\"\\\"\\\"\n        return self.inventory.get(item_name, 0)\n\n    def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a summary report of the inventory.\\\"\\\"\\\"\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        report_lines = [f\"Inventory Report for {self.warehouse_id} at {timestamp}\"]\n        if not self.inventory:\n            report_lines.append(\"Inventory is empty.\")\n        else:\n            for item, quantity in self.inventory.items():\n                report_lines.append(f\"- {item}: {quantity}\")\n        return \"\\\\n\".join(report_lines)\n\"\"\",\n        imports=[\"from datetime import datetime\"],\n        context=ClassContextInput(\n            dependencies=[\"datetime\"],\n            instantiated_by=[\"main_app_startup\", \"warehouse_worker_script\"],\n            methods_analysis=[\n                add_item_analysis,\n                check_stock_analysis,\n                generate_report_analysis\n            ]\n        )\n    )\n\n    \n    # The helper methods expect a LIST of inputs, so we wrap our single object in a list.\n    input = [add_item_input, check_stock_input, generate_report_input]\n    analysis = [add_item_analysis, check_stock_analysis, generate_report_analysis] \n\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    llm_helper = LLMHelper(api_key=GEMINI_API_KEY, function_prompt_path=function_prompt_file, class_prompt_path=class_prompt_file)\n\n    # This will be the final JSON containing all documentation\n    final_documentation = {}\n\n    # The analysis of the methods is already provided in the context.\n    logging.info(\"\\n--- Generating documentation for classes ---\")\n    \n    # The `generate_for_classes` method returns a list of results.\n    analysis_results = llm_helper.generate_for_functions(input)\n\n    # --- Step 4: Process the results ---\n    \n    # We loop through the list of results (even though there's only one in this case).\n    for doc in analysis_results:\n        if doc:\n            logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n            if \"classes\" not in final_documentation:\n                final_documentation[\"classes\"] = {}\n            # Use .model_dump() to convert the Pydantic object back to a dict for JSON serialization\n            final_documentation[\"classes\"][doc.identifier] = doc.model_dump() \n        else:\n            logging.warning(f\"Failed to generate doc for a class\")\n\n    # --- Step 5: Display the final aggregated result ---\n    logging.info(\"\\n--- Final Generated Documentation ---\")\n    print(json.dumps(final_documentation, indent=2))"
            start_line: 227
            end_line: 413
            context:
              calls[0]:
              called_by[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.HelperLLM.LLMHelper
            name: LLMHelper
            docstring: "A class to interact with Google Gemini for generating code snippet documentation.\nIt centralizes API interaction, error handling, and validates I/O using Pydantic."
            source_code: "class LLMHelper:\n    \"\"\"\n    A class to interact with Google Gemini for generating code snippet documentation.\n    It centralizes API interaction, error handling, and validates I/O using Pydantic.\n    \"\"\"\n    def __init__(self, api_key: str, function_prompt_path: str, class_prompt_path: str, model_name: str = \"gemini-2.0-flash-lite\", base_url: str = None):\n        if not api_key:\n            raise ValueError(\"Gemini API Key must be set.\")\n        \n        try:\n            with open(function_prompt_path, 'r', encoding='utf-8') as f:\n                self.function_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Function system prompt file not found at: {function_prompt_path}\")\n            raise\n\n        # Handle the second file\n        try:\n            with open(class_prompt_path, 'r', encoding='utf-8') as f:\n                self.class_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Class system prompt file not found at: {class_prompt_path}\")\n            raise\n        \n        # Batch-Size config\n        self.model_name = model_name\n        self._configure_batch_settings(model_name)\n\n\n        if model_name.startswith(\"gemini-\"):\n            base_llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=0.3, \n            )\n        \n        elif model_name.startswith(\"gpt-\") and \"openGPT\" not in model_name:\n            base_llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=0.3,\n            )\n\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            if not SCADSLLM_URL:\n                raise ValueError(f\"SCADSLLM_URL environment variable is required for model {model_name}\")\n            \n            logging.info(f\"Connecting to Custom API at {SCADSLLM_URL} for model {model_name}\")\n            \n            base_llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                base_url=SCADSLLM_URL,\n                temperature=0.3,\n            )\n\n        else:\n            target_url = base_url if base_url else OLLAMA_BASE_URL\n            logging.info(f\"Using Ollama at {target_url} for model {model_name}\")\n            base_llm = ChatOllama(\n                model=model_name,\n                temperature=0.3,\n                base_url=target_url,\n            )\n\n        self.function_llm = base_llm.with_structured_output(FunctionAnalysis, method=\"json_schema\")\n        self.class_llm = base_llm.with_structured_output(ClassAnalysis, method=\"json_schema\")\n\n        self.raw_llm = base_llm\n\n        logging.info(f\"LLMHelper initialized with model '{model_name}'. Batch Size: {self.batch_size}\")\n\n    def _configure_batch_settings(self, model_name: str):\n\n        if model_name == \"gemini-2.0-flash-lite\":\n            self.batch_size = 30\n        \n        elif model_name == \"gemini-flash-latest\":\n            self.batch_size = 10\n\n        elif model_name == \"gemini-2.5-flash\":\n            self.batch_size = 10\n            \n        elif model_name == \"gemini-2.5-pro\":\n            self.batch_size = 2\n        \n        elif model_name == \"gemini-2.0-flash\":\n            self.batch_size = 15\n\n        elif model_name == \"gemini-2.5-flash-lite\":\n            self.batch_size = 15\n\n        elif model_name == \"llama3\":\n            self.batch_size = 50\n\n        elif model_name == \"gpt-5.1\":\n            self.batch_size = 500\n\n        elif model_name == \"gpt-5-mini\":\n            self.batch_size = 500\n\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            self.batch_size = 500\n            \n        else:\n            logging.warning(f\"Unknown model '{model_name}', using conservative defaults.\")\n            self.batch_size = 2\n\n    def generate_for_functions(self, function_inputs: List[FunctionAnalysisInput]) -> List[Optional[FunctionAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of functions.\"\"\"\n\n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 62\n\n        if not function_inputs:\n            return []\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(function_input.model_dump(), indent=2) \n            for function_input in function_inputs\n        ]\n       \n        conversations = [[SystemMessage(content=self.function_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_functions = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n        \n            try:\n                batch_results = self.function_llm.batch(batch_conversations, config={\"max_concurrency\": self.batch_size})\n                all_validated_functions.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                all_validated_functions.extend([None] * len(batch_conversations))\n            \n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_functions\n        \n            \n    \n\n    def generate_for_classes(self, class_inputs: List[ClassAnalysisInput]) -> List[Optional[ClassAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of classes.\"\"\"\n        if not class_inputs:\n            return []\n        \n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 62\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(class_input.model_dump(), indent=2)\n            for class_input in class_inputs\n        ]\n\n        conversations = [[SystemMessage(content=self.class_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_classes = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n\n            try:\n                batch_results = self.class_llm.batch(batch_conversations , config={\"max_concurrency\": self.batch_size})\n                all_validated_classes.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                # Falls ein Fehler auftritt, f√ºllen wir die Liste mit None auf, um die Reihenfolge zu wahren\n                all_validated_classes.extend([None] * len(batch_conversations))\n\n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_classes"
            start_line: 31
            end_line: 221
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[4]:
                - identifier: backend.HelperLLM.LLMHelper.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[6]: self,api_key,function_prompt_path,class_prompt_path,model_name,base_url
                  docstring: null
                  start_line: 36
                  end_line: 101
                - identifier: backend.HelperLLM.LLMHelper._configure_batch_settings
                  name: _configure_batch_settings
                  calls[0]:
                  called_by[0]:
                  args[2]: self,model_name
                  docstring: null
                  start_line: 103
                  end_line: 137
                - identifier: backend.HelperLLM.LLMHelper.generate_for_functions
                  name: generate_for_functions
                  calls[0]:
                  called_by[0]:
                  args[2]: self,function_inputs
                  docstring: Generates and validates documentation for a batch of functions.
                  start_line: 139
                  end_line: 178
                - identifier: backend.HelperLLM.LLMHelper.generate_for_classes
                  name: generate_for_classes
                  calls[0]:
                  called_by[0]:
                  args[2]: self,class_inputs
                  docstring: Generates and validates documentation for a batch of classes.
                  start_line: 183
                  end_line: 221
    "backend/MainLLM.py":
      ast_nodes:
        imports[9]: os,logging,sys,dotenv.load_dotenv,langchain_google_genai.ChatGoogleGenerativeAI,langchain_ollama.ChatOllama,langchain_openai.ChatOpenAI,langchain.messages.HumanMessage,langchain.messages.SystemMessage
        functions[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.MainLLM.MainLLM
            name: MainLLM
            docstring: Hauptklasse f√ºr die Interaktion mit dem LLM.
            source_code: "class MainLLM:\n    \"\"\"\n    Hauptklasse f√ºr die Interaktion mit dem LLM.\n    \"\"\"\n    def __init__(self, api_key: str, prompt_file_path: str, model_name: str = \"gemini-2.5-pro\", base_url: str = None):\n        if not api_key:\n            raise ValueError(\"Gemini API Key must be set.\")\n        \n        try:\n            with open(prompt_file_path, 'r', encoding='utf-8') as f:\n                self.system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"System prompt file not found at: {prompt_file_path}\")\n            raise\n        \n        self.model_name = model_name\n\n        if model_name.startswith(\"gemini-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n\n        elif model_name.startswith(\"gpt-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            if not SCADSLLM_URL:\n                raise ValueError(f\"SCADSLLM_URL environment variable is required for model {model_name}\")\n            \n            logging.info(f\"Connecting to Custom API at {SCADSLLM_URL} for model {model_name}\")\n            \n            self.llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                base_url=SCADSLLM_URL,\n                temperature=1.0,\n            )\n\n        else:\n            target_url = base_url if base_url else OLLAMA_BASE_URL\n            self.llm = ChatOllama(\n                model=model_name,\n                temperature=1.0,\n                base_url=target_url,\n            )\n\n        logging.info(f\"Main LLM initialized with model '{model_name}'.\")\n    \n    def call_llm(self, user_input: str):\n\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with HelperLLM input...\")\n\n        try:\n            response = self.llm.invoke(messages)\n            logging.info(\"LLM call successful.\")\n            return response.content\n        except Exception as e:\n            logging.error(f\"Error during LLM call: {e}\")\n            return None\n\n    def stream_llm(self, user_input: str):\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with 'stream'...\")\n\n        try:\n            stream_iterator = self.llm.stream(messages)\n            \n            for chunk in stream_iterator:\n                yield chunk.content\n        except Exception as e:\n            error_message = f\"\\n--- Error during LLM stream call: {e} ---\"\n            logging.error(error_message)\n            yield error_message"
            start_line: 22
            end_line: 106
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[3]:
                - identifier: backend.MainLLM.MainLLM.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[5]: self,api_key,prompt_file_path,model_name,base_url
                  docstring: null
                  start_line: 26
                  end_line: 73
                - identifier: backend.MainLLM.MainLLM.call_llm
                  name: call_llm
                  calls[0]:
                  called_by[0]:
                  args[2]: self,user_input
                  docstring: null
                  start_line: 75
                  end_line: 89
                - identifier: backend.MainLLM.MainLLM.stream_llm
                  name: stream_llm
                  calls[0]:
                  called_by[0]:
                  args[2]: self,user_input
                  docstring: null
                  start_line: 91
                  end_line: 106
    "backend/basic_info.py":
      ast_nodes:
        imports[7]: re,os,tomllib,typing.List,typing.Dict,typing.Any,typing.Optional
        functions[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.basic_info.ProjektInfoExtractor
            name: ProjektInfoExtractor
            docstring: "Extrahiert grundlegende Projektinformationen aus g√§ngigen Projektdateien\nwie README, pyproject.toml und requirements.txt."
            source_code: "class ProjektInfoExtractor:\n    \"\"\"\n    Extrahiert grundlegende Projektinformationen aus g√§ngigen Projektdateien\n    wie README, pyproject.toml und requirements.txt.\n    \"\"\"\n    def __init__(self):\n        self.INFO_NICHT_GEFUNDEN = \"Information not found\"\n        # Initialisiert die Struktur mit Platzhaltern\n        self.info = {\n            \"projekt_uebersicht\": {\n                \"titel\": self.INFO_NICHT_GEFUNDEN,\n                \"beschreibung\": self.INFO_NICHT_GEFUNDEN,\n                \"aktueller_status\": self.INFO_NICHT_GEFUNDEN,\n                \"key_features\": self.INFO_NICHT_GEFUNDEN,\n                \"tech_stack\": self.INFO_NICHT_GEFUNDEN,\n            },\n            \"installation\": {\n                \"dependencies\": self.INFO_NICHT_GEFUNDEN,\n                \"setup_anleitung\": self.INFO_NICHT_GEFUNDEN,\n                \"quick_start_guide\": self.INFO_NICHT_GEFUNDEN,\n            }\n        }\n\n    def _clean_content(self, content: str) -> str:\n        \"\"\"Entfernt Null-Bytes, die durch Encoding-Fehler (UTF-16 als UTF-8 gelesen) entstehen.\"\"\"\n        if not content:\n            return \"\"\n        return content.replace('\\x00', '')\n\n    def _finde_datei(self, patterns: List[str], dateien: List[Any]) -> Optional[Any]:\n        \"\"\"Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht.\"\"\"\n        for datei in dateien:\n            for pattern in patterns:\n                if datei.path.lower().endswith(pattern.lower()):\n                    return datei\n        return None\n\n    def _extrahiere_sektion_aus_markdown(self, inhalt: str, keywords: List[str]) -> Optional[str]:\n        \"\"\"\n        Extrahiert den Text unter einer Markdown-√úberschrift (##).\n        \"\"\"\n        # Erstellt ein Regex-Pattern, das auf jedes der Schl√ºsselw√∂rter reagiert\n        keyword_pattern = \"|\".join(re.escape(k) for k in keywords)\n        \n        # Sucht nach \"## Schl√ºsselwort\" und erfasst alles bis zur n√§chsten \"##\" oder dem Dateiende\n        pattern = re.compile(\n            rf\"##\\s*({keyword_pattern})\\s*\\n(.*?)(?=\\n##|\\Z)\",\n            re.IGNORECASE | re.DOTALL\n        )\n        match = pattern.search(inhalt)\n        if match:\n            return match.group(2).strip()\n        return None\n\n    def _parse_readme(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer README-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        if self.info[\"projekt_uebersicht\"][\"titel\"] == self.INFO_NICHT_GEFUNDEN:\n            title_match = re.search(r\"^\\s*#\\s*(.*)\", inhalt)\n            if title_match:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = title_match.group(1).strip()\n\n        # Beschreibung (Fallback)\n        if self.info[\"projekt_uebersicht\"][\"beschreibung\"] == self.INFO_NICHT_GEFUNDEN:\n            desc_match = re.search(r\"^\\s*#\\s*.*\\n+([^#\\n].*)\", inhalt, re.DOTALL)\n            if desc_match:\n                 self.info[\"projekt_uebersicht\"][\"beschreibung\"] = desc_match.group(1).strip().split('\\n\\n')[0]\n\n        # Key Features\n        features = self._extrahiere_sektion_aus_markdown(inhalt, [\"Features\", \"Key Features\", \"Merkmale\"])\n        if features:\n            self.info[\"projekt_uebersicht\"][\"key_features\"] = features\n            \n        # Tech Stack\n        tech_stack = self._extrahiere_sektion_aus_markdown(inhalt, [\"Tech Stack\", \"Technology\", \"Technologien\"])\n        if tech_stack:\n            self.info[\"projekt_uebersicht\"][\"tech_stack\"] = tech_stack\n            \n        # Status\n        status = self._extrahiere_sektion_aus_markdown(inhalt, [\"Status\", \"Current Status\"])\n        if status:\n            self.info[\"projekt_uebersicht\"][\"aktueller_status\"] = status\n\n        # Setup-Anleitung\n        setup = self._extrahiere_sektion_aus_markdown(inhalt, [\"Installation\", \"Setup\", \"Getting Started\"])\n        if setup:\n            self.info[\"installation\"][\"setup_anleitung\"] = setup\n            \n        # Quick Start\n        quick_start = self._extrahiere_sektion_aus_markdown(inhalt, [\"Quick Start\", \"Schnellstart\"])\n        if quick_start:\n            self.info[\"installation\"][\"quick_start_guide\"] = quick_start\n\n    def _parse_toml(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer pyproject.toml-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        if not tomllib:\n            print(\"Warnung: 'tomli' ist nicht installiert. pyproject.toml kann nicht analysiert werden.\")\n            return\n            \n        try:\n            data = tomllib.loads(inhalt)\n            project_data = data.get(\"project\", {})\n            \n            if \"name\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = project_data[\"name\"]\n            if \"description\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"beschreibung\"] = project_data[\"description\"]\n            if \"dependencies\" in project_data:\n                self.info[\"installation\"][\"dependencies\"] = project_data[\"dependencies\"]\n        except tomllib.TOMLDecodeError as e:\n            print(f\"Warnung: Fehler beim Parsen der pyproject.toml: {e}\")\n\n    def _parse_requirements(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer requirements.txt-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        # Nur f√ºllen, wenn noch keine Dependencies aus toml gefunden wurden\n        if self.info[\"installation\"][\"dependencies\"] == self.INFO_NICHT_GEFUNDEN:\n            lines = inhalt.splitlines()\n            dependencies = [\n                line.strip() for line in lines \n                if line.strip() and not line.strip().startswith('#')\n            ]\n            if dependencies:\n                self.info[\"installation\"][\"dependencies\"] = dependencies\n\n    def extrahiere_info(self, dateien: List[Any], repo_url: str) -> Dict[str, Any]:\n        \"\"\"\n        Orchestriert die Extraktion von Informationen.\n        \"\"\"\n        # Relevante Dateien finden\n        readme_datei = self._finde_datei([\"readme.md\", \"readme.rst\", \"readme.txt\", \"readme\"], dateien)\n        toml_datei = self._finde_datei([\"pyproject.toml\"], dateien)\n        req_datei = self._finde_datei([\"requirements.txt\"], dateien)\n\n        # Dateien parsen (mit Priorisierung)\n        if toml_datei:\n            self._parse_toml(toml_datei.content)\n\n        if req_datei:\n            self._parse_requirements(req_datei.content)\n            \n        if readme_datei:\n            self._parse_readme(readme_datei.content)\n            \n        # Finale Formatierung der Dependencies\n        deps = self.info[\"installation\"][\"dependencies\"]\n        if isinstance(deps, list):\n            if not deps:\n                self.info[\"installation\"][\"dependencies\"] = self.INFO_NICHT_GEFUNDEN\n            else:\n                self.info[\"installation\"][\"dependencies\"] = \"\\n\".join(f\"- {dep}\" for dep in deps)\n        \n        # Titel aus URL ableiten, falls nichts gefunden wurde (oder als Zusatzinfo)\n        if repo_url:\n            repo_name = os.path.basename(repo_url.removesuffix('.git'))\n            # Wenn noch kein Titel da ist oder er generisch war\n            if self.info[\"projekt_uebersicht\"][\"titel\"] == self.INFO_NICHT_GEFUNDEN:\n                 self.info[\"projekt_uebersicht\"][\"titel\"] = f\"{repo_name} documentation\"\n\n        return self.info"
            start_line: 9
            end_line: 172
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[8]:
                - identifier: backend.basic_info.ProjektInfoExtractor.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 14
                  end_line: 30
                - identifier: backend.basic_info.ProjektInfoExtractor._clean_content
                  name: _clean_content
                  calls[0]:
                  called_by[0]:
                  args[2]: self,content
                  docstring: "Entfernt Null-Bytes, die durch Encoding-Fehler (UTF-16 als UTF-8 gelesen) entstehen."
                  start_line: 32
                  end_line: 36
                - identifier: backend.basic_info.ProjektInfoExtractor._finde_datei
                  name: _finde_datei
                  calls[0]:
                  called_by[0]:
                  args[3]: self,patterns,dateien
                  docstring: "Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht."
                  start_line: 38
                  end_line: 44
                - identifier: backend.basic_info.ProjektInfoExtractor._extrahiere_sektion_aus_markdown
                  name: _extrahiere_sektion_aus_markdown
                  calls[0]:
                  called_by[0]:
                  args[3]: self,inhalt,keywords
                  docstring: Extrahiert den Text unter einer Markdown-√úberschrift (##).
                  start_line: 46
                  end_line: 61
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_readme
                  name: _parse_readme
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer README-Datei.
                  start_line: 63
                  end_line: 101
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_toml
                  name: _parse_toml
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer pyproject.toml-Datei.
                  start_line: 103
                  end_line: 122
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_requirements
                  name: _parse_requirements
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer requirements.txt-Datei.
                  start_line: 124
                  end_line: 136
                - identifier: backend.basic_info.ProjektInfoExtractor.extrahiere_info
                  name: extrahiere_info
                  calls[0]:
                  called_by[0]:
                  args[3]: self,dateien,repo_url
                  docstring: Orchestriert die Extraktion von Informationen.
                  start_line: 138
                  end_line: 172
    "backend/callgraph.py":
      ast_nodes:
        imports[9]: ast,networkx,os,pathlib.Path,typing.Dict,getRepo.GitRepository,getRepo.GitRepository,basic_info.ProjektInfoExtractor,os
        functions[2]:
          - mode: function_analysis
            identifier: backend.callgraph.make_safe_dot
            name: make_safe_dot
            args[2]: graph,out_path
            docstring: null
            source_code: "def make_safe_dot(graph: nx.DiGraph, out_path: str):\n    mapping = {}\n    H = graph.copy()\n    for i, n in enumerate(list(graph.nodes())):\n        safe = f\"n{i}\"           \n        mapping[n] = safe\n    H = nx.relabel_nodes(H, mapping)\n    for orig, safe in mapping.items():\n        H.nodes[safe][\"label\"] = orig\n    nx.drawing.nx_pydot.write_dot(H, out_path)"
            start_line: 173
            end_line: 182
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.callgraph.build_filtered_callgraph
            name: build_filtered_callgraph
            args[1]: repo
            docstring: Baut den globalen Call-Graphen und filtert ihn auf selbst geschriebene Funktionen.
            source_code: "def build_filtered_callgraph(repo: GitRepository) -> nx.DiGraph:\n    \"\"\"\n    Baut den globalen Call-Graphen und filtert ihn auf selbst geschriebene Funktionen.\n    \"\"\"\n    all_file_objects = repo.get_all_files()\n    own_functions = set()\n    file_trees = {}\n\n    for file in all_file_objects:\n        if not file.path.endswith(\".py\"):\n            continue\n        filename = Path(file.path).stem\n        tree = ast.parse(file.content)\n        file_trees[filename] = tree\n        visitor = CallGraph(filename)\n        visitor.visit(tree)\n        own_functions.update(visitor.function_set)\n\n    # Globalen Call-Graph bauen\n    global_graph = nx.DiGraph()\n    for filename, tree in file_trees.items():\n        visitor = CallGraph(filename)\n        visitor.visit(tree)\n        for caller, callees in visitor.edges.items():\n            if caller not in own_functions:\n                continue \n            for callee in callees:\n                if callee in own_functions:\n                    global_graph.add_edge(caller, callee)\n                    global_graph.add_node(caller)\n                    global_graph.add_node(callee)\n    return global_graph"
            start_line: 185
            end_line: 216
            context:
              calls[0]:
              called_by[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.callgraph.CallGraph
            name: CallGraph
            docstring: null
            source_code: "class CallGraph(ast.NodeVisitor):\n    def __init__(self, filename: str):\n        self.filename = filename\n        self.current_function = None\n        self.current_class = None\n        \n        self.local_defs: dict[str, str] = {}\n        self.graph = nx.DiGraph()\n        self.import_mapping: dict[str, str] = {}\n        self.function_set: set[str] = set()\n        self.edges: Dict[str, set[str]] = {}\n\n    def _recursive_call(self, node):\n        \"\"\"\n        Liefert eine Liste der Namenskomponenten als dotted string z.B. ['pkg', 'mod', 'Class', 'method']\n        \"\"\"\n        if isinstance(node, ast.Call):\n            return self._recursive_call(node.func)\n        if isinstance(node, ast.Name):\n            return [node.id]\n        if isinstance(node, ast.Attribute):\n            parts = self._recursive_call(node.value)\n            parts.append(node.attr)\n            return parts\n        return []\n\n    def _resolve_all_callee_names(self, callee_nodes: list[list[str]]) -> list[str]:\n        \"\"\"\n        callee_nodes ist jetzt eine Liste von Listen (Name-Steps).\n        Wir pr√ºfen zuerst lokale Definitionen, dann import_mapping.\n        \"\"\"\n        resolved = []\n        for parts in callee_nodes:\n            if not parts:\n                continue\n            simple = parts[-1]\n            dotted = \".\".join(parts[-2:]) if len(parts) >= 2 else simple\n\n            if simple in self.local_defs:\n                resolved.append(self.local_defs[simple])\n                continue\n            if dotted in self.local_defs:\n                resolved.append(self.local_defs[dotted])\n                continue\n\n            first = parts[0]\n            if first in self.import_mapping:\n                mod = self.import_mapping[first]\n                rest = \"::\".join(parts[1:]) if len(parts) > 1 else simple\n                resolved.append(f\"{mod}::{rest}\")\n                continue\n\n            if self.current_class:\n                resolved.append(f\"{self.filename}::{parts[0]}::{simple}\" if len(parts)==1 else f\"{self.filename}::{'::'.join(parts)}\")\n            else:\n                resolved.append(f\"{self.filename}::{ '::'.join(parts)}\")\n        return resolved\n\n    def _make_full_name(self, basename: str, class_name: str | None = None) -> str:\n        if class_name:\n            return f\"{self.filename}::{class_name}::{basename}\"\n        return f\"{self.filename}::{basename}\"\n    \n    def _current_caller(self) -> str:\n        if self.current_function:\n            return self.current_function\n        return f\"<{self.filename}>\" if self.filename else \"<global-scope>\"\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            module_name = alias.name\n            module_asname = alias.asname if alias.asname else module_name\n            self.import_mapping[module_asname] = module_name\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        module_name = node.module.split(\".\")[-1] if node.module else \"\"\n        for alias in node.names:\n            self.import_mapping[alias.asname or alias.name] = f\"{module_name}\" if module_name else alias.name\n\n    def visit_ClassDef(self, node: ast.ClassDef):\n        prev_class = self.current_class\n        self.current_class = node.name\n        self.generic_visit(node)\n        self.current_class = prev_class\n        \n    def visit_FunctionDef(self, node):\n        prev_function = self.current_function\n        full_name = self._make_full_name(node.name, self.current_class)\n        self.local_defs[node.name] = full_name\n        if self.current_class:\n            self.local_defs[f\"{self.current_class}.{node.name}\"] = full_name\n\n        self.current_function = full_name\n        self.graph.add_node(self.current_function)\n        self.generic_visit(node)\n        self.function_set.add(self.current_function)\n        self.current_function = prev_function\n\n    def visit_AsyncFunctionDef(self, node):\n        self.visit_FunctionDef(node)\n\n    def visit_Call(self, node):\n        caller = self._current_caller()\n        parts = self._recursive_call(node)\n        resolved_callees = self._resolve_all_callee_names([parts])\n\n        if caller not in self.edges:\n            self.edges[caller] = set()\n\n        for callee in resolved_callees:\n            if callee:\n                self.edges[caller].add(callee)\n        self.generic_visit(node)\n\n    def visit_If(self, node):\n        if (isinstance(node.test, ast.Compare) and\n            isinstance(node.test.left, ast.Name) and \n            node.test.left.id == \"__name__\"):\n            caller_backup = self.current_function\n            self.current_function = \"<main_block>\"\n            self.generic_visit(node)\n            self.current_function = caller_backup\n        else:\n            self.generic_visit(node)"
            start_line: 10
            end_line: 134
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[12]:
                - identifier: backend.callgraph.CallGraph.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,filename
                  docstring: null
                  start_line: 11
                  end_line: 20
                - identifier: backend.callgraph.CallGraph._recursive_call
                  name: _recursive_call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: "Liefert eine Liste der Namenskomponenten als dotted string z.B. ['pkg', 'mod', 'Class', 'method']"
                  start_line: 22
                  end_line: 34
                - identifier: backend.callgraph.CallGraph._resolve_all_callee_names
                  name: _resolve_all_callee_names
                  calls[0]:
                  called_by[0]:
                  args[2]: self,callee_nodes
                  docstring: "callee_nodes ist jetzt eine Liste von Listen (Name-Steps).\nWir pr√ºfen zuerst lokale Definitionen, dann import_mapping."
                  start_line: 36
                  end_line: 66
                - identifier: backend.callgraph.CallGraph._make_full_name
                  name: _make_full_name
                  calls[0]:
                  called_by[0]:
                  args[3]: self,basename,class_name
                  docstring: null
                  start_line: 68
                  end_line: 71
                - identifier: backend.callgraph.CallGraph._current_caller
                  name: _current_caller
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 73
                  end_line: 76
                - identifier: backend.callgraph.CallGraph.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 78
                  end_line: 83
                - identifier: backend.callgraph.CallGraph.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 85
                  end_line: 88
                - identifier: backend.callgraph.CallGraph.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 90
                  end_line: 94
                - identifier: backend.callgraph.CallGraph.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 96
                  end_line: 107
                - identifier: backend.callgraph.CallGraph.visit_AsyncFunctionDef
                  name: visit_AsyncFunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 109
                  end_line: 110
                - identifier: backend.callgraph.CallGraph.visit_Call
                  name: visit_Call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 112
                  end_line: 123
                - identifier: backend.callgraph.CallGraph.visit_If
                  name: visit_If
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 125
                  end_line: 134
    "backend/converter.py":
      ast_nodes:
        imports[4]: logging,nbformat,base64,nbformat.reader.NotJSONError
        functions[5]:
          - mode: function_analysis
            identifier: backend.converter.wrap_cdata
            name: wrap_cdata
            args[1]: content
            docstring: Wraps content in CDATA tags.
            source_code: "def wrap_cdata(content):\n    \"\"\"Wraps content in CDATA tags.\"\"\"\n    return f\"<![CDATA[\\n{content}\\n]]>\""
            start_line: 8
            end_line: 10
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.converter.extract_output_content
            name: extract_output_content
            args[2]: outputs,image_list
            docstring: "Extracts text and handles images by decoding Base64 to bytes.\nReturns: A list of text strings or placeholders."
            source_code: "def extract_output_content(outputs, image_list):\n    \"\"\"\n    Extracts text and handles images by decoding Base64 to bytes.\n    Returns: A list of text strings or placeholders.\n    \"\"\"\n    extracted_xml_snippets = []\n    \n    for output in outputs:\n\n        if output.output_type in ('display_data', 'execute_result') and hasattr(output, 'data'):\n            data = output.data\n            \n            # Helper to process image\n            def process_image(mime_type):\n                if mime_type in data:\n                    try:\n                        b64_str = data[mime_type]\n                        b64_str = b64_str.replace('\\n', '')\n\n                        image_index = len(image_list)\n                        image_list.append({\n                            \"mime_type\": mime_type,\n                            \"data\": b64_str\n                        })\n\n                        return f'\\n<IMAGE_PLACEHOLDER index=\"{image_index}\" mime=\"{mime_type}\"/>\\n'\n                    except Exception as e:\n                        return f\"<ERROR>Could not decode image: {e}</ERROR>\"\n                return None\n\n            # Ignore jpeg if png is found\n            img_xml = process_image('image/png')\n            if not img_xml:\n                img_xml = process_image('image/jpeg')\n            \n            if img_xml:\n                extracted_xml_snippets.append(img_xml)\n            elif 'text/plain' in data:\n                extracted_xml_snippets.append(data['text/plain'])\n\n        elif output.output_type == 'stream':\n            extracted_xml_snippets.append(output.text)\n            \n        elif output.output_type == 'error':\n            extracted_xml_snippets.append(f\"{output.ename}: {output.evalue}\")\n\n    return extracted_xml_snippets"
            start_line: 12
            end_line: 58
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.converter.process_image
            name: process_image
            args[1]: mime_type
            docstring: null
            source_code: "def process_image(mime_type):\n                if mime_type in data:\n                    try:\n                        b64_str = data[mime_type]\n                        b64_str = b64_str.replace('\\n', '')\n\n                        image_index = len(image_list)\n                        image_list.append({\n                            \"mime_type\": mime_type,\n                            \"data\": b64_str\n                        })\n\n                        return f'\\n<IMAGE_PLACEHOLDER index=\"{image_index}\" mime=\"{mime_type}\"/>\\n'\n                    except Exception as e:\n                        return f\"<ERROR>Could not decode image: {e}</ERROR>\"\n                return None"
            start_line: 25
            end_line: 40
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.converter.convert_notebook_to_xml
            name: convert_notebook_to_xml
            args[1]: file_content
            docstring: null
            source_code: "def convert_notebook_to_xml(file_content):\n    try:\n        nb = nbformat.reads(file_content, as_version=4)\n    except NotJSONError:\n        return \"<ERROR>Could not parse file as JSON/Notebook</ERROR>\", []\n\n    xml_parts = []\n    extracted_images = [] \n\n    for cell in nb.cells:\n        if cell.cell_type == 'markdown':\n            cell_xml = f'<CELL type=\"markdown\">\\n{cell.source}\\n</CELL>'\n            xml_parts.append(cell_xml)\n\n        elif cell.cell_type == 'code':\n            source_code = wrap_cdata(cell.source)\n            xml_parts.append(f'<CELL type=\"code\">\\n{source_code}\\n</CELL>')\n\n            if hasattr(cell, 'outputs') and cell.outputs:\n                snippets = extract_output_content(cell.outputs, extracted_images)\n                \n                content = \"\\n\".join(snippets)\n                \n                if content.strip():\n                    wrapped_output = wrap_cdata(content)\n                    xml_parts.append(f'<CELL type=\"output\">\\n{wrapped_output}\\n</CELL>')\n\n    return \"\\n\\n\".join(xml_parts), extracted_images"
            start_line: 60
            end_line: 87
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.converter.process_repo_notebooks
            name: process_repo_notebooks
            args[1]: repo_files
            docstring: null
            source_code: "def process_repo_notebooks(repo_files):\n    notebook_files = [f for f in repo_files if f.path.endswith('.ipynb')]\n    logging.info(f\"Found {len(notebook_files)} notebooks.\")\n        \n    results = {}\n\n    for nb_file in notebook_files:\n        logging.info(f\"Processing: {nb_file.path}\")\n\n        xml_output, images = convert_notebook_to_xml(nb_file.content)\n        results[nb_file.path] = {\n            \"xml\": xml_output,\n            \"images\": images\n        }\n            \n    return results"
            start_line: 90
            end_line: 105
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "backend/getRepo.py":
      ast_nodes:
        imports[5]: tempfile,git.Repo,git.GitCommandError,logging,os
        functions[0]:
        classes[2]:
          - mode: class_analysis
            identifier: backend.getRepo.RepoFile
            name: RepoFile
            docstring: "Repr√§sentiert eine einzelne Datei in einem Git-Repository.\n\nDer Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tats√§chlichem Zugriff."
            source_code: "class RepoFile:\n    \"\"\"\n    Repr√§sentiert eine einzelne Datei in einem Git-Repository.\n    \n    Der Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tats√§chlichem Zugriff.\n    \"\"\"\n    def __init__(self, file_path, commit_tree):\n        \"\"\"\n        Initialisiert das RepoFile-Objekt.\n\n        Args:\n            file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n            commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt.\n        \"\"\"\n        self.path = file_path\n        self._tree = commit_tree\n        \n\n        self._blob = None\n        self._content = None\n        self._size = None\n\n    @property\n    def blob(self):\n        \"\"\"Lazy-l√§dt das Git-Blob-Objekt.\"\"\"\n        if self._blob is None:\n            try:\n                self._blob = self._tree[self.path]\n            except KeyError:\n                raise FileNotFoundError(f\"Datei '{self.path}' konnte im Commit-Tree nicht gefunden werden.\")\n        return self._blob\n\n    @property\n    def content(self):\n        \"\"\"Lazy-l√§dt und gibt den dekodierten Inhalt der Datei zur√ºck.\"\"\"\n        if self._content is None:\n            self._content = self.blob.data_stream.read().decode('utf-8', errors='ignore')\n        return self._content\n\n    @property\n    def size(self):\n        \"\"\"Lazy-l√§dt und gibt die Gr√∂√üe der Datei in Bytes zur√ºck.\"\"\"\n        if self._size is None:\n            self._size = self.blob.size\n        return self._size\n        \n    def analyze_word_count(self):\n        \"\"\"\n        Eine Beispiel-Analyse-Methode. Z√§hlt die W√∂rter im Dateiinhalt.\n        \"\"\"\n        return len(self.content.split())\n\n    def __repr__(self):\n        \"\"\"Gibt eine n√ºtzliche String-Repr√§sentation des Objekts zur√ºck.\"\"\"\n        return f\"<RepoFile(path='{self.path}')>\"\n    \n    def to_dict(self, include_content=False):\n        data = {\n            \"path\": self.path,\n            \"name\": os.path.basename(self.path),\n            \"size\": self.size,\n            \"type\": \"file\"\n        }\n        if include_content:\n            data[\"content\"] = self.content\n        return data"
            start_line: 7
            end_line: 72
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[7]:
                - identifier: backend.getRepo.RepoFile.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[3]: self,file_path,commit_tree
                  docstring: "Initialisiert das RepoFile-Objekt.\n\nArgs:\n    file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n    commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt."
                  start_line: 13
                  end_line: 27
                - identifier: backend.getRepo.RepoFile.blob
                  name: blob
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt das Git-Blob-Objekt.
                  start_line: 30
                  end_line: 37
                - identifier: backend.getRepo.RepoFile.content
                  name: content
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt und gibt den dekodierten Inhalt der Datei zur√ºck.
                  start_line: 40
                  end_line: 44
                - identifier: backend.getRepo.RepoFile.size
                  name: size
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt und gibt die Gr√∂√üe der Datei in Bytes zur√ºck.
                  start_line: 47
                  end_line: 51
                - identifier: backend.getRepo.RepoFile.analyze_word_count
                  name: analyze_word_count
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Eine Beispiel-Analyse-Methode. Z√§hlt die W√∂rter im Dateiinhalt.
                  start_line: 53
                  end_line: 57
                - identifier: backend.getRepo.RepoFile.__repr__
                  name: __repr__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Gibt eine n√ºtzliche String-Repr√§sentation des Objekts zur√ºck.
                  start_line: 59
                  end_line: 61
                - identifier: backend.getRepo.RepoFile.to_dict
                  name: to_dict
                  calls[0]:
                  called_by[0]:
                  args[2]: self,include_content
                  docstring: null
                  start_line: 63
                  end_line: 72
          - mode: class_analysis
            identifier: backend.getRepo.GitRepository
            name: GitRepository
            docstring: "Verwaltet ein Git-Repository, einschlie√ülich Klonen in ein tempor√§res\nVerzeichnis und Bereitstellung von RepoFile-Objekten."
            source_code: "class GitRepository:\n    \"\"\"\n    Verwaltet ein Git-Repository, einschlie√ülich Klonen in ein tempor√§res\n    Verzeichnis und Bereitstellung von RepoFile-Objekten.\n    \"\"\"\n    def __init__(self, repo_url):\n        self.repo_url = repo_url\n        self.temp_dir = tempfile.mkdtemp()\n        self.repo = None\n\n        self.files = []\n        \n        try:\n            logging.info(f\"Clone Repository {self.repo_url}...\")\n            self.repo = Repo.clone_from(self.repo_url, self.temp_dir)\n            self.latest_commit = self.repo.head.commit\n            self.commit_tree = self.latest_commit.tree\n            logging.info(\"Cloning successful.\")\n        except GitCommandError as e:\n            self.close()\n            raise RuntimeError(f\"Error cloning repository: {e}\") from e\n\n    def get_all_files(self):\n        \"\"\"\n        Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zur√ºck.\n\n        Returns:\n            list[RepoFile]: Eine Liste von RepoFile-Instanzen.\n        \"\"\"\n        file_paths = self.repo.git.ls_files().split('\\n')\n        self.files = [RepoFile(path, self.commit_tree) for path in file_paths if path]\n        return self.files\n\n    def close(self):\n        \"\"\"L√∂scht das tempor√§re Verzeichnis und dessen Inhalt.\"\"\"\n        if self.temp_dir:\n            print(f\"\\nL√∂sche tempor√§res Verzeichnis: {self.temp_dir}\")\n            self.temp_dir = None\n            \n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def get_file_tree(self, include_content=False):\n\n        if not self.files:\n            self.get_all_files()\n\n        tree = {\"name\": \"root\", \"type\": \"directory\", \"children\": []}\n\n        for file_obj in self.files:\n            parts = file_obj.path.split('/')\n            current_level = tree[\"children\"]\n            \n            # Iteriere durch die Ordnerstruktur\n            for part in parts[:-1]:\n                # Suche, ob Ordner schon existiert\n                found = next((item for item in current_level if item[\"name\"] == part and item[\"type\"] == \"directory\"), None)\n                if not found:\n                    new_dir = {\"name\": part, \"type\": \"directory\", \"children\": []}\n                    current_level.append(new_dir)\n                    current_level = new_dir[\"children\"]\n                else:\n                    current_level = found[\"children\"]\n            \n            # Datei am Ende hinzuf√ºgen\n            current_level.append(file_obj.to_dict(include_content=include_content))\n\n        return tree"
            start_line: 78
            end_line: 148
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[6]:
                - identifier: backend.getRepo.GitRepository.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,repo_url
                  docstring: null
                  start_line: 83
                  end_line: 98
                - identifier: backend.getRepo.GitRepository.get_all_files
                  name: get_all_files
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: "Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zur√ºck.\n\nReturns:\n    list[RepoFile]: Eine Liste von RepoFile-Instanzen."
                  start_line: 100
                  end_line: 109
                - identifier: backend.getRepo.GitRepository.close
                  name: close
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: L√∂scht das tempor√§re Verzeichnis und dessen Inhalt.
                  start_line: 111
                  end_line: 115
                - identifier: backend.getRepo.GitRepository.__enter__
                  name: __enter__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 117
                  end_line: 118
                - identifier: backend.getRepo.GitRepository.__exit__
                  name: __exit__
                  calls[0]:
                  called_by[0]:
                  args[4]: self,exc_type,exc_val,exc_tb
                  docstring: null
                  start_line: 120
                  end_line: 121
                - identifier: backend.getRepo.GitRepository.get_file_tree
                  name: get_file_tree
                  calls[0]:
                  called_by[0]:
                  args[2]: self,include_content
                  docstring: null
                  start_line: 123
                  end_line: 148
    "backend/main.py":
      ast_nodes:
        imports[28]: json,math,logging,os,re,time,math,datetime.datetime,matplotlib.pyplot,datetime.datetime,pathlib.Path,dotenv.load_dotenv,getRepo.GitRepository,AST_Schema.ASTAnalyzer,MainLLM.MainLLM,basic_info.ProjektInfoExtractor,HelperLLM.LLMHelper,relationship_analyzer.ProjectAnalyzer,schemas.types.FunctionContextInput,schemas.types.FunctionAnalysisInput,schemas.types.ClassContextInput,schemas.types.ClassAnalysisInput,schemas.types.MethodContextInput,toon_format.encode,toon_format.count_tokens,toon_format.estimate_savings,toon_format.compare_formats,converter.process_repo_notebooks
        functions[7]:
          - mode: function_analysis
            identifier: backend.main.create_savings_chart
            name: create_savings_chart
            args[4]: json_tokens,toon_tokens,savings_percent,output_path
            docstring: Erstellt ein Balkendiagramm f√ºr den Token-Vergleich und speichert es.
            source_code: "def create_savings_chart(json_tokens, toon_tokens, savings_percent, output_path):\n    \"\"\"Erstellt ein Balkendiagramm f√ºr den Token-Vergleich und speichert es.\"\"\"\n    labels = ['JSON', 'TOON']\n    values = [json_tokens, toon_tokens]\n    colors = ['#ff9999', '#66b3ff']\n\n    plt.figure(figsize=(8, 6))\n    bars = plt.bar(labels, values, color=colors, width=0.5)\n\n    # Titel und Beschriftungen\n    plt.title(f'Token-Vergleich: {savings_percent:.2f}% Einsparung', fontsize=14)\n    plt.ylabel('Anzahl Token')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n\n    # Werte √ºber den Balken anzeigen\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width() / 2.0, height, \n                 f'{int(height):,}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n\n    # Speichern\n    plt.savefig(output_path)\n    plt.close()"
            start_line: 33
            end_line: 55
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.calculate_net_time
            name: calculate_net_time
            args[5]: start_time,end_time,total_items,batch_size,model_name
            docstring: Berechnet die Dauer abz√ºglich der Sleep-Zeiten f√ºr Rate-Limits.
            source_code: "def calculate_net_time(start_time, end_time, total_items, batch_size, model_name):\n    \"\"\"Berechnet die Dauer abz√ºglich der Sleep-Zeiten f√ºr Rate-Limits.\"\"\"\n    total_duration = end_time - start_time\n    \n    if not model_name.startswith(\"gemini-\"):\n        return total_duration\n\n    if total_items == 0:\n        return 0\n\n    num_batches = math.ceil(total_items / batch_size)\n    sleep_count = max(0, num_batches - 1)\n    total_sleep_time = sleep_count * 61\n    \n    net_time = total_duration - total_sleep_time\n    return max(0, net_time)"
            start_line: 57
            end_line: 72
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.main_workflow
            name: main_workflow
            args[4]: input,api_keys,model_names,status_callback
            docstring: null
            source_code: "def main_workflow(input, api_keys: dict, model_names: dict, status_callback=None):\n    \n    def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)\n\n    update_status(\"üîç Analysiere Input...\")\n    \n    user_input = input\n    \n    # API Key & Ollama Base URL aus Frontend holen\n    gemini_api_key = api_keys.get(\"gemini\")\n    openai_api_key = api_keys.get(\"gpt\")\n    scadsllm_api_key = api_keys.get(\"scadsllm\")\n    scadsllm_base_url = api_keys.get(\"scadsllm_base_url\")\n    ollama_base_url = api_keys.get(\"ollama\")\n    base_url = None\n\n    if model_names[\"helper\"].startswith(\"gpt-\"):\n        helper_api_key = openai_api_key\n    elif model_names[\"helper\"].startswith(\"gemini-\"):\n        helper_api_key = gemini_api_key\n    elif \"/\" in model_names[\"helper\"] or model_names[\"helper\"].startswith(\"alias-\") or any(x in model_names[\"helper\"] for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        helper_api_key = scadsllm_api_key\n        base_url = scadsllm_base_url\n    else:\n        helper_api_key = None\n        base_url = ollama_base_url\n    if model_names[\"main\"].startswith(\"gpt-\"):\n        main_api_key = openai_api_key\n    elif model_names[\"main\"].startswith(\"gemini-\"):\n        main_api_key = gemini_api_key\n    elif \"/\" in model_names[\"main\"] or model_names[\"main\"].startswith(\"alias-\") or any(x in model_names[\"main\"] for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        main_api_key = scadsllm_api_key\n        base_url = scadsllm_base_url\n    else:\n        main_api_key = None\n        base_url = ollama_base_url\n\n    # Standardeinstellungen f√ºr Modelle\n    helper_model = model_names.get(\"helper\", \"gpt-5-mini\")\n    main_model = model_names.get(\"main\", \"gpt-5.1\")\n\n    # Error Handling f√ºr fehlende API Keys\n    if not gemini_api_key and \"gemini\" in helper_model:\n        raise ValueError(\"Gemini API Key was not provided in api_keys dictionary.\")\n\n    # URL Extraktion\n    repo_url = None\n    url_pattern = r\"https?://(?:www\\.)?github\\.com/[^\\s]+\"\n    match = re.search(url_pattern, user_input)\n\n    if match:\n        repo_url = match.group(0)\n        logging.info(f\"Extracted repository URL: {repo_url}\")\n    else:\n        raise ValueError(\"Could not find a valid URL in the provided input.\")\n    \n    # Repo klonen und Dateien extrahieren\n    update_status(f\"‚¨áÔ∏è Klone Repository: {repo_url} ...\")\n    \n    repo_files = []\n    local_repo_path = \"\" \n\n    try: \n\n        with GitRepository(repo_url) as repo:\n            repo_files = repo.get_all_files()\n            local_repo_path = repo.temp_dir\n\n            logging.info(f\"Total files retrieved: {len(repo_files)}\")\n\n    except Exception as e:\n        logging.error(f\"Error cloning repository: {e}\")\n        raise \n\n\n    # Extrahiere Basic Infos\n    update_status(\"‚ÑπÔ∏è Extrahiere Basis-Informationen...\")\n    try:\n        info_extractor = ProjektInfoExtractor()\n        basic_project_info = info_extractor.extrahiere_info(dateien=repo_files, repo_url=repo_url)\n        logging.info(\"Basic project info extracted\")\n    except Exception as e:\n        logging.error(f\"Error extracting basic project info: {e}\")\n        basic_project_info = \"Could not extract basic info.\"\n\n    # Erstelle Repository Dateibaum\n    update_status(\"üå≤ Erstelle Repository Dateibaum...\")\n    try:\n        repo_file_tree = repo.get_file_tree()\n        logging.info(\"Repository file tree constructed\")\n    except Exception as e:\n        logging.error(f\"Error constructing repository file tree: {e}\")\n        repo_file_tree = \"Could not create file tree.\"\n\n    # Relationship Analyse durchf√ºhren\n    update_status(\"üîó Analysiere Beziehungen (Calls & Instanziierungen)...\")\n    try:\n        rel_analyzer = ProjectAnalyzer(project_root=local_repo_path)\n        rel_analyzer.analyze()\n        \n        raw_relationships = rel_analyzer.get_raw_relationships()\n        \n        logging.info(f\"Relationships analyzed.\")\n    except Exception as e:\n        logging.error(f\"Error in relationship analyzer: {e}\")\n        raw_relationships = {\"outgoing\": {}, \"incoming\": {}}\n\n    # Erstelle AST Schema\n    update_status(\"üå≥ Erstelle Abstract Syntax Tree (AST)...\")\n    try:        \n        ast_analyzer = ASTAnalyzer()   \n        ast_schema = ast_analyzer.analyze_repository(files=repo_files, repo=repo)\n        logging.info(\"AST schema created\")\n    except Exception as e:\n        logging.error(f\"Error retrieving repository files: {e}\")\n        raise\n\n    # Anreichern des AST Schemas mit Relationship Daten\n    update_status(\"‚ûï Reiche AST mit Beziehungsdaten an...\")            \n    try:   \n        ast_schema = ast_analyzer.merge_relationship_data(ast_schema, raw_relationships)\n        logging.info(\"AST schema created and enriched\")\n\n    except Exception as e:\n        logging.error(f\"Error processing repository: {e}\")\n        raise\n\n    # Vorbereitung der HelperLLM Eingaben\n    update_status(\"‚öôÔ∏è Bereite Daten f√ºr Helper LLM vor...\")\n    \n    helper_llm_function_input = []\n    helper_llm_class_input = []\n\n    try:\n        for filename, file_data in ast_schema['files'].items():\n            ast_nodes = file_data.get('ast_nodes', {})\n            imports = ast_nodes.get('imports', [])\n            functions = ast_nodes.get('functions', [])\n            classes = ast_nodes.get('classes', [])\n\n            for function in functions:\n                context = function.get('context', {})\n                \n                raw_called_by = context.get('called_by', [])\n                clean_called_by = [cb for cb in raw_called_by if isinstance(cb, dict)]\n\n                function_context = FunctionContextInput(\n                    calls = context.get('calls', []),\n                    called_by = clean_called_by \n                )\n                \n                function_input = FunctionAnalysisInput(\n                    mode = function.get('mode', 'function_analysis'),\n                    identifier = function.get('identifier'),\n                    source_code = function.get('source_code'),\n                    imports = imports,\n                    context = function_context\n                )\n                \n                helper_llm_function_input.append(function_input)\n\n            for _class in classes:\n                context = _class.get('context', {})\n                \n                method_context_inputs = []\n                for method in context.get('method_context', []):\n                    \n                    raw_method_called_by = method.get('called_by', [])\n                    clean_method_called_by = [cb for cb in raw_method_called_by if isinstance(cb, dict)]\n\n                    method_context_inputs.append(\n                        MethodContextInput(\n                            identifier=method.get('identifier'),\n                            calls=method.get('calls', []),\n                            called_by=clean_method_called_by, \n                            args=method.get('args', []),\n                            docstring=method.get('docstring')\n                        )\n                    )\n\n                raw_instantiated_by = context.get('instantiated_by', [])\n                clean_instantiated_by = [ib for ib in raw_instantiated_by if isinstance(ib, dict)]\n\n                class_context = ClassContextInput(\n                    dependencies = context.get('dependencies', []),\n                    instantiated_by = clean_instantiated_by, \n                    method_context = method_context_inputs\n                )\n\n                class_input = ClassAnalysisInput(\n                    mode = _class.get('mode', 'class_analysis'),\n                    identifier =_class.get('identifier'),\n                    source_code = _class.get('source_code'), \n                    imports = imports, \n                    context = class_context\n                )\n                \n                helper_llm_class_input.append(class_input)\n\n    except Exception as e:\n        logging.error(f\"Error preparing inputs for Helper LLM: {e}\")\n        raise\n    \n    # Initialisiere HelperLLM\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    \n    llm_helper = LLMHelper(\n        api_key=helper_api_key, \n        function_prompt_path=function_prompt_file, \n        class_prompt_path=class_prompt_file,\n        model_name=helper_model,\n        base_url=base_url,\n    )\n    \n    if ollama_base_url:\n        os.environ[\"OLLAMA_BASE_URL\"] = ollama_base_url\n\n    # Initialisiere Ergebniscontainer\n    analysis_results = {}\n    function_analysis_results = []\n    class_analysis_results = []\n\n    # Call HelperLLM f√ºr Funktionen\n    update_status(f\"ü§ñ Helper LLM: Analysiere {len(helper_llm_function_input)} Funktionen ({helper_model})...\")\n\n    try:\n        net_time_func = 0\n        if len(helper_llm_function_input) > 0:\n\n            logging.info(\"\\n--- Generating documentation for Functions ---\")\n            t_start_func = time.time()    \n            function_analysis_results = llm_helper.generate_for_functions(helper_llm_function_input)    \n            t_end_func = time.time()\n            net_time_func = calculate_net_time(t_start_func, t_end_func, len(helper_llm_function_input), llm_helper.batch_size, helper_model)\n\n        if len(function_analysis_results) != 0:\n            for doc in function_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"functions\" not in analysis_results:\n                        analysis_results[\"functions\"] = {}\n                    analysis_results[\"functions\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(f\"Failed to generate doc for a function\") \n    except Exception as e:\n        logging.error(f\"Error during Helper LLM function analysis: {e}\")\n        raise\n    \n\n    # Call HelperLLM f√ºr Klassen\n    try:\n        net_time_class = 0\n        if len(helper_llm_class_input) > 0:\n            # Rate Limit Sleep f√ºr Gemini Modelle\n            if llm_helper.model_name.startswith(\"gemini-\") & (len(helper_llm_function_input) > 0):\n                update_status(\"üí§ Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n                time.sleep(65)\n            \n            update_status(f\"ü§ñ Helper LLM: Analysiere {len(helper_llm_class_input)} Klassen ({helper_model})...\")\n            \n            logging.info(\"\\n--- Generating documentation for Classes ---\")\n            t_start_class = time.time()\n            class_analysis_results = llm_helper.generate_for_classes(helper_llm_class_input)\n            t_end_class = time.time()\n            net_time_class = calculate_net_time(t_start_class, t_end_class, len(helper_llm_class_input), llm_helper.batch_size, helper_model)\n\n        if len(class_analysis_results) != 0:\n            for doc in class_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"classes\" not in analysis_results:\n                        analysis_results[\"classes\"] = {}\n                    analysis_results[\"classes\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(f\"Failed to generate doc for a class\")\n    except Exception as e:\n        logging.error(f\"Error during Helper LLM class analysis: {e}\")\n        raise\n\n    total_helper_time = net_time_func + net_time_class\n\n    # MainLLM Input Vorbereitung\n    main_llm_input = {\n        \"basic_info\": basic_project_info,\n        \"file_tree\": repo_file_tree,\n        \"ast_schema\": ast_schema,\n        \"analysis_results\": analysis_results\n    }\n\n    # Speichern als JSON (Optional)\n    main_llm_input_json = json.dumps(main_llm_input, indent=2)\n    # with open(\"output.json\", \"w\", encoding=\"utf-8\") as f:\n    #     f.write(main_llm_input_json)\n    #     logging.info(\"JSON-Datei wurde gespeichert.\")\n\n    # Konvertiere Input in Toon Format\n    main_llm_input_toon = encode(main_llm_input)\n\n    #Speichern in TOON Format (Optional)\n    # with open(\"output.toon\", \"w\", encoding=\"utf-8\") as f:\n    #     f.write(main_llm_input_toon)\n    #     logging.info(\"output.toon erfolgreich gespeichert.\")\n    \n    # Token Evaluation\n    savings_data = None\n    try:\n        logging.info(\"--- Evaluierung der Token-Ersparnis ---\")\n        savings_data = estimate_savings(main_llm_input)\n        logging.info(f\"JSON Tokens: {savings_data['json_tokens']}\")\n        logging.info(f\"TOON Tokens: {savings_data['toon_tokens']}\")\n        logging.info(f\"Ersparnis:   {savings_data['savings_percent']:.2f}%\")\n        \n    except Exception as e:\n        logging.warning(f\"Token evaluation could not be performed: {e}\")    \n\n\n\n    prompt_file_mainllm = \"SystemPrompts/SystemPromptMainLLM.txt\"\n    prompt_file_mainllm_toon = \"SystemPrompts/SystemPromptMainLLMToon.txt\"\n    # MainLLM Ausf√ºhrung\n    main_llm = MainLLM(\n        api_key=main_api_key, \n        prompt_file_path=prompt_file_mainllm_toon,\n        model_name=main_model,\n        base_url=base_url,\n    )\n\n\n    # RPM Limit Sleep f√ºr Gemini Modelle\n    if llm_helper.model_name == main_llm.model_name and main_llm.model_name.startswith(\"gemini-\"):\n        time.sleep(65)\n        update_status(\"üí§ Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n\n    # Call MainLLM f√ºr finalen Report\n    update_status(f\"üß† Main LLM: Generiere finalen Report ({main_model})...\")\n    try:\n        total_main_time = 0\n        logging.info(\"\\n--- Generating Final Report ---\")\n        t_start_main = time.time()\n        final_report = main_llm.call_llm(main_llm_input_toon)\n        t_end_main = time.time()\n        total_main_time = t_end_main - t_start_main\n    except Exception as e:\n        logging.error(f\"Error during Main LLM final report generation: {e}\")\n        raise\n\n    \n    # --- Speichern der Ergebnisse ---\n    output_dir = \"result\"\n    stats_dir = \"Statistics\" # Neuer Ordner\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(stats_dir, exist_ok=True) # Stelle sicher, dass Statistics Ordner existiert\n\n    total_active_time = total_helper_time + total_main_time\n    \n    # Dateiname f√ºr Report\n    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M-%S\")\n    report_filename = f\"report_{timestamp}_Helper_{llm_helper.model_name}_MainLLM_{main_llm.model_name}.md\"\n    report_filepath = os.path.join(output_dir, report_filename)\n    \n    if final_report:\n        with open(report_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(final_report)\n        logging.info(f\"Final report saved to '{report_filepath}'.\")\n        \n        if savings_data:\n            try:\n                savings_filename = report_filename.replace(\"report_\", \"savings_\").replace(\".md\", \".png\")\n                savings_filepath = os.path.join(stats_dir, savings_filename)\n                \n                logging.info(f\"Erstelle Token-Chart: {savings_filepath}\")\n                create_savings_chart(\n                    json_tokens=savings_data['json_tokens'], \n                    toon_tokens=savings_data['toon_tokens'], \n                    savings_percent=savings_data['savings_percent'],\n                    output_path=savings_filepath\n                )\n            except Exception as chart_error:\n                logging.error(f\"Konnte Diagramm nicht erstellen: {chart_error}\")\n\n    else:\n        final_report = \"Error: Report generation failed or returned empty.\"\n\n    metrics = {\n        \"helper_time\": round(total_helper_time, 2),\n        \"main_time\": round(total_main_time, 2),\n        \"total_time\": round(total_active_time, 2),\n        \"helper_model\": helper_model,\n        \"main_model\": main_model,\n        \"json_tokens\": savings_data['json_tokens'] if savings_data else None,\n        \"toon_tokens\": savings_data['toon_tokens'] if savings_data else None,\n        \"savings_percent\": round(savings_data['savings_percent'], 2) if savings_data else None\n    \n    }\n\n    return {\n        \"report\": final_report,\n        \"metrics\": metrics\n    }"
            start_line: 75
            end_line: 477
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.update_status
            name: update_status
            args[1]: msg
            docstring: null
            source_code: "def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)"
            start_line: 77
            end_line: 80
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.notebook_workflow
            name: notebook_workflow
            args[4]: input,api_keys,model,status_callback
            docstring: null
            source_code: "def notebook_workflow(input, api_keys, model, status_callback=None):\n    t_start = time.time()\n    final_report = \"keine Notebooks gefunden\"\n    def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)\n\n    def gemini_payload(basic_info, nb_path, xml_content, images):\n        \n        intro_json = json.dumps({\n            \"basic_info\": basic_info,\n            \"current_notebook_path\": nb_path\n        }, indent=2)\n        \n        payload_content = []\n\n        payload_content.append({\n            \"type\": \"text\",\n            \"text\": f\"Context Information:\\n{intro_json}\\n\\nNotebook XML Structure:\\n\"\n        })\n\n        # Regex to find: <IMAGE_PLACEHOLDER index=\"0\" mime=\"image/png\"/>\n        pattern = r'(<IMAGE_PLACEHOLDER index=\"(\\d+)\" mime=\"([^\"]+)\"/>)'\n        last_pos = 0\n        \n        for match in re.finditer(pattern, xml_content):\n            text_segment = xml_content[last_pos:match.start()]\n            if text_segment.strip():\n                payload_content.append({\n                    \"type\": \"text\",\n                    \"text\": text_segment\n                })\n                \n            image_index = int(match.group(2))\n            mime_type = match.group(3)\n            \n            if image_index < len(images):\n                img_data = images[image_index]\n                b64_string = img_data['data']\n                \n                payload_content.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:{mime_type};base64,{b64_string}\"\n                    }\n                })\n                \n            last_pos = match.end()\n\n        # Add any remaining text after the last image\n        remaining_text = xml_content[last_pos:]\n        if remaining_text.strip():\n            payload_content.append({\n                \"type\": \"text\",\n                \"text\": remaining_text\n            })\n\n        return payload_content\n\n\n    base_url = None\n\n    if model.startswith(\"gpt-\"):\n        input_api_key = api_keys.get(\"gpt\")\n    elif model.startswith(\"gemini-\"):\n        input_api_key = api_keys.get(\"gemini\")\n    elif \"/\" in model or model.startswith(\"alias-\") or any(x in model for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        input_api_key = api_keys.get(\"scadsllm\")\n        base_url = api_keys.get(\"scadsllm_base_url\")\n    else:\n        input_api_key = None\n        base_url = api_keys.get(\"ollama\")\n\n    update_status(\"üîç Analysiere Input...\")\n\n    # URL Extraktion\n    repo_url = None\n    url_pattern = r\"https?://(?:www\\.)?github\\.com/[^\\s]+\"\n    match = re.search(url_pattern, input)\n\n    if match:\n        repo_url = match.group(0)\n        logging.info(f\"Extracted repository URL: {repo_url}\")\n    else:\n        raise ValueError(\"Could not find a valid URL in the provided input.\")\n    \n    update_status(f\"‚¨áÔ∏è Klone Repository: {repo_url} ...\")\n\n    try: \n        with GitRepository(repo_url) as repo:\n            repo_files = repo.get_all_files()\n    except Exception as e:\n        logging.error(f\"Error cloning repository: {e}\")\n        raise\n    \n    update_status(f\"üîó Bereite Input vor...\")\n\n    # convert to XML\n    processed_data = process_repo_notebooks(repo_files)\n\n    \n    # Extrahiere Basic Infos\n    update_status(\"‚ÑπÔ∏è Extrahiere Basis-Informationen...\")\n    try:\n        info_extractor = ProjektInfoExtractor()\n        basic_project_info = info_extractor.extrahiere_info(dateien=repo_files, repo_url=repo_url)\n        logging.info(\"Basic project info extracted\")\n    except Exception as e:\n        logging.error(f\"Error extracting basic project info: {e}\")\n        basic_project_info = \"Could not extract basic info.\"\n\n\n\n    prompt_file_notebook_llm = \"SystemPrompts/SystemPromptNotebookLLM.txt\"\n    notebook_llm = MainLLM(\n        api_key=input_api_key, \n        prompt_file_path=prompt_file_notebook_llm,\n        model_name=model,\n        base_url=base_url,\n    )\n\n    notebook_reports = []\n    total_notebooks = len(processed_data)\n    \n    logging.info(f\"Starting sequential processing of {total_notebooks} notebooks...\")\n\n    # Iterate over each notebook file individually\n    for index, (nb_path, nb_data) in enumerate(processed_data.items(), 1):\n        \n        update_status(f\"üß† Generiere Report ({index}/{total_notebooks}): {os.path.basename(nb_path)}\")\n        \n        nb_xml = nb_data['xml']\n        nb_images = nb_data['images']\n\n        llm_payload = gemini_payload(basic_project_info, nb_path, nb_xml, nb_images)\n\n        try:\n            single_report = notebook_llm.call_llm(llm_payload)\n            if single_report and isinstance(single_report, str):\n                notebook_reports.append(single_report)\n            else:\n                logging.warning(f\"LLM lieferte kein Ergebnis f√ºr {nb_path}\")\n                notebook_reports.append(f\"## Analyse f√ºr {os.path.basename(nb_path)}\\nFehler: Das Modell lieferte keine Antwort.\")\n            \n        except Exception as e:\n            logging.error(f\"Error generating report for {nb_path}: {e}\")\n            notebook_reports.append(f\"# Error processing {nb_path}\\n\\nError: {str(e)}\\n\\n---\\n\")\n\n    # Concatenate all reports\n    final_report = \"\\n\\n<br>\\n<br>\\n<br>\\n\\n\".join([str(r) for r in notebook_reports if r is not None])\n\n    \n    # --- Speichern der Ergebnisse ---\n    output_dir = \"result\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Dateiname f√ºr Report\n    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M-%S\")\n    report_filename = f\"notebook_report_{timestamp}_NotebookLLM_{notebook_llm.model_name}.md\"\n    report_filepath = os.path.join(output_dir, report_filename)\n    \n    if final_report:\n        with open(report_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(final_report)\n        logging.info(f\"Final report saved to '{report_filepath}'.\")\n\n    # Am Ende der Funktion nach dem Speichern:\n    total_time = time.time() - t_start\n    \n    # Metriken f√ºr das Frontend bauen\n    metrics = {\n        \"helper_time\": \"0\", # Notebook-Workflow hat meist keinen separaten Helper\n        \"main_time\": round(total_time, 2),\n        \"total_time\": round(total_time, 2),\n        \"helper_model\": \"None\",\n        \"main_model\": model,\n        \"json_tokens\": 0,\n        \"toon_tokens\": 0,\n        \"savings_percent\": 0\n    }\n\n    return {\n        \"report\": final_report,\n        \"metrics\": metrics\n    }"
            start_line: 483
            end_line: 668
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.update_status
            name: update_status
            args[1]: msg
            docstring: null
            source_code: "def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)"
            start_line: 486
            end_line: 489
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.gemini_payload
            name: gemini_payload
            args[4]: basic_info,nb_path,xml_content,images
            docstring: null
            source_code: "def gemini_payload(basic_info, nb_path, xml_content, images):\n        \n        intro_json = json.dumps({\n            \"basic_info\": basic_info,\n            \"current_notebook_path\": nb_path\n        }, indent=2)\n        \n        payload_content = []\n\n        payload_content.append({\n            \"type\": \"text\",\n            \"text\": f\"Context Information:\\n{intro_json}\\n\\nNotebook XML Structure:\\n\"\n        })\n\n        # Regex to find: <IMAGE_PLACEHOLDER index=\"0\" mime=\"image/png\"/>\n        pattern = r'(<IMAGE_PLACEHOLDER index=\"(\\d+)\" mime=\"([^\"]+)\"/>)'\n        last_pos = 0\n        \n        for match in re.finditer(pattern, xml_content):\n            text_segment = xml_content[last_pos:match.start()]\n            if text_segment.strip():\n                payload_content.append({\n                    \"type\": \"text\",\n                    \"text\": text_segment\n                })\n                \n            image_index = int(match.group(2))\n            mime_type = match.group(3)\n            \n            if image_index < len(images):\n                img_data = images[image_index]\n                b64_string = img_data['data']\n                \n                payload_content.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:{mime_type};base64,{b64_string}\"\n                    }\n                })\n                \n            last_pos = match.end()\n\n        # Add any remaining text after the last image\n        remaining_text = xml_content[last_pos:]\n        if remaining_text.strip():\n            payload_content.append({\n                \"type\": \"text\",\n                \"text\": remaining_text\n            })\n\n        return payload_content"
            start_line: 491
            end_line: 541
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "backend/relationship_analyzer.py":
      ast_nodes:
        imports[4]: ast,os,logging,collections.defaultdict
        functions[1]:
          - mode: function_analysis
            identifier: backend.relationship_analyzer.path_to_module
            name: path_to_module
            args[2]: filepath,project_root
            docstring: Wandelt einen Dateipfad in einen Python-Modulpfad um.
            source_code: "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    module_path = rel_path.replace(os.path.sep, '.')\n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n    return module_path"
            start_line: 6
            end_line: 18
            context:
              calls[0]:
              called_by[0]:
        classes[2]:
          - mode: class_analysis
            identifier: backend.relationship_analyzer.ProjectAnalyzer
            name: ProjectAnalyzer
            docstring: null
            source_code: "class ProjectAnalyzer:\n    \n    def __init__(self, project_root):\n        self.project_root = os.path.abspath(project_root)\n        self.definitions = {}\n        self.call_graph = defaultdict(list)\n        self.file_asts = {} \n        self.ignore_dirs = {'.git', '.venv', 'venv', '__pycache__', 'node_modules', 'dist', 'build', 'docs'}\n\n    def analyze(self):\n        py_files = self._find_py_files()\n        \n        for filepath in py_files:\n            self._collect_definitions(filepath)\n            \n        for filepath in py_files:\n            self._resolve_calls(filepath)\n            \n        self.file_asts.clear()\n        \n        return self.call_graph\n\n    def get_raw_relationships(self):\n\n        outgoing = defaultdict(set)\n        incoming = defaultdict(set)\n\n        for callee_id, callers_info_list in self.call_graph.items():\n            for info in callers_info_list:\n                caller_id = info['caller']\n                \n                if caller_id and callee_id:\n                    outgoing[caller_id].add(callee_id)\n                    incoming[callee_id].add(caller_id)\n        \n        return {\n            \"outgoing\": {k: sorted(list(v)) for k, v in outgoing.items()},\n            \"incoming\": {k: sorted(list(v)) for k, v in incoming.items()}\n        }\n\n    def _find_py_files(self):\n        py_files = []\n        for root, dirs, files in os.walk(self.project_root):\n            dirs[:] = [d for d in dirs if d not in self.ignore_dirs]\n            for file in files:\n                if file.endswith(\".py\"):\n                    py_files.append(os.path.join(root, file))\n        return py_files\n\n    def _collect_definitions(self, filepath):\n        try:\n            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n                source = f.read()\n                tree = ast.parse(source, filename=filepath)\n                \n            self.file_asts[filepath] = tree\n            module_path = path_to_module(filepath, self.project_root)\n            \n            for node in ast.walk(tree):\n                if isinstance(node, ast.FunctionDef):\n                    parent = self._get_parent(tree, node)\n                    if isinstance(parent, ast.ClassDef):\n                        path_name = f\"{module_path}.{parent.name}.{node.name}\"\n                        def_type = 'method'\n                    else:\n                        path_name = f\"{module_path}.{node.name}\"\n                        def_type = 'function'\n                    self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': def_type}\n                elif isinstance(node, ast.ClassDef):\n                    path_name = f\"{module_path}.{node.name}\"\n                    self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': 'class'}\n        except Exception as e:\n            logging.error(f\"Error collecting definitions in {filepath}: {e}\")\n            self.file_asts[filepath] = None\n\n    def _get_parent(self, tree, node):\n        for parent in ast.walk(tree):\n            for child in ast.iter_child_nodes(parent):\n                if child is node:\n                    return parent\n        return None\n\n    def _resolve_calls(self, filepath):\n        tree = self.file_asts.get(filepath)\n        if not tree:\n            return\n\n        try:\n            resolver = CallResolverVisitor(filepath, self.project_root, self.definitions)\n            resolver.visit(tree)\n            \n            for callee_pathname, caller_info_list in resolver.calls.items():\n                self.call_graph[callee_pathname].extend(caller_info_list)\n        except Exception as e:\n            logging.error(f\"Error resolving calls in {filepath}: {e}\")"
            start_line: 20
            end_line: 114
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[7]:
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,project_root
                  docstring: null
                  start_line: 22
                  end_line: 27
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.analyze
                  name: analyze
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 29
                  end_line: 40
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.get_raw_relationships
                  name: get_raw_relationships
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 42
                  end_line: 58
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._find_py_files
                  name: _find_py_files
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 60
                  end_line: 67
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._collect_definitions
                  name: _collect_definitions
                  calls[0]:
                  called_by[0]:
                  args[2]: self,filepath
                  docstring: null
                  start_line: 69
                  end_line: 93
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._get_parent
                  name: _get_parent
                  calls[0]:
                  called_by[0]:
                  args[3]: self,tree,node
                  docstring: null
                  start_line: 95
                  end_line: 100
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._resolve_calls
                  name: _resolve_calls
                  calls[0]:
                  called_by[0]:
                  args[2]: self,filepath
                  docstring: null
                  start_line: 102
                  end_line: 114
          - mode: class_analysis
            identifier: backend.relationship_analyzer.CallResolverVisitor
            name: CallResolverVisitor
            docstring: null
            source_code: "class CallResolverVisitor(ast.NodeVisitor):\n    def __init__(self, filepath, project_root, definitions):\n        self.filepath = filepath\n        self.module_path = path_to_module(filepath, project_root)\n        self.definitions = definitions\n        self.scope = {}\n        self.instance_types = {}\n        self.current_caller_name = self.module_path\n        self.current_class_name = None\n        self.calls = defaultdict(list)\n\n    def visit_ClassDef(self, node):\n        old_class_name = self.current_class_name\n        self.current_class_name = node.name\n        self.generic_visit(node)\n        self.current_class_name = old_class_name\n\n    def visit_FunctionDef(self, node):\n        old_caller = self.current_caller_name\n        \n        if self.current_class_name:\n            full_identifier = f\"{self.module_path}.{self.current_class_name}.{node.name}\"\n        else:\n            full_identifier = f\"{self.module_path}.{node.name}\"\n            \n        self.current_caller_name = full_identifier\n        self.generic_visit(node)\n        self.current_caller_name = old_caller\n\n    def visit_Call(self, node):\n        callee_pathname = self._resolve_call_qname(node.func)\n        if callee_pathname and callee_pathname in self.definitions:\n            \n            if self.current_caller_name == self.module_path:\n                caller_type = 'module'\n            elif '.<locals>.' in self.current_caller_name:\n                caller_type = 'local_function'\n            elif self.current_class_name:\n                caller_type = 'method'\n            else:\n                caller_type = 'function'\n            \n            caller_info = {\n                'file': os.path.basename(self.filepath),\n                'line': node.lineno,\n                'caller': self.current_caller_name,\n                'caller_type': caller_type \n            }\n            self.calls[callee_pathname].append(caller_info)\n        self.generic_visit(node)\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.scope[alias.asname or alias.name] = alias.name\n        self.generic_visit(node)\n        \n    def visit_ImportFrom(self, node):\n        module = node.module or ''\n        for alias in node.names:\n            name = alias.asname or alias.name\n            if node.level > 0:\n                base = self.module_path.split('.')\n                prefix = '.'.join(base[:-node.level])\n                full_module_path = f\"{prefix}.{module}\" if module else prefix\n            else:\n                full_module_path = module\n            self.scope[name] = f\"{full_module_path}.{alias.name}\"\n        self.generic_visit(node)\n\n    def visit_Assign(self, node):\n        if isinstance(node.value, ast.Call) and isinstance(node.value.func, ast.Name):\n            class_name = node.value.func.id\n            if class_name in self.scope:\n                qualified_class_name = self.scope[class_name]\n                if qualified_class_name in self.definitions:\n                    for target in node.targets:\n                        if isinstance(target, ast.Name):\n                            self.instance_types[target.id] = qualified_class_name\n        self.generic_visit(node)\n\n    def _resolve_call_qname(self, func_node):\n        if isinstance(func_node, ast.Name):\n            name = func_node.id\n            if name in self.scope:\n                return self.scope[name]\n            local_pathname = f\"{self.module_path}.{name}\"\n            if local_pathname in self.definitions:\n                return local_pathname\n        elif isinstance(func_node, ast.Attribute) and isinstance(func_node.value, ast.Name):\n            var_name = func_node.value.id\n            method_name = func_node.attr\n            if var_name in self.instance_types:\n                class_pathname = self.instance_types[var_name]\n                return f\"{class_pathname}.{method_name}\"\n            if var_name in self.scope:\n                module_pathname = self.scope[var_name]\n                return f\"{module_pathname}.{method_name}\"\n        return None"
            start_line: 117
            end_line: 214
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[8]:
                - identifier: backend.relationship_analyzer.CallResolverVisitor.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[4]: self,filepath,project_root,definitions
                  docstring: null
                  start_line: 118
                  end_line: 126
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 128
                  end_line: 132
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 134
                  end_line: 144
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Call
                  name: visit_Call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 146
                  end_line: 166
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 168
                  end_line: 171
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 173
                  end_line: 184
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Assign
                  name: visit_Assign
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 186
                  end_line: 195
                - identifier: backend.relationship_analyzer.CallResolverVisitor._resolve_call_qname
                  name: _resolve_call_qname
                  calls[0]:
                  called_by[0]:
                  args[2]: self,func_node
                  docstring: null
                  start_line: 197
                  end_line: 214
    "backend/scads_key_test.py":
      ast_nodes:
        imports[3]: os,dotenv.load_dotenv,openai.OpenAI
        functions[0]:
        classes[0]:
    "database/db.py":
      ast_nodes:
        imports[7]: datetime.datetime,pymongo.MongoClient,dotenv.load_dotenv,streamlit_authenticator,cryptography.fernet.Fernet,uuid,os
        functions[29]:
          - mode: function_analysis
            identifier: database.db.encrypt_text
            name: encrypt_text
            args[1]: text
            docstring: null
            source_code: "def encrypt_text(text: str) -> str:\n    if not text or not cipher_suite: return text\n    return cipher_suite.encrypt(text.strip().encode()).decode()"
            start_line: 28
            end_line: 30
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.decrypt_text
            name: decrypt_text
            args[1]: text
            docstring: null
            source_code: "def decrypt_text(text: str) -> str:\n    if not text or not cipher_suite: return text\n    try:\n        return cipher_suite.decrypt(text.strip().encode()).decode()\n    except Exception:\n        return text"
            start_line: 32
            end_line: 37
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.insert_user
            name: insert_user
            args[3]: username,name,password
            docstring: null
            source_code: "def insert_user(username: str, name: str, password: str):\n    user = {\n        \"_id\": username,\n        \"name\": name, \n        \"hashed_password\": stauth.Hasher.hash(password),\n        \"gemini_api_key\": \"\",\n        \"ollama_base_url\": \"\",\n        \"gpt_api_key\": \"\"\n    }\n    result = dbusers.insert_one(user)\n    return result.inserted_id"
            start_line: 43
            end_line: 53
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_all_users
            name: fetch_all_users
            args[0]:
            docstring: null
            source_code: "def fetch_all_users():\n    return list(dbusers.find())"
            start_line: 55
            end_line: 56
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_user
            name: fetch_user
            args[1]: username
            docstring: null
            source_code: "def fetch_user(username: str):\n    return dbusers.find_one({\"_id\": username})"
            start_line: 58
            end_line: 59
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_user_name
            name: update_user_name
            args[2]: username,new_name
            docstring: null
            source_code: "def update_user_name(username: str, new_name: str):\n    # Achtung: _id kann in Mongo nicht einfach so ge√§ndert werden. \n    # Hier wird nur das Name-Feld geupdated.\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"name\": new_name}})\n    return result.modified_count"
            start_line: 61
            end_line: 65
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_gemini_key
            name: update_gemini_key
            args[2]: username,gemini_api_key
            docstring: null
            source_code: "def update_gemini_key(username: str, gemini_api_key: str):\n    encrypted_key = encrypt_text(gemini_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"gemini_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 67
            end_line: 70
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_gpt_key
            name: update_gpt_key
            args[2]: username,gpt_api_key
            docstring: null
            source_code: "def update_gpt_key(username: str, gpt_api_key: str):\n    encrypted_key = encrypt_text(gpt_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"gpt_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 72
            end_line: 75
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_ollama_url
            name: update_ollama_url
            args[2]: username,ollama_base_url
            docstring: null
            source_code: "def update_ollama_url(username: str, ollama_base_url: str):\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"ollama_base_url\": ollama_base_url.strip()}})\n    return result.modified_count"
            start_line: 77
            end_line: 79
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_opensrc_key
            name: update_opensrc_key
            args[2]: username,opensrc_api_key
            docstring: null
            source_code: "def update_opensrc_key(username: str, opensrc_api_key: str):\n    encrypted_key = encrypt_text(opensrc_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"opensrc_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 81
            end_line: 84
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_opensrc_url
            name: update_opensrc_url
            args[2]: username,opensrc_base_url
            docstring: null
            source_code: "def update_opensrc_url(username: str, opensrc_base_url: str):\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"opensrc_base_url\": opensrc_base_url.strip()}})\n    return result.modified_count"
            start_line: 86
            end_line: 88
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_gemini_key
            name: fetch_gemini_key
            args[1]: username
            docstring: null
            source_code: "def fetch_gemini_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"gemini_api_key\": 1, \"_id\": 0})\n    return user.get(\"gemini_api_key\") if user else None"
            start_line: 90
            end_line: 92
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_ollama_url
            name: fetch_ollama_url
            args[1]: username
            docstring: null
            source_code: "def fetch_ollama_url(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"ollama_base_url\": 1, \"_id\": 0})\n    return user.get(\"ollama_base_url\") if user else None"
            start_line: 94
            end_line: 96
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_gpt_key
            name: fetch_gpt_key
            args[1]: username
            docstring: null
            source_code: "def fetch_gpt_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"gpt_api_key\": 1, \"_id\": 0})\n    return user.get(\"gpt_api_key\") if user else None"
            start_line: 98
            end_line: 100
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_opensrc_key
            name: fetch_opensrc_key
            args[1]: username
            docstring: null
            source_code: "def fetch_opensrc_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"opensrc_api_key\": 1, \"_id\": 0})\n    return user.get(\"opensrc_api_key\") if user else None"
            start_line: 102
            end_line: 104
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_opensrc_url
            name: fetch_opensrc_url
            args[1]: username
            docstring: null
            source_code: "def fetch_opensrc_url(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"opensrc_base_url\": 1, \"_id\": 0})\n    return user.get(\"opensrc_base_url\") if user else None"
            start_line: 106
            end_line: 108
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.delete_user
            name: delete_user
            args[1]: username
            docstring: null
            source_code: "def delete_user(username: str):\n    return dbusers.delete_one({\"_id\": username}).deleted_count"
            start_line: 110
            end_line: 111
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.get_decrypted_api_keys
            name: get_decrypted_api_keys
            args[1]: username
            docstring: null
            source_code: "def get_decrypted_api_keys(username: str):\n    user = dbusers.find_one({\"_id\": username})\n    if not user: return None, None\n    gemini_plain = decrypt_text(user.get(\"gemini_api_key\", \"\"))\n    ollama_plain = user.get(\"ollama_base_url\", \"\")\n    gpt_plain = decrypt_text(user.get(\"gpt_api_key\", \"\"))\n    opensrc_plain = decrypt_text(user.get(\"opensrc_api_key\", \"\"))\n    opensrc_url = user.get(\"opensrc_base_url\", \"\")\n    return gemini_plain, ollama_plain, gpt_plain, opensrc_plain, opensrc_url"
            start_line: 113
            end_line: 121
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.insert_chat
            name: insert_chat
            args[2]: username,chat_name
            docstring: Erstellt einen neuen Chat-Eintrag.
            source_code: "def insert_chat(username: str, chat_name: str):\n    \"\"\"Erstellt einen neuen Chat-Eintrag.\"\"\"\n    chat = {\n        \"_id\": str(uuid.uuid4()),\n        \"username\": username,\n        \"chat_name\": chat_name,\n        \"created_at\": datetime.now()\n    }\n    result = dbchats.insert_one(chat)\n    return result.inserted_id"
            start_line: 127
            end_line: 136
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_chats_by_user
            name: fetch_chats_by_user
            args[1]: username
            docstring: Holt alle definierten Chats eines Users.
            source_code: "def fetch_chats_by_user(username: str):\n    \"\"\"Holt alle definierten Chats eines Users.\"\"\"\n    # Sortieren nach Erstellung macht Sinn\n    chats = list(dbchats.find({\"username\": username}).sort(\"created_at\", 1))\n    return chats"
            start_line: 138
            end_line: 142
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.check_chat_exists
            name: check_chat_exists
            args[2]: username,chat_name
            docstring: null
            source_code: "def check_chat_exists(username: str, chat_name: str):\n    return dbchats.find_one({\"username\": username, \"chat_name\": chat_name}) is not None"
            start_line: 144
            end_line: 145
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.rename_chat_fully
            name: rename_chat_fully
            args[3]: username,old_name,new_name
            docstring: Benennt einen Chat und alle zugeh√∂rigen Exchanges um.
            source_code: "def rename_chat_fully(username: str, old_name: str, new_name: str):\n    \"\"\"\n    Benennt einen Chat und alle zugeh√∂rigen Exchanges um.\n    \"\"\"\n    # 1. Chat-Eintrag umbenennen\n    result = dbchats.update_one(\n        {\"username\": username, \"chat_name\": old_name}, \n        {\"$set\": {\"chat_name\": new_name}}\n    )\n    \n    # 2. Alle Messages (Exchanges) umh√§ngen\n    dbexchanges.update_many(\n        {\"username\": username, \"chat_name\": old_name},\n        {\"$set\": {\"chat_name\": new_name}}\n    )\n    return result.modified_count"
            start_line: 148
            end_line: 163
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.insert_exchange
            name: insert_exchange
            args[13]: question,answer,feedback,username,chat_name,helper_used,main_used,total_time,helper_time,main_time,json_tokens,toon_tokens,savings_percent
            docstring: null
            source_code: "def insert_exchange(question: str, answer: str, feedback: str, username: str, chat_name: str, \n                    helper_used: str=\"\", main_used: str=\"\", total_time: str=\"\", helper_time: str=\"\", main_time: str=\"\",\n                    json_tokens=0, toon_tokens=0, savings_percent=0.0):\n    new_id = str(uuid.uuid4())\n    exchange = {\n        \"_id\": new_id,\n        \"question\": question,\n        \"answer\": answer,\n        \"feedback\": feedback,\n        \"feedback_message\": \"\",\n        \"chat_name\": chat_name,\n        \"username\": username,\n        \"helper_used\": helper_used,\n        \"main_used\": main_used,\n        \"total_time\": total_time,\n        \"helper_time\": helper_time,\n        \"main_time\": main_time,\n        \"json_tokens\": json_tokens,\n        \"toon_tokens\": toon_tokens,\n        \"savings_percent\": savings_percent,\n        \"created_at\": datetime.now()\n    }\n    try:\n        dbexchanges.insert_one(exchange)\n        return new_id\n    except Exception as e:\n        print(f\"DB Error: {e}\")\n        return None"
            start_line: 169
            end_line: 196
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_exchanges_by_user
            name: fetch_exchanges_by_user
            args[1]: username
            docstring: null
            source_code: "def fetch_exchanges_by_user(username: str):\n    # Sortieren nach Zeitstempel wichtig f√ºr die Anzeige\n    exchanges = list(dbexchanges.find({\"username\": username}).sort(\"created_at\", 1))\n    return exchanges"
            start_line: 198
            end_line: 201
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_exchanges_by_chat
            name: fetch_exchanges_by_chat
            args[2]: username,chat_name
            docstring: null
            source_code: "def fetch_exchanges_by_chat(username: str, chat_name: str):\n    exchanges = list(dbexchanges.find({\"username\": username, \"chat_name\": chat_name}).sort(\"created_at\", 1))\n    return exchanges"
            start_line: 203
            end_line: 205
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_exchange_feedback
            name: update_exchange_feedback
            args[2]: exchange_id,feedback
            docstring: null
            source_code: "def update_exchange_feedback(exchange_id, feedback: int):\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback\": feedback}})\n    return result.modified_count"
            start_line: 207
            end_line: 209
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_exchange_feedback_message
            name: update_exchange_feedback_message
            args[2]: exchange_id,feedback_message
            docstring: null
            source_code: "def update_exchange_feedback_message(exchange_id, feedback_message: str):\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback_message\": feedback_message}})\n    return result.modified_count"
            start_line: 211
            end_line: 213
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.delete_exchange_by_id
            name: delete_exchange_by_id
            args[1]: exchange_id
            docstring: null
            source_code: "def delete_exchange_by_id(exchange_id: str):\n    result = dbexchanges.delete_one({\"_id\": exchange_id})\n    return result.deleted_count"
            start_line: 215
            end_line: 217
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.delete_full_chat
            name: delete_full_chat
            args[2]: username,chat_name
            docstring: "L√∂scht den Chat UND alle zugeh√∂rigen Exchanges.\nDas sorgt f√ºr Konsistenz zwischen Frontend und Backend."
            source_code: "def delete_full_chat(username: str, chat_name: str):\n    \"\"\"\n    L√∂scht den Chat UND alle zugeh√∂rigen Exchanges.\n    Das sorgt f√ºr Konsistenz zwischen Frontend und Backend.\n    \"\"\"\n    # 1. Alle Nachrichten in diesem Chat l√∂schen\n    del_exchanges = dbexchanges.delete_many({\"username\": username, \"chat_name\": chat_name})\n    \n    # 2. Den Chat selbst aus der Chat-Liste l√∂schen\n    del_chat = dbchats.delete_one({\"username\": username, \"chat_name\": chat_name})\n    \n    return del_chat.deleted_count"
            start_line: 221
            end_line: 232
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "frontend/frontend.py":
      ast_nodes:
        imports[16]: numpy,datetime.datetime,time,pymongo.MongoClient,dotenv.load_dotenv,os,sys,urllib.parse.urlparse,logging,traceback,re,streamlit_mermaid.st_mermaid,backend.main,database.db,streamlit,streamlit_authenticator
        functions[12]:
          - mode: function_analysis
            identifier: frontend.frontend.clean_names
            name: clean_names
            args[1]: model_list
            docstring: null
            source_code: "def clean_names(model_list):\n    return [m.split(\"/\")[-1] for m in model_list]"
            start_line: 54
            end_line: 55
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.get_filtered_models
            name: get_filtered_models
            args[2]: source_list,category_name
            docstring: Filtert eine Liste basierend auf der gew√§hlten Kategorie.
            source_code: "def get_filtered_models(source_list, category_name):\n    \"\"\"Filtert eine Liste basierend auf der gew√§hlten Kategorie.\"\"\"\n    keywords = CATEGORY_KEYWORDS.get(category_name, [\"\"])\n    \n    if \"STANDARD\" in keywords:\n        # Nur Modelle zur√ºckgeben, die auch in der Standard-Liste sind\n        return [m for m in source_list if m in STANDARD_MODELS]\n    \n    filtered = []\n    for model in source_list:\n        # Pr√ºfen ob ein Keyword im Namen steckt\n        if any(k in model.lower() for k in keywords):\n            filtered.append(model)\n            \n    return filtered if filtered else source_list"
            start_line: 86
            end_line: 100
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.save_gemini_cb
            name: save_gemini_cb
            args[0]:
            docstring: null
            source_code: "def save_gemini_cb():\n    new_key = st.session_state.get(\"in_gemini_key\", \"\")\n    if new_key:\n        db.update_gemini_key(st.session_state[\"username\"], new_key)\n        st.session_state[\"in_gemini_key\"] = \"\"\n        st.toast(\"Gemini Key erfolgreich gespeichert! ‚úÖ\")"
            start_line: 143
            end_line: 148
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.save_ollama_cb
            name: save_ollama_cb
            args[0]:
            docstring: null
            source_code: "def save_ollama_cb():\n    new_url = st.session_state.get(\"in_ollama_url\", \"\")\n    if new_url:\n        db.update_ollama_url(st.session_state[\"username\"], new_url)\n        st.toast(\"Ollama URL gespeichert! ‚úÖ\")"
            start_line: 150
            end_line: 154
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.load_data_from_db
            name: load_data_from_db
            args[1]: username
            docstring: L√§dt Chats und Exchanges konsistent aus der DB.
            source_code: "def load_data_from_db(username: str):\n    \"\"\"L√§dt Chats und Exchanges konsistent aus der DB.\"\"\"\n    \n    # Nur laden, wenn neuer User oder noch nicht geladen\n    if \"loaded_user\" not in st.session_state or st.session_state.loaded_user != username:\n        st.session_state.chats = {}\n        \n        # 1. Erst die definierten Chats laden (damit auch leere Chats da sind)\n        db_chats = db.fetch_chats_by_user(username)\n        for c in db_chats:\n            c_name = c.get(\"chat_name\")\n            if c_name:\n                st.session_state.chats[c_name] = {\"exchanges\": []}\n\n        # 2. Dann die Exchanges laden und einsortieren\n        db_exchanges = db.fetch_exchanges_by_user(username)\n        for ex in db_exchanges:\n            c_name = ex.get(\"chat_name\", \"Unbenannt\")\n            \n            # Falls Exchanges existieren f√ºr Chats, die nicht in dbchats sind (Legacy Support)\n            if c_name not in st.session_state.chats:\n                st.session_state.chats[c_name] = {\"exchanges\": []}\n            \n            if \"feedback\" not in ex or ex[\"feedback\"] is None:\n                ex[\"feedback\"] = np.nan\n            \n            st.session_state.chats[c_name][\"exchanges\"].append(ex)\n\n        # 3. Default Chat erstellen, falls gar nichts existiert\n        if not st.session_state.chats:\n            initial_name = \"Chat 1\"\n            # Konsistent in DB anlegen\n            db.insert_chat(username, initial_name)\n            st.session_state.chats[initial_name] = {\"exchanges\": []}\n            st.session_state.active_chat = initial_name\n        else:\n            # Active Chat setzen, falls n√∂tig\n            if \"active_chat\" not in st.session_state or st.session_state.active_chat not in st.session_state.chats:\n                # Nimm den ersten verf√ºgbaren Chat\n                st.session_state.active_chat = list(st.session_state.chats.keys())[0]\n        \n        st.session_state.loaded_user = username"
            start_line: 160
            end_line: 201
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.handle_feedback_change
            name: handle_feedback_change
            args[2]: ex,val
            docstring: null
            source_code: "def handle_feedback_change(ex, val):\n    ex[\"feedback\"] = val\n    db.update_exchange_feedback(ex[\"_id\"], val)\n    st.rerun()"
            start_line: 207
            end_line: 210
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.handle_delete_exchange
            name: handle_delete_exchange
            args[2]: chat_name,ex
            docstring: null
            source_code: "def handle_delete_exchange(chat_name, ex):\n    db.delete_exchange_by_id(ex[\"_id\"])\n    if chat_name in st.session_state.chats:\n        if ex in st.session_state.chats[chat_name][\"exchanges\"]:\n            st.session_state.chats[chat_name][\"exchanges\"].remove(ex)\n    st.rerun()"
            start_line: 212
            end_line: 217
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.handle_delete_chat
            name: handle_delete_chat
            args[2]: username,chat_name
            docstring: null
            source_code: "def handle_delete_chat(username, chat_name):\n    # KONSISTENZ: Ruft jetzt delete_full_chat in DB auf\n    db.delete_full_chat(username, chat_name)\n    \n    # State bereinigen\n    if chat_name in st.session_state.chats:\n        del st.session_state.chats[chat_name]\n    \n    # Active Chat neu setzen\n    if len(st.session_state.chats) > 0:\n        st.session_state.active_chat = list(st.session_state.chats.keys())[0]\n    else:\n        # Wenn alles weg ist, neuen leeren Chat anlegen\n        new_name = \"Chat 1\"\n        db.insert_chat(username, new_name)\n        st.session_state.chats[new_name] = {\"exchanges\": []}\n        st.session_state.active_chat = new_name\n        \n    st.rerun()"
            start_line: 219
            end_line: 237
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.extract_repo_name
            name: extract_repo_name
            args[1]: text
            docstring: null
            source_code: "def extract_repo_name(text):\n    url_match = re.search(r'(https?://[^\\s]+)', text)\n    if url_match:\n        url = url_match.group(0)\n        parsed = urlparse(url)\n        path = parsed.path.strip(\"/\")\n        if path:\n            repo_name = path.split(\"/\")[-1]\n            if repo_name.endswith(\".git\"):\n                repo_name = repo_name[:-4]\n            return repo_name\n    return None"
            start_line: 243
            end_line: 254
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.stream_text_generator
            name: stream_text_generator
            args[1]: text
            docstring: null
            source_code: "def stream_text_generator(text):\n    for word in text.split(\" \"):\n        yield word + \" \"\n        time.sleep(0.01)"
            start_line: 256
            end_line: 259
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.render_text_with_mermaid
            name: render_text_with_mermaid
            args[2]: markdown_text,should_stream
            docstring: null
            source_code: "def render_text_with_mermaid(markdown_text, should_stream=False):\n    if not markdown_text:\n        return\n\n    parts = re.split(r\"```mermaid\\s+(.*?)\\s+```\", markdown_text, flags=re.DOTALL)\n\n    for i, part in enumerate(parts):\n        if i % 2 == 0:\n            if part.strip():\n                if should_stream:\n                    st.write_stream(stream_text_generator(part))\n                else:\n                    st.markdown(part)\n        else:\n            try:\n                st_mermaid(part, key=f\"mermaid_{hash(part)}_{i}\")\n            except Exception:\n                st.code(part, language=\"mermaid\")"
            start_line: 261
            end_line: 278
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.render_exchange
            name: render_exchange
            args[2]: ex,current_chat_name
            docstring: null
            source_code: "def render_exchange(ex, current_chat_name):\n    st.chat_message(\"user\").write(ex[\"question\"])\n\n    with st.chat_message(\"assistant\"):\n        answer_text = ex.get(\"answer\", \"\")\n        is_error = answer_text.startswith(\"Fehler\") or answer_text.startswith(\"Error\")\n\n        # --- 1. TOOLBAR CONTAINER ---\n        # Nutzt die neuen Parameter horizontal=True und alignment\n        # border=True rahmt die Toolbar ein\n        # horizontal_alignment=\"right\" schiebt alles nach rechts\n        with st.container(horizontal=True, horizontal_alignment=\"right\"):\n            \n            if not is_error:\n                # 1. Status Text (wird jetzt links von den Buttons angezeigt, aber rechtsb√ºndig im Container)\n                st.write(\"hier die Antwort:\")\n                if ex.get(\"feedback\") == 1:\n                    st.caption(\"‚úÖ Hilfreich\")\n                elif ex.get(\"feedback\") == 0:\n                    st.caption(\"‚ùå Nicht hilfreich\")\n                \n                # 2. Die Buttons (einfach untereinander im Code, erscheinen nebeneinander im UI)\n                \n                # Like\n                type_primary_up = ex.get(\"feedback\") == 1\n                if st.button(\"üëç\", key=f\"up_{ex['_id']}\", type=\"primary\" if type_primary_up else \"secondary\", help=\"Hilfreich\"):\n                    handle_feedback_change(ex, 1)\n\n                # Dislike\n                type_primary_down = ex.get(\"feedback\") == 0\n                if st.button(\"üëé\", key=f\"down_{ex['_id']}\", type=\"primary\" if type_primary_down else \"secondary\", help=\"Nicht hilfreich\"):\n                    handle_feedback_change(ex, 0)\n\n                # Comment Popover\n                with st.popover(\"üí¨\", help=\"Notiz hinzuf√ºgen\"):\n                    msg = st.text_area(\"Notiz:\", value=ex.get(\"feedback_message\", \"\"), key=f\"txt_{ex['_id']}\")\n                    if st.button(\"Speichern\", key=f\"save_{ex['_id']}\"):\n                        ex[\"feedback_message\"] = msg\n                        db.update_exchange_feedback_message(ex[\"_id\"], msg)\n                        st.success(\"Gespeichert!\")\n                        time.sleep(0.5)\n                        st.rerun()\n\n                # Download\n                st.download_button(\n                    label=\"üì•\",\n                    data=ex[\"answer\"],\n                    file_name=f\"response_{ex['_id']}.md\",\n                    mime=\"text/markdown\",\n                    key=f\"dl_{ex['_id']}\",\n                    help=\"Als Markdown herunterladen\"\n                )\n\n                # Delete\n                if st.button(\"üóëÔ∏è\", key=f\"del_{ex['_id']}\", help=\"Nachricht l√∂schen\", type=\"secondary\"):\n                    handle_delete_exchange(current_chat_name, ex)\n\n            else:\n                # Fehlerfall\n                st.error(\"‚ö†Ô∏è Fehler\")\n                if st.button(\"üóëÔ∏è L√∂schen\", key=f\"del_err_{ex['_id']}\"):\n                    handle_delete_exchange(current_chat_name, ex)\n\n        # --- 2. CONTENT (Antwort) ---\n        with st.container(height=500, border=True):\n             render_text_with_mermaid(ex[\"answer\"], should_stream=False)"
            start_line: 284
            end_line: 349
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "schemas/types.py":
      ast_nodes:
        imports[5]: typing.List,typing.Optional,typing.Literal,pydantic.BaseModel,pydantic.ValidationError
        functions[0]:
        classes[15]:
          - mode: class_analysis
            identifier: schemas.types.ParameterDescription
            name: ParameterDescription
            docstring: Describes a single parameter of a function.
            source_code: "class ParameterDescription(BaseModel):\n    \"\"\"Describes a single parameter of a function.\"\"\"\n    name: str\n    type: str\n    description: str"
            start_line: 6
            end_line: 10
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ReturnDescription
            name: ReturnDescription
            docstring: Describes the return value of a function.
            source_code: "class ReturnDescription(BaseModel):\n    \"\"\"Describes the return value of a function.\"\"\"\n    name: str\n    type: str\n    description: str"
            start_line: 12
            end_line: 16
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.UsageContext
            name: UsageContext
            docstring: Describes the calling context of a function.
            source_code: "class UsageContext(BaseModel):\n    \"\"\"Describes the calling context of a function.\"\"\"\n    calls: str\n    called_by: str"
            start_line: 18
            end_line: 21
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionDescription
            name: FunctionDescription
            docstring: Contains the detailed analysis of a function's purpose and signature.
            source_code: "class FunctionDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a function's purpose and signature.\"\"\"\n    overall: str\n    parameters: List[ParameterDescription]\n    returns: List[ReturnDescription]\n    usage_context: UsageContext"
            start_line: 23
            end_line: 28
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionAnalysis
            name: FunctionAnalysis
            docstring: The main model representing the entire JSON schema for a function.
            source_code: "class FunctionAnalysis(BaseModel):\n    \"\"\"The main model representing the entire JSON schema for a function.\"\"\"\n    identifier: str\n    description: FunctionDescription\n    error: Optional[str] = None"
            start_line: 30
            end_line: 34
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ConstructorDescription
            name: ConstructorDescription
            docstring: Describes the __init__ method of a class.
            source_code: "class ConstructorDescription(BaseModel):\n    \"\"\"Describes the __init__ method of a class.\"\"\"\n    description: str\n    parameters: List[ParameterDescription]"
            start_line: 39
            end_line: 42
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassContext
            name: ClassContext
            docstring: Describes the class's external dependencies and primary points of instantiation.
            source_code: "class ClassContext(BaseModel):\n    \"\"\"Describes the class's external dependencies and primary points of instantiation.\"\"\"\n    dependencies: str\n    instantiated_by: str"
            start_line: 44
            end_line: 47
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassDescription
            name: ClassDescription
            docstring: "Contains the detailed analysis of a class's purpose, constructor, and methods."
            source_code: "class ClassDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a class's purpose, constructor, and methods.\"\"\"\n    overall: str\n    init_method: ConstructorDescription\n    methods: List[FunctionAnalysis]\n    usage_context: ClassContext"
            start_line: 49
            end_line: 54
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassAnalysis
            name: ClassAnalysis
            docstring: The main model for the entire JSON schema for a class.
            source_code: "class ClassAnalysis(BaseModel):\n    \"\"\"The main model for the entire JSON schema for a class.\"\"\"\n    identifier: str\n    description: ClassDescription\n    error: Optional[str] = None"
            start_line: 56
            end_line: 60
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.CallInfo
            name: CallInfo
            docstring: "Represents a specific call event from the relationship analyzer.\nUsed in 'called_by' and 'instantiated_by' lists."
            source_code: "class CallInfo(BaseModel):\n    \"\"\"\n    Represents a specific call event from the relationship analyzer.\n    Used in 'called_by' and 'instantiated_by' lists.\n    \"\"\"\n    file: str\n    function: str  # Name des Aufrufers\n    mode: str      # z.B. 'method', 'function', 'module'\n    line: int"
            start_line: 65
            end_line: 73
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionContextInput
            name: FunctionContextInput
            docstring: Structured context for analyzing a function.
            source_code: "class FunctionContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a function.\"\"\"\n    calls: List[str]\n    called_by: List[CallInfo]"
            start_line: 78
            end_line: 81
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionAnalysisInput
            name: FunctionAnalysisInput
            docstring: The required input to generate a FunctionAnalysis object.
            source_code: "class FunctionAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a FunctionAnalysis object.\"\"\"\n    mode: Literal[\"function_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: FunctionContextInput"
            start_line: 83
            end_line: 89
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.MethodContextInput
            name: MethodContextInput
            docstring: Structured context for a classes methods
            source_code: "class MethodContextInput(BaseModel):\n    \"\"\"Structured context for a classes methods\"\"\"\n    identifier: str\n    calls: List[str]\n    called_by: List[CallInfo]\n    args: List[str]\n    docstring: Optional[str]"
            start_line: 94
            end_line: 100
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassContextInput
            name: ClassContextInput
            docstring: Structured context for analyzing a class.
            source_code: "class ClassContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a class.\"\"\"\n    dependencies: List[str]\n    instantiated_by: List[CallInfo]\n    method_context: List[MethodContextInput]"
            start_line: 102
            end_line: 106
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassAnalysisInput
            name: ClassAnalysisInput
            docstring: The required input to generate a ClassAnalysis object.
            source_code: "class ClassAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a ClassAnalysis object.\"\"\"\n    mode: Literal[\"class_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: ClassContextInput"
            start_line: 108
            end_line: 114
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
analysis_results:
  functions:
    backend.AST_Schema.path_to_module:
      identifier: backend.AST_Schema.path_to_module
      description:
        overall: "This function converts a given file system path into a Python module path string. It first attempts to determine the relative path from a specified project root, falling back to the base filename if a relative path cannot be established. It then removes the '.py' extension if present and replaces directory separators with dots. Finally, it handles '__init__.py' files by removing the '.__init__' suffix to correctly represent the package."
        parameters[2]{name,type,description}:
          filepath,str,The absolute or relative path to a file that needs to be converted to a module path.
          project_root,str,"The root directory of the project, used as a reference to calculate the relative path for the module."
        returns[1]{name,type,description}:
          module_path,str,The Python module path string derived from the input filepath.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.File_Dependency.build_file_dependency_graph:
      identifier: backend.File_Dependency.build_file_dependency_graph
      description:
        overall: "This function constructs a directed graph representing file import dependencies within a given Abstract Syntax Tree (AST). It initializes a NetworkX directed graph and creates an instance of a `FileDependencyGraph` visitor. The visitor then traverses the provided AST to collect import relationships. Finally, the function populates the graph with nodes for files and edges representing these import dependencies, returning the complete dependency graph."
        parameters[3]{name,type,description}:
          filename,str,The path to the file being analyzed for dependencies.
          tree,AST,The Abstract Syntax Tree (AST) of the file to be analyzed.
          repo_root,str,"The root directory of the repository, used for resolving relative paths."
        returns[1]{name,type,description}:
          graph,nx.DiGraph,A directed graph representing the file import dependencies.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by other functions in the provided context.
      error: null
    backend.File_Dependency.build_repository_graph:
      identifier: backend.File_Dependency.build_repository_graph
      description:
        overall: "This function constructs a directed graph representing dependencies across an entire Git repository. It iterates through all Python files within the provided repository, parsing each file's content into an Abstract Syntax Tree (AST). For each file, it builds a local dependency graph using a helper function and then integrates the nodes and edges from these local graphs into a single, global NetworkX directed graph. The final graph illustrates the relationships between various components (e.g., functions, classes) found across the repository's Python files."
        parameters[1]{name,type,description}:
          repository,GitRepository,The GitRepository object representing the code repository to be analyzed for dependencies.
        returns[1]{name,type,description}:
          global_graph,nx.DiGraph,"A NetworkX directed graph where nodes represent code components (e.g., files, functions, classes) and edges represent dependencies between them across the entire repository."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.File_Dependency.get_all_temp_files:
      identifier: backend.File_Dependency.get_all_temp_files
      description:
        overall: "This function is designed to locate all Python files within a specified directory and its subdirectories. It takes a directory path as input and converts it into an absolute `Path` object. The function then recursively searches for all files ending with the \".py\" extension. Finally, it returns a list of these Python files, with each file's path represented as a `Path` object relative to the initial input directory."
        parameters[1]{name,type,description}:
          directory,str,The path to the root directory from which to start searching for Python files.
        returns[1]{name,type,description}:
          all_files,"list[pathlib.Path]","A list of `pathlib.Path` objects, where each path is relative to the input `directory`, representing all found Python files."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.HelperLLM.main_orchestrator:
      identifier: backend.HelperLLM.main_orchestrator
      description:
        overall: "This function serves as a dummy orchestrator for testing the LLMHelper class. It defines pre-computed analysis inputs and outputs for several example functions, such as 'add_item', 'check_stock', and 'generate_report'. It also sets up a ClassAnalysisInput for an 'InventoryManager' class, incorporating the pre-computed function analyses. The orchestrator then initializes an LLMHelper instance and simulates the process of generating documentation for these functions. Finally, it aggregates and prints the simulated documentation results."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    backend.callgraph.make_safe_dot:
      identifier: backend.callgraph.make_safe_dot
      description:
        overall: "This function takes a NetworkX directed graph and an output file path, then generates a 'safe' DOT file representation of the graph. It achieves this by creating a copy of the input graph and relabeling its nodes with generic, safe identifiers (e.g., 'n0', 'n1'). The original node names are preserved by assigning them as 'label' attributes to the new safe nodes. Finally, the modified graph, with sanitized node names, is written to the specified output path using the NetworkX pydot drawing utility."
        parameters[2]{name,type,description}:
          graph,nx.DiGraph,The NetworkX directed graph object to be converted into a safe DOT file format.
          out_path,str,The file path where the sanitized DOT graph representation will be saved.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    backend.callgraph.build_filtered_callgraph:
      identifier: backend.callgraph.build_filtered_callgraph
      description:
        overall: "This function constructs a filtered call graph for a given Git repository. It first iterates through all Python files in the repository to identify and collect all functions defined within the project's own codebase. Subsequently, it re-processes these files to detect function calls. A NetworkX directed graph is then built, where edges are added exclusively between functions that were previously identified as belonging to the project's own code. The final graph represents the internal call structure, focusing only on self-written functions."
        parameters[1]{name,type,description}:
          repo,GitRepository,The Git repository object containing the source code files to be analyzed.
        returns[1]{name,type,description}:
          global_graph,nx.DiGraph,A NetworkX directed graph representing the call relationships between functions defined within the repository's own Python files.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.wrap_cdata:
      identifier: backend.converter.wrap_cdata
      description:
        overall: "This function, `wrap_cdata`, is designed to encapsulate a given string `content` within XML CDATA tags. It constructs a new string that begins with `<![CDATA[`, followed by a newline, the provided content, another newline, and finally `]]>`. This ensures that the content is treated as character data by an XML parser, preventing interpretation of any special characters within it. The primary purpose is to safely embed raw text, potentially containing XML-sensitive characters, into an XML document."
        parameters[1]{name,type,description}:
          content,str,The string content to be wrapped in CDATA tags.
        returns[1]{name,type,description}:
          wrapped_content,str,"A string with the provided content wrapped in CDATA tags, including leading and trailing newlines within the CDATA block."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    backend.converter.extract_output_content:
      identifier: backend.converter.extract_output_content
      description:
        overall: "This function processes a list of notebook output objects to extract their content, primarily focusing on text and image data. It iterates through each output, identifying its type (display_data, execute_result, stream, or error). For display_data and execute_result types, it prioritizes extracting image data (PNG then JPEG), converting Base64 encoded images into a structured format and storing them in a provided image_list, while generating XML-like placeholders for the images. If no image is found, it extracts plain text. For stream outputs, it extracts the raw text, and for error outputs, it formats the error name and value. The function accumulates all extracted content, including image placeholders and text, into a list of strings."
        parameters[2]{name,type,description}:
          outputs,list,"A list of output objects, typically from a notebook cell execution, each containing various data types such as text, images, or error information."
          image_list,"list[dict]","A mutable list passed by reference, used to store dictionaries of extracted image data (mime_type and Base64 string) and their corresponding metadata. This list is modified in-place."
        returns[1]{name,type,description}:
          extracted_content,"list[str]","A list of strings, where each string represents either extracted plain text, a formatted error message, or an XML-like placeholder for an image that was processed and added to `image_list`."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.process_image:
      identifier: backend.converter.process_image
      description:
        overall: "The process_image function is designed to extract and format base64 encoded image data based on a given MIME type. It first checks for the presence of the mime_type as a key in an external 'data' dictionary. If found, it retrieves the corresponding base64 string, removes any newline characters, and then appends a dictionary containing the mime_type and processed data to an external 'image_list'. Finally, it returns a formatted string representing an image placeholder, including an index derived from the 'image_list' length and the mime_type. In case of any exception during this process, it returns an error message. If the mime_type is not present in 'data', the function returns None."
        parameters[1]{name,type,description}:
          mime_type,str,"The MIME type string, used as a key to locate the image's base64 data within an external 'data' dictionary."
        returns[1]{name,type,description}:
          result,str | None,"Returns a formatted string containing an image placeholder if the image data is successfully processed. If an error occurs during processing, it returns an error message string. If the mime_type is not found in the external 'data' dictionary, it returns None."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.convert_notebook_to_xml:
      identifier: backend.converter.convert_notebook_to_xml
      description:
        overall: "This function converts the content of a Jupyter Notebook, provided as a string, into a custom XML format. It iterates through each cell, processing markdown cells by embedding their source directly and code cells by wrapping their source in CDATA. If code cells have outputs, it extracts and processes them, potentially identifying and collecting images. The function handles parsing errors by returning a specific error message."
        parameters[1]{name,type,description}:
          file_content,str,"The raw content of a Jupyter Notebook file, expected as a string in JSON format."
        returns[2]{name,type,description}:
          xml_output,str,"A string containing the converted notebook content in a custom XML format, or an error message if the input cannot be parsed as a notebook."
          extracted_images,list,A list of extracted image data or references found within the notebook's output cells.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    backend.converter.process_repo_notebooks:
      identifier: backend.converter.process_repo_notebooks
      description:
        overall: "This function processes a list of repository files to identify and convert Jupyter notebooks. It filters the input `repo_files` to find items with a '.ipynb' extension. For each identified notebook, it logs its path and then calls an external utility to convert its content into XML and extract associated images. The function stores the resulting XML and image data in a dictionary, keyed by the notebook's original file path. Finally, it returns this dictionary containing all processed notebook data."
        parameters[1]{name,type,description}:
          repo_files,"List[object]","An iterable collection of file-like objects, where each object is expected to have a 'path' attribute (string) and a 'content' attribute (string), representing files from a repository."
        returns[1]{name,type,description}:
          results,"Dict[str, Dict[str, Any]]","A dictionary where keys are the paths of the processed notebook files (strings) and values are dictionaries. Each inner dictionary contains two keys: 'xml' (string, representing the converted XML content) and 'images' (representing extracted image data)."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.create_savings_chart:
      identifier: backend.main.create_savings_chart
      description:
        overall: "This function generates a bar chart to visually compare the number of JSON tokens against TOON tokens. It takes the token counts and a savings percentage as input, then uses `matplotlib.pyplot` to create the visualization. The chart includes a title indicating the token comparison and savings, labeled axes, and displays the exact token counts above each bar. Finally, the generated chart is saved to a specified output file path and the plot is closed to free up memory."
        parameters[4]{name,type,description}:
          json_tokens,int,The number of tokens for the JSON format.
          toon_tokens,int,The number of tokens for the TOON format.
          savings_percent,float,The calculated percentage of savings between the two token counts.
          output_path,str,The file path where the generated chart image will be saved.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.calculate_net_time:
      identifier: backend.main.calculate_net_time
      description:
        overall: "This function calculates the net operational time between a start and end time, specifically accounting for potential sleep durations due to rate limits when a 'gemini-' model is used. It first determines the total elapsed duration. If the model is not a 'gemini-' type, the total duration is returned directly. For 'gemini-' models, it computes the number of batches, estimates the total sleep time based on a fixed sleep duration per batch, and subtracts this from the total duration. The final net time is guaranteed to be non-negative."
        parameters[5]{name,type,description}:
          start_time,float,The starting timestamp or time value for the operation.
          end_time,float,The ending timestamp or time value for the operation.
          total_items,int,The total number of items processed during the operation.
          batch_size,int,The number of items processed per batch.
          model_name,str,"The name of the model being used, which determines if rate-limiting sleep adjustments are applied."
        returns[1]{name,type,description}:
          net_time,float,"The calculated net duration in seconds, adjusted for rate-limiting sleep times if applicable, or the total duration if no adjustments are made. The value is always non-negative."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    backend.main.main_workflow:
      identifier: backend.main.main_workflow
      description:
        overall: "The `main_workflow` function orchestrates a comprehensive process for analyzing a GitHub repository, generating documentation using Helper LLMs, and compiling a final report with a Main LLM. It handles API key and model selection, clones the specified repository, extracts project information, constructs a file tree, and performs relationship and AST analysis. The function then prepares structured inputs for LLMs, calls them to generate function and class analyses, and finally synthesizes these into a comprehensive report, including token usage metrics and saving the output."
        parameters[4]{name,type,description}:
          input,str,"The initial user input, expected to contain a GitHub repository URL for analysis."
          api_keys,dict,"A dictionary containing various API keys (e.g., 'gemini', 'gpt', 'scadsllm') and base URLs required for LLM interactions."
          model_names,dict,A dictionary specifying the names of the 'helper' and 'main' LLM models to be used in the workflow.
          status_callback,None,An optional callback function used to provide real-time status updates during the workflow execution.
        returns[1]{name,type,description}:
          result,dict,A dictionary containing the generated 'report' (string) and 'metrics' (dict) related to the workflow's execution and performance.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by other functions in the provided context.
      error: null
    backend.main.update_status:
      identifier: backend.main.update_status
      description:
        overall: "This function, `update_status`, is designed to handle and propagate status messages. It accepts a single message as input. The function first checks if a `status_callback` function is available and, if so, invokes it with the provided message. Regardless of the callback's presence, it logs the message at the info level using the `logging` module. Its main purpose is to ensure status updates are both logged and optionally dispatched to a custom handler."
        parameters[1]{name,type,description}:
          msg,Any,"The status message to be processed, logged, and potentially passed to a callback."
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.notebook_workflow:
      identifier: backend.main.notebook_workflow
      description:
        overall: "This function orchestrates a workflow to analyze Jupyter notebooks within a Git repository. It clones the repository, extracts basic project information, converts notebooks to an XML-like structure, and then processes each notebook individually using a specified Large Language Model (LLM). The function dynamically selects the appropriate API key and base URL based on the chosen LLM model. Finally, it concatenates the reports generated for each notebook, saves the combined report to a markdown file, and returns the report along with execution metrics."
        parameters[4]{name,type,description}:
          input,str,A string containing the URL to a Git repository that will be cloned and analyzed.
          api_keys,dict,"A dictionary containing various API keys, such as 'gpt', 'gemini', 'scadsllm', and 'ollama', used for authenticating with different LLM services."
          model,str,"The name of the Large Language Model to be used for generating notebook reports (e.g., 'gpt-4', 'gemini-pro', 'alias-model')."
          status_callback,callable,"An optional callback function that receives status messages during the workflow execution, allowing for real-time updates. Defaults to None."
        returns[1]{name,type,description}:
          report_and_metrics,dict,"A dictionary containing two keys: 'report', which holds the concatenated markdown string of all generated notebook reports, and 'metrics', which is a dictionary of performance metrics for the workflow execution."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    backend.main.gemini_payload:
      identifier: backend.main.gemini_payload
      description:
        overall: "This function constructs a multimodal payload suitable for the Gemini API, combining textual context with embedded images. It serializes basic project information and the notebook path into an initial JSON string. The function then parses an XML content string, identifying image placeholders using a regular expression. For each placeholder, it extracts the corresponding base64 image data from a provided list and formats it as an image_url entry in the payload. Text segments found between or around these image placeholders are added as text entries, resulting in a structured list of dictionaries ready for a multimodal API request."
        parameters[4]{name,type,description}:
          basic_info,dict,A dictionary or object containing basic project information to be included in the payload.
          nb_path,str,"The current notebook's file path, included in the context information."
          xml_content,str,"An XML string representing the notebook structure, which may contain <IMAGE_PLACEHOLDER/> tags."
          images,"List[dict]","A list of dictionaries, where each dictionary contains image data (e.g., base64 string) corresponding to image placeholders in `xml_content`."
        returns[1]{name,type,description}:
          payload_content,"List[dict]","A list of dictionaries, each representing a part of the multimodal Gemini payload, containing either text content or image URLs with base64 encoded data."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.relationship_analyzer.path_to_module:
      identifier: backend.relationship_analyzer.path_to_module
      description:
        overall: "This function converts a given file system path into its corresponding Python module path string. It first determines the path relative to the project root, handling cases where the file is not within the root by falling back to the base filename. It then removes the '.py' extension if present and replaces directory separators with dots. Finally, it specifically handles '__init__.py' files by removing the '.__init__' suffix from the module path to represent the package itself."
        parameters[2]{name,type,description}:
          filepath,str,The absolute or relative path to the Python file.
          project_root,str,"The root directory of the project, used to calculate the relative path."
        returns[1]{name,type,description}:
          module_path,str,"The converted Python module path string (e.g., 'my_package.my_module')."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.encrypt_text:
      identifier: database.db.encrypt_text
      description:
        overall: "This function encrypts a given string using a `cipher_suite` object. It first checks if the input text or the `cipher_suite` is empty or null; if either is, it returns the original text without encryption. Otherwise, it prepares the text by stripping leading/trailing whitespace and encoding it to bytes. The byte string is then encrypted, and the resulting encrypted bytes are decoded back into a string before being returned."
        parameters[1]{name,type,description}:
          text,str,The string value to be encrypted.
        returns[1]{name,type,description}:
          encrypted_text,str,"The encrypted version of the input text, or the original text if encryption was skipped due to empty input or missing cipher suite."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.decrypt_text:
      identifier: database.db.decrypt_text
      description:
        overall: "This function attempts to decrypt a given text string using a global `cipher_suite` object. It first checks if the input text or the `cipher_suite` is empty, returning the original text if either condition is true. If both are present, it proceeds to strip whitespace from the text, encode it to bytes, decrypt it using `cipher_suite.decrypt`, and then decode the result back into a string. The function includes error handling, returning the original text in case any exception occurs during the decryption process."
        parameters[1]{name,type,description}:
          text,str,The string value to be decrypted.
        returns[1]{name,type,description}:
          decrypted_or_original_text,str,"The decrypted string if successful, or the original string if decryption is not performed (due to empty input or missing cipher_suite) or if an error occurs during decryption."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.insert_user:
      identifier: database.db.insert_user
      description:
        overall: "This function creates a new user document with the provided username, name, and a hashed password. It initializes fields for Gemini API key, Ollama base URL, and GPT API key as empty strings. The function then inserts this user document into the `dbusers` collection, which is presumed to be a MongoDB collection. It leverages `stauth.Hasher.hash` to secure the password before storage."
        parameters[3]{name,type,description}:
          username,str,"The unique username for the new user, which will also serve as the document's `_id`."
          name,str,The full name of the user.
          password,str,"The plain-text password for the user, which will be hashed before being stored."
        returns[1]{name,type,description}:
          inserted_id,str,"The `_id` of the newly inserted user document, which corresponds to the provided username."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_all_users:
      identifier: database.db.fetch_all_users
      description:
        overall: "The `fetch_all_users` function is responsible for retrieving all user records from a database collection. It executes a `find()` operation on the `dbusers` object, which is expected to be a database collection. The results, typically a cursor of user documents, are then converted into a standard Python list before being returned. This provides a comprehensive list of all stored user data."
        parameters[0]:
        returns[1]{name,type,description}:
          users,list,A list containing all user documents retrieved from the 'dbusers' collection.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_user:
      identifier: database.db.fetch_user
      description:
        overall: This function retrieves a single user record from a database collection named `dbusers`. It queries the collection for a document where the `_id` field matches the provided `username`. The function is designed to return the first document that satisfies this criterion.
        parameters[1]{name,type,description}:
          username,str,"The unique identifier for the user to be fetched, which is used to match the `_id` field in the database."
        returns[1]{name,type,description}:
          user_document,dict | None,"A dictionary representing the user document if a matching record is found, otherwise `None`."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_user_name:
      identifier: database.db.update_user_name
      description:
        overall: "This function updates the 'name' field for a user identified by their 'username' in the 'dbusers' collection. It performs a MongoDB update_one operation, setting the 'name' field to the provided new_name for the document where the '_id' matches the given username. The function returns the count of modified documents. It specifically notes that the '_id' field itself is not changed, only the 'name' attribute."
        parameters[2]{name,type,description}:
          username,str,The unique identifier (MongoDB _id) of the user whose name is to be updated.
          new_name,str,The new name to be set for the specified user.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.update_gemini_key:
      identifier: database.db.update_gemini_key
      description:
        overall: "This function is responsible for updating a user's Gemini API key within a database. It takes a username and the new API key as input. The provided API key is first stripped of any leading or trailing whitespace and then encrypted. Finally, the function updates the `gemini_api_key` field for the specified user in the `dbusers` collection with the encrypted key."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose Gemini API key needs to be updated.
          gemini_api_key,str,"The new Gemini API key to be stored for the user, which will be encrypted before storage."
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation in the database.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.update_gpt_key:
      identifier: database.db.update_gpt_key
      description:
        overall: "This function `update_gpt_key` is designed to securely update a user's GPT API key in the database. It takes a username and a new API key as input. The provided API key is first stripped of any leading/trailing whitespace and then encrypted using an external `encrypt_text` function. Finally, the encrypted key is stored in the `gpt_api_key` field for the specified user in the `dbusers` collection."
        parameters[2]{name,type,description}:
          username,str,The unique identifier of the user whose GPT API key needs to be updated.
          gpt_api_key,str,The new GPT API key to be stored for the user. It will be stripped of whitespace and encrypted before being saved.
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents that were modified by the update operation. This will typically be 1 if the user was found and the key was updated, or 0 if the user was not found."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.update_ollama_url:
      identifier: database.db.update_ollama_url
      description:
        overall: This function updates the Ollama base URL for a specific user in a database. It takes a username and a new Ollama base URL as input. The provided URL is first stripped of any leading or trailing whitespace before being stored. The function then updates the corresponding user's document in the 'dbusers' collection. It returns the count of documents that were modified by this operation.
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose Ollama base URL is to be updated.
          ollama_base_url,str,"The new base URL for Ollama, which will be stripped of whitespace before being saved."
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.update_opensrc_key:
      identifier: database.db.update_opensrc_key
      description:
        overall: "This function updates a user's OpenSRC API key within the database. It takes a username and the new API key as input. The provided API key is first stripped of any leading or trailing whitespace, then encrypted before being stored. Finally, the function performs an update operation on the `dbusers` collection, setting the `opensrc_api_key` field for the specified user. It returns the count of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          username,str,The username of the user whose OpenSRC API key needs to be updated.
          opensrc_api_key,str,The new OpenSRC API key to be stored for the user.
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents modified by the update operation, typically 0 or 1."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_opensrc_url:
      identifier: database.db.update_opensrc_url
      description:
        overall: "This function updates the 'opensrc_base_url' for a specific user in a database. It takes a username and a new base URL as input. The function uses the provided username to locate the user's record and then updates their 'opensrc_base_url' field with the new URL, ensuring any leading or trailing whitespace is removed from the URL. It returns the count of modified documents."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose 'opensrc_base_url' is to be updated.
          opensrc_base_url,str,"The new base URL for the open-source repository, which will have leading/trailing whitespace removed before being stored."
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation. Typically 0 or 1.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_gemini_key:
      identifier: database.db.fetch_gemini_key
      description:
        overall: "The fetch_gemini_key function retrieves a user's Gemini API key from a database. It takes a username as input to identify the specific user. The function queries a 'dbusers' collection, searching for a document where the '_id' field matches the provided username. If a user document is found, it extracts and returns the 'gemini_api_key' field. If no user is found with the given username, the function returns None."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose Gemini API key is to be fetched.
        returns[1]{name,type,description}:
          gemini_api_key,str | None,"The Gemini API key associated with the user, or None if the user or key is not found in the database."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions based on the provided context.
      error: null
    database.db.fetch_ollama_url:
      identifier: database.db.fetch_ollama_url
      description:
        overall: "This function retrieves the Ollama base URL associated with a specific user from a database. It queries the `dbusers` collection using the provided username as the `_id`. If a user document is found, it extracts and returns the `ollama_base_url` field. If no user is found, the function returns `None`."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose Ollama base URL is to be fetched.
        returns[1]{name,type,description}:
          ollama_base_url,str | None,"The Ollama base URL associated with the user, or `None` if the user is not found in the database."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_gpt_key:
      identifier: database.db.fetch_gpt_key
      description:
        overall: "This function is designed to retrieve a user's GPT API key from a database. It accepts a username as an argument and uses it to query a collection named 'dbusers'. The function specifically looks for a document matching the provided username and extracts the 'gpt_api_key' field. If a user document is found, the corresponding API key is returned; otherwise, the function returns None."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose GPT API key is to be fetched from the database.
        returns[1]{name,type,description}:
          gpt_api_key,str | None,"The GPT API key associated with the specified username, or None if the user is not found in the database."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.fetch_opensrc_key:
      identifier: database.db.fetch_opensrc_key
      description:
        overall: "This function, `fetch_opensrc_key`, is designed to retrieve an Open Source API key for a specified user from a database. It queries the `dbusers` collection using the provided username as the document's `_id`. The function projects only the `opensrc_api_key` field. If a user document is found, it extracts the `opensrc_api_key` value; otherwise, it returns `None`."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose Open Source API key is to be fetched.
        returns[1]{name,type,description}:
          opensrc_api_key,str | None,"The Open Source API key associated with the user, or None if the user is not found or the key does not exist."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.fetch_opensrc_url:
      identifier: database.db.fetch_opensrc_url
      description:
        overall: "This function retrieves the 'opensrc_base_url' for a specified user from a database. It queries the 'dbusers' collection using the provided username as the document's '_id'. If a matching user document is found, it extracts the 'opensrc_base_url' field. If no user is found or the field is absent, the function returns None."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose opensrc base URL is to be fetched.
        returns[1]{name,type,description}:
          opensrc_base_url,str | None,"The base URL for opensrc associated with the user, or None if the user is not found or the 'opensrc_base_url' field is not present."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.delete_user:
      identifier: database.db.delete_user
      description:
        overall: "This function is designed to remove a user record from a database collection. It takes a username as input and uses it to identify the document to be deleted. The function performs a delete operation on the `dbusers` collection, specifically targeting a document where the `_id` field matches the provided username. It then returns the number of documents that were successfully deleted."
        parameters[1]{name,type,description}:
          username,str,The unique identifier (username) of the user to be deleted from the database. This value is used to match the '_id' field in the database.
        returns[1]{name,type,description}:
          deleted_count,int,"An integer representing the number of documents that were deleted from the collection. A value of 1 indicates successful deletion of the user, while 0 indicates the user was not found or not deleted."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.get_decrypted_api_keys:
      identifier: database.db.get_decrypted_api_keys
      description:
        overall: "This function retrieves a user's API keys and base URLs from a database based on a provided username. It queries the `dbusers` collection for a matching user. If no user is found, it returns `None` for all expected values. Otherwise, it decrypts specific API keys (Gemini, GPT, Open Source) and retrieves base URLs (Ollama, Open Source) before returning all five values."
        parameters[1]{name,type,description}:
          username,str,The username used to identify and retrieve the user's data from the database.
        returns[5]{name,type,description}:
          gemini_plain,str | None,"The decrypted Gemini API key, or None if the user is not found or the key is absent."
          ollama_plain,str | None,"The Ollama base URL, or None if the user is not found or the URL is absent."
          gpt_plain,str | None,"The decrypted GPT API key, or None if the user is not found or the key is absent."
          opensrc_plain,str | None,"The decrypted open-source API key, or None if the user is not found or the key is absent."
          opensrc_url,str | None,"The open-source base URL, or None if the user is not found or the URL is absent."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.insert_chat:
      identifier: database.db.insert_chat
      description:
        overall: "This function creates a new chat entry in a database. It generates a unique identifier for the chat using UUID, records the provided username and chat name, and captures the current timestamp for creation. The constructed chat dictionary is then inserted into the 'dbchats' collection. Finally, it returns the unique ID of the newly inserted document."
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat entry.
          chat_name,str,The name of the chat.
        returns[1]{name,type,description}:
          inserted_id,str,The unique identifier of the newly created chat entry in the database.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_chats_by_user:
      identifier: database.db.fetch_chats_by_user
      description:
        overall: "This function, `fetch_chats_by_user`, is designed to retrieve all chat documents associated with a specific user from a database. It queries the `dbchats` collection, filtering documents where the 'username' field matches the provided input. The results are then sorted in ascending order based on their 'created_at' timestamp. Finally, the function returns the collection of matching chat documents as a list."
        parameters[1]{name,type,description}:
          username,str,The username for which to fetch chat documents.
        returns[1]{name,type,description}:
          chats,list,"A list of chat documents associated with the specified username, sorted by their creation date."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.check_chat_exists:
      identifier: database.db.check_chat_exists
      description:
        overall: "This function, `check_chat_exists`, is designed to verify the existence of a specific chat within a database collection named `dbchats`. It takes a username and a chat name as input. The function queries the `dbchats` collection to find a document that matches both the provided username and chat name. It then returns a boolean value indicating whether such a chat was found."
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to be checked for existence.
          chat_name,str,The name of the chat to be checked for existence.
        returns[1]{name,type,description}:
          exists,bool,True if a chat matching the provided username and chat name is found in the database; False otherwise.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.rename_chat_fully:
      identifier: database.db.rename_chat_fully
      description:
        overall: "This function renames a chat and all its associated exchanges (messages) within the database for a specified user. It first updates the `chat_name` field for a single chat entry in the `dbchats` collection. Subsequently, it updates the `chat_name` field for all related exchange entries in the `dbexchanges` collection. The function returns the count of chat documents that were modified during the initial rename operation."
        parameters[3]{name,type,description}:
          username,str,The username associated with the chat to be renamed.
          old_name,str,The current name of the chat that needs to be changed.
          new_name,str,The desired new name for the chat.
        returns[1]{name,type,description}:
          modified_count,int,The number of chat documents modified by the initial rename operation (from dbchats.update_one).
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.insert_exchange:
      identifier: database.db.insert_exchange
      description:
        overall: "This function is designed to create and persist a new exchange record into a database. It generates a unique identifier using UUID, constructs a comprehensive dictionary containing the provided question, answer, feedback, user details, chat name, and various optional metrics such as helper/main components used, time taken, token counts, and savings percentage. A timestamp for creation is automatically added. The function attempts to insert this structured record into the `dbexchanges` collection. Upon successful insertion, it returns the unique ID of the new record; otherwise, it catches any exceptions during the database operation and returns `None`."
        parameters[13]{name,type,description}:
          question,str,The textual question associated with the exchange.
          answer,str,The textual answer generated for the exchange.
          feedback,str,The feedback string provided for the exchange.
          username,str,The username of the individual involved in the exchange.
          chat_name,str,The name of the chat session where the exchange occurred.
          helper_used,str,"An optional string indicating which helper component was utilized, defaulting to an empty string."
          main_used,str,"An optional string indicating which main component was utilized, defaulting to an empty string."
          total_time,str,"An optional string representing the total time taken for the exchange, defaulting to an empty string."
          helper_time,str,"An optional string representing the time taken by the helper component, defaulting to an empty string."
          main_time,str,"An optional string representing the time taken by the main component, defaulting to an empty string."
          json_tokens,int,"An optional integer representing the number of JSON tokens used, defaulting to 0."
          toon_tokens,int,"An optional integer representing the number of Toon tokens used, defaulting to 0."
          savings_percent,float,"An optional float representing the percentage of savings, defaulting to 0.0."
        returns[2]{name,type,description}:
          new_id,str,The unique identifier of the newly created exchange record upon successful insertion.
          None,NoneType,Returned if an error occurs during the database insertion process.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_exchanges_by_user:
      identifier: database.db.fetch_exchanges_by_user
      description:
        overall: This function retrieves all exchange records associated with a specific username from a database collection named `dbexchanges`. It queries the database using the provided username and sorts the results by their 'created_at' timestamp in ascending order. The function then converts the database cursor into a list and returns these sorted exchange records.
        parameters[1]{name,type,description}:
          username,str,The username for which to fetch exchange records.
        returns[1]{name,type,description}:
          exchanges,list,"A list of exchange records associated with the specified username, sorted by creation timestamp."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_exchanges_by_chat:
      identifier: database.db.fetch_exchanges_by_chat
      description:
        overall: This function retrieves a list of exchange documents from the 'dbexchanges' collection. It filters these exchanges based on a provided username and chat name. The results are then sorted by their 'created_at' timestamp in ascending order before being returned as a list.
        parameters[2]{name,type,description}:
          username,str,The username to filter the exchanges by.
          chat_name,str,The name of the chat to filter the exchanges by.
        returns[1]{name,type,description}:
          exchanges,list,"A list of exchange documents (dictionaries) that match the specified username and chat name, sorted by their creation timestamp."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_exchange_feedback:
      identifier: database.db.update_exchange_feedback
      description:
        overall: This function updates the feedback value for a specific exchange record in a database. It takes an exchange identifier and an integer feedback value. The function uses `dbexchanges.update_one` to locate the document by its `_id` and sets the 'feedback' field to the provided value. It then returns the count of documents that were successfully modified.
        parameters[2]{name,type,description}:
          exchange_id,Any,The unique identifier of the exchange record to be updated.
          feedback,int,The integer value representing the feedback to be set for the exchange.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_exchange_feedback_message:
      identifier: database.db.update_exchange_feedback_message
      description:
        overall: This function updates an existing exchange record in the database. It takes an exchange identifier and a new feedback message string. The function uses the `dbexchanges` collection to find the document matching the provided `exchange_id` and sets its `feedback_message` field to the new value. It then returns the count of documents that were successfully modified by this operation.
        parameters[2]{name,type,description}:
          exchange_id,Any,The unique identifier of the exchange record to be updated in the database.
          feedback_message,str,The new feedback message to be stored for the specified exchange record.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified as a result of the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.delete_exchange_by_id:
      identifier: database.db.delete_exchange_by_id
      description:
        overall: This function is responsible for deleting a single exchange record from the database. It takes an exchange ID as input and uses it to locate and remove the corresponding document from the 'dbexchanges' collection. The function returns an integer indicating how many documents were successfully deleted.
        parameters[1]{name,type,description}:
          exchange_id,str,The unique identifier of the exchange record to be deleted.
        returns[1]{name,type,description}:
          deleted_count,int,"The number of documents deleted from the collection. Typically 0 if no matching document was found, or 1 if a document was successfully deleted."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.delete_full_chat:
      identifier: database.db.delete_full_chat
      description:
        overall: "This function is designed to completely delete a specific chat and all its associated message exchanges for a given user. It first removes all messages linked to the chat from the 'dbexchanges' collection using `dbexchanges.delete_many`. Subsequently, it deletes the chat entry itself from the 'dbchats' collection using `dbchats.delete_one`. This two-step process ensures data consistency between the frontend and backend. The function returns the count of chat entries that were deleted."
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to be deleted.
          chat_name,str,The name of the chat to be deleted.
        returns[1]{name,type,description}:
          deleted_count,int,The number of chat documents deleted from the 'dbchats' collection. This will typically be 0 or 1 for a delete_one operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.clean_names:
      identifier: frontend.frontend.clean_names
      description:
        overall: "The `clean_names` function processes a list of strings, `model_list`. It iterates through each string, splitting it by the '/' character. For each string, it extracts the last segment after the final '/' and compiles these segments into a new list. This effectively cleans path-like names by returning only the base name."
        parameters[1]{name,type,description}:
          model_list,list,"A list of strings, where each string is expected to be a path-like identifier that may contain '/' characters."
        returns[1]{name,type,description}:
          cleaned_names,"list[str]",A new list containing the last segment of each string in the input `model_list` after splitting by '/'.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.get_filtered_models:
      identifier: frontend.frontend.get_filtered_models
      description:
        overall: "This function filters a given list of models (`source_list`) based on a specified `category_name`. It retrieves associated keywords from a global `CATEGORY_KEYWORDS` mapping. If the \"STANDARD\" keyword is present for the category, it returns only those models from `source_list` that are also found in a global `STANDARD_MODELS` list. Otherwise, it iterates through the `source_list` and appends models whose names contain any of the category's keywords (case-insensitive) to a `filtered` list. Finally, it returns the `filtered` list if it contains any items, otherwise it returns the original `source_list`."
        parameters[2]{name,type,description}:
          source_list,list,The list of models to be filtered.
          category_name,str,The name of the category to use for filtering.
        returns[1]{name,type,description}:
          filtered_models,list,"A list of models filtered according to the specified category, or the original source list if no models match the filter criteria."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.save_gemini_cb:
      identifier: frontend.frontend.save_gemini_cb
      description:
        overall: "This function, `save_gemini_cb`, is designed to handle the saving of a new Gemini API key. It retrieves the potential new key from the Streamlit session state. If a new key is present, it updates the Gemini key in the database for the current user and then clears the key from the session state. Finally, it displays a success toast notification to the user."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    frontend.frontend.save_ollama_cb:
      identifier: frontend.frontend.save_ollama_cb
      description:
        overall: "This function, `save_ollama_cb`, acts as a callback to process and save a user-provided Ollama URL. It retrieves the URL from the Streamlit session state, specifically from the key 'in_ollama_url'. If a non-empty URL is found, it proceeds to update this URL in the database associated with the current user's username. Upon successful update, it displays a confirmation toast message to the user."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.load_data_from_db:
      identifier: frontend.frontend.load_data_from_db
      description:
        overall: "This function `load_data_from_db` is responsible for loading chat and exchange data for a specified user from the database and populating the Streamlit session state. It first checks if the user's data is already loaded to prevent redundant operations. If not loaded, it initializes the `chats` dictionary in `st.session_state`, then fetches predefined chats and exchanges from the database. The fetched exchanges are organized into their corresponding chats within the session state, with a fallback for unnamed chats and handling of `feedback` values. Finally, it ensures a default chat exists if none are loaded, inserting \"Chat 1\" into the database if necessary, and sets an active chat for the user interface."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose chat and exchange data needs to be loaded from the database.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    frontend.frontend.handle_feedback_change:
      identifier: frontend.frontend.handle_feedback_change
      description:
        overall: "This function updates the 'feedback' key of an exchange object (`ex`) with a new value (`val`). It then persists this updated feedback to a database using `db.update_exchange_feedback`, identifying the record by `ex[\"_id\"]`. Finally, it triggers a full re-run of the Streamlit application using `st.rerun()` to reflect the changes in the UI."
        parameters[2]{name,type,description}:
          ex,dict,"A dictionary-like object representing an exchange, expected to contain 'feedback' and '_id' keys."
          val,Any,The new feedback value to be assigned.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.handle_delete_exchange:
      identifier: frontend.frontend.handle_delete_exchange
      description:
        overall: "This function handles the deletion of a specific exchange. It first removes the exchange from the database using its ID. Subsequently, it checks if the associated chat exists in the Streamlit session state and, if the exchange is present within that chat's exchanges list, it removes it from the session state. Finally, it triggers a Streamlit rerun to refresh the UI."
        parameters[2]{name,type,description}:
          chat_name,str,The name of the chat to which the exchange belongs.
          ex,dict,"A dictionary representing the exchange to be deleted, expected to contain an '_id' key."
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    frontend.frontend.handle_delete_chat:
      identifier: frontend.frontend.handle_delete_chat
      description:
        overall: "This function, `handle_delete_chat`, is designed to remove a specific chat for a given user. It first deletes the chat from the database using `db.delete_full_chat`. Subsequently, it cleans up the Streamlit session state by removing the chat from `st.session_state.chats`. If other chats remain, the first one is set as the active chat; otherwise, a new default chat named 'Chat 1' is created in both the database and the session state. Finally, it triggers a Streamlit rerun to update the UI."
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to be deleted.
          chat_name,str,The name of the chat to be deleted.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    frontend.frontend.extract_repo_name:
      identifier: frontend.frontend.extract_repo_name
      description:
        overall: "The `extract_repo_name` function processes an input string to identify and extract a repository name. It first uses a regular expression to locate a URL within the provided text. If a URL is found, it parses the URL to isolate the path component. From this path, the function extracts the last segment, which is treated as the repository name, and removes any trailing \".git\" suffix. If no URL is found or a repository name cannot be successfully extracted, the function returns None."
        parameters[1]{name,type,description}:
          text,str,"The input string, which may contain a URL from which a repository name needs to be extracted."
        returns[1]{name,type,description}:
          repo_name,str | None,"The extracted repository name as a string, or None if no URL is found or a repository name cannot be determined."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.stream_text_generator:
      identifier: frontend.frontend.stream_text_generator
      description:
        overall: "This function acts as a generator that takes a string of text and yields its words sequentially. It splits the input text by spaces and iterates through each word. For every word, it yields the word followed by a space, introducing a small delay of 0.01 seconds between each yield. This creates a streaming effect, delivering the text word by word over time."
        parameters[1]{name,type,description}:
          text,str,The input string of text to be processed and streamed word by word.
        returns[1]{name,type,description}:
          word_chunk,str,"A single word from the input text, followed by a space, yielded sequentially."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.render_text_with_mermaid:
      identifier: frontend.frontend.render_text_with_mermaid
      description:
        overall: "This function processes a given markdown text, identifying and rendering embedded Mermaid diagrams. It splits the input text into parts based on '```mermaid' delimiters. Non-Mermaid text sections are rendered using `st.markdown` or `st.write_stream` if streaming is enabled. Mermaid code blocks are rendered using `st_mermaid`, with a fallback to `st.code` if an error occurs during Mermaid rendering."
        parameters[2]{name,type,description}:
          markdown_text,str,"The input text, which may contain markdown and embedded Mermaid diagram definitions."
          should_stream,bool,A flag indicating whether non-Mermaid text content should be streamed (True) or rendered directly (False). Defaults to False.
        returns[1]{name,type,description}:
          None,None,The function does not explicitly return a value. It performs side effects by rendering content to a Streamlit application. It returns implicitly if 'markdown_text' is empty.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.render_exchange:
      identifier: frontend.frontend.render_exchange
      description:
        overall: "This function `render_exchange` is designed to display a single chat exchange, comprising a user's question and an assistant's answer, within a Streamlit application. It renders the user's input and then presents the assistant's response, which includes an interactive toolbar. This toolbar offers functionalities such as providing feedback (like/dislike), adding comments through a popover, downloading the response, and deleting the exchange. The function also manages error states, displaying an error message if the answer indicates an issue and providing a delete option for erroneous exchanges. It leverages various Streamlit components to construct the user interface."
        parameters[2]{name,type,description}:
          ex,dict,"A dictionary-like object representing a single chat exchange. It is expected to contain keys such as 'question', 'answer', 'feedback', '_id', and 'feedback_message' for rendering and interaction."
          current_chat_name,str,"A string representing the name of the current chat session, primarily used when invoking the `handle_delete_exchange` function."
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
  classes:
    backend.AST_Schema.ASTVisitor:
      identifier: backend.AST_Schema.ASTVisitor
      description:
        overall: "The ASTVisitor class extends ast.NodeVisitor to systematically traverse an Abstract Syntax Tree (AST) and extract structured information about imports, functions, and classes from Python source code. It populates an internal schema dictionary with details such as identifiers, docstrings, and source code segments for each encountered element. The visitor manages context, like the current module path and the class being processed, to accurately scope and identify nested elements like methods and functions. This class serves as a foundational component for static code analysis, providing a structured representation of a Python file's contents."
        init_method:
          description: "The constructor initializes the ASTVisitor instance with the raw source code, the file's absolute path, and the project's root directory. It calculates the module's relative path and sets up an empty schema dictionary to store parsed imports, functions, and classes. An internal attribute, `_current_class`, is also initialized to `None` to track the class currently being visited during AST traversal."
          parameters[3]{name,type,description}:
            source_code,str,The raw source code of the Python file being analyzed.
            file_path,str,The absolute file path to the Python file.
            project_root,str,The root directory of the entire project.
        methods[5]:
          - identifier: visit_Import
            description:
              overall: "This method is invoked by the `ast.NodeVisitor` framework when an `ast.Import` node is encountered during AST traversal. It iterates through each alias within the import statement, extracting the module name. Each identified module name is then appended to the 'imports' list within the visitor's `schema` dictionary. Finally, it calls `self.generic_visit(node)` to ensure that the traversal continues to any child nodes."
              parameters[1]{name,type,description}:
                node,ast.Import,The AST node representing an 'import' statement.
              returns[0]:
              usage_context:
                calls: This method calls self.generic_visit(node).
                called_by: This method is called by the ast.NodeVisitor framework when an ast.Import node is encountered during AST traversal.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method handles `ast.ImportFrom` nodes, which correspond to 'from ... import ...' statements in Python. It processes each alias within the import statement, constructing a fully qualified name by combining the module name from the node with the alias name. This fully qualified import string is then added to the 'imports' list within the visitor's `schema`. The method concludes by calling `self.generic_visit(node)` to ensure proper traversal of any nested nodes."
              parameters[1]{name,type,description}:
                node,ast.ImportFrom,The AST node representing a 'from ... import ...' statement.
              returns[0]:
              usage_context:
                calls: This method calls self.generic_visit(node).
                called_by: This method is called by the ast.NodeVisitor framework when an ast.ImportFrom node is encountered during AST traversal.
            error: null
          - identifier: visit_ClassDef
            description:
              overall: "This method is responsible for processing `ast.ClassDef` nodes, representing class definitions in the source code. It constructs a unique identifier for the class using the module path and class name, then gathers essential information such as the class's name, docstring, and its exact source code segment. This collected data is stored in a `class_info` dictionary, which is then appended to the 'classes' list in the visitor's `schema`. The method temporarily sets `_current_class` to this `class_info` to correctly scope nested methods, performs a generic visit for child nodes, and finally resets `_current_class` to `None`."
              parameters[1]{name,type,description}:
                node,ast.ClassDef,The AST node representing a class definition.
              returns[0]:
              usage_context:
                calls: "This method calls ast.get_docstring(node), ast.get_source_segment(self.source_code, node), and self.generic_visit(node)."
                called_by: This method is called by the ast.NodeVisitor framework when an ast.ClassDef node is encountered during AST traversal.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method processes `ast.FunctionDef` nodes, distinguishing between methods defined within a class and standalone functions. If a class is currently being visited (indicated by `_current_class`), it extracts method-specific details like its identifier, name, arguments, docstring, and line numbers, appending this information to the `method_context` of the current class. Otherwise, for standalone functions, it gathers similar details and appends them to the 'functions' list in the visitor's `schema`. The method ensures continued AST traversal by calling `self.generic_visit(node)`."
              parameters[1]{name,type,description}:
                node,ast.FunctionDef,The AST node representing a function definition (either a standalone function or a class method).
              returns[0]:
              usage_context:
                calls: "This method calls ast.get_docstring(node), ast.get_source_segment(self.source_code, node) (for standalone functions), and self.generic_visit(node)."
                called_by: "This method is called by the ast.NodeVisitor framework when an ast.FunctionDef node is encountered during AST traversal, and also by visit_AsyncFunctionDef."
            error: null
          - identifier: visit_AsyncFunctionDef
            description:
              overall: "This method is designed to handle `ast.AsyncFunctionDef` nodes, which represent asynchronous function definitions. To avoid code duplication and ensure consistent processing, it delegates the entire analysis of the asynchronous function directly to the `visit_FunctionDef` method. This approach ensures that both synchronous and asynchronous functions are processed uniformly, collecting their metadata into the visitor's schema."
              parameters[1]{name,type,description}:
                node,ast.AsyncFunctionDef,The AST node representing an asynchronous function definition.
              returns[0]:
              usage_context:
                calls: This method calls self.visit_FunctionDef(node).
                called_by: This method is called by the ast.NodeVisitor framework when an ast.AsyncFunctionDef node is encountered during AST traversal.
            error: null
        usage_context:
          dependencies: "The ASTVisitor class depends on the 'ast' module for its core functionality, inheriting from ast.NodeVisitor and utilizing functions like ast.get_docstring and ast.get_source_segment. It also implicitly depends on a 'path_to_module' function, which is called during initialization."
          instantiated_by: It is not explicitly known where this class is instantiated within the provided context.
      error: null
    backend.AST_Schema.ASTAnalyzer:
      identifier: backend.AST_Schema.ASTAnalyzer
      description:
        overall: "The ASTAnalyzer class is designed to perform static analysis on a Python codebase, focusing on building an Abstract Syntax Tree (AST) schema and integrating relationship data. It processes a collection of files from a repository, parses their Python content, and extracts structural information like functions, classes, and imports. Furthermore, it enriches this AST schema by merging call graph data, identifying incoming/outgoing calls for functions and methods, and determining class-level dependencies."
        init_method:
          description: "This constructor initializes the ASTAnalyzer class. It currently performs no specific setup or attribute assignments, serving as a placeholder or indicating that the class's state is managed entirely through its methods."
          parameters[1]{name,type,description}:
            self,ASTAnalyzer,The instance of the class.
        methods[2]:
          - identifier: merge_relationship_data
            description:
              overall: "This method integrates relationship data (incoming and outgoing calls) into a pre-existing AST schema. It iterates through files, functions, and classes within the `full_schema`, populating their respective 'calls', 'called_by', and 'instantiated_by' contexts using information from `raw_relationships`. Additionally, it identifies and lists external dependencies for each class based on its methods' outgoing calls."
              parameters[3]{name,type,description}:
                self,ASTAnalyzer,The instance of the class.
                full_schema,dict,"A dictionary representing the complete AST schema of the repository, including files, functions, and classes."
                raw_relationships,dict,A dictionary containing raw incoming and outgoing call relationships for various identifiers.
              returns[1]{name,type,description}:
                full_schema,dict,The updated full schema dictionary with integrated relationship data.
              usage_context:
                calls: This method does not explicitly call any other functions or methods.
                called_by: This method is not explicitly called by any other functions or methods in the provided context.
            error: null
          - identifier: analyze_repository
            description:
              overall: "This method processes a list of file objects from a Git repository to build a comprehensive AST schema. It filters for Python files, parses their content using the `ast` module, and then uses an `ASTVisitor` to extract structured AST nodes (imports, functions, classes). The extracted schema for each file is then added to a `full_schema` dictionary, handling potential parsing errors."
              parameters[3]{name,type,description}:
                self,ASTAnalyzer,The instance of the class.
                files,list,"A list of file objects, each expected to have `path` and `content` attributes."
                repo,GitRepository,"An object representing the Git repository, though its direct attributes are not accessed in the provided method body."
              returns[1]{name,type,description}:
                full_schema,dict,"A dictionary representing the AST schema of the entire repository, structured by file paths."
              usage_context:
                calls: This method does not explicitly call any other functions or methods.
                called_by: This method is not explicitly called by any other functions or methods in the provided context.
            error: null
        usage_context:
          dependencies: The class does not explicitly declare any external functional dependencies in the provided context.
          instantiated_by: This class is not explicitly instantiated by any known entities in the provided context.
      error: null
    backend.File_Dependency.FileDependencyGraph:
      identifier: backend.File_Dependency.FileDependencyGraph
      description:
        overall: "The FileDependencyGraph class extends NodeVisitor to analyze Python source code and build a graph of file-level import dependencies. It processes import statements, resolving both absolute and relative imports to identify which files depend on others. The class maintains a dictionary, `import_dependencies`, to store these relationships, mapping a filename to a set of files it imports."
        init_method:
          description: The constructor initializes the FileDependencyGraph instance with the path to the current file being analyzed and the root directory of the repository. These paths are stored as instance attributes to be used during the dependency resolution process.
          parameters[2]{name,type,description}:
            filename,str,The path to the Python file currently being analyzed for dependencies.
            repo_root,str,"The root directory of the repository, used for resolving relative import paths and locating files."
        methods[3]:
          - identifier: _resolve_module_name
            description:
              overall: "This method is responsible for resolving relative import statements (e.g., `from .. import name`). It calculates the correct base directory based on the import level and then checks if the imported names correspond to existing module files or symbols exported by `__init__.py` files. It uses two nested helper functions, `module_file_exists` and `init_exports_symbol`, to perform these checks. If no modules or symbols can be resolved, an `ImportError` is raised."
              parameters[1]{name,type,description}:
                node,ImportFrom,The AST node representing the 'from ... import ...' statement.
              returns[1]{name,type,description}:
                resolved_names,"list[str]",A list of resolved module or symbol names that actually exist.
              usage_context:
                calls: This method does not make any external calls according to the provided context.
                called_by: This method is not explicitly called by other methods within the provided context.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method is part of the AST NodeVisitor pattern and is invoked for `Import` and `ImportFrom` nodes. Its primary function is to record the detected import dependencies. It adds the imported module or symbol name to the `import_dependencies` dictionary, associating it with the current file being analyzed. It then calls `generic_visit` to continue traversing the AST."
              parameters[2]{name,type,description}:
                node,Import | ImportFrom,The AST node representing an import statement.
                base_name,str | None,"An optional base name for the import, typically used for `from ... import ...` statements where the module name is explicitly provided."
              returns[0]:
              usage_context:
                calls: This method does not make any external calls according to the provided context.
                called_by: This method is not explicitly called by other methods within the provided context.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method is a specialized visitor for `ImportFrom` AST nodes. It distinguishes between absolute and relative imports. For absolute imports (where `node.module` is present), it extracts the last part of the module name and passes it to `visit_Import`. For relative imports (where `node.module` is `None`), it calls `_resolve_module_name` to determine the actual module paths, handling potential `ImportError` exceptions by printing a message. Finally, it ensures the AST traversal continues by calling `generic_visit`."
              parameters[1]{name,type,description}:
                node,ImportFrom,The AST node representing a 'from ... import ...' statement.
              returns[0]:
              usage_context:
                calls: This method does not make any external calls according to the provided context.
                called_by: This method is not explicitly called by other methods within the provided context.
            error: null
        usage_context:
          dependencies: This class has no explicitly listed external functional dependencies in the provided context.
          instantiated_by: This class has no explicitly listed instantiation points in the provided context.
      error: null
    backend.HelperLLM.LLMHelper:
      identifier: backend.HelperLLM.LLMHelper
      description:
        overall: "The LLMHelper class provides a centralized interface for interacting with various Large Language Models (LLMs) to generate structured documentation for Python functions and classes. It handles the configuration of different LLM providers (Gemini, OpenAI, custom, Ollama), manages system prompts, and implements batch processing with rate limiting. The class ensures that LLM outputs conform to predefined Pydantic schemas for FunctionAnalysis and ClassAnalysis, streamlining the documentation generation workflow."
        init_method:
          description: "This constructor initializes the LLMHelper with the necessary API key, paths to system prompt files for function and class analysis, and optional model configuration. It loads the system prompts from the specified files, configures the appropriate LLM client (Gemini, OpenAI, custom, or Ollama) based on the model name, and sets up structured output parsers. It also calls a private method to configure batch processing settings based on the chosen model."
          parameters[5]{name,type,description}:
            api_key,str,The API key required for authenticating with the chosen LLM service.
            function_prompt_path,str,The file path to the system prompt used for generating function documentation.
            class_prompt_path,str,The file path to the system prompt used for generating class documentation.
            model_name,str,"The name of the LLM model to be used, defaulting to 'gemini-2.0-flash-lite'."
            base_url,str,An optional base URL for custom or Ollama LLM endpoints.
        methods[3]:
          - identifier: _configure_batch_settings
            description:
              overall: "This private method dynamically sets the `batch_size` attribute for the LLMHelper instance based on the provided model name. It contains a series of conditional statements to assign specific batch sizes optimized for different LLM models, including various Gemini, Llama, and GPT versions, as well as custom or alias models. If the model name is not explicitly recognized, it logs a warning and assigns a conservative default batch size."
              parameters[1]{name,type,description}:
                model_name,str,The name of the LLM model for which batch processing settings need to be configured.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call other methods or functions within its body, beyond standard Python operations."
                called_by: This method is called by the `__init__` method of the `LLMHelper` class.
            error: null
          - identifier: generate_for_functions
            description:
              overall: "This method generates and validates documentation for a batch of functions using the configured LLM. It takes a list of FunctionAnalysisInput objects, converts them into JSON payloads, and constructs conversations with the `function_system_prompt`. The method then processes these conversations in batches, calling the `function_llm.batch` method, and includes a waiting period between batches to respect API rate limits. Any errors encountered during batch processing result in `None` being added to the results for the corresponding input."
              parameters[1]{name,type,description}:
                function_inputs,"List[FunctionAnalysisInput]","A list of input objects, each containing details of a function for which documentation is to be generated."
              returns[1]{name,type,description}:
                "null","List[Optional[FunctionAnalysis]]","A list of `FunctionAnalysis` objects, where each object represents the generated documentation for a function, or `None` if an error occurred during its processing."
              usage_context:
                calls: "This method calls `json.dumps` for serialization, `SystemMessage` and `HumanMessage` to build LLM conversations, `self.function_llm.batch` to execute batch LLM calls, `logging.info` and `logging.error` for logging, and `time.sleep` for rate limiting."
                called_by: This method is not explicitly called by other methods within the provided `method_context`.
            error: null
          - identifier: generate_for_classes
            description:
              overall: "This method is responsible for generating and validating documentation for a batch of classes using the configured LLM. It accepts a list of `ClassAnalysisInput` objects, serializes them into JSON payloads, and prepares them as `SystemMessage` and `HumanMessage` conversations using the `class_system_prompt`. The method then iterates through these conversations in batches, invoking `self.class_llm.batch` to obtain LLM responses. It incorporates error handling for batch calls and a `time.sleep` mechanism to manage API rate limits, ultimately returning a list of `ClassAnalysis` objects or `None` for inputs that failed during processing."
              parameters[1]{name,type,description}:
                class_inputs,"List[ClassAnalysisInput]","A list of input objects, each containing details of a class for which documentation is to be generated."
              returns[1]{name,type,description}:
                "null","List[Optional[ClassAnalysis]]","A list of `ClassAnalysis` objects, where each object represents the generated documentation for a class, or `None` if an error occurred during its processing."
              usage_context:
                calls: "This method calls `json.dumps` for serialization, `SystemMessage` and `HumanMessage` to build LLM conversations, `self.class_llm.batch` to execute batch LLM calls, `logging.info` and `logging.error` for logging, and `time.sleep` for rate limiting."
                called_by: This method is not explicitly called by other methods within the provided `method_context`.
            error: null
        usage_context:
          dependencies: "The class depends on `logging` for output, `time` for rate limiting, `json` for serialization, `langchain_google_genai.ChatGoogleGenerativeAI`, `langchain_ollama.ChatOllama`, `langchain_openai.ChatOpenAI` for LLM integrations, `langchain.messages.HumanMessage`, `langchain.messages.SystemMessage` for conversation construction, and `schemas.types.FunctionAnalysis`, `schemas.types.ClassAnalysis`, `schemas.types.FunctionAnalysisInput`, `schemas.types.ClassAnalysisInput` for structured input/output. It also implicitly depends on `SCADSLLM_URL` and `OLLAMA_BASE_URL` environment variables for certain model types."
          instantiated_by: The provided context does not specify where this class is instantiated.
      error: null
    backend.MainLLM.MainLLM:
      identifier: backend.MainLLM.MainLLM
      description:
        overall: "The MainLLM class serves as a central interface for interacting with various large language models (LLMs). It abstracts away the specifics of different LLM providers (Gemini, OpenAI, Ollama, or custom APIs) by dynamically configuring the appropriate client based on the model name. The class manages a system prompt loaded from a file and provides methods for both direct (blocking) and streaming interactions with the chosen LLM, ensuring robust error handling for communication."
        init_method:
          description: "This constructor initializes the MainLLM instance by setting up the system prompt from a specified file and configuring the appropriate LLM client based on the provided model name. It supports various LLM providers like Google Generative AI, OpenAI, and Ollama, handling their respective API keys and base URLs. The method also includes validation for the API key and prompt file path."
          parameters[4]{name,type,description}:
            api_key,str,The API key required for authenticating with the chosen LLM service.
            prompt_file_path,str,The file path to the system prompt that will be used for all LLM interactions.
            model_name,str,"The name of the LLM model to be used, defaulting to 'gemini-2.5-pro'."
            base_url,str,"An optional base URL for custom LLM API endpoints, if applicable."
        methods[2]:
          - identifier: call_llm
            description:
              overall: "This method sends a user's input along with the pre-configured system prompt to the initialized LLM for a single, blocking response. It constructs a list of messages, invokes the LLM, logs the process, and returns the content of the LLM's response. The method includes error handling to catch and report exceptions during the LLM call, returning None in case of failure."
              parameters[1]{name,type,description}:
                user_input,str,The user's query or message to be sent to the LLM.
              returns[1]{name,type,description}:
                content,str,"The text content of the LLM's response, or None if an error occurred."
              usage_context:
                calls: "This method calls SystemMessage, HumanMessage, self.llm.invoke, logging.info, and logging.error."
                called_by: This method is not explicitly called by other functions or methods in the provided context.
            error: null
          - identifier: stream_llm
            description:
              overall: "This method streams the LLM's response for a given user input, allowing for real-time processing of the output. It constructs messages similar to call_llm but utilizes the stream method of the LLM client, yielding each chunk of content as it becomes available. The method also incorporates error handling for streaming operations, yielding an error message if an exception occurs."
              parameters[1]{name,type,description}:
                user_input,str,The user's query or message for which to stream the LLM response.
              returns[2]{name,type,description}:
                chunk.content,str,Yields individual text chunks from the LLM's streaming response.
                error_message,str,Yields an error message if an exception occurs during streaming.
              usage_context:
                calls: "This method calls SystemMessage, HumanMessage, self.llm.stream, logging.info, and logging.error."
                called_by: This method is not explicitly called by other functions or methods in the provided context.
            error: null
        usage_context:
          dependencies: "The class depends on external libraries such as langchain_google_genai, langchain_ollama, langchain_openai, and langchain.messages for LLM integration, as well as logging for operational insights. It also relies on environment variables like SCADSLLM_URL and OLLAMA_BASE_URL for custom API configurations."
          instantiated_by: This class is not explicitly instantiated by other functions or methods in the provided context.
      error: null
    backend.basic_info.ProjektInfoExtractor:
      identifier: backend.basic_info.ProjektInfoExtractor
      description:
        overall: "The ProjektInfoExtractor class is designed to systematically extract and consolidate essential project information from various common project files and a repository URL. It acts as a central intelligence unit for gathering metadata such as project title, description, features, tech stack, installation instructions, and dependencies. By prioritizing information sources (e.g., pyproject.toml over requirements.txt for dependencies), it ensures a robust and consistent data collection process, ultimately providing a comprehensive overview of a project."
        init_method:
          description: "The constructor initializes the ProjektInfoExtractor instance by setting a default 'Information not found' string and establishing the self.info dictionary structure. This dictionary serves as a container for all extracted project details, pre-filled with placeholder values to indicate missing information."
          parameters[0]:
        methods[7]:
          - identifier: _clean_content
            description:
              overall: "This private utility method is responsible for sanitizing string content by removing null bytes (\\x00). Null bytes can appear due to encoding errors, such as reading UTF-16 encoded files as UTF-8, and can interfere with text processing. The method ensures that the content is clean before further parsing by replacing all occurrences of null bytes with an empty string."
              parameters[1]{name,type,description}:
                content,str,The input string content to be cleaned.
              returns[1]{name,type,description}:
                "null",str,"The cleaned string content with null bytes removed, or an empty string if the input content was empty."
              usage_context:
                calls: "This method does not call any other functions or methods, performing only string manipulation."
                called_by: "This method is called by _parse_readme, _parse_toml, and _parse_requirements to preprocess file contents."
            error: null
          - identifier: _finde_datei
            description:
              overall: "This private helper method searches through a list of file objects to find one whose path matches any of the provided patterns. The search is case-insensitive, making it robust to variations in file naming conventions. It iterates through each file and each pattern, returning the first file object that satisfies a match or None if no match is found after checking all possibilities."
              parameters[2]{name,type,description}:
                patterns,"List[str]","A list of string patterns (e.g., file extensions or names) to match against file paths."
                dateien,"List[Any]","A list of file-like objects, each expected to have a 'path' attribute."
              returns[1]{name,type,description}:
                "null","Optional[Any]","The first file object that matches a pattern, or None if no match is found."
              usage_context:
                calls: "This method does not call any other functions or methods, performing only string and list operations."
                called_by: "This method is called by extrahiere_info to locate specific project files like READMEs, pyproject.toml, and requirements.txt."
            error: null
          - identifier: _extrahiere_sektion_aus_markdown
            description:
              overall: "This private method extracts content from a Markdown string that appears under a level 2 heading (##). It takes a list of keywords and constructs a regular expression to find any of these keywords in an H2 heading. The method then captures all text following that heading until the next H2 heading or the end of the document, returning the stripped content if a match is found."
              parameters[2]{name,type,description}:
                inhalt,str,The Markdown content string to parse.
                keywords,"List[str]",A list of keywords to match against Markdown H2 headings.
              returns[1]{name,type,description}:
                "null","Optional[str]","The stripped content found under the matched Markdown H2 heading, or None if no matching section is found."
              usage_context:
                calls: "This method calls re.escape to prepare keywords for regex, re.compile to create a regex pattern, and re.search to find the pattern in the content."
                called_by: "This method is called by _parse_readme to extract specific sections like features, tech stack, status, installation instructions, and quick start guides."
            error: null
          - identifier: _parse_readme
            description:
              overall: "This private method processes the content of a README file to populate various project information fields within the self.info dictionary. It first cleans the content, then attempts to extract the project title from an H1 heading and a general description. Subsequently, it utilizes _extrahiere_sektion_aus_markdown to find and extract specific sections like 'Features', 'Tech Stack', 'Status', 'Installation', and 'Quick Start' based on predefined keywords."
              parameters[1]{name,type,description}:
                inhalt,str,The content of the README file as a string.
              returns[0]:
              usage_context:
                calls: "This method calls _clean_content to sanitize the input, re.search for title and description extraction, and _extrahiere_sektion_aus_markdown for extracting specific sections."
                called_by: This method is called by extrahiere_info after a README file has been identified.
            error: null
          - identifier: _parse_toml
            description:
              overall: "This private method parses the content of a pyproject.toml file to extract project-related metadata. It first cleans the content and then attempts to load it using the tomllib module. If tomllib is not available or a decoding error occurs, it prints a warning. Upon successful parsing, it extracts the project name, description, and dependencies from the '[project]' section and updates the self.info dictionary."
              parameters[1]{name,type,description}:
                inhalt,str,The content of the pyproject.toml file as a string.
              returns[0]:
              usage_context:
                calls: "This method calls _clean_content for content sanitization, tomllib.loads to parse TOML content, and data.get to safely access dictionary keys."
                called_by: This method is called by extrahiere_info if a pyproject.toml file is found.
            error: null
          - identifier: _parse_requirements
            description:
              overall: "This private method processes the content of a requirements.txt file to extract project dependencies. It first cleans the content. It only populates the dependencies field in self.info if it hasn't already been filled by a pyproject.toml file, ensuring that pyproject.toml takes precedence. It filters out empty lines and comments before storing the dependencies as a list."
              parameters[1]{name,type,description}:
                inhalt,str,The content of the requirements.txt file as a string.
              returns[0]:
              usage_context:
                calls: This method calls _clean_content to sanitize the input content.
                called_by: This method is called by extrahiere_info if a requirements.txt file is found.
            error: null
          - identifier: extrahiere_info
            description:
              overall: "This is the main public method of the class, orchestrating the entire information extraction process. It first uses _finde_datei to locate README, pyproject.toml, and requirements.txt files within a provided list of file objects. It then parses these files in a prioritized order (pyproject.toml first, then requirements.txt, then README) to populate the self.info dictionary. Finally, it formats the extracted dependencies and attempts to derive a project title from the repo_url if no title was found in the files, returning the complete information dictionary."
              parameters[2]{name,type,description}:
                dateien,"List[Any]","A list of file-like objects, each expected to have 'path' and 'content' attributes."
                repo_url,str,"The URL of the repository, used as a fallback for the project title."
              returns[1]{name,type,description}:
                "null","Dict[str, Any]",A dictionary containing all extracted project information.
              usage_context:
                calls: "This method calls _finde_datei to locate files, _parse_toml, _parse_requirements, and _parse_readme to process file contents. It also uses os.path.basename and string methods like removesuffix to process the repository URL."
                called_by: "This is the primary public method, expected to be called by external components to initiate project information extraction."
            error: null
        usage_context:
          dependencies: "This class does not explicitly list external functional dependencies in the provided context, but it internally relies on the 're' module for regular expressions, the 'os' module for path manipulation, and the 'tomllib' module for TOML parsing."
          instantiated_by: The class is not explicitly shown to be instantiated by any other components in the provided context.
      error: null
    backend.callgraph.CallGraph:
      identifier: backend.callgraph.CallGraph
      description:
        overall: "The CallGraph class is an AST visitor designed to construct a directed call graph for a given Python source file. It traverses the Abstract Syntax Tree, identifying function and class definitions, import statements, and function calls. The class maintains context about the current function and class to accurately resolve full names of callers and callees, ultimately building a graph of dependencies between functions and methods."
        init_method:
          description: "Initializes the CallGraph instance by setting the filename and various internal state variables required for graph construction. These include tracking the current function and class context, local definitions, import mappings, a NetworkX graph object, a set of discovered functions, and a dictionary to store call edges."
          parameters[1]{name,type,description}:
            filename,str,The path to the Python file being analyzed.
        methods[11]:
          - identifier: _recursive_call
            description:
              overall: "This private helper method recursively extracts the name components from an Abstract Syntax Tree node representing a call, name, or attribute access. It breaks down complex expressions like `obj.method()` or `module.submodule.function` into a list of strings, such as `['module', 'submodule', 'function']`, which are then used for name resolution."
              parameters[1]{name,type,description}:
                node,ast.AST,"The Abstract Syntax Tree node to analyze, typically an `ast.Call`, `ast.Name`, or `ast.Attribute`."
              returns[1]{name,type,description}:
                parts,"list[str]",A list of strings representing the dotted name components of the entity being called.
              usage_context:
                calls: This method calls itself recursively to process nested AST nodes.
                called_by: This method is called by `visit_Call` to extract callee name components.
            error: null
          - identifier: _resolve_all_callee_names
            description:
              overall: "This private method takes a list of potential callee name components and resolves them into fully qualified names. It prioritizes resolution by first checking local definitions, then import mappings, and finally constructs a full path based on the current filename and class context. This ensures that calls to local functions, imported modules, or methods within the current class are correctly identified."
              parameters[1]{name,type,description}:
                callee_nodes,"list[list[str]]","A list where each inner list represents the name components of a potential callee, e.g., `[['module', 'function']]`."
              returns[1]{name,type,description}:
                resolved,"list[str]",A list of fully qualified names for the resolved callees.
              usage_context:
                calls: "This method accesses `self.local_defs`, `self.import_mapping`, `self.current_class`, and `self.filename` for name resolution."
                called_by: This method is called by `visit_Call` to resolve the names of called entities.
            error: null
          - identifier: _make_full_name
            description:
              overall: "This private helper method constructs a fully qualified name for a function or method. It combines the filename, an optional class name, and the base name of the entity to create a unique identifier within the project's scope, following a `filename::ClassName::methodName` or `filename::functionName` format."
              parameters[2]{name,type,description}:
                basename,str,"The base name of the function or method (e.g., 'my_function')."
                class_name,str | None,"The name of the class if the entity is a method, otherwise `None`."
              returns[1]{name,type,description}:
                full_name,str,The fully qualified name of the function or method.
              usage_context:
                calls: This method accesses `self.filename` to construct the full name.
                called_by: This method is called by `visit_FunctionDef` to create unique identifiers for functions and methods.
            error: null
          - identifier: _current_caller
            description:
              overall: "This private helper method determines the identifier of the currently active caller context. It returns the `self.current_function` if set, otherwise it provides a placeholder indicating the global scope within the current filename. This is crucial for correctly attributing calls to their originating context."
              parameters[0]:
              returns[1]{name,type,description}:
                caller_identifier,str,The identifier of the current function or a placeholder for the global scope.
              usage_context:
                calls: This method accesses `self.current_function` and `self.filename` to determine the caller.
                called_by: This method is called by `visit_Call` to identify the source of a function call.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method is an AST visitor for `ast.Import` nodes. It processes `import module as alias` statements, recording the mapping from the alias (or original module name) to the actual module name in `self.import_mapping`. After processing the import, it delegates to `generic_visit` to continue the AST traversal."
              parameters[1]{name,type,description}:
                node,ast.Import,The `ast.Import` node representing an `import` statement.
              returns[0]:
              usage_context:
                calls: This method calls `self.generic_visit` to continue the AST traversal.
                called_by: This method is implicitly called by the `ast.NodeVisitor` mechanism when an `ast.Import` node is encountered.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method is an AST visitor for `ast.ImportFrom` nodes. It handles `from module import name as alias` statements, extracting the module name and mapping the imported name (or its alias) to the module in `self.import_mapping`. This helps in resolving fully qualified names for imported entities."
              parameters[1]{name,type,description}:
                node,ast.ImportFrom,The `ast.ImportFrom` node representing a `from ... import ...` statement.
              returns[0]:
              usage_context:
                calls: This method accesses `self.import_mapping` to store import information.
                called_by: This method is implicitly called by the `ast.NodeVisitor` mechanism when an `ast.ImportFrom` node is encountered.
            error: null
          - identifier: visit_ClassDef
            description:
              overall: "This method is an AST visitor for `ast.ClassDef` nodes. It manages the `self.current_class` context, saving the previous class name before setting the current one to the newly defined class. It then performs a generic visit to process nested elements (like methods) within the class and restores the previous class context upon exiting the class definition."
              parameters[1]{name,type,description}:
                node,ast.ClassDef,The `ast.ClassDef` node representing a class definition.
              returns[0]:
              usage_context:
                calls: This method calls `self.generic_visit` to traverse the class's body.
                called_by: This method is implicitly called by the `ast.NodeVisitor` mechanism when an `ast.ClassDef` node is encountered.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method is an AST visitor for `ast.FunctionDef` nodes. It captures the definition of a function or method, constructs its full qualified name using `_make_full_name`, and updates `self.local_defs` to map its simple name to the full name. It sets the `self.current_function` context, adds the function as a node to the call graph, performs a generic visit to process its body, and finally restores the previous function context."
              parameters[1]{name,type,description}:
                node,ast.FunctionDef,The `ast.FunctionDef` node representing a function definition.
              returns[0]:
              usage_context:
                calls: "This method calls `self._make_full_name` to get the function's full name, `self.graph.add_node` to add it to the graph, and `self.generic_visit` to traverse its body."
                called_by: "This method is implicitly called by the `ast.NodeVisitor` mechanism when an `ast.FunctionDef` node is encountered, and explicitly by `visit_AsyncFunctionDef`."
            error: null
          - identifier: visit_AsyncFunctionDef
            description:
              overall: "This method is an AST visitor for `ast.AsyncFunctionDef` nodes. It handles asynchronous function definitions by simply delegating the processing to the `visit_FunctionDef` method. This ensures that async functions are treated similarly to regular functions for the purpose of call graph construction, capturing their definition and adding them to the graph."
              parameters[1]{name,type,description}:
                node,ast.AsyncFunctionDef,The `ast.AsyncFunctionDef` node representing an asynchronous function definition.
              returns[0]:
              usage_context:
                calls: This method calls `self.visit_FunctionDef` to process the async function.
                called_by: This method is implicitly called by the `ast.NodeVisitor` mechanism when an `ast.AsyncFunctionDef` node is encountered.
            error: null
          - identifier: visit_Call
            description:
              overall: "This method is an AST visitor for `ast.Call` nodes, which represent function or method calls. It identifies the caller using `_current_caller`, extracts the callee's name components using `_recursive_call`, and resolves the callee's full name using `_resolve_all_callee_names`. Finally, it records the call as an edge in the `self.edges` dictionary, linking the caller to the resolved callee."
              parameters[1]{name,type,description}:
                node,ast.Call,The `ast.Call` node representing a function or method call.
              returns[0]:
              usage_context:
                calls: "This method calls `self._current_caller`, `self._recursive_call`, `self._resolve_all_callee_names`, and `self.generic_visit`."
                called_by: This method is implicitly called by the `ast.NodeVisitor` mechanism when an `ast.Call` node is encountered.
            error: null
          - identifier: visit_If
            description:
              overall: "This method is an AST visitor for `ast.If` nodes. It includes special handling for the `if __name__ == \"__main__\"` block, temporarily setting the `self.current_function` to `<main_block>` to correctly attribute calls within this entry point. For all other `if` statements, it simply performs a generic visit to continue traversing the AST."
              parameters[1]{name,type,description}:
                node,ast.If,The `ast.If` node representing an `if` statement.
              returns[0]:
              usage_context:
                calls: This method calls `self.generic_visit` to continue the AST traversal.
                called_by: This method is implicitly called by the `ast.NodeVisitor` mechanism when an `ast.If` node is encountered.
            error: null
        usage_context:
          dependencies: This class depends on the `ast` module for parsing Python code and the `networkx` library for graph manipulation.
          instantiated_by: This class is not explicitly shown to be instantiated by other components in the provided context.
      error: null
    backend.getRepo.RepoFile:
      identifier: backend.getRepo.RepoFile
      description:
        overall: "The RepoFile class represents a single file within a Git repository, designed for efficient handling of file data through lazy loading. It provides properties to access the Git blob object, the file's content, and its size only when they are first requested. Additionally, it includes methods for basic analysis, such as word counting, and for converting the file's metadata into a dictionary format."
        init_method:
          description: "This method initializes a RepoFile object by setting its file path and the Git Tree object from which the file originates. It also initializes internal attributes for the Git blob, file content, and file size to `None`, indicating that these values will be loaded lazily upon first access."
          parameters[2]{name,type,description}:
            file_path,str,The path to the file within the repository.
            commit_tree,git.Tree,The Tree object of the commit from which the file originates.
        methods[6]:
          - identifier: blob
            description:
              overall: "This property provides lazy loading for the Git blob object associated with the file. It checks if the `_blob` attribute is already populated; if not, it attempts to retrieve the blob from the `_tree` using the file's path. If the file is not found in the commit tree, a `FileNotFoundError` is raised."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",git.Blob,The Git blob object for the file.
              usage_context:
                calls: This method does not explicitly call other functions or methods.
                called_by: This method is not explicitly called by other functions or methods within the provided context.
            error: null
          - identifier: content
            description:
              overall: "This property provides lazy loading for the decoded content of the file. It first checks if the `_content` attribute is already loaded. If not, it accesses the `blob` property to get the Git blob, reads its data stream, and decodes it using UTF-8, ignoring any decoding errors."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",str,The decoded content of the file.
              usage_context:
                calls: This method does not explicitly call other functions or methods.
                called_by: This method is not explicitly called by other functions or methods within the provided context.
            error: null
          - identifier: size
            description:
              overall: "This property provides lazy loading for the size of the file in bytes. It checks if the `_size` attribute is already loaded. If not, it accesses the `blob` property to get the Git blob and retrieves its `size` attribute, storing it for future access."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",int,The size of the file in bytes.
              usage_context:
                calls: This method does not explicitly call other functions or methods.
                called_by: This method is not explicitly called by other functions or methods within the provided context.
            error: null
          - identifier: analyze_word_count
            description:
              overall: "This method serves as an example analysis function, demonstrating how to process the file's content. It retrieves the file's content using the `content` property, splits the string into individual words, and then returns the total count of these words."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",int,The total number of words in the file content.
              usage_context:
                calls: This method does not explicitly call other functions or methods.
                called_by: This method is not explicitly called by other functions or methods within the provided context.
            error: null
          - identifier: __repr__
            description:
              overall: "This special method provides a developer-friendly string representation of the RepoFile object. It returns a string that includes the class name and the file's path, which is useful for debugging and logging purposes."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",str,"A string representation of the RepoFile object, including its path."
              usage_context:
                calls: This method does not explicitly call other functions or methods.
                called_by: This method is not explicitly called by other functions or methods within the provided context.
            error: null
          - identifier: to_dict
            description:
              overall: "This method converts the RepoFile object into a dictionary representation, providing structured access to its metadata. It includes the file's path, its base name, its size, and its type. Optionally, if `include_content` is set to `True`, the file's content is also added to the dictionary."
              parameters[1]{name,type,description}:
                include_content,bool,A boolean flag indicating whether the file's content should be included in the dictionary.
              returns[1]{name,type,description}:
                data,dict,A dictionary containing the file's metadata and optionally its content.
              usage_context:
                calls: This method does not explicitly call other functions or methods.
                called_by: This method is not explicitly called by other functions or methods within the provided context.
            error: null
        usage_context:
          dependencies: This class does not explicitly declare any external functional dependencies within the provided context.
          instantiated_by: This class is not explicitly instantiated by other components within the provided context.
      error: null
    backend.getRepo.GitRepository:
      identifier: backend.getRepo.GitRepository
      description:
        overall: "The GitRepository class provides a comprehensive interface for managing Git repositories. It handles the cloning of a remote repository into a temporary local directory, manages the lifecycle of this directory, and offers methods to access and structure the repository's files. The class also implements the context manager protocol, ensuring proper cleanup of temporary resources upon exiting a 'with' statement."
        init_method:
          description: "The `__init__` method initializes a GitRepository instance by cloning the specified Git repository URL into a newly created temporary directory. It sets up various instance attributes including the repository URL, the path to the temporary directory, the GitPython Repo object, an empty list for `RepoFile` objects, and the latest commit and its tree. Error handling is included to catch and raise `RuntimeError` if the cloning process fails."
          parameters[1]{name,type,description}:
            repo_url,str,The URL of the Git repository to be cloned.
        methods[5]:
          - identifier: get_all_files
            description:
              overall: "This method is responsible for retrieving all file paths present in the cloned Git repository. It utilizes the GitPython library's `ls-files` command to get a list of all tracked files. Each file path is then used to instantiate a `RepoFile` object, which is assumed to be an external class capable of representing a file within the repository context. The method populates the `self.files` attribute with these `RepoFile` instances and returns the complete list."
              parameters[0]:
              returns[1]{name,type,description}:
                files,"list[RepoFile]","A list of RepoFile instances, each representing a file within the repository."
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions based on the provided context."
                called_by: This method is not explicitly called by other functions or methods based on the provided context.
            error: null
          - identifier: close
            description:
              overall: "The `close` method is designed for resource cleanup, specifically for deleting the temporary directory where the Git repository was cloned. It first checks if `self.temp_dir` is set to ensure a directory exists to be removed. After printing a message indicating the deletion, it sets `self.temp_dir` to `None`, effectively marking the resource as cleaned up."
              parameters[0]:
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions based on the provided context."
                called_by: This method is not explicitly called by other functions or methods based on the provided context.
            error: null
          - identifier: __enter__
            description:
              overall: "This method implements the `__enter__` part of Python's context manager protocol. When a `GitRepository` instance is used in a `with` statement, this method is implicitly called upon entry to the block. Its primary function is to return the instance itself, allowing it to be bound to a variable in the `with` statement for use within the block."
              parameters[0]:
              returns[1]{name,type,description}:
                self,GitRepository,The instance of the GitRepository class itself.
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions based on the provided context."
                called_by: This method is not explicitly called by other functions or methods based on the provided context.
            error: null
          - identifier: __exit__
            description:
              overall: "This method implements the `__exit__` part of Python's context manager protocol. It is implicitly called when execution leaves the `with` statement block, regardless of whether an exception occurred. Its main purpose is to ensure that the `close()` method is invoked to clean up the temporary repository directory, thereby guaranteeing proper resource management."
              parameters[3]{name,type,description}:
                exc_type,type,"The type of exception that occurred, or None if no exception."
                exc_val,Exception,"The exception instance that occurred, or None."
                exc_tb,traceback,"The traceback object associated with the exception, or None."
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions based on the provided context."
                called_by: This method is not explicitly called by other functions or methods based on the provided context.
            error: null
          - identifier: get_file_tree
            description:
              overall: "This method generates a hierarchical dictionary representation of the repository's file structure, mimicking a file system tree. If the `self.files` attribute is not already populated with `RepoFile` objects, it first calls `get_all_files()` to retrieve them. It then iterates through these `RepoFile` objects, splitting their paths to construct a nested dictionary where each node represents either a directory or a file. Files are added to the appropriate directory level, optionally including their content."
              parameters[1]{name,type,description}:
                include_content,bool,A boolean flag indicating whether the content of the files should be included in the dictionary representation. Defaults to False.
              returns[1]{name,type,description}:
                tree,dict,"A dictionary representing the hierarchical file tree of the repository, with 'name', 'type', and 'children' keys for directories, and file details for files."
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions based on the provided context."
                called_by: This method is not explicitly called by other functions or methods based on the provided context.
            error: null
        usage_context:
          dependencies: The class does not explicitly list any external functional dependencies in the provided context.
          instantiated_by: The class is not explicitly instantiated by other known components in the provided context.
      error: null
    backend.relationship_analyzer.ProjectAnalyzer:
      identifier: backend.relationship_analyzer.ProjectAnalyzer
      description:
        overall: "The ProjectAnalyzer class is designed to analyze a Python project's source code to build a comprehensive call graph. It identifies all Python files, collects definitions of functions, methods, and classes, and then resolves calls between these entities. The class provides methods to initiate the analysis and retrieve the raw relationships in a structured format, enabling a deep understanding of code dependencies within the project."
        init_method:
          description: "The constructor initializes the ProjectAnalyzer instance by setting the project's root directory and preparing several internal data structures. It sets up dictionaries for storing definitions, the call graph, and file ASTs, along with a set of directories to be ignored during file system traversal."
          parameters[1]{name,type,description}:
            project_root,string,The absolute path to the root directory of the project to be analyzed.
        methods[6]:
          - identifier: analyze
            description:
              overall: "This method orchestrates the entire project analysis process. It first identifies all Python files within the project, then iterates through them to collect function, method, and class definitions. Subsequently, it resolves calls within these files to build a comprehensive call graph. Finally, it clears temporary AST storage and returns the generated call graph."
              parameters[0]:
              returns[1]{name,type,description}:
                call_graph,defaultdict(list),"A dictionary representing the call graph, where keys are callee identifiers and values are lists of caller information."
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions based on the provided context."
                called_by: This method is not explicitly called by any other functions or methods based on the provided context.
            error: null
          - identifier: get_raw_relationships
            description:
              overall: "This method processes the internal call_graph to generate a structured representation of incoming and outgoing relationships between code entities. It iterates through the call graph, populating two dictionaries: 'outgoing' (what each entity calls) and 'incoming' (what calls each entity). The results are then sorted and returned in a dictionary format."
              parameters[0]:
              returns[1]{name,type,description}:
                relationships_data,dict,"A dictionary containing two keys, 'outgoing' and 'incoming', each mapping entity identifiers to sorted lists of related entity identifiers."
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions based on the provided context."
                called_by: This method is not explicitly called by any other functions or methods based on the provided context.
            error: null
          - identifier: _find_py_files
            description:
              overall: "This private helper method is responsible for recursively traversing the project directory to locate all Python files. It uses os.walk to navigate the file system, explicitly skipping directories specified in self.ignore_dirs. It compiles a list of absolute paths to all discovered Python files."
              parameters[0]:
              returns[1]{name,type,description}:
                py_files,"list[str]","A list of absolute file paths to all Python files found within the project root, excluding ignored directories."
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions based on the provided context."
                called_by: This method is not explicitly called by any other functions or methods based on the provided context.
            error: null
          - identifier: _collect_definitions
            description:
              overall: "This private method parses a given Python file to identify and store definitions of functions, methods, and classes. It reads the file, parses it into an Abstract Syntax Tree (AST), and then walks the AST to find FunctionDef and ClassDef nodes. For each definition, it constructs a unique path name and stores its file path, line number, and type in self.definitions. It also stores the AST in self.file_asts. Error handling is included for file processing."
              parameters[1]{name,type,description}:
                filepath,str,The absolute path to the Python file being analyzed.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions based on the provided context."
                called_by: This method is not explicitly called by any other functions or methods based on the provided context.
            error: null
          - identifier: _get_parent
            description:
              overall: "This private helper method traverses an Abstract Syntax Tree (AST) to find the parent node of a given child node. It iterates through all nodes in the tree and checks their children to identify if any child matches the provided node. If a match is found, the current parent node is returned. If no parent is found, it returns None."
              parameters[2]{name,type,description}:
                tree,ast.AST,The root of the Abstract Syntax Tree to search within.
                node,ast.AST,The child node for which to find the parent.
              returns[1]{name,type,description}:
                parent_node,ast.AST or None,"The parent AST node of the given node, or None if no parent is found."
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions based on the provided context."
                called_by: This method is not explicitly called by any other functions or methods based on the provided context.
            error: null
          - identifier: _resolve_calls
            description:
              overall: "This private method is responsible for resolving function and method calls within a specified Python file. It retrieves the AST for the given filepath from self.file_asts. It then instantiates a CallResolverVisitor with the file context and definitions, and uses it to visit the AST. The resolved calls from the visitor are then extended into the self.call_graph. Error handling is included for the call resolution process."
              parameters[1]{name,type,description}:
                filepath,str,The absolute path to the Python file whose calls are to be resolved.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call any other methods, classes, or functions based on the provided context."
                called_by: This method is not explicitly called by any other functions or methods based on the provided context.
            error: null
        usage_context:
          dependencies: This class has no explicit external functional dependencies listed in the provided context.
          instantiated_by: This class is not explicitly instantiated by any other components based on the provided context.
      error: null
    backend.relationship_analyzer.CallResolverVisitor:
      identifier: backend.relationship_analyzer.CallResolverVisitor
      description:
        overall: "The CallResolverVisitor class extends ast.NodeVisitor to perform a detailed analysis of Python Abstract Syntax Trees (ASTs). Its primary function is to identify and resolve all function and method calls within a given source file, mapping them to their fully qualified names. By tracking module paths, class definitions, function scopes, and object instantiations, it builds a comprehensive record of call relationships across the codebase."
        init_method:
          description: "The constructor initializes the visitor with the current file path, the project root, and a dictionary of known definitions. It sets up internal state variables like `module_path`, `scope` for imports, `instance_types` for tracking assigned object types, and `calls` (a defaultdict) to store detected call relationships."
          parameters[3]{name,type,description}:
            filepath,str,The path to the source file being analyzed.
            project_root,str,"The root directory of the project, used for module path resolution."
            definitions,dict,A dictionary containing known qualified names of definitions in the project.
        methods[7]:
          - identifier: visit_ClassDef
            description:
              overall: "This method is called when an ast.ClassDef node is encountered during AST traversal. It temporarily updates the `current_class_name` attribute to reflect the class being visited, allowing nested methods to correctly form their fully qualified names. After visiting the class's children, it restores the previous class name."
              parameters[1]{name,type,description}:
                node,ast.ClassDef,The AST node representing a class definition.
              returns[0]:
              usage_context:
                calls: No specific calls made by this method were identified in the provided context.
                called_by: No specific callers of this method were identified in the provided context.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method is invoked when an ast.FunctionDef node is visited. It constructs the full qualified identifier for the function or method, considering whether it's nested within a class. It then sets this as the `current_caller_name` for subsequent call resolution within the function's body and restores the previous caller name after processing."
              parameters[1]{name,type,description}:
                node,ast.FunctionDef,The AST node representing a function or method definition.
              returns[0]:
              usage_context:
                calls: No specific calls made by this method were identified in the provided context.
                called_by: No specific callers of this method were identified in the provided context.
            error: null
          - identifier: visit_Call
            description:
              overall: "When an ast.Call node is encountered, this method attempts to resolve the fully qualified name of the called function or method using `_resolve_call_qname`. If a definition is found and recognized, it records the call, including the caller's file, line number, full identifier, and type (module, local function, method, or function), storing this information in the `self.calls` defaultdict."
              parameters[1]{name,type,description}:
                node,ast.Call,The AST node representing a function or method call.
              returns[0]:
              usage_context:
                calls: No specific calls made by this method were identified in the provided context.
                called_by: No specific callers of this method were identified in the provided context.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method handles ast.Import nodes, which represent `import module_name [as alias]` statements. It iterates through the imported names and their aliases, storing the mapping from the local name (alias or original name) to the module's original name in the `self.scope` dictionary."
              parameters[1]{name,type,description}:
                node,ast.Import,The AST node representing an import statement.
              returns[0]:
              usage_context:
                calls: No specific calls made by this method were identified in the provided context.
                called_by: No specific callers of this method were identified in the provided context.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method processes ast.ImportFrom nodes, which correspond to `from module import name [as alias]` statements. It correctly resolves the full module path, accounting for relative imports (`node.level`), and then maps the imported names (or their aliases) to their fully qualified paths in the `self.scope` dictionary."
              parameters[1]{name,type,description}:
                node,ast.ImportFrom,The AST node representing a 'from ... import ...' statement.
              returns[0]:
              usage_context:
                calls: No specific calls made by this method were identified in the provided context.
                called_by: No specific callers of this method were identified in the provided context.
            error: null
          - identifier: visit_Assign
            description:
              overall: "This method is triggered by ast.Assign nodes, representing assignment statements. It specifically looks for assignments where the right-hand side is a call to a constructor (e.g., `x = MyClass()`). If such an instantiation is found and the class name is resolvable, it records the qualified class name associated with the assigned variable in `self.instance_types`."
              parameters[1]{name,type,description}:
                node,ast.Assign,The AST node representing an assignment statement.
              returns[0]:
              usage_context:
                calls: No specific calls made by this method were identified in the provided context.
                called_by: No specific callers of this method were identified in the provided context.
            error: null
          - identifier: _resolve_call_qname
            description:
              overall: "This private helper method attempts to determine the fully qualified name (QName) of a function or method being called. It handles direct name calls (e.g., `func()`), resolving them against the current scope or local module definitions. It also handles attribute calls (e.g., `obj.method()`), using `self.instance_types` to find the class of the object or `self.scope` for module-level attributes."
              parameters[1]{name,type,description}:
                func_node,ast.expr,"The AST node representing the function or method being called (e.g., ast.Name or ast.Attribute)."
              returns[1]{name,type,description}:
                qualified_name,str | None,"The fully qualified name of the callee as a string if resolved, otherwise None."
              usage_context:
                calls: No specific calls made by this method were identified in the provided context.
                called_by: No specific callers of this method were identified in the provided context.
            error: null
        usage_context:
          dependencies: This class does not explicitly list any external dependencies in the provided context.
          instantiated_by: This class is not explicitly shown to be instantiated by any other components in the provided context.
      error: null
    schemas.types.ParameterDescription:
      identifier: schemas.types.ParameterDescription
      description:
        overall: "This class serves as a Pydantic data model designed to precisely describe a single parameter of a function. It encapsulates the essential attributes of a function parameter, including its name, data type, and a descriptive explanation. By inheriting from BaseModel, it provides robust data validation and serialization capabilities for structured parameter information."
        init_method:
          description: "This class, inheriting from Pydantic's BaseModel, implicitly generates a constructor. It initializes instances with values for 'name', 'type', and 'description', enforcing type validation based on the defined annotations."
          parameters[3]{name,type,description}:
            name,str,The name of the parameter.
            type,str,The data type of the parameter.
            description,str,A brief explanation of the parameter's purpose.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly depend on other components within the provided context.
          instantiated_by: This class is not explicitly instantiated by other components within the provided context.
      error: null
    schemas.types.ReturnDescription:
      identifier: schemas.types.ReturnDescription
      description:
        overall: "The ReturnDescription class is a Pydantic model designed to structure information about the return value of a function. It serves as a data container, ensuring that return value descriptions consistently include a name, its type, and a textual explanation. This class facilitates standardized documentation or programmatic representation of function outputs within a larger system."
        init_method:
          description: "The `__init__` method for `ReturnDescription` is implicitly generated by Pydantic's BaseModel. It initializes an instance of `ReturnDescription` by accepting `name`, `type`, and `description` as keyword arguments, validating them against their respective string types."
          parameters[3]{name,type,description}:
            name,str,The name or identifier of the return value.
            type,str,The Python type hint or description of the return value's type.
            description,str,A detailed explanation of what the return value represents.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in its provided context.
          instantiated_by: The specific locations where this class is instantiated are not provided in the current context.
      error: null
    schemas.types.UsageContext:
      identifier: schemas.types.UsageContext
      description:
        overall: "The UsageContext class is a Pydantic BaseModel designed to standardize the description of how a function or method interacts with other parts of a system. It serves as a data structure to capture both the outgoing calls made by an entity and the incoming calls it receives, providing a clear overview of its operational context."
        init_method:
          description: "This class, being a Pydantic BaseModel, implicitly defines its constructor to accept `calls` and `called_by` as keyword arguments. It initializes these attributes directly based on the provided values, ensuring they conform to the `str` type and enabling data validation upon instantiation."
          parameters[2]{name,type,description}:
            calls,str,"A string summarizing the functions, methods, or classes that this entity calls."
            called_by,str,A string summarizing the functions or methods that call this entity.
        methods[0]:
        usage_context:
          dependencies: This class primarily depends on `pydantic.BaseModel` for its structural definition and validation capabilities. No other explicit functional dependencies are listed in the provided context.
          instantiated_by: The specific points where this `UsageContext` class is instantiated are not provided in the current context.
      error: null
    schemas.types.FunctionDescription:
      identifier: schemas.types.FunctionDescription
      description:
        overall: "The `FunctionDescription` class is a Pydantic BaseModel designed to provide a structured and detailed analysis of a Python function. It serves as a data schema to encapsulate various aspects of a function, including its high-level purpose, the parameters it accepts, the values it returns, and its operational context within a larger system. This class is crucial for standardizing the representation of function metadata, enabling systematic documentation and analysis."
        init_method:
          description: "As a Pydantic BaseModel, the `__init__` method for `FunctionDescription` is implicitly generated by Pydantic. It handles the instantiation of a `FunctionDescription` object by validating and assigning values to its defined fields: `overall`, `parameters`, `returns`, and `usage_context`. This ensures that all instances conform to the specified data types and structure."
          parameters[4]{name,type,description}:
            overall,str,A comprehensive summary describing the function's purpose and its high-level implementation details.
            parameters,"List[ParameterDescription]","A list containing detailed descriptions for each parameter accepted by the function, including their names, types, and individual purposes."
            returns,"List[ReturnDescription]","A list detailing the values returned by the function, specifying their types and descriptions."
            usage_context,UsageContext,"An object providing context on how the function interacts with other components, including what it calls and where it is called from."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list external functional dependencies in the provided context.
          instantiated_by: The provided context does not specify where this class is instantiated.
      error: null
    schemas.types.FunctionAnalysis:
      identifier: schemas.types.FunctionAnalysis
      description:
        overall: "The FunctionAnalysis class is a Pydantic BaseModel designed to encapsulate a comprehensive analysis of a Python function. It serves as a structured data container for machine-readable function analysis results, including the function's unique identifier, a detailed description of its purpose, parameters, return values, and usage context, along with an optional field for error reporting. This model ensures consistency in how function analysis data is represented."
        init_method:
          description: "The FunctionAnalysis class, being a Pydantic BaseModel, implicitly initializes its instances through Pydantic's generated constructor. This constructor sets up the `identifier`, `description`, and an optional `error` field based on the provided arguments, ensuring data validation upon instantiation."
          parameters[3]{name,type,description}:
            identifier,str,The unique name or identifier of the function being analyzed.
            description,FunctionDescription,"An object containing a detailed analysis of the function's purpose, parameters, return values, and usage context."
            error,"Optional[str]",An optional field to store an error message if the function analysis encountered issues or failed.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list external functional dependencies in the provided context.
          instantiated_by: The specific points of instantiation for this class are not provided in the current context.
      error: null
    schemas.types.ConstructorDescription:
      identifier: schemas.types.ConstructorDescription
      description:
        overall: "The ConstructorDescription class is a Pydantic BaseModel designed to encapsulate structured information about the `__init__` method of another class. It serves as a data schema, providing fields to store a high-level textual description of the constructor's purpose and a detailed list of its individual parameters. This model is crucial for standardizing the representation of constructor metadata within a larger system, enabling consistent processing and documentation of class initialization logic."
        init_method:
          description: "This class, being a Pydantic BaseModel, automatically generates an `__init__` method. This constructor is responsible for initializing instances of ConstructorDescription with a string description of a constructor and a list of ParameterDescription objects, ensuring type validation and data integrity."
          parameters[2]{name,type,description}:
            description,str,A textual summary or explanation of the constructor being described.
            parameters,"List[ParameterDescription]","A list of ParameterDescription objects, each detailing an individual parameter of the constructor."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list external functional dependencies in the provided context.
          instantiated_by: The specific locations where this class is instantiated are not provided in the current context.
      error: null
    schemas.types.ClassContext:
      identifier: schemas.types.ClassContext
      description:
        overall: "The ClassContext class is a Pydantic BaseModel designed to encapsulate contextual information about another class. It defines two string attributes, `dependencies` and `instantiated_by`, which are intended to describe the external components a class relies on and the locations where it is instantiated, respectively. This model provides a structured and type-hinted way to store metadata crucial for understanding a class's role and integration within a larger system."
        init_method:
          description: "As a Pydantic BaseModel, the `__init__` method for ClassContext is automatically generated. It initializes an instance of ClassContext by accepting keyword arguments corresponding to its defined fields: `dependencies` and `instantiated_by`. Pydantic handles validation and assignment of these values upon instantiation."
          parameters[2]{name,type,description}:
            dependencies,str,A string summarizing the external dependencies of the class being described.
            instantiated_by,str,A string summarizing the primary locations or modules where the class being described is instantiated.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external dependencies in the provided context.
          instantiated_by: The provided context does not specify any locations where this class is instantiated.
      error: null
    schemas.types.ClassDescription:
      identifier: schemas.types.ClassDescription
      description:
        overall: "The ClassDescription class is a Pydantic BaseModel designed to structure and store a comprehensive analysis of a Python class. It encapsulates various aspects such as an overall description of the class, details about its constructor, a list of analyses for its methods, and information regarding its usage context. This model serves as a standardized data structure for representing the analytical output of a class."
        init_method:
          description: "This class does not define an explicit `__init__` method. Initialization is handled implicitly by Pydantic's `BaseModel`, which typically involves assigning values to its defined fields upon instantiation."
          parameters[0]:
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies within the provided context.
          instantiated_by: The instantiation points for this class are not specified within the provided context.
      error: null
    schemas.types.ClassAnalysis:
      identifier: schemas.types.ClassAnalysis
      description:
        overall: "The ClassAnalysis class serves as the root data model for representing a complete analysis of a Python class. It is a Pydantic BaseModel designed to structure information such as the class's unique identifier, a detailed ClassDescription object containing its constructor, methods, and usage context, and an optional field for capturing any errors encountered during the analysis process. This model ensures a standardized format for machine-readable class analysis."
        init_method:
          description: "This class, being a Pydantic BaseModel, implicitly generates an __init__ method. It initializes instances with 'identifier', 'description', and an optional 'error' field, enforcing type validation based on the defined schema."
          parameters[3]{name,type,description}:
            identifier,str,A unique string identifying the class being analyzed.
            description,ClassDescription,"An object containing the detailed analysis of the class, including its constructor, methods, and usage context."
            error,"Optional[str]","An optional string to store any error messages encountered during the analysis of the class, defaulting to None."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly declare external functional dependencies within its provided source code.
          instantiated_by: The specific locations where this class is instantiated are not provided in the current context.
      error: null
    schemas.types.CallInfo:
      identifier: schemas.types.CallInfo
      description:
        overall: "The CallInfo class is a Pydantic BaseModel designed to encapsulate details about a specific call event identified by a relationship analyzer. It acts as a structured data record, storing critical information such as the source file, the name of the calling function, the mode of the caller (e.g., method, function), and the exact line number where the call occurs. This class provides a standardized format for representing call origins within a system."
        init_method:
          description: "This class, inheriting from Pydantic's BaseModel, does not define an explicit __init__ method. Pydantic automatically generates a constructor that initializes its fields (file, function, mode, line) based on the provided arguments, ensuring data validation and type enforcement."
          parameters[4]{name,type,description}:
            file,str,The path to the source file where the call event occurred.
            function,str,The name of the function or method that made the call.
            mode,str,"The classification of the calling entity, such as 'method', 'function', or 'module'."
            line,int,The specific line number within the file where the call event is located.
        methods[0]:
        usage_context:
          dependencies: This class primarily depends on pydantic.BaseModel for its data validation and serialization capabilities.
          instantiated_by: The specific points of instantiation for this class are not provided in the current context. It is designed to be instantiated by components that analyze code relationships and identify call events.
      error: null
    schemas.types.FunctionContextInput:
      identifier: schemas.types.FunctionContextInput
      description:
        overall: "The `FunctionContextInput` class is a Pydantic BaseModel designed to provide a structured context for analyzing a function. It encapsulates two primary pieces of information: a list of identifiers for other functions or methods that the analyzed function calls, and a list of `CallInfo` objects detailing where the analyzed function itself is called. This model ensures a standardized format for representing a function's operational context, facilitating consistent data handling and analysis within a larger system."
        init_method:
          description: "This class does not define an explicit `__init__` method. As a Pydantic `BaseModel`, its initialization is handled automatically by Pydantic, which parses input data to populate the `calls` and `called_by` attributes based on their type hints."
          parameters[0]:
        methods[0]:
        usage_context:
          dependencies: "This class does not explicitly list any external functional dependencies in the provided context, relying on Pydantic's BaseModel for its core functionality."
          instantiated_by: The specific locations where this class is instantiated are not provided in the current context.
      error: null
    schemas.types.FunctionAnalysisInput:
      identifier: schemas.types.FunctionAnalysisInput
      description:
        overall: "The `FunctionAnalysisInput` class is a Pydantic BaseModel designed to serve as a structured input for a function analysis process. It encapsulates all necessary data, including the analysis mode, the function's identifier, its source code, relevant import statements, and additional contextual information. This class acts as a data contract, ensuring that all required components for a comprehensive function analysis are provided in a standardized format."
        init_method:
          description: "This class inherits from `BaseModel` and does not explicitly define an `__init__` method. Pydantic automatically generates a constructor to validate and initialize its fields: `mode`, `identifier`, `source_code`, `imports`, and `context` based on the provided type hints."
          parameters[0]:
        methods[0]:
        usage_context:
          dependencies: This class has no explicitly listed external functional dependencies.
          instantiated_by: This class has no explicitly listed instantiation points.
      error: null
    schemas.types.MethodContextInput:
      identifier: schemas.types.MethodContextInput
      description:
        overall: "The `MethodContextInput` class is a Pydantic BaseModel designed to provide a structured schema for capturing contextual information about a method. It defines fields such as the method's identifier, a list of other functions or methods it invokes, a list of entities that call this method, its arguments, and an optional docstring. This class serves as a data structure to represent the operational context and inter-method relationships within a larger system."
        init_method:
          description: "As a Pydantic BaseModel, the `__init__` method for `MethodContextInput` is automatically generated by Pydantic. It initializes an instance by accepting keyword arguments that correspond to its defined fields: `identifier`, `calls`, `called_by`, `args`, and `docstring`. This allows for straightforward creation and validation of method context data."
          parameters[5]{name,type,description}:
            identifier,str,A unique string identifier for the method.
            calls,"List[str]","A list of identifiers for other methods, classes, or functions that this method calls."
            called_by,"List[CallInfo]",A list of `CallInfo` objects detailing where this method is called from.
            args,"List[str]",A list of string representations of the arguments accepted by the method.
            docstring,"Optional[str]",An optional string containing the method's docstring.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies.
          instantiated_by: The specific points of instantiation for this class are not provided in the context.
      error: null
    schemas.types.ClassContextInput:
      identifier: schemas.types.ClassContextInput
      description:
        overall: "The `ClassContextInput` class is a Pydantic model designed to structure contextual information relevant for analyzing a Python class. It encapsulates details about the class's external dependencies, where it is instantiated within the codebase, and specific contextual information for its methods. This model serves as a standardized input format for tools or systems that require a comprehensive understanding of a class's operational environment and internal structure."
        init_method:
          description: "The `__init__` method for `ClassContextInput` is implicitly generated by Pydantic's `BaseModel`. It handles the instantiation of a `ClassContextInput` object by accepting keyword arguments corresponding to its defined fields: `dependencies`, `instantiated_by`, and `method_context`. Pydantic automatically performs data validation and type coercion based on the provided type hints during object creation."
          parameters[3]{name,type,description}:
            dependencies,"List[str]","A list of strings, where each string represents an external dependency of the class being analyzed. These are typically other modules, functions, or classes that the analyzed class relies upon."
            instantiated_by,"List[CallInfo]","A list of `CallInfo` objects, indicating the locations or contexts where the class being analyzed is instantiated. This helps in understanding the class's usage patterns across the codebase."
            method_context,"List[MethodContextInput]","A list of `MethodContextInput` objects, each providing specific contextual details for individual methods within the class being analyzed. This includes information about what each method calls and where it is called from."
        methods[0]:
        usage_context:
          dependencies: "This class does not explicitly list any direct functional dependencies within the provided context, but it relies on Pydantic's BaseModel for its core functionality."
          instantiated_by: The `ClassContextInput` class is not explicitly shown to be instantiated by any other components in the provided context.
      error: null
    schemas.types.ClassAnalysisInput:
      identifier: schemas.types.ClassAnalysisInput
      description:
        overall: "The ClassAnalysisInput class is a Pydantic BaseModel that defines the structured input required for generating a ClassAnalysis object. It acts as a data transfer object (DTO), encapsulating all necessary information, such as the class identifier, its source code, relevant imports, and additional context, to facilitate a comprehensive analysis of a Python class. This model ensures that all data passed for class analysis conforms to a predefined schema."
        init_method:
          description: "The ClassAnalysisInput class does not explicitly define an `__init__` method. As a Pydantic BaseModel, its constructor is implicitly generated by Pydantic, allowing instantiation by providing keyword arguments corresponding to its defined fields: `mode`, `identifier`, `source_code`, `imports`, and `context`."
          parameters[5]{name,type,description}:
            mode,"Literal[\"class_analysis\"]","A literal string indicating the analysis mode, which must be 'class_analysis' to specify a class analysis request."
            identifier,str,The unique name or identifier of the class that is to be analyzed.
            source_code,str,The complete raw source code of the entire class definition to be analyzed.
            imports,"List[str]","A list of import statements from the source file, which may include imports relevant to the class or its methods."
            context,ClassContextInput,"An object containing additional contextual information for the class analysis, such as dependencies and instantiation points."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies within the provided context.
          instantiated_by: The specific points of instantiation for this class are not provided in the current context.
      error: null