basic_info:
  projekt_uebersicht:
    titel: Repo Onboarding Agent üöÄ
    beschreibung: ```
    aktueller_status: Information not found
    key_features: Information not found
    tech_stack: Information not found
  installation:
    dependencies: "- altair==4.2.2\n- annotated-types==0.7.0\n- anyio==4.11.0\n- attrs==25.4.0\n- bcrypt==5.0.0\n- blinker==1.9.0\n- cachetools==6.2.2\n- captcha==0.7.1\n- certifi==2025.11.12\n- cffi==2.0.0\n- charset-normalizer==3.4.4\n- click==8.3.1\n- colorama==0.4.6\n- contourpy==1.3.3\n- cryptography==46.0.3\n- cycler==0.12.1\n- distro==1.9.0\n- dnspython==2.8.0\n- dotenv==0.9.9\n- entrypoints==0.4\n- extra-streamlit-components==0.1.81\n- filetype==1.2.0\n- fonttools==4.61.0\n- gitdb==4.0.12\n- GitPython==3.1.45\n- google-ai-generativelanguage==0.9.0\n- google-api-core==2.28.1\n- google-auth==2.43.0\n- googleapis-common-protos==1.72.0\n- grpcio==1.76.0\n- grpcio-status==1.76.0\n- h11==0.16.0\n- httpcore==1.0.9\n- httpx==0.28.1\n- idna==3.11\n- Jinja2==3.1.6\n- jiter==0.12.0\n- jsonpatch==1.33\n- jsonpointer==3.0.0\n- jsonschema==4.25.1\n- jsonschema-specifications==2025.9.1\n- kiwisolver==1.4.9\n- langchain==1.0.8\n- langchain-core==1.1.0\n- langchain-google-genai==3.1.0\n- langchain-ollama==1.0.0\n- langchain-openai==1.1.0\n- langgraph==1.0.3\n- langgraph-checkpoint==3.0.1\n- langgraph-prebuilt==1.0.5\n- langgraph-sdk==0.2.9\n- langsmith==0.4.46\n- MarkupSafe==3.0.3\n- matplotlib==3.10.7\n- narwhals==2.12.0\n- networkx==3.6\n- numpy==2.3.5\n- ollama==0.6.1\n- openai==2.8.1\n- orjson==3.11.4\n- ormsgpack==1.12.0\n- packaging==25.0\n- pandas==2.3.3\n- pillow==12.0.0\n- proto-plus==1.26.1\n- protobuf==6.33.1\n- pyarrow==21.0.0\n- pyasn1==0.6.1\n- pyasn1_modules==0.4.2\n- pycparser==2.23\n- pydantic==2.12.4\n- pydantic_core==2.41.5\n- pydeck==0.9.1\n- PyJWT==2.10.1\n- pymongo==4.15.4\n- pyparsing==3.2.5\n- python-dateutil==2.9.0.post0\n- python-dotenv==1.2.1\n- pytz==2025.2\n- PyYAML==6.0.3\n- referencing==0.37.0\n- regex==2025.11.3\n- requests==2.32.5\n- requests-toolbelt==1.0.0\n- rpds-py==0.29.0\n- rsa==4.9.1\n- setuptools==75.9.1\n- six==1.17.0\n- smmap==5.0.2\n- sniffio==1.3.1\n- streamlit==1.51.0\n- streamlit-authenticator==0.4.2\n- streamlit-mermaid==0.3.0\n- tenacity==9.1.2\n- tiktoken==0.12.0\n- toml==0.10.2\n- toolz==1.1.0\n- toon_format @ git+https://github.com/toon-format/toon-python.git@9c4f0c0c24f2a0b0b376315f4b8707f8c9006de6\n- tornado==6.5.2\n- tqdm==4.67.1\n- typing-inspection==0.4.2\n- typing_extensions==4.15.0\n- tzdata==2025.2\n- urllib3==2.5.0\n- watchdog==6.0.0\n- xxhash==3.6.0\n- zstandard==0.25.0\n- nbformat"
    setup_anleitung: Information not found
    quick_start_guide: Information not found
file_tree:
  name: root
  type: directory
  children[16]:
    - path: .env.example
      name: .env.example
      size: 48
      type: file
    - path: .gitignore
      name: .gitignore
      size: 184
      type: file
    - name: SystemPrompts
      type: directory
      children[6]{path,name,size,type}:
        SystemPrompts/SystemPromptClassHelperLLM.txt,SystemPromptClassHelperLLM.txt,6645,file
        SystemPrompts/SystemPromptFunctionHelperLLM.txt,SystemPromptFunctionHelperLLM.txt,5546,file
        SystemPrompts/SystemPromptHelperLLM.txt,SystemPromptHelperLLM.txt,9350,file
        SystemPrompts/SystemPromptMainLLM.txt,SystemPromptMainLLM.txt,8380,file
        SystemPrompts/SystemPromptMainLLMToon.txt,SystemPromptMainLLMToon.txt,8456,file
        SystemPrompts/SystemPromptNotebookLLM.txt,SystemPromptNotebookLLM.txt,6921,file
    - path: analysis_output.json
      name: analysis_output.json
      size: 569396
      type: file
    - name: backend
      type: directory
      children[12]{path,name,size,type}:
        backend/AST_Schema.py,AST_Schema.py,6622,file
        backend/File_Dependency.py,File_Dependency.py,7384,file
        backend/HelperLLM.py,HelperLLM.py,17280,file
        backend/MainLLM.py,MainLLM.py,4281,file
        backend/__init__.py,__init__.py,0,file
        backend/basic_info.py,basic_info.py,7284,file
        backend/callgraph.py,callgraph.py,9020,file
        backend/converter.py,converter.py,3594,file
        backend/getRepo.py,getRepo.py,4739,file
        backend/main.py,main.py,25591,file
        backend/relationship_analyzer.py,relationship_analyzer.py,8279,file
        backend/scads_key_test.py,scads_key_test.py,663,file
    - name: database
      type: directory
      children[1]{path,name,size,type}:
        database/db.py,db.py,8222,file
    - name: frontend
      type: directory
      children[4]:
        - name: .streamlit
          type: directory
          children[1]{path,name,size,type}:
            frontend/.streamlit/config.toml,config.toml,165,file
        - path: frontend/__init__.py
          name: __init__.py
          size: 0
          type: file
        - path: frontend/frontend.py
          name: frontend.py
          size: 31176
          type: file
        - name: gifs
          type: directory
          children[1]{path,name,size,type}:
            frontend/gifs/4j.gif,4j.gif,3306723,file
    - name: notizen
      type: directory
      children[8]:
        - path: notizen/Report Agenda.txt
          name: Report Agenda.txt
          size: 3492
          type: file
        - path: notizen/Zwischenpraesentation Agenda.txt
          name: Zwischenpraesentation Agenda.txt
          size: 809
          type: file
        - path: notizen/doc_bestandteile.md
          name: doc_bestandteile.md
          size: 847
          type: file
        - name: grafiken
          type: directory
          children[4]:
            - name: "1"
              type: directory
              children[5]{path,name,size,type}:
                notizen/grafiken/1/File_Dependency_Graph_Repo.dot,File_Dependency_Graph_Repo.dot,1227,file
                notizen/grafiken/1/global_callgraph.png,global_callgraph.png,940354,file
                notizen/grafiken/1/global_graph.png,global_graph.png,4756519,file
                notizen/grafiken/1/global_graph_2.png,global_graph_2.png,242251,file
                notizen/grafiken/1/repo.dot,repo.dot,13870,file
            - name: "2"
              type: directory
              children[12]{path,name,size,type}:
                notizen/grafiken/2/FDG_repo.dot,FDG_repo.dot,9150,file
                notizen/grafiken/2/fdg_graph.png,fdg_graph.png,425374,file
                notizen/grafiken/2/fdg_graph_2.png,fdg_graph_2.png,4744718,file
                notizen/grafiken/2/filtered_callgraph_flask.png,filtered_callgraph_flask.png,614631,file
                notizen/grafiken/2/filtered_callgraph_repo-agent.png,filtered_callgraph_repo-agent.png,280428,file
                notizen/grafiken/2/filtered_callgraph_repo-agent_3.png,filtered_callgraph_repo-agent_3.png,1685838,file
                notizen/grafiken/2/filtered_repo_callgraph_flask.dot,filtered_repo_callgraph_flask.dot,19984,file
                notizen/grafiken/2/filtered_repo_callgraph_repo-agent-3.dot,filtered_repo_callgraph_repo-agent-3.dot,20120,file
                notizen/grafiken/2/filtered_repo_callgraph_repo-agent.dot,filtered_repo_callgraph_repo-agent.dot,3118,file
                notizen/grafiken/2/global_callgraph.png,global_callgraph.png,608807,file
                notizen/grafiken/2/graph_flask.md,graph_flask.md,17398,file
                notizen/grafiken/2/repo.dot,repo.dot,19984,file
            - name: Flask-Repo
              type: directory
              children[65]{path,name,size,type}:
                notizen/grafiken/Flask-Repo/__init__.dot,__init__.dot,89,file
                notizen/grafiken/Flask-Repo/__main__.dot,__main__.dot,87,file
                notizen/grafiken/Flask-Repo/app.dot,app.dot,78,file
                notizen/grafiken/Flask-Repo/auth.dot,auth.dot,1126,file
                notizen/grafiken/Flask-Repo/blog.dot,blog.dot,939,file
                notizen/grafiken/Flask-Repo/blueprints.dot,blueprints.dot,4601,file
                notizen/grafiken/Flask-Repo/cli.dot,cli.dot,8269,file
                notizen/grafiken/Flask-Repo/conf.dot,conf.dot,489,file
                notizen/grafiken/Flask-Repo/config.dot,config.dot,2130,file
                notizen/grafiken/Flask-Repo/conftest.dot,conftest.dot,1756,file
                notizen/grafiken/Flask-Repo/ctx.dot,ctx.dot,3009,file
                notizen/grafiken/Flask-Repo/db.dot,db.dot,782,file
                notizen/grafiken/Flask-Repo/debughelpers.dot,debughelpers.dot,1903,file
                notizen/grafiken/Flask-Repo/factory.dot,factory.dot,220,file
                notizen/grafiken/Flask-Repo/flask.dot,flask.dot,82,file
                notizen/grafiken/Flask-Repo/globals.dot,globals.dot,370,file
                notizen/grafiken/Flask-Repo/hello.dot,hello.dot,152,file
                notizen/grafiken/Flask-Repo/helpers.dot,helpers.dot,2510,file
                notizen/grafiken/Flask-Repo/importerrorapp.dot,importerrorapp.dot,155,file
                notizen/grafiken/Flask-Repo/logging.dot,logging.dot,564,file
                notizen/grafiken/Flask-Repo/make_celery.dot,make_celery.dot,99,file
                notizen/grafiken/Flask-Repo/multiapp.dot,multiapp.dot,88,file
                notizen/grafiken/Flask-Repo/provider.dot,provider.dot,1769,file
                notizen/grafiken/Flask-Repo/scaffold.dot,scaffold.dot,3700,file
                notizen/grafiken/Flask-Repo/sessions.dot,sessions.dot,4178,file
                notizen/grafiken/Flask-Repo/signals.dot,signals.dot,133,file
                notizen/grafiken/Flask-Repo/tag.dot,tag.dot,3750,file
                notizen/grafiken/Flask-Repo/tasks.dot,tasks.dot,312,file
                notizen/grafiken/Flask-Repo/templating.dot,templating.dot,2277,file
                notizen/grafiken/Flask-Repo/test_appctx.dot,test_appctx.dot,2470,file
                notizen/grafiken/Flask-Repo/test_async.dot,test_async.dot,1919,file
                notizen/grafiken/Flask-Repo/test_auth.dot,test_auth.dot,725,file
                notizen/grafiken/Flask-Repo/test_basic.dot,test_basic.dot,17383,file
                notizen/grafiken/Flask-Repo/test_blog.dot,test_blog.dot,1105,file
                notizen/grafiken/Flask-Repo/test_blueprints.dot,test_blueprints.dot,11515,file
                notizen/grafiken/Flask-Repo/test_cli.dot,test_cli.dot,6435,file
                notizen/grafiken/Flask-Repo/test_config.dot,test_config.dot,2854,file
                notizen/grafiken/Flask-Repo/test_config.png,test_config.png,288693,file
                notizen/grafiken/Flask-Repo/test_converters.dot,test_converters.dot,937,file
                notizen/grafiken/Flask-Repo/test_db.dot,test_db.dot,481,file
                notizen/grafiken/Flask-Repo/test_factory.dot,test_factory.dot,201,file
                notizen/grafiken/Flask-Repo/test_helpers.dot,test_helpers.dot,5857,file
                notizen/grafiken/Flask-Repo/test_instance_config.dot,test_instance_config.dot,1249,file
                notizen/grafiken/Flask-Repo/test_js_example.dot,test_js_example.dot,453,file
                notizen/grafiken/Flask-Repo/test_json.dot,test_json.dot,4021,file
                notizen/grafiken/Flask-Repo/test_json_tag.dot,test_json_tag.dot,1452,file
                notizen/grafiken/Flask-Repo/test_logging.dot,test_logging.dot,1348,file
                notizen/grafiken/Flask-Repo/test_regression.dot,test_regression.dot,763,file
                notizen/grafiken/Flask-Repo/test_reqctx.dot,test_reqctx.dot,3809,file
                notizen/grafiken/Flask-Repo/test_request.dot,test_request.dot,668,file
                notizen/grafiken/Flask-Repo/test_session_interface.dot,test_session_interface.dot,667,file
                notizen/grafiken/Flask-Repo/test_signals.dot,test_signals.dot,1671,file
                notizen/grafiken/Flask-Repo/test_subclassing.dot,test_subclassing.dot,612,file
                notizen/grafiken/Flask-Repo/test_templating.dot,test_templating.dot,5129,file
                notizen/grafiken/Flask-Repo/test_testing.dot,test_testing.dot,4081,file
                notizen/grafiken/Flask-Repo/test_user_error_handler.dot,test_user_error_handler.dot,5984,file
                notizen/grafiken/Flask-Repo/test_views.dot,test_views.dot,2641,file
                notizen/grafiken/Flask-Repo/testing.dot,testing.dot,2785,file
                notizen/grafiken/Flask-Repo/typing.dot,typing.dot,86,file
                notizen/grafiken/Flask-Repo/typing_app_decorators.dot,typing_app_decorators.dot,500,file
                notizen/grafiken/Flask-Repo/typing_error_handler.dot,typing_error_handler.dot,420,file
                notizen/grafiken/Flask-Repo/typing_route.dot,typing_route.dot,1717,file
                notizen/grafiken/Flask-Repo/views.dot,views.dot,1125,file
                notizen/grafiken/Flask-Repo/wrappers.dot,wrappers.dot,911,file
                notizen/grafiken/Flask-Repo/wsgi.dot,wsgi.dot,19,file
            - name: Repo-onboarding
              type: directory
              children[15]{path,name,size,type}:
                notizen/grafiken/Repo-onboarding/AST.dot,AST.dot,1361,file
                notizen/grafiken/Repo-onboarding/Frontend.dot,Frontend.dot,538,file
                notizen/grafiken/Repo-onboarding/HelperLLM.dot,HelperLLM.dot,1999,file
                notizen/grafiken/Repo-onboarding/HelperLLM.png,HelperLLM.png,234861,file
                notizen/grafiken/Repo-onboarding/MainLLM.dot,MainLLM.dot,2547,file
                notizen/grafiken/Repo-onboarding/agent.dot,agent.dot,2107,file
                notizen/grafiken/Repo-onboarding/basic_info.dot,basic_info.dot,1793,file
                notizen/grafiken/Repo-onboarding/callgraph.dot,callgraph.dot,1478,file
                notizen/grafiken/Repo-onboarding/getRepo.dot,getRepo.dot,1431,file
                notizen/grafiken/Repo-onboarding/graph_AST.png,graph_AST.png,132743,file
                notizen/grafiken/Repo-onboarding/graph_AST2.png,graph_AST2.png,12826,file
                notizen/grafiken/Repo-onboarding/graph_AST3.png,graph_AST3.png,136016,file
                notizen/grafiken/Repo-onboarding/main.dot,main.dot,1091,file
                notizen/grafiken/Repo-onboarding/tools.dot,tools.dot,1328,file
                notizen/grafiken/Repo-onboarding/types.dot,types.dot,133,file
        - path: notizen/notizen.md
          name: notizen.md
          size: 4937
          type: file
        - path: notizen/paul_notizen.md
          name: paul_notizen.md
          size: 2033
          type: file
        - path: notizen/praesentation_notizen.md
          name: praesentation_notizen.md
          size: 2738
          type: file
        - path: notizen/technische_notizen.md
          name: technische_notizen.md
          size: 3616
          type: file
    - path: output.json
      name: output.json
      size: 454008
      type: file
    - path: output.toon
      name: output.toon
      size: 341854
      type: file
    - path: readme.md
      name: readme.md
      size: 1905
      type: file
    - path: requirements.txt
      name: requirements.txt
      size: 4286
      type: file
    - name: result
      type: directory
      children[28]{path,name,size,type}:
        result/ast_schema_01_12_2025_11-49-24.json,ast_schema_01_12_2025_11-49-24.json,201215,file
        result/notebook_report_23_12_2025_12-56-24_NotebookLLM_gemini-2.5-flash.md,notebook_report_23_12_2025_12-56-24_NotebookLLM_gemini-2.5-flash.md,15776,file
        result/notebook_report_27_12_2025_15-06-09_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-06-09_NotebookLLM_gemini-2.5-flash.md,14804,file
        result/notebook_report_27_12_2025_15-09-29_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-09-29_NotebookLLM_gemini-2.5-flash.md,17598,file
        result/notebook_report_27_12_2025_15-26-34_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-26-34_NotebookLLM_gemini-2.5-flash.md,20458,file
        result/notebook_report_27_12_2025_15-33-06_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-33-06_NotebookLLM_gemini-2.5-flash.md,15268,file
        result/notebook_report_29_12_2025_15-03-21_NotebookLLM_gemini-2.5-flash.md,notebook_report_29_12_2025_15-03-21_NotebookLLM_gemini-2.5-flash.md,16111,file
        result/report_01_12_2025_12-26-46_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_12-26-46_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,79523,file
        result/report_01_12_2025_12-55-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_12-55-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,143883,file
        result/report_01_12_2025_13-37-30_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_13-37-30_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,141924,file
        result/report_01_12_2025_14-15-04_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_14-15-04_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,129176,file
        result/report_01_12_2025_14-42-38_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_14-42-38_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,38933,file
        result/report_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,138015,file
        result/report_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.md,report_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.md,127747,file
        result/report_03_12_2025_22-46-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_03_12_2025_22-46-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,80305,file
        result/report_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,163921,file
        result/report_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.md,report_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.md,128520,file
        result/report_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.md,report_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.md,7245,file
        result/report_14_11_2025_14-52-36.md,report_14_11_2025_14-52-36.md,16393,file
        result/report_14_11_2025_15-21-53.md,report_14_11_2025_15-21-53.md,108585,file
        result/report_14_11_2025_15-26-24.md,report_14_11_2025_15-26-24.md,11659,file
        result/report_21_11_2025_15-43-30.md,report_21_11_2025_15-43-30.md,57940,file
        result/report_21_11_2025_16-06-12.md,report_21_11_2025_16-06-12.md,76423,file
        result/report_22_11_2025_14-01-50_Helper_llama3_Main_geminipro.md,report_22_11_2025_14-01-50_Helper_llama3_Main_geminipro.md,39595,file
        result/report_22_11_2025_14-39-55_Helper_llama3_MainLLM_llama3.md,report_22_11_2025_14-39-55_Helper_llama3_MainLLM_llama3.md,2013,file
        result/result_2025-11-11_12-30-53.md,result_2025-11-11_12-30-53.md,1232,file
        result/result_2025-11-11_12-43-51.md,result_2025-11-11_12-43-51.md,33683,file
        result/result_2025-11-11_12-45-37.md,result_2025-11-11_12-45-37.md,1193,file
    - name: schemas
      type: directory
      children[1]{path,name,size,type}:
        schemas/types.py,types.py,7559,file
    - name: statistics
      type: directory
      children[5]{path,name,size,type}:
        statistics/savings_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.png,savings_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.png,23279,file
        statistics/savings_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.png,savings_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.png,23650,file
        statistics/savings_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.png,savings_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.png,23805,file
        statistics/savings_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.png,savings_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.png,22698,file
        statistics/savings_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.png,savings_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.png,22539,file
    - path: test.json
      name: test.json
      size: 241669
      type: file
ast_schema:
  files:
    "backend/AST_Schema.py":
      ast_nodes:
        imports[3]: ast,os,getRepo.GitRepository
        functions[1]:
          - mode: function_analysis
            identifier: backend.AST_Schema.path_to_module
            name: path_to_module
            args[2]: filepath,project_root
            docstring: Wandelt einen Dateipfad in einen Python-Modulpfad um.
            source_code: "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    \n    module_path = rel_path.replace(os.path.sep, '.')\n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n    return module_path"
            start_line: 5
            end_line: 18
            context:
              calls[0]:
              called_by[1]: backend.AST_Schema.ASTVisitor.__init__
        classes[2]:
          - mode: class_analysis
            identifier: backend.AST_Schema.ASTVisitor
            name: ASTVisitor
            docstring: null
            source_code: "class ASTVisitor(ast.NodeVisitor):\n    def __init__(self, source_code: str, file_path: str, project_root: str):\n        self.source_code = source_code\n        self.file_path = file_path\n        self.project_root = project_root\n        self.module_path = path_to_module(self.file_path, self.project_root)\n        self.schema = {\"imports\": [], \"functions\": [], \"classes\": []}\n        self._current_class = None\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(f\"{node.module}.{alias.name}\")\n        self.generic_visit(node)\n\n    def visit_ClassDef(self, node):\n        class_identifier = f\"{self.module_path}.{node.name}\"\n        class_info = {\n            \"mode\": \"class_analysis\",\n            \"identifier\": class_identifier,\n            \"name\": node.name,\n            \"docstring\": ast.get_docstring(node),\n            \"source_code\": ast.get_source_segment(self.source_code, node),\n            \"start_line\": node.lineno,\n            \"end_line\": node.end_lineno,\n            \"context\": {\n                \"dependencies\": [],\n                \"instantiated_by\": [],\n                \"method_context\": []\n            },   \n        }\n        self.schema[\"classes\"].append(class_info)\n        self._current_class = class_info \n        self.generic_visit(node)\n        self._current_class = None\n\n    def visit_FunctionDef(self, node):\n        if self._current_class:\n            method_identifier = f\"{self._current_class['identifier']}.{node.name}\"\n            method_context_info = {\n                \"identifier\": method_identifier,\n                \"name\": node.name,\n                \"calls\": [],\n                \"called_by\": [],\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n            }\n            self._current_class[\"context\"][\"method_context\"].append(method_context_info)\n        else:\n            func_identifier = f\"{self.module_path}.{node.name}\"\n            func_info = {\n                \"mode\": \"function_analysis\",\n                \"identifier\": func_identifier,\n                \"name\": node.name,\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"source_code\": ast.get_source_segment(self.source_code, node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n                \"context\": {\n                    \"calls\": [],\n                    \"called_by\": []\n                }\n            }\n            self.schema[\"functions\"].append(func_info)\n        self.generic_visit(node)\n    \n    def visit_AsyncFunctionDef(self, node):\n        self.visit_FunctionDef(node)"
            start_line: 20
            end_line: 94
            context:
              dependencies[1]: backend.AST_Schema.path_to_module
              instantiated_by[1]: backend.AST_Schema.ASTAnalyzer.analyze_repository
              method_context[6]:
                - identifier: backend.AST_Schema.ASTVisitor.__init__
                  name: __init__
                  calls[1]: backend.AST_Schema.path_to_module
                  called_by[0]:
                  args[4]: self,source_code,file_path,project_root
                  docstring: null
                  start_line: 21
                  end_line: 27
                - identifier: backend.AST_Schema.ASTVisitor.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 29
                  end_line: 32
                - identifier: backend.AST_Schema.ASTVisitor.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 34
                  end_line: 37
                - identifier: backend.AST_Schema.ASTVisitor.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 39
                  end_line: 58
                - identifier: backend.AST_Schema.ASTVisitor.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 60
                  end_line: 91
                - identifier: backend.AST_Schema.ASTVisitor.visit_AsyncFunctionDef
                  name: visit_AsyncFunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 93
                  end_line: 94
          - mode: class_analysis
            identifier: backend.AST_Schema.ASTAnalyzer
            name: ASTAnalyzer
            docstring: null
            source_code: "class ASTAnalyzer:\n\n    def __init__(self):\n        pass\n\n    def merge_relationship_data(self, full_schema: dict, raw_relationships: dict) -> dict:\n\n        outgoing_calls = raw_relationships.get(\"outgoing\", {})\n        incoming_calls = raw_relationships.get(\"incoming\", {})\n\n        for file_path, file_data in full_schema.get(\"files\", {}).items():\n            ast_nodes = file_data.get(\"ast_nodes\", {})\n            \n            for func in ast_nodes.get(\"functions\", []):\n                func_id = func.get(\"identifier\")\n                if func_id:\n                    func[\"context\"][\"calls\"] = outgoing_calls.get(func_id, [])\n                    func[\"context\"][\"called_by\"] = incoming_calls.get(func_id, [])\n\n            for cls in ast_nodes.get(\"classes\", []):\n                cls_id = cls.get(\"identifier\")\n                \n                if cls_id:\n                    cls[\"context\"][\"instantiated_by\"] = incoming_calls.get(cls_id, [])\n\n                class_dependencies = set()\n                \n                for method in cls[\"context\"].get(\"method_context\", []):\n                    method_id = method.get(\"identifier\")\n                    if method_id:\n                        m_calls = outgoing_calls.get(method_id, [])\n                        method[\"calls\"] = m_calls\n                        method[\"called_by\"] = incoming_calls.get(method_id, [])\n                        \n                        for callee in m_calls:\n                            if cls_id and not callee.startswith(f\"{cls_id}.\"):\n                                class_dependencies.add(callee)\n                \n                cls[\"context\"][\"dependencies\"] = sorted(list(class_dependencies))\n        \n        return full_schema\n\n    def analyze_repository(self, files: list, repo: GitRepository) -> dict:\n        full_schema = {\n            \"files\": {}\n        }\n\n        all_paths = [file_obj.path for file_obj in files]\n        if not all_paths:\n            return full_schema\n        \n        project_root = os.path.commonpath(all_paths)\n        if os.path.isfile(project_root):\n            project_root = os.path.dirname(project_root)\n\n        for file_obj in files:\n            if not file_obj.path.endswith('.py'):\n                continue\n            \n            file_content = file_obj.content\n            if not file_content.strip():\n                continue\n\n            try:\n                tree = ast.parse(file_content)\n                visitor = ASTVisitor(\n                    source_code=file_content, \n                    file_path=file_obj.path, \n                    project_root=project_root\n                )\n                visitor.visit(tree)\n                file_schema_nodes = visitor.schema\n\n                if file_schema_nodes[\"imports\"] or file_schema_nodes[\"functions\"] or file_schema_nodes[\"classes\"]:\n                    full_schema[\"files\"][file_obj.path] = {\n                        \"ast_nodes\": file_schema_nodes\n                    }\n\n            except (SyntaxError, ValueError) as e:\n                print(f\"Warnung: Konnte Datei '{file_obj.path}' nicht parsen. Fehler: {e}\")\n\n        return full_schema"
            start_line: 97
            end_line: 178
            context:
              dependencies[1]: backend.AST_Schema.ASTVisitor
              instantiated_by[1]: backend.main.main_workflow
              method_context[3]:
                - identifier: backend.AST_Schema.ASTAnalyzer.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 99
                  end_line: 100
                - identifier: backend.AST_Schema.ASTAnalyzer.merge_relationship_data
                  name: merge_relationship_data
                  calls[0]:
                  called_by[1]: backend.main.main_workflow
                  args[3]: self,full_schema,raw_relationships
                  docstring: null
                  start_line: 102
                  end_line: 137
                - identifier: backend.AST_Schema.ASTAnalyzer.analyze_repository
                  name: analyze_repository
                  calls[1]: backend.AST_Schema.ASTVisitor
                  called_by[1]: backend.main.main_workflow
                  args[3]: self,files,repo
                  docstring: null
                  start_line: 139
                  end_line: 178
    "backend/File_Dependency.py":
      ast_nodes:
        imports[17]: networkx,os,ast.Assign,ast.AST,ast.ClassDef,ast.FunctionDef,ast.Import,ast.ImportFrom,ast.Name,ast.NodeVisitor,ast.literal_eval,ast.parse,ast.walk,keyword.iskeyword,pathlib.Path,getRepo.GitRepository,callgraph.make_safe_dot
        functions[3]:
          - mode: function_analysis
            identifier: backend.File_Dependency.build_file_dependency_graph
            name: build_file_dependency_graph
            args[3]: filename,tree,repo_root
            docstring: null
            source_code: "def build_file_dependency_graph(filename: str, tree: AST, repo_root: str) -> nx.DiGraph:\n    graph = nx.DiGraph()\n\n    tree_visitor = FileDependencyGraph(filename, repo_root)\n    tree_visitor.visit(tree)\n\n    for caller, callees in tree_visitor.import_dependencies.items():\n        graph.add_node(caller)\n        graph.add_nodes_from(callees)\n        for callee in callees:\n            graph.add_edge(caller, callee)\n    \n    return graph"
            start_line: 153
            end_line: 165
            context:
              calls[1]: backend.File_Dependency.FileDependencyGraph
              called_by[1]: backend.File_Dependency.build_repository_graph
          - mode: function_analysis
            identifier: backend.File_Dependency.build_repository_graph
            name: build_repository_graph
            args[1]: repository
            docstring: null
            source_code: "def build_repository_graph(repository: GitRepository) -> nx.DiGraph:\n    all_files = repository.get_all_files()\n    repo_root = repository.temp_dir\n    global_graph = nx.DiGraph()\n\n    for file in all_files: \n        if not file.path.endswith(\".py\"):\n            continue\n        filename = str(os.path.basename(file.path)).removesuffix(\".py\")\n        tree = parse(file.content)\n        graph = build_file_dependency_graph(filename, tree, repo_root)\n        \n        for node in graph.nodes:\n            global_graph.add_node(node)\n\n        for caller, callee in graph.edges:\n            if callee:\n                global_graph.add_node(callee)\n                global_graph.add_edge(caller, callee)\n    \n    return global_graph"
            start_line: 167
            end_line: 187
            context:
              calls[1]: backend.File_Dependency.build_file_dependency_graph
              called_by[1]: backend.File_Dependency
          - mode: function_analysis
            identifier: backend.File_Dependency.get_all_temp_files
            name: get_all_temp_files
            args[1]: directory
            docstring: null
            source_code: "def get_all_temp_files(directory: str) -> list[Path]:\n    root_path = Path(directory).resolve()\n    all_files = [file.relative_to(root_path) for file in root_path.rglob(\"*.py\")]\n\n    return all_files"
            start_line: 189
            end_line: 193
            context:
              calls[0]:
              called_by[1]: backend.File_Dependency.FileDependencyGraph._resolve_module_name
        classes[1]:
          - mode: class_analysis
            identifier: backend.File_Dependency.FileDependencyGraph
            name: FileDependencyGraph
            docstring: null
            source_code: "class FileDependencyGraph(NodeVisitor):\n\n    import_dependencies: dict[str, set[str]] = {}\n    def __init__(self, filename: str, repo_root):\n        \"\"\"\n        Initialisiert den File Dependency Graphen\n\n        Args:\n\n        \"\"\"\n        self.filename = filename\n        self.repo_root = repo_root\n\n    def _resolve_module_name(self, node: ImportFrom) -> list[str]:\n        \"\"\"\n        L√∂st relative Imports der Form `from .. import name1, name2` auf.\n        Liefert die Liste der tats√§chlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\n        Wirft ImportError, wenn nichts aufgel√∂st werden konnte.\n        \"\"\"\n        level_depth = node.level\n        names = [alias.name for alias in node.names]  \n        all_files = get_all_temp_files(self.repo_root)  \n\n        candidates = [p for p in all_files if p.stem == Path(self.filename).stem or p.name == f\"{self.filename}.py\"]\n\n        if not candidates:\n            raise ImportError(f\"Kann aktuelle Datei '{self.filename}' im Repo nicht finden.\")\n\n\n        candidates.sort(key=lambda p: len(p.parts))\n        current_rel_path = candidates[0]\n        base_dir = current_rel_path.parent\n        if level_depth <= 0:\n            raise ImportError(\"Erwarteter relativer Import (level >= 1).\")\n        for _ in range(level_depth - 1):\n            if base_dir == base_dir.parent and len(base_dir.parts) == 0:\n                raise ImportError(f\"Relative Import-Ebene ({level_depth}) zu gro√ü f√ºr Datei '{current_rel_path}'.\")\n            base_dir = base_dir.parent\n\n        repo_root_path = Path(self.repo_root).resolve()\n        resolved: list[str] = []\n\n        def module_file_exists(rel_base: Path, name: str) -> bool:\n            file_path = repo_root_path / rel_base / f\"{name}.py\"\n            pkg_init = repo_root_path / rel_base / name / \"__init__.py\"\n            return file_path.exists() or pkg_init.exists()\n\n        def init_exports_symbol(rel_base: Path, symbol: str) -> bool:\n            \"\"\"\n            Pr√ºft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\n            oder als Name (funktion/klasse/assign) definiert ist.\n            \"\"\"\n            init_path = repo_root_path / rel_base / \"__init__.py\"\n            if not init_path.exists():\n                return False\n            try:\n                src = init_path.read_text(encoding=\"utf-8\")\n                mod = parse(src, filename=str(init_path))\n            except Exception:\n                return False\n\n            for node_ in walk(mod):\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == \"__all__\":\n                            try:\n                                value = literal_eval(node_.value)\n                                if isinstance(value, (list, tuple)) and symbol in value:\n                                    return True\n                            except Exception:\n                                pass\n                if isinstance(node_, (FunctionDef, ClassDef)) and node_.name == symbol:\n                    return True\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == symbol:\n                            return True\n            return False\n\n        for name in names:\n            if not name.isidentifier() or iskeyword(name):\n                continue\n\n            if module_file_exists(base_dir, name):\n                resolved.append(name)\n                continue\n            if init_exports_symbol(base_dir, name):\n                resolved.append(name)\n                continue\n\n        resolved = sorted(set(resolved))\n\n        if not resolved:\n            raise ImportError(\n                f\"Kein passendes Modul/Symbol f√ºr relative Import-Aufl√∂sung gefunden \"\n                f\"(level={level_depth}, names={names}, base_dir={base_dir})\"\n            )\n\n        return resolved\n    \n    def visit_Import(self, node: Import | ImportFrom, base_name: str | None = None):\n        for alias in node.names:\n        \n            if self.filename not in self.import_dependencies:\n                self.import_dependencies[self.filename] = set()\n\n            if base_name:\n                self.import_dependencies[self.filename].add(base_name)\n            else:\n                self.import_dependencies[self.filename].add(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node: ImportFrom):\n        \"\"\"\n        Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\n        also c, und dieser wird als callee f√ºr den caller, das File, gesetzt.\n        \"\"\"\n        module_name = node.module\n        if module_name:\n            module_base = module_name.split(\".\")[-1]\n            self.visit_Import(node, module_base)\n        else:\n            try:\n                resolved = self._resolve_module_name(node)\n                for base in resolved:\n                    self.visit_Import(node, base)\n            except ImportError as e:\n                print(f\"Aufl√∂sung eines relativen Imports fehlgeschlagen: {e}\")\n\n        self.generic_visit(node)"
            start_line: 22
            end_line: 151
            context:
              dependencies[3]: backend.File_Dependency.get_all_temp_files,backend.File_Dependency.init_exports_symbol,backend.File_Dependency.module_file_exists
              instantiated_by[1]: backend.File_Dependency.build_file_dependency_graph
              method_context[6]:
                - identifier: backend.File_Dependency.FileDependencyGraph.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[3]: self,filename,repo_root
                  docstring: "Initialisiert den File Dependency Graphen\n\nArgs:"
                  start_line: 25
                  end_line: 33
                - identifier: backend.File_Dependency.FileDependencyGraph._resolve_module_name
                  name: _resolve_module_name
                  calls[3]: backend.File_Dependency.get_all_temp_files,backend.File_Dependency.init_exports_symbol,backend.File_Dependency.module_file_exists
                  called_by[0]:
                  args[2]: self,node
                  docstring: "L√∂st relative Imports der Form `from .. import name1, name2` auf.\nLiefert die Liste der tats√§chlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\nWirft ImportError, wenn nichts aufgel√∂st werden konnte."
                  start_line: 35
                  end_line: 120
                - identifier: backend.File_Dependency.FileDependencyGraph.module_file_exists
                  name: module_file_exists
                  calls[0]:
                  called_by[0]:
                  args[2]: rel_base,name
                  docstring: null
                  start_line: 64
                  end_line: 67
                - identifier: backend.File_Dependency.FileDependencyGraph.init_exports_symbol
                  name: init_exports_symbol
                  calls[0]:
                  called_by[0]:
                  args[2]: rel_base,symbol
                  docstring: "Pr√ºft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\noder als Name (funktion/klasse/assign) definiert ist."
                  start_line: 69
                  end_line: 99
                - identifier: backend.File_Dependency.FileDependencyGraph.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[3]: self,node,base_name
                  docstring: null
                  start_line: 122
                  end_line: 132
                - identifier: backend.File_Dependency.FileDependencyGraph.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: "Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\nalso c, und dieser wird als callee f√ºr den caller, das File, gesetzt."
                  start_line: 134
                  end_line: 151
    "backend/HelperLLM.py":
      ast_nodes:
        imports[23]: os,json,logging,time,typing.List,typing.Dict,typing.Any,typing.Optional,typing.Union,dotenv.load_dotenv,langchain_google_genai.ChatGoogleGenerativeAI,langchain_ollama.ChatOllama,langchain_openai.ChatOpenAI,langchain.messages.HumanMessage,langchain.messages.SystemMessage,langchain.messages.AIMessage,pydantic.ValidationError,schemas.types.FunctionAnalysis,schemas.types.ClassAnalysis,schemas.types.FunctionAnalysisInput,schemas.types.FunctionContextInput,schemas.types.ClassAnalysisInput,schemas.types.ClassContextInput
        functions[1]:
          - mode: function_analysis
            identifier: backend.HelperLLM.main_orchestrator
            name: main_orchestrator
            args[0]:
            docstring: "Dummy Data and processing loop for testing the LLMHelper class.\nThis version is syntactically correct and logically matches the Pydantic models."
            source_code: "def main_orchestrator():\n    \"\"\"\n    Dummy Data and processing loop for testing the LLMHelper class.\n    This version is syntactically correct and logically matches the Pydantic models.\n    \"\"\"\n    \n    # --- Step 1: Define the pre-computed analysis for each method ---\n    \n    # Example Input 1: For the 'add_item' function\n    add_item_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"source_code\": \"\"\"def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\\\"\\\"\\\"\n        if not isinstance(quantity, int) or quantity <= 0:\n            raise ValueError(\"Quantity must be a positive integer.\")\n    \n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n        print(f\"Added {quantity} of {item_name}. New total: {self.inventory[item_name]}\")\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"process_shipment\", \"restock_api_endpoint\"]\n        }\n    })\n\n    # Example Input 2: For the 'check_stock' function\n    check_stock_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"source_code\": \"\"\"def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Retrieves the current stock quantity for a given item.\\\"\\\"\\\"\n        stock_level = self.inventory.get(item_name, 0)\n        return stock_level\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"fulfill_order\", \"ui_display_handler\"]\n        }\n    })\n\n    # Example Input 3: For the 'generate_report' function\n    generate_report_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"source_code\": \"\"\"def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a timestamped string summary of the current inventory state.\\\"\\\"\\\"\n        timestamp = datetime.now()\n        report_header = \"Inventory Report as of: {ts}\\\\n\".format(ts=timestamp)\n        report_lines = [report_header]\n        \n        for item, quantity in self.inventory.items():\n            line = \"- {name}: {qty}\".format(name=item, qty=quantity)\n            report_lines.append(line)\n            \n        return \"\\\\n\".join(report_lines)\"\"\",\n        \"imports\": [\"from datetime import datetime\"],\n        \"context\": {\n            \"calls\": [\"datetime.now\", \"str.format\"],\n            \"called_by\": [\"daily_cron_job\", \"admin_dashboard_export\"]\n        }\n    })\n\n    add_item_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"description\": {\n            \"overall\": \"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to add.\"},\n                {\"name\": \"quantity\", \"type\": \"int\", \"description\": \"The number of units to add. Must be a positive integer.\"}\n            ],\n            \"returns\": [],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"process_shipment and restock_api_endpoint\" }\n        },\n        \"error\": None\n    })\n\n    check_stock_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"description\": {\n            \"overall\": \"Retrieves the current stock quantity for a given item.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to check.\"}\n            ],\n            \"returns\": [\n                {\"name\": \"stock_level\", \"type\": \"int\", \"description\": \"The quantity of the item in stock. Returns 0 if the item is not found.\"}\n            ],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"fulfill_order and ui_display_handler\" }\n        },\n        \"error\": None\n    })\n\n    generate_report_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"description\": {\n            \"overall\": \"Generates a timestamped string summary of the current inventory state.\",\n            \"parameters\": [ {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"} ],\n            \"returns\": [\n                {\"name\": \"report\", \"type\": \"str\", \"description\": \"A formatted string detailing all items and their quantities.\"}\n            ],\n            \"usage_context\": { \"calls\": \"datetime.now and str.format\", \"called_by\": \"daily_cron_job and admin_dashboard_export\" }\n        },\n        \"error\": None\n    })\n\n    class_input = ClassAnalysisInput(\n        mode=\"class_analysis\",\n        identifier=\"InventoryManager\",\n        # NOTE: The indentation of this string is now fixed. It starts at column 0.\n        source_code=\"\"\"class InventoryManager:\n    \\\"\\\"\\\"Manages stock levels for products in a warehouse.\\\"\\\"\\\"\n    def __init__(self, warehouse_id: str):\n        self.warehouse_id = warehouse_id\n        self.inventory = {}  # item_name: quantity\n\n    def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds an item to the inventory.\\\"\\\"\\\"\n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n\n    def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Checks the stock of a specific item.\\\"\\\"\\\"\n        return self.inventory.get(item_name, 0)\n\n    def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a summary report of the inventory.\\\"\\\"\\\"\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        report_lines = [f\"Inventory Report for {self.warehouse_id} at {timestamp}\"]\n        if not self.inventory:\n            report_lines.append(\"Inventory is empty.\")\n        else:\n            for item, quantity in self.inventory.items():\n                report_lines.append(f\"- {item}: {quantity}\")\n        return \"\\\\n\".join(report_lines)\n\"\"\",\n        imports=[\"from datetime import datetime\"],\n        context=ClassContextInput(\n            dependencies=[\"datetime\"],\n            instantiated_by=[\"main_app_startup\", \"warehouse_worker_script\"],\n            methods_analysis=[\n                add_item_analysis,\n                check_stock_analysis,\n                generate_report_analysis\n            ]\n        )\n    )\n\n    \n    # The helper methods expect a LIST of inputs, so we wrap our single object in a list.\n    input = [add_item_input, check_stock_input, generate_report_input]\n    analysis = [add_item_analysis, check_stock_analysis, generate_report_analysis] \n\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    llm_helper = LLMHelper(api_key=GEMINI_API_KEY, function_prompt_path=function_prompt_file, class_prompt_path=class_prompt_file)\n\n    # This will be the final JSON containing all documentation\n    final_documentation = {}\n\n    # The analysis of the methods is already provided in the context.\n    logging.info(\"\\n--- Generating documentation for classes ---\")\n    \n    # The `generate_for_classes` method returns a list of results.\n    analysis_results = llm_helper.generate_for_functions(input)\n\n    # --- Step 4: Process the results ---\n    \n    # We loop through the list of results (even though there's only one in this case).\n    for doc in analysis_results:\n        if doc:\n            logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n            if \"classes\" not in final_documentation:\n                final_documentation[\"classes\"] = {}\n            # Use .model_dump() to convert the Pydantic object back to a dict for JSON serialization\n            final_documentation[\"classes\"][doc.identifier] = doc.model_dump() \n        else:\n            logging.warning(f\"Failed to generate doc for a class\")\n\n    # --- Step 5: Display the final aggregated result ---\n    logging.info(\"\\n--- Final Generated Documentation ---\")\n    print(json.dumps(final_documentation, indent=2))"
            start_line: 227
            end_line: 413
            context:
              calls[3]: backend.HelperLLM.LLMHelper,schemas.types.ClassAnalysisInput,schemas.types.ClassContextInput
              called_by[1]: backend.HelperLLM
        classes[1]:
          - mode: class_analysis
            identifier: backend.HelperLLM.LLMHelper
            name: LLMHelper
            docstring: "A class to interact with Google Gemini for generating code snippet documentation.\nIt centralizes API interaction, error handling, and validates I/O using Pydantic."
            source_code: "class LLMHelper:\n    \"\"\"\n    A class to interact with Google Gemini for generating code snippet documentation.\n    It centralizes API interaction, error handling, and validates I/O using Pydantic.\n    \"\"\"\n    def __init__(self, api_key: str, function_prompt_path: str, class_prompt_path: str, model_name: str = \"gemini-2.0-flash-lite\", base_url: str = None):\n        if not api_key:\n            raise ValueError(\"Gemini API Key must be set.\")\n        \n        try:\n            with open(function_prompt_path, 'r', encoding='utf-8') as f:\n                self.function_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Function system prompt file not found at: {function_prompt_path}\")\n            raise\n\n        # Handle the second file\n        try:\n            with open(class_prompt_path, 'r', encoding='utf-8') as f:\n                self.class_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Class system prompt file not found at: {class_prompt_path}\")\n            raise\n        \n        # Batch-Size config\n        self.model_name = model_name\n        self._configure_batch_settings(model_name)\n\n\n        if model_name.startswith(\"gemini-\"):\n            base_llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=0.3, \n            )\n        \n        elif model_name.startswith(\"gpt-\") and \"openGPT\" not in model_name:\n            base_llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=0.3,\n            )\n\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            if not SCADSLLM_URL:\n                raise ValueError(f\"SCADSLLM_URL environment variable is required for model {model_name}\")\n            \n            logging.info(f\"Connecting to Custom API at {SCADSLLM_URL} for model {model_name}\")\n            \n            base_llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                base_url=SCADSLLM_URL,\n                temperature=0.3,\n            )\n\n        else:\n            target_url = base_url if base_url else OLLAMA_BASE_URL\n            logging.info(f\"Using Ollama at {target_url} for model {model_name}\")\n            base_llm = ChatOllama(\n                model=model_name,\n                temperature=0.3,\n                base_url=target_url,\n            )\n\n        self.function_llm = base_llm.with_structured_output(FunctionAnalysis, method=\"json_schema\")\n        self.class_llm = base_llm.with_structured_output(ClassAnalysis, method=\"json_schema\")\n\n        self.raw_llm = base_llm\n\n        logging.info(f\"LLMHelper initialized with model '{model_name}'. Batch Size: {self.batch_size}\")\n\n    def _configure_batch_settings(self, model_name: str):\n\n        if model_name == \"gemini-2.0-flash-lite\":\n            self.batch_size = 30\n        \n        elif model_name == \"gemini-flash-latest\":\n            self.batch_size = 10\n\n        elif model_name == \"gemini-2.5-flash\":\n            self.batch_size = 10\n            \n        elif model_name == \"gemini-2.5-pro\":\n            self.batch_size = 2\n        \n        elif model_name == \"gemini-2.0-flash\":\n            self.batch_size = 15\n\n        elif model_name == \"gemini-2.5-flash-lite\":\n            self.batch_size = 15\n\n        elif model_name == \"llama3\":\n            self.batch_size = 50\n\n        elif model_name == \"gpt-5.1\":\n            self.batch_size = 500\n\n        elif model_name == \"gpt-5-mini\":\n            self.batch_size = 500\n\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            self.batch_size = 500\n            \n        else:\n            logging.warning(f\"Unknown model '{model_name}', using conservative defaults.\")\n            self.batch_size = 2\n\n    def generate_for_functions(self, function_inputs: List[FunctionAnalysisInput]) -> List[Optional[FunctionAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of functions.\"\"\"\n\n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 62\n\n        if not function_inputs:\n            return []\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(function_input.model_dump(), indent=2) \n            for function_input in function_inputs\n        ]\n       \n        conversations = [[SystemMessage(content=self.function_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_functions = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n        \n            try:\n                batch_results = self.function_llm.batch(batch_conversations, config={\"max_concurrency\": self.batch_size})\n                all_validated_functions.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                all_validated_functions.extend([None] * len(batch_conversations))\n            \n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_functions\n        \n            \n    \n\n    def generate_for_classes(self, class_inputs: List[ClassAnalysisInput]) -> List[Optional[ClassAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of classes.\"\"\"\n        if not class_inputs:\n            return []\n        \n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 62\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(class_input.model_dump(), indent=2)\n            for class_input in class_inputs\n        ]\n\n        conversations = [[SystemMessage(content=self.class_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_classes = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n\n            try:\n                batch_results = self.class_llm.batch(batch_conversations , config={\"max_concurrency\": self.batch_size})\n                all_validated_classes.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                # Falls ein Fehler auftritt, f√ºllen wir die Liste mit None auf, um die Reihenfolge zu wahren\n                all_validated_classes.extend([None] * len(batch_conversations))\n\n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_classes"
            start_line: 31
            end_line: 221
            context:
              dependencies[0]:
              instantiated_by[2]: backend.HelperLLM.main_orchestrator,backend.main.main_workflow
              method_context[4]:
                - identifier: backend.HelperLLM.LLMHelper.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[6]: self,api_key,function_prompt_path,class_prompt_path,model_name,base_url
                  docstring: null
                  start_line: 36
                  end_line: 101
                - identifier: backend.HelperLLM.LLMHelper._configure_batch_settings
                  name: _configure_batch_settings
                  calls[0]:
                  called_by[0]:
                  args[2]: self,model_name
                  docstring: null
                  start_line: 103
                  end_line: 137
                - identifier: backend.HelperLLM.LLMHelper.generate_for_functions
                  name: generate_for_functions
                  calls[0]:
                  called_by[1]: backend.main.main_workflow
                  args[2]: self,function_inputs
                  docstring: Generates and validates documentation for a batch of functions.
                  start_line: 139
                  end_line: 178
                - identifier: backend.HelperLLM.LLMHelper.generate_for_classes
                  name: generate_for_classes
                  calls[0]:
                  called_by[1]: backend.main.main_workflow
                  args[2]: self,class_inputs
                  docstring: Generates and validates documentation for a batch of classes.
                  start_line: 183
                  end_line: 221
    "backend/MainLLM.py":
      ast_nodes:
        imports[9]: os,logging,sys,dotenv.load_dotenv,langchain_google_genai.ChatGoogleGenerativeAI,langchain_ollama.ChatOllama,langchain_openai.ChatOpenAI,langchain.messages.HumanMessage,langchain.messages.SystemMessage
        functions[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.MainLLM.MainLLM
            name: MainLLM
            docstring: Hauptklasse f√ºr die Interaktion mit dem LLM.
            source_code: "class MainLLM:\n    \"\"\"\n    Hauptklasse f√ºr die Interaktion mit dem LLM.\n    \"\"\"\n    def __init__(self, api_key: str, prompt_file_path: str, model_name: str = \"gemini-2.5-pro\", base_url: str = None):\n        if not api_key:\n            raise ValueError(\"Gemini API Key must be set.\")\n        \n        try:\n            with open(prompt_file_path, 'r', encoding='utf-8') as f:\n                self.system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"System prompt file not found at: {prompt_file_path}\")\n            raise\n        \n        self.model_name = model_name\n\n        if model_name.startswith(\"gemini-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n\n        elif model_name.startswith(\"gpt-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            if not SCADSLLM_URL:\n                raise ValueError(f\"SCADSLLM_URL environment variable is required for model {model_name}\")\n            \n            logging.info(f\"Connecting to Custom API at {SCADSLLM_URL} for model {model_name}\")\n            \n            self.llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                base_url=SCADSLLM_URL,\n                temperature=1.0,\n            )\n\n        else:\n            target_url = base_url if base_url else OLLAMA_BASE_URL\n            self.llm = ChatOllama(\n                model=model_name,\n                temperature=1.0,\n                base_url=target_url,\n            )\n\n        logging.info(f\"Main LLM initialized with model '{model_name}'.\")\n    \n    def call_llm(self, user_input: str):\n\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with HelperLLM input...\")\n\n        try:\n            response = self.llm.invoke(messages)\n            logging.info(\"LLM call successful.\")\n            return response.content\n        except Exception as e:\n            logging.error(f\"Error during LLM call: {e}\")\n            return None\n\n    def stream_llm(self, user_input: str):\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with 'stream'...\")\n\n        try:\n            stream_iterator = self.llm.stream(messages)\n            \n            for chunk in stream_iterator:\n                yield chunk.content\n        except Exception as e:\n            error_message = f\"\\n--- Error during LLM stream call: {e} ---\"\n            logging.error(error_message)\n            yield error_message"
            start_line: 22
            end_line: 106
            context:
              dependencies[0]:
              instantiated_by[2]: backend.main.main_workflow,backend.main.notebook_workflow
              method_context[3]:
                - identifier: backend.MainLLM.MainLLM.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[5]: self,api_key,prompt_file_path,model_name,base_url
                  docstring: null
                  start_line: 26
                  end_line: 73
                - identifier: backend.MainLLM.MainLLM.call_llm
                  name: call_llm
                  calls[0]:
                  called_by[2]: backend.main.main_workflow,backend.main.notebook_workflow
                  args[2]: self,user_input
                  docstring: null
                  start_line: 75
                  end_line: 89
                - identifier: backend.MainLLM.MainLLM.stream_llm
                  name: stream_llm
                  calls[0]:
                  called_by[0]:
                  args[2]: self,user_input
                  docstring: null
                  start_line: 91
                  end_line: 106
    "backend/basic_info.py":
      ast_nodes:
        imports[7]: re,os,tomllib,typing.List,typing.Dict,typing.Any,typing.Optional
        functions[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.basic_info.ProjektInfoExtractor
            name: ProjektInfoExtractor
            docstring: "Extrahiert grundlegende Projektinformationen aus g√§ngigen Projektdateien\nwie README, pyproject.toml und requirements.txt."
            source_code: "class ProjektInfoExtractor:\n    \"\"\"\n    Extrahiert grundlegende Projektinformationen aus g√§ngigen Projektdateien\n    wie README, pyproject.toml und requirements.txt.\n    \"\"\"\n    def __init__(self):\n        self.INFO_NICHT_GEFUNDEN = \"Information not found\"\n        # Initialisiert die Struktur mit Platzhaltern\n        self.info = {\n            \"projekt_uebersicht\": {\n                \"titel\": self.INFO_NICHT_GEFUNDEN,\n                \"beschreibung\": self.INFO_NICHT_GEFUNDEN,\n                \"aktueller_status\": self.INFO_NICHT_GEFUNDEN,\n                \"key_features\": self.INFO_NICHT_GEFUNDEN,\n                \"tech_stack\": self.INFO_NICHT_GEFUNDEN,\n            },\n            \"installation\": {\n                \"dependencies\": self.INFO_NICHT_GEFUNDEN,\n                \"setup_anleitung\": self.INFO_NICHT_GEFUNDEN,\n                \"quick_start_guide\": self.INFO_NICHT_GEFUNDEN,\n            }\n        }\n\n    def _clean_content(self, content: str) -> str:\n        \"\"\"Entfernt Null-Bytes, die durch Encoding-Fehler (UTF-16 als UTF-8 gelesen) entstehen.\"\"\"\n        if not content:\n            return \"\"\n        return content.replace('\\x00', '')\n\n    def _finde_datei(self, patterns: List[str], dateien: List[Any]) -> Optional[Any]:\n        \"\"\"Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht.\"\"\"\n        for datei in dateien:\n            for pattern in patterns:\n                if datei.path.lower().endswith(pattern.lower()):\n                    return datei\n        return None\n\n    def _extrahiere_sektion_aus_markdown(self, inhalt: str, keywords: List[str]) -> Optional[str]:\n        \"\"\"\n        Extrahiert den Text unter einer Markdown-√úberschrift (##).\n        \"\"\"\n        # Erstellt ein Regex-Pattern, das auf jedes der Schl√ºsselw√∂rter reagiert\n        keyword_pattern = \"|\".join(re.escape(k) for k in keywords)\n        \n        # Sucht nach \"## Schl√ºsselwort\" und erfasst alles bis zur n√§chsten \"##\" oder dem Dateiende\n        pattern = re.compile(\n            rf\"##\\s*({keyword_pattern})\\s*\\n(.*?)(?=\\n##|\\Z)\",\n            re.IGNORECASE | re.DOTALL\n        )\n        match = pattern.search(inhalt)\n        if match:\n            return match.group(2).strip()\n        return None\n\n    def _parse_readme(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer README-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        if self.info[\"projekt_uebersicht\"][\"titel\"] == self.INFO_NICHT_GEFUNDEN:\n            title_match = re.search(r\"^\\s*#\\s*(.*)\", inhalt)\n            if title_match:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = title_match.group(1).strip()\n\n        # Beschreibung (Fallback)\n        if self.info[\"projekt_uebersicht\"][\"beschreibung\"] == self.INFO_NICHT_GEFUNDEN:\n            desc_match = re.search(r\"^\\s*#\\s*.*\\n+([^#\\n].*)\", inhalt, re.DOTALL)\n            if desc_match:\n                 self.info[\"projekt_uebersicht\"][\"beschreibung\"] = desc_match.group(1).strip().split('\\n\\n')[0]\n\n        # Key Features\n        features = self._extrahiere_sektion_aus_markdown(inhalt, [\"Features\", \"Key Features\", \"Merkmale\"])\n        if features:\n            self.info[\"projekt_uebersicht\"][\"key_features\"] = features\n            \n        # Tech Stack\n        tech_stack = self._extrahiere_sektion_aus_markdown(inhalt, [\"Tech Stack\", \"Technology\", \"Technologien\"])\n        if tech_stack:\n            self.info[\"projekt_uebersicht\"][\"tech_stack\"] = tech_stack\n            \n        # Status\n        status = self._extrahiere_sektion_aus_markdown(inhalt, [\"Status\", \"Current Status\"])\n        if status:\n            self.info[\"projekt_uebersicht\"][\"aktueller_status\"] = status\n\n        # Setup-Anleitung\n        setup = self._extrahiere_sektion_aus_markdown(inhalt, [\"Installation\", \"Setup\", \"Getting Started\"])\n        if setup:\n            self.info[\"installation\"][\"setup_anleitung\"] = setup\n            \n        # Quick Start\n        quick_start = self._extrahiere_sektion_aus_markdown(inhalt, [\"Quick Start\", \"Schnellstart\"])\n        if quick_start:\n            self.info[\"installation\"][\"quick_start_guide\"] = quick_start\n\n    def _parse_toml(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer pyproject.toml-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        if not tomllib:\n            print(\"Warnung: 'tomli' ist nicht installiert. pyproject.toml kann nicht analysiert werden.\")\n            return\n            \n        try:\n            data = tomllib.loads(inhalt)\n            project_data = data.get(\"project\", {})\n            \n            if \"name\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = project_data[\"name\"]\n            if \"description\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"beschreibung\"] = project_data[\"description\"]\n            if \"dependencies\" in project_data:\n                self.info[\"installation\"][\"dependencies\"] = project_data[\"dependencies\"]\n        except tomllib.TOMLDecodeError as e:\n            print(f\"Warnung: Fehler beim Parsen der pyproject.toml: {e}\")\n\n    def _parse_requirements(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer requirements.txt-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        # Nur f√ºllen, wenn noch keine Dependencies aus toml gefunden wurden\n        if self.info[\"installation\"][\"dependencies\"] == self.INFO_NICHT_GEFUNDEN:\n            lines = inhalt.splitlines()\n            dependencies = [\n                line.strip() for line in lines \n                if line.strip() and not line.strip().startswith('#')\n            ]\n            if dependencies:\n                self.info[\"installation\"][\"dependencies\"] = dependencies\n\n    def extrahiere_info(self, dateien: List[Any], repo_url: str) -> Dict[str, Any]:\n        \"\"\"\n        Orchestriert die Extraktion von Informationen.\n        \"\"\"\n        # Relevante Dateien finden\n        readme_datei = self._finde_datei([\"readme.md\", \"readme.rst\", \"readme.txt\", \"readme\"], dateien)\n        toml_datei = self._finde_datei([\"pyproject.toml\"], dateien)\n        req_datei = self._finde_datei([\"requirements.txt\"], dateien)\n\n        # Dateien parsen (mit Priorisierung)\n        if toml_datei:\n            self._parse_toml(toml_datei.content)\n\n        if req_datei:\n            self._parse_requirements(req_datei.content)\n            \n        if readme_datei:\n            self._parse_readme(readme_datei.content)\n            \n        # Finale Formatierung der Dependencies\n        deps = self.info[\"installation\"][\"dependencies\"]\n        if isinstance(deps, list):\n            if not deps:\n                self.info[\"installation\"][\"dependencies\"] = self.INFO_NICHT_GEFUNDEN\n            else:\n                self.info[\"installation\"][\"dependencies\"] = \"\\n\".join(f\"- {dep}\" for dep in deps)\n        \n        # Titel aus URL ableiten, falls nichts gefunden wurde (oder als Zusatzinfo)\n        if repo_url:\n            repo_name = os.path.basename(repo_url.removesuffix('.git'))\n            # Wenn noch kein Titel da ist oder er generisch war\n            if self.info[\"projekt_uebersicht\"][\"titel\"] == self.INFO_NICHT_GEFUNDEN:\n                 self.info[\"projekt_uebersicht\"][\"titel\"] = f\"{repo_name} documentation\"\n\n        return self.info"
            start_line: 9
            end_line: 172
            context:
              dependencies[0]:
              instantiated_by[2]: backend.main.main_workflow,backend.main.notebook_workflow
              method_context[8]:
                - identifier: backend.basic_info.ProjektInfoExtractor.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 14
                  end_line: 30
                - identifier: backend.basic_info.ProjektInfoExtractor._clean_content
                  name: _clean_content
                  calls[0]:
                  called_by[0]:
                  args[2]: self,content
                  docstring: "Entfernt Null-Bytes, die durch Encoding-Fehler (UTF-16 als UTF-8 gelesen) entstehen."
                  start_line: 32
                  end_line: 36
                - identifier: backend.basic_info.ProjektInfoExtractor._finde_datei
                  name: _finde_datei
                  calls[0]:
                  called_by[0]:
                  args[3]: self,patterns,dateien
                  docstring: "Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht."
                  start_line: 38
                  end_line: 44
                - identifier: backend.basic_info.ProjektInfoExtractor._extrahiere_sektion_aus_markdown
                  name: _extrahiere_sektion_aus_markdown
                  calls[0]:
                  called_by[0]:
                  args[3]: self,inhalt,keywords
                  docstring: Extrahiert den Text unter einer Markdown-√úberschrift (##).
                  start_line: 46
                  end_line: 61
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_readme
                  name: _parse_readme
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer README-Datei.
                  start_line: 63
                  end_line: 101
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_toml
                  name: _parse_toml
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer pyproject.toml-Datei.
                  start_line: 103
                  end_line: 122
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_requirements
                  name: _parse_requirements
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer requirements.txt-Datei.
                  start_line: 124
                  end_line: 136
                - identifier: backend.basic_info.ProjektInfoExtractor.extrahiere_info
                  name: extrahiere_info
                  calls[0]:
                  called_by[2]: backend.main.main_workflow,backend.main.notebook_workflow
                  args[3]: self,dateien,repo_url
                  docstring: Orchestriert die Extraktion von Informationen.
                  start_line: 138
                  end_line: 172
    "backend/callgraph.py":
      ast_nodes:
        imports[9]: ast,networkx,os,pathlib.Path,typing.Dict,getRepo.GitRepository,getRepo.GitRepository,basic_info.ProjektInfoExtractor,os
        functions[2]:
          - mode: function_analysis
            identifier: backend.callgraph.make_safe_dot
            name: make_safe_dot
            args[2]: graph,out_path
            docstring: null
            source_code: "def make_safe_dot(graph: nx.DiGraph, out_path: str):\n    mapping = {}\n    H = graph.copy()\n    for i, n in enumerate(list(graph.nodes())):\n        safe = f\"n{i}\"           \n        mapping[n] = safe\n    H = nx.relabel_nodes(H, mapping)\n    for orig, safe in mapping.items():\n        H.nodes[safe][\"label\"] = orig\n    nx.drawing.nx_pydot.write_dot(H, out_path)"
            start_line: 173
            end_line: 182
            context:
              calls[0]:
              called_by[1]: backend.callgraph
          - mode: function_analysis
            identifier: backend.callgraph.build_filtered_callgraph
            name: build_filtered_callgraph
            args[1]: repo
            docstring: Baut den globalen Call-Graphen und filtert ihn auf selbst geschriebene Funktionen.
            source_code: "def build_filtered_callgraph(repo: GitRepository) -> nx.DiGraph:\n    \"\"\"\n    Baut den globalen Call-Graphen und filtert ihn auf selbst geschriebene Funktionen.\n    \"\"\"\n    all_file_objects = repo.get_all_files()\n    own_functions = set()\n    file_trees = {}\n\n    for file in all_file_objects:\n        if not file.path.endswith(\".py\"):\n            continue\n        filename = Path(file.path).stem\n        tree = ast.parse(file.content)\n        file_trees[filename] = tree\n        visitor = CallGraph(filename)\n        visitor.visit(tree)\n        own_functions.update(visitor.function_set)\n\n    # Globalen Call-Graph bauen\n    global_graph = nx.DiGraph()\n    for filename, tree in file_trees.items():\n        visitor = CallGraph(filename)\n        visitor.visit(tree)\n        for caller, callees in visitor.edges.items():\n            if caller not in own_functions:\n                continue \n            for callee in callees:\n                if callee in own_functions:\n                    global_graph.add_edge(caller, callee)\n                    global_graph.add_node(caller)\n                    global_graph.add_node(callee)\n    return global_graph"
            start_line: 185
            end_line: 216
            context:
              calls[1]: backend.callgraph.CallGraph
              called_by[1]: backend.callgraph
        classes[1]:
          - mode: class_analysis
            identifier: backend.callgraph.CallGraph
            name: CallGraph
            docstring: null
            source_code: "class CallGraph(ast.NodeVisitor):\n    def __init__(self, filename: str):\n        self.filename = filename\n        self.current_function = None\n        self.current_class = None\n        \n        self.local_defs: dict[str, str] = {}\n        self.graph = nx.DiGraph()\n        self.import_mapping: dict[str, str] = {}\n        self.function_set: set[str] = set()\n        self.edges: Dict[str, set[str]] = {}\n\n    def _recursive_call(self, node):\n        \"\"\"\n        Liefert eine Liste der Namenskomponenten als dotted string z.B. ['pkg', 'mod', 'Class', 'method']\n        \"\"\"\n        if isinstance(node, ast.Call):\n            return self._recursive_call(node.func)\n        if isinstance(node, ast.Name):\n            return [node.id]\n        if isinstance(node, ast.Attribute):\n            parts = self._recursive_call(node.value)\n            parts.append(node.attr)\n            return parts\n        return []\n\n    def _resolve_all_callee_names(self, callee_nodes: list[list[str]]) -> list[str]:\n        \"\"\"\n        callee_nodes ist jetzt eine Liste von Listen (Name-Steps).\n        Wir pr√ºfen zuerst lokale Definitionen, dann import_mapping.\n        \"\"\"\n        resolved = []\n        for parts in callee_nodes:\n            if not parts:\n                continue\n            simple = parts[-1]\n            dotted = \".\".join(parts[-2:]) if len(parts) >= 2 else simple\n\n            if simple in self.local_defs:\n                resolved.append(self.local_defs[simple])\n                continue\n            if dotted in self.local_defs:\n                resolved.append(self.local_defs[dotted])\n                continue\n\n            first = parts[0]\n            if first in self.import_mapping:\n                mod = self.import_mapping[first]\n                rest = \"::\".join(parts[1:]) if len(parts) > 1 else simple\n                resolved.append(f\"{mod}::{rest}\")\n                continue\n\n            if self.current_class:\n                resolved.append(f\"{self.filename}::{parts[0]}::{simple}\" if len(parts)==1 else f\"{self.filename}::{'::'.join(parts)}\")\n            else:\n                resolved.append(f\"{self.filename}::{ '::'.join(parts)}\")\n        return resolved\n\n    def _make_full_name(self, basename: str, class_name: str | None = None) -> str:\n        if class_name:\n            return f\"{self.filename}::{class_name}::{basename}\"\n        return f\"{self.filename}::{basename}\"\n    \n    def _current_caller(self) -> str:\n        if self.current_function:\n            return self.current_function\n        return f\"<{self.filename}>\" if self.filename else \"<global-scope>\"\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            module_name = alias.name\n            module_asname = alias.asname if alias.asname else module_name\n            self.import_mapping[module_asname] = module_name\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        module_name = node.module.split(\".\")[-1] if node.module else \"\"\n        for alias in node.names:\n            self.import_mapping[alias.asname or alias.name] = f\"{module_name}\" if module_name else alias.name\n\n    def visit_ClassDef(self, node: ast.ClassDef):\n        prev_class = self.current_class\n        self.current_class = node.name\n        self.generic_visit(node)\n        self.current_class = prev_class\n        \n    def visit_FunctionDef(self, node):\n        prev_function = self.current_function\n        full_name = self._make_full_name(node.name, self.current_class)\n        self.local_defs[node.name] = full_name\n        if self.current_class:\n            self.local_defs[f\"{self.current_class}.{node.name}\"] = full_name\n\n        self.current_function = full_name\n        self.graph.add_node(self.current_function)\n        self.generic_visit(node)\n        self.function_set.add(self.current_function)\n        self.current_function = prev_function\n\n    def visit_AsyncFunctionDef(self, node):\n        self.visit_FunctionDef(node)\n\n    def visit_Call(self, node):\n        caller = self._current_caller()\n        parts = self._recursive_call(node)\n        resolved_callees = self._resolve_all_callee_names([parts])\n\n        if caller not in self.edges:\n            self.edges[caller] = set()\n\n        for callee in resolved_callees:\n            if callee:\n                self.edges[caller].add(callee)\n        self.generic_visit(node)\n\n    def visit_If(self, node):\n        if (isinstance(node.test, ast.Compare) and\n            isinstance(node.test.left, ast.Name) and \n            node.test.left.id == \"__name__\"):\n            caller_backup = self.current_function\n            self.current_function = \"<main_block>\"\n            self.generic_visit(node)\n            self.current_function = caller_backup\n        else:\n            self.generic_visit(node)"
            start_line: 10
            end_line: 134
            context:
              dependencies[0]:
              instantiated_by[1]: backend.callgraph.build_filtered_callgraph
              method_context[12]:
                - identifier: backend.callgraph.CallGraph.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,filename
                  docstring: null
                  start_line: 11
                  end_line: 20
                - identifier: backend.callgraph.CallGraph._recursive_call
                  name: _recursive_call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: "Liefert eine Liste der Namenskomponenten als dotted string z.B. ['pkg', 'mod', 'Class', 'method']"
                  start_line: 22
                  end_line: 34
                - identifier: backend.callgraph.CallGraph._resolve_all_callee_names
                  name: _resolve_all_callee_names
                  calls[0]:
                  called_by[0]:
                  args[2]: self,callee_nodes
                  docstring: "callee_nodes ist jetzt eine Liste von Listen (Name-Steps).\nWir pr√ºfen zuerst lokale Definitionen, dann import_mapping."
                  start_line: 36
                  end_line: 66
                - identifier: backend.callgraph.CallGraph._make_full_name
                  name: _make_full_name
                  calls[0]:
                  called_by[0]:
                  args[3]: self,basename,class_name
                  docstring: null
                  start_line: 68
                  end_line: 71
                - identifier: backend.callgraph.CallGraph._current_caller
                  name: _current_caller
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 73
                  end_line: 76
                - identifier: backend.callgraph.CallGraph.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 78
                  end_line: 83
                - identifier: backend.callgraph.CallGraph.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 85
                  end_line: 88
                - identifier: backend.callgraph.CallGraph.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 90
                  end_line: 94
                - identifier: backend.callgraph.CallGraph.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 96
                  end_line: 107
                - identifier: backend.callgraph.CallGraph.visit_AsyncFunctionDef
                  name: visit_AsyncFunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 109
                  end_line: 110
                - identifier: backend.callgraph.CallGraph.visit_Call
                  name: visit_Call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 112
                  end_line: 123
                - identifier: backend.callgraph.CallGraph.visit_If
                  name: visit_If
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 125
                  end_line: 134
    "backend/converter.py":
      ast_nodes:
        imports[4]: logging,nbformat,base64,nbformat.reader.NotJSONError
        functions[5]:
          - mode: function_analysis
            identifier: backend.converter.wrap_cdata
            name: wrap_cdata
            args[1]: content
            docstring: Wraps content in CDATA tags.
            source_code: "def wrap_cdata(content):\n    \"\"\"Wraps content in CDATA tags.\"\"\"\n    return f\"<![CDATA[\\n{content}\\n]]>\""
            start_line: 8
            end_line: 10
            context:
              calls[0]:
              called_by[1]: backend.converter.convert_notebook_to_xml
          - mode: function_analysis
            identifier: backend.converter.extract_output_content
            name: extract_output_content
            args[2]: outputs,image_list
            docstring: "Extracts text and handles images by decoding Base64 to bytes.\nReturns: A list of text strings or placeholders."
            source_code: "def extract_output_content(outputs, image_list):\n    \"\"\"\n    Extracts text and handles images by decoding Base64 to bytes.\n    Returns: A list of text strings or placeholders.\n    \"\"\"\n    extracted_xml_snippets = []\n    \n    for output in outputs:\n\n        if output.output_type in ('display_data', 'execute_result') and hasattr(output, 'data'):\n            data = output.data\n            \n            # Helper to process image\n            def process_image(mime_type):\n                if mime_type in data:\n                    try:\n                        b64_str = data[mime_type]\n                        b64_str = b64_str.replace('\\n', '')\n\n                        image_index = len(image_list)\n                        image_list.append({\n                            \"mime_type\": mime_type,\n                            \"data\": b64_str\n                        })\n\n                        return f'\\n<IMAGE_PLACEHOLDER index=\"{image_index}\" mime=\"{mime_type}\"/>\\n'\n                    except Exception as e:\n                        return f\"<ERROR>Could not decode image: {e}</ERROR>\"\n                return None\n\n            # Ignore jpeg if png is found\n            img_xml = process_image('image/png')\n            if not img_xml:\n                img_xml = process_image('image/jpeg')\n            \n            if img_xml:\n                extracted_xml_snippets.append(img_xml)\n            elif 'text/plain' in data:\n                extracted_xml_snippets.append(data['text/plain'])\n\n        elif output.output_type == 'stream':\n            extracted_xml_snippets.append(output.text)\n            \n        elif output.output_type == 'error':\n            extracted_xml_snippets.append(f\"{output.ename}: {output.evalue}\")\n\n    return extracted_xml_snippets"
            start_line: 12
            end_line: 58
            context:
              calls[1]: backend.converter.process_image
              called_by[1]: backend.converter.convert_notebook_to_xml
          - mode: function_analysis
            identifier: backend.converter.process_image
            name: process_image
            args[1]: mime_type
            docstring: null
            source_code: "def process_image(mime_type):\n                if mime_type in data:\n                    try:\n                        b64_str = data[mime_type]\n                        b64_str = b64_str.replace('\\n', '')\n\n                        image_index = len(image_list)\n                        image_list.append({\n                            \"mime_type\": mime_type,\n                            \"data\": b64_str\n                        })\n\n                        return f'\\n<IMAGE_PLACEHOLDER index=\"{image_index}\" mime=\"{mime_type}\"/>\\n'\n                    except Exception as e:\n                        return f\"<ERROR>Could not decode image: {e}</ERROR>\"\n                return None"
            start_line: 25
            end_line: 40
            context:
              calls[0]:
              called_by[1]: backend.converter.extract_output_content
          - mode: function_analysis
            identifier: backend.converter.convert_notebook_to_xml
            name: convert_notebook_to_xml
            args[1]: file_content
            docstring: null
            source_code: "def convert_notebook_to_xml(file_content):\n    try:\n        nb = nbformat.reads(file_content, as_version=4)\n    except NotJSONError:\n        return \"<ERROR>Could not parse file as JSON/Notebook</ERROR>\", []\n\n    xml_parts = []\n    extracted_images = [] \n\n    for cell in nb.cells:\n        if cell.cell_type == 'markdown':\n            cell_xml = f'<CELL type=\"markdown\">\\n{cell.source}\\n</CELL>'\n            xml_parts.append(cell_xml)\n\n        elif cell.cell_type == 'code':\n            source_code = wrap_cdata(cell.source)\n            xml_parts.append(f'<CELL type=\"code\">\\n{source_code}\\n</CELL>')\n\n            if hasattr(cell, 'outputs') and cell.outputs:\n                snippets = extract_output_content(cell.outputs, extracted_images)\n                \n                content = \"\\n\".join(snippets)\n                \n                if content.strip():\n                    wrapped_output = wrap_cdata(content)\n                    xml_parts.append(f'<CELL type=\"output\">\\n{wrapped_output}\\n</CELL>')\n\n    return \"\\n\\n\".join(xml_parts), extracted_images"
            start_line: 60
            end_line: 87
            context:
              calls[2]: backend.converter.extract_output_content,backend.converter.wrap_cdata
              called_by[1]: backend.converter.process_repo_notebooks
          - mode: function_analysis
            identifier: backend.converter.process_repo_notebooks
            name: process_repo_notebooks
            args[1]: repo_files
            docstring: null
            source_code: "def process_repo_notebooks(repo_files):\n    notebook_files = [f for f in repo_files if f.path.endswith('.ipynb')]\n    logging.info(f\"Found {len(notebook_files)} notebooks.\")\n        \n    results = {}\n\n    for nb_file in notebook_files:\n        logging.info(f\"Processing: {nb_file.path}\")\n\n        xml_output, images = convert_notebook_to_xml(nb_file.content)\n        results[nb_file.path] = {\n            \"xml\": xml_output,\n            \"images\": images\n        }\n            \n    return results"
            start_line: 90
            end_line: 105
            context:
              calls[1]: backend.converter.convert_notebook_to_xml
              called_by[1]: backend.main.notebook_workflow
        classes[0]:
    "backend/getRepo.py":
      ast_nodes:
        imports[5]: tempfile,git.Repo,git.GitCommandError,logging,os
        functions[0]:
        classes[2]:
          - mode: class_analysis
            identifier: backend.getRepo.RepoFile
            name: RepoFile
            docstring: "Repr√§sentiert eine einzelne Datei in einem Git-Repository.\n\nDer Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tats√§chlichem Zugriff."
            source_code: "class RepoFile:\n    \"\"\"\n    Repr√§sentiert eine einzelne Datei in einem Git-Repository.\n    \n    Der Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tats√§chlichem Zugriff.\n    \"\"\"\n    def __init__(self, file_path, commit_tree):\n        \"\"\"\n        Initialisiert das RepoFile-Objekt.\n\n        Args:\n            file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n            commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt.\n        \"\"\"\n        self.path = file_path\n        self._tree = commit_tree\n        \n\n        self._blob = None\n        self._content = None\n        self._size = None\n\n    @property\n    def blob(self):\n        \"\"\"Lazy-l√§dt das Git-Blob-Objekt.\"\"\"\n        if self._blob is None:\n            try:\n                self._blob = self._tree[self.path]\n            except KeyError:\n                raise FileNotFoundError(f\"Datei '{self.path}' konnte im Commit-Tree nicht gefunden werden.\")\n        return self._blob\n\n    @property\n    def content(self):\n        \"\"\"Lazy-l√§dt und gibt den dekodierten Inhalt der Datei zur√ºck.\"\"\"\n        if self._content is None:\n            self._content = self.blob.data_stream.read().decode('utf-8', errors='ignore')\n        return self._content\n\n    @property\n    def size(self):\n        \"\"\"Lazy-l√§dt und gibt die Gr√∂√üe der Datei in Bytes zur√ºck.\"\"\"\n        if self._size is None:\n            self._size = self.blob.size\n        return self._size\n        \n    def analyze_word_count(self):\n        \"\"\"\n        Eine Beispiel-Analyse-Methode. Z√§hlt die W√∂rter im Dateiinhalt.\n        \"\"\"\n        return len(self.content.split())\n\n    def __repr__(self):\n        \"\"\"Gibt eine n√ºtzliche String-Repr√§sentation des Objekts zur√ºck.\"\"\"\n        return f\"<RepoFile(path='{self.path}')>\"\n    \n    def to_dict(self, include_content=False):\n        data = {\n            \"path\": self.path,\n            \"name\": os.path.basename(self.path),\n            \"size\": self.size,\n            \"type\": \"file\"\n        }\n        if include_content:\n            data[\"content\"] = self.content\n        return data"
            start_line: 7
            end_line: 72
            context:
              dependencies[0]:
              instantiated_by[1]: backend.getRepo.GitRepository.get_all_files
              method_context[7]:
                - identifier: backend.getRepo.RepoFile.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[3]: self,file_path,commit_tree
                  docstring: "Initialisiert das RepoFile-Objekt.\n\nArgs:\n    file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n    commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt."
                  start_line: 13
                  end_line: 27
                - identifier: backend.getRepo.RepoFile.blob
                  name: blob
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt das Git-Blob-Objekt.
                  start_line: 30
                  end_line: 37
                - identifier: backend.getRepo.RepoFile.content
                  name: content
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt und gibt den dekodierten Inhalt der Datei zur√ºck.
                  start_line: 40
                  end_line: 44
                - identifier: backend.getRepo.RepoFile.size
                  name: size
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt und gibt die Gr√∂√üe der Datei in Bytes zur√ºck.
                  start_line: 47
                  end_line: 51
                - identifier: backend.getRepo.RepoFile.analyze_word_count
                  name: analyze_word_count
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Eine Beispiel-Analyse-Methode. Z√§hlt die W√∂rter im Dateiinhalt.
                  start_line: 53
                  end_line: 57
                - identifier: backend.getRepo.RepoFile.__repr__
                  name: __repr__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Gibt eine n√ºtzliche String-Repr√§sentation des Objekts zur√ºck.
                  start_line: 59
                  end_line: 61
                - identifier: backend.getRepo.RepoFile.to_dict
                  name: to_dict
                  calls[0]:
                  called_by[0]:
                  args[2]: self,include_content
                  docstring: null
                  start_line: 63
                  end_line: 72
          - mode: class_analysis
            identifier: backend.getRepo.GitRepository
            name: GitRepository
            docstring: "Verwaltet ein Git-Repository, einschlie√ülich Klonen in ein tempor√§res\nVerzeichnis und Bereitstellung von RepoFile-Objekten."
            source_code: "class GitRepository:\n    \"\"\"\n    Verwaltet ein Git-Repository, einschlie√ülich Klonen in ein tempor√§res\n    Verzeichnis und Bereitstellung von RepoFile-Objekten.\n    \"\"\"\n    def __init__(self, repo_url):\n        self.repo_url = repo_url\n        self.temp_dir = tempfile.mkdtemp()\n        self.repo = None\n\n        self.files = []\n        \n        try:\n            logging.info(f\"Clone Repository {self.repo_url}...\")\n            self.repo = Repo.clone_from(self.repo_url, self.temp_dir)\n            self.latest_commit = self.repo.head.commit\n            self.commit_tree = self.latest_commit.tree\n            logging.info(\"Cloning successful.\")\n        except GitCommandError as e:\n            self.close()\n            raise RuntimeError(f\"Error cloning repository: {e}\") from e\n\n    def get_all_files(self):\n        \"\"\"\n        Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zur√ºck.\n\n        Returns:\n            list[RepoFile]: Eine Liste von RepoFile-Instanzen.\n        \"\"\"\n        file_paths = self.repo.git.ls_files().split('\\n')\n        self.files = [RepoFile(path, self.commit_tree) for path in file_paths if path]\n        return self.files\n\n    def close(self):\n        \"\"\"L√∂scht das tempor√§re Verzeichnis und dessen Inhalt.\"\"\"\n        if self.temp_dir:\n            print(f\"\\nL√∂sche tempor√§res Verzeichnis: {self.temp_dir}\")\n            self.temp_dir = None\n            \n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def get_file_tree(self, include_content=False):\n\n        if not self.files:\n            self.get_all_files()\n\n        tree = {\"name\": \"root\", \"type\": \"directory\", \"children\": []}\n\n        for file_obj in self.files:\n            parts = file_obj.path.split('/')\n            current_level = tree[\"children\"]\n            \n            # Iteriere durch die Ordnerstruktur\n            for part in parts[:-1]:\n                # Suche, ob Ordner schon existiert\n                found = next((item for item in current_level if item[\"name\"] == part and item[\"type\"] == \"directory\"), None)\n                if not found:\n                    new_dir = {\"name\": part, \"type\": \"directory\", \"children\": []}\n                    current_level.append(new_dir)\n                    current_level = new_dir[\"children\"]\n                else:\n                    current_level = found[\"children\"]\n            \n            # Datei am Ende hinzuf√ºgen\n            current_level.append(file_obj.to_dict(include_content=include_content))\n\n        return tree"
            start_line: 78
            end_line: 148
            context:
              dependencies[1]: backend.getRepo.RepoFile
              instantiated_by[2]: backend.main.main_workflow,backend.main.notebook_workflow
              method_context[6]:
                - identifier: backend.getRepo.GitRepository.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,repo_url
                  docstring: null
                  start_line: 83
                  end_line: 98
                - identifier: backend.getRepo.GitRepository.get_all_files
                  name: get_all_files
                  calls[1]: backend.getRepo.RepoFile
                  called_by[0]:
                  args[1]: self
                  docstring: "Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zur√ºck.\n\nReturns:\n    list[RepoFile]: Eine Liste von RepoFile-Instanzen."
                  start_line: 100
                  end_line: 109
                - identifier: backend.getRepo.GitRepository.close
                  name: close
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: L√∂scht das tempor√§re Verzeichnis und dessen Inhalt.
                  start_line: 111
                  end_line: 115
                - identifier: backend.getRepo.GitRepository.__enter__
                  name: __enter__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 117
                  end_line: 118
                - identifier: backend.getRepo.GitRepository.__exit__
                  name: __exit__
                  calls[0]:
                  called_by[0]:
                  args[4]: self,exc_type,exc_val,exc_tb
                  docstring: null
                  start_line: 120
                  end_line: 121
                - identifier: backend.getRepo.GitRepository.get_file_tree
                  name: get_file_tree
                  calls[0]:
                  called_by[0]:
                  args[2]: self,include_content
                  docstring: null
                  start_line: 123
                  end_line: 148
    "backend/main.py":
      ast_nodes:
        imports[28]: json,math,logging,os,re,time,math,datetime.datetime,matplotlib.pyplot,datetime.datetime,pathlib.Path,dotenv.load_dotenv,getRepo.GitRepository,AST_Schema.ASTAnalyzer,MainLLM.MainLLM,basic_info.ProjektInfoExtractor,HelperLLM.LLMHelper,relationship_analyzer.ProjectAnalyzer,schemas.types.FunctionContextInput,schemas.types.FunctionAnalysisInput,schemas.types.ClassContextInput,schemas.types.ClassAnalysisInput,schemas.types.MethodContextInput,toon_format.encode,toon_format.count_tokens,toon_format.estimate_savings,toon_format.compare_formats,converter.process_repo_notebooks
        functions[7]:
          - mode: function_analysis
            identifier: backend.main.create_savings_chart
            name: create_savings_chart
            args[4]: json_tokens,toon_tokens,savings_percent,output_path
            docstring: Erstellt ein Balkendiagramm f√ºr den Token-Vergleich und speichert es.
            source_code: "def create_savings_chart(json_tokens, toon_tokens, savings_percent, output_path):\n    \"\"\"Erstellt ein Balkendiagramm f√ºr den Token-Vergleich und speichert es.\"\"\"\n    labels = ['JSON', 'TOON']\n    values = [json_tokens, toon_tokens]\n    colors = ['#ff9999', '#66b3ff']\n\n    plt.figure(figsize=(8, 6))\n    bars = plt.bar(labels, values, color=colors, width=0.5)\n\n    # Titel und Beschriftungen\n    plt.title(f'Token-Vergleich: {savings_percent:.2f}% Einsparung', fontsize=14)\n    plt.ylabel('Anzahl Token')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n\n    # Werte √ºber den Balken anzeigen\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width() / 2.0, height, \n                 f'{int(height):,}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n\n    # Speichern\n    plt.savefig(output_path)\n    plt.close()"
            start_line: 33
            end_line: 55
            context:
              calls[0]:
              called_by[1]: backend.main.main_workflow
          - mode: function_analysis
            identifier: backend.main.calculate_net_time
            name: calculate_net_time
            args[5]: start_time,end_time,total_items,batch_size,model_name
            docstring: Berechnet die Dauer abz√ºglich der Sleep-Zeiten f√ºr Rate-Limits.
            source_code: "def calculate_net_time(start_time, end_time, total_items, batch_size, model_name):\n    \"\"\"Berechnet die Dauer abz√ºglich der Sleep-Zeiten f√ºr Rate-Limits.\"\"\"\n    total_duration = end_time - start_time\n    \n    if not model_name.startswith(\"gemini-\"):\n        return total_duration\n\n    if total_items == 0:\n        return 0\n\n    num_batches = math.ceil(total_items / batch_size)\n    sleep_count = max(0, num_batches - 1)\n    total_sleep_time = sleep_count * 61\n    \n    net_time = total_duration - total_sleep_time\n    return max(0, net_time)"
            start_line: 57
            end_line: 72
            context:
              calls[0]:
              called_by[1]: backend.main.main_workflow
          - mode: function_analysis
            identifier: backend.main.main_workflow
            name: main_workflow
            args[4]: input,api_keys,model_names,status_callback
            docstring: null
            source_code: "def main_workflow(input, api_keys: dict, model_names: dict, status_callback=None):\n    \n    def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)\n\n    update_status(\"üîç Analysiere Input...\")\n    \n    user_input = input\n    \n    # API Key & Ollama Base URL aus Frontend holen\n    gemini_api_key = api_keys.get(\"gemini\")\n    openai_api_key = api_keys.get(\"gpt\")\n    scadsllm_api_key = api_keys.get(\"scadsllm\")\n    scadsllm_base_url = api_keys.get(\"scadsllm_base_url\")\n    ollama_base_url = api_keys.get(\"ollama\")\n    base_url = None\n\n    if model_names[\"helper\"].startswith(\"gpt-\"):\n        helper_api_key = openai_api_key\n    elif model_names[\"helper\"].startswith(\"gemini-\"):\n        helper_api_key = gemini_api_key\n    elif \"/\" in model_names[\"helper\"] or model_names[\"helper\"].startswith(\"alias-\") or any(x in model_names[\"helper\"] for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        helper_api_key = scadsllm_api_key\n        base_url = scadsllm_base_url\n    else:\n        helper_api_key = None\n        base_url = ollama_base_url\n    if model_names[\"main\"].startswith(\"gpt-\"):\n        main_api_key = openai_api_key\n    elif model_names[\"main\"].startswith(\"gemini-\"):\n        main_api_key = gemini_api_key\n    elif \"/\" in model_names[\"main\"] or model_names[\"main\"].startswith(\"alias-\") or any(x in model_names[\"main\"] for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        main_api_key = scadsllm_api_key\n        base_url = scadsllm_base_url\n    else:\n        main_api_key = None\n        base_url = ollama_base_url\n\n    # Standardeinstellungen f√ºr Modelle\n    helper_model = model_names.get(\"helper\", \"gpt-5-mini\")\n    main_model = model_names.get(\"main\", \"gpt-5.1\")\n\n    # Error Handling f√ºr fehlende API Keys\n    if not gemini_api_key and \"gemini\" in helper_model:\n        raise ValueError(\"Gemini API Key was not provided in api_keys dictionary.\")\n\n    # URL Extraktion\n    repo_url = None\n    url_pattern = r\"https?://(?:www\\.)?github\\.com/[^\\s]+\"\n    match = re.search(url_pattern, user_input)\n\n    if match:\n        repo_url = match.group(0)\n        logging.info(f\"Extracted repository URL: {repo_url}\")\n    else:\n        raise ValueError(\"Could not find a valid URL in the provided input.\")\n    \n    # Repo klonen und Dateien extrahieren\n    update_status(f\"‚¨áÔ∏è Klone Repository: {repo_url} ...\")\n    \n    repo_files = []\n    local_repo_path = \"\" \n\n    try: \n\n        with GitRepository(repo_url) as repo:\n            repo_files = repo.get_all_files()\n            local_repo_path = repo.temp_dir\n\n            logging.info(f\"Total files retrieved: {len(repo_files)}\")\n\n    except Exception as e:\n        logging.error(f\"Error cloning repository: {e}\")\n        raise \n\n\n    # Extrahiere Basic Infos\n    update_status(\"‚ÑπÔ∏è Extrahiere Basis-Informationen...\")\n    try:\n        info_extractor = ProjektInfoExtractor()\n        basic_project_info = info_extractor.extrahiere_info(dateien=repo_files, repo_url=repo_url)\n        logging.info(\"Basic project info extracted\")\n    except Exception as e:\n        logging.error(f\"Error extracting basic project info: {e}\")\n        basic_project_info = \"Could not extract basic info.\"\n\n    # Erstelle Repository Dateibaum\n    update_status(\"üå≤ Erstelle Repository Dateibaum...\")\n    try:\n        repo_file_tree = repo.get_file_tree()\n        logging.info(\"Repository file tree constructed\")\n    except Exception as e:\n        logging.error(f\"Error constructing repository file tree: {e}\")\n        repo_file_tree = \"Could not create file tree.\"\n\n    # Relationship Analyse durchf√ºhren\n    update_status(\"üîó Analysiere Beziehungen (Calls & Instanziierungen)...\")\n    try:\n        rel_analyzer = ProjectAnalyzer(project_root=local_repo_path)\n        rel_analyzer.analyze()\n        \n        raw_relationships = rel_analyzer.get_raw_relationships()\n        \n        logging.info(f\"Relationships analyzed.\")\n    except Exception as e:\n        logging.error(f\"Error in relationship analyzer: {e}\")\n        raw_relationships = {\"outgoing\": {}, \"incoming\": {}}\n\n    # Erstelle AST Schema\n    update_status(\"üå≥ Erstelle Abstract Syntax Tree (AST)...\")\n    try:        \n        ast_analyzer = ASTAnalyzer()   \n        ast_schema = ast_analyzer.analyze_repository(files=repo_files, repo=repo)\n        logging.info(\"AST schema created\")\n    except Exception as e:\n        logging.error(f\"Error retrieving repository files: {e}\")\n        raise\n\n    # Anreichern des AST Schemas mit Relationship Daten\n    update_status(\"‚ûï Reiche AST mit Beziehungsdaten an...\")            \n    try:   \n        ast_schema = ast_analyzer.merge_relationship_data(ast_schema, raw_relationships)\n        logging.info(\"AST schema created and enriched\")\n\n    except Exception as e:\n        logging.error(f\"Error processing repository: {e}\")\n        raise\n\n    # Vorbereitung der HelperLLM Eingaben\n    update_status(\"‚öôÔ∏è Bereite Daten f√ºr Helper LLM vor...\")\n    \n    helper_llm_function_input = []\n    helper_llm_class_input = []\n\n    try:\n        for filename, file_data in ast_schema['files'].items():\n            ast_nodes = file_data.get('ast_nodes', {})\n            imports = ast_nodes.get('imports', [])\n            functions = ast_nodes.get('functions', [])\n            classes = ast_nodes.get('classes', [])\n\n            for function in functions:\n                context = function.get('context', {})\n                \n                raw_called_by = context.get('called_by', [])\n                clean_called_by = [cb for cb in raw_called_by if isinstance(cb, dict)]\n\n                function_context = FunctionContextInput(\n                    calls = context.get('calls', []),\n                    called_by = clean_called_by \n                )\n                \n                function_input = FunctionAnalysisInput(\n                    mode = function.get('mode', 'function_analysis'),\n                    identifier = function.get('identifier'),\n                    source_code = function.get('source_code'),\n                    imports = imports,\n                    context = function_context\n                )\n                \n                helper_llm_function_input.append(function_input)\n\n            for _class in classes:\n                context = _class.get('context', {})\n                \n                method_context_inputs = []\n                for method in context.get('method_context', []):\n                    \n                    raw_method_called_by = method.get('called_by', [])\n                    clean_method_called_by = [cb for cb in raw_method_called_by if isinstance(cb, dict)]\n\n                    method_context_inputs.append(\n                        MethodContextInput(\n                            identifier=method.get('identifier'),\n                            calls=method.get('calls', []),\n                            called_by=clean_method_called_by, \n                            args=method.get('args', []),\n                            docstring=method.get('docstring')\n                        )\n                    )\n\n                raw_instantiated_by = context.get('instantiated_by', [])\n                clean_instantiated_by = [ib for ib in raw_instantiated_by if isinstance(ib, dict)]\n\n                class_context = ClassContextInput(\n                    dependencies = context.get('dependencies', []),\n                    instantiated_by = clean_instantiated_by, \n                    method_context = method_context_inputs\n                )\n\n                class_input = ClassAnalysisInput(\n                    mode = _class.get('mode', 'class_analysis'),\n                    identifier =_class.get('identifier'),\n                    source_code = _class.get('source_code'), \n                    imports = imports, \n                    context = class_context\n                )\n                \n                helper_llm_class_input.append(class_input)\n\n    except Exception as e:\n        logging.error(f\"Error preparing inputs for Helper LLM: {e}\")\n        raise\n    \n    # Initialisiere HelperLLM\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    \n    llm_helper = LLMHelper(\n        api_key=helper_api_key, \n        function_prompt_path=function_prompt_file, \n        class_prompt_path=class_prompt_file,\n        model_name=helper_model,\n        base_url=base_url,\n    )\n    \n    if ollama_base_url:\n        os.environ[\"OLLAMA_BASE_URL\"] = ollama_base_url\n\n    # Initialisiere Ergebniscontainer\n    analysis_results = {}\n    function_analysis_results = []\n    class_analysis_results = []\n\n    # Call HelperLLM f√ºr Funktionen\n    update_status(f\"ü§ñ Helper LLM: Analysiere {len(helper_llm_function_input)} Funktionen ({helper_model})...\")\n\n    try:\n        net_time_func = 0\n        if len(helper_llm_function_input) > 0:\n\n            logging.info(\"\\n--- Generating documentation for Functions ---\")\n            t_start_func = time.time()    \n            function_analysis_results = llm_helper.generate_for_functions(helper_llm_function_input)    \n            t_end_func = time.time()\n            net_time_func = calculate_net_time(t_start_func, t_end_func, len(helper_llm_function_input), llm_helper.batch_size, helper_model)\n\n        if len(function_analysis_results) != 0:\n            for doc in function_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"functions\" not in analysis_results:\n                        analysis_results[\"functions\"] = {}\n                    analysis_results[\"functions\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(f\"Failed to generate doc for a function\") \n    except Exception as e:\n        logging.error(f\"Error during Helper LLM function analysis: {e}\")\n        raise\n    \n\n    # Call HelperLLM f√ºr Klassen\n    try:\n        net_time_class = 0\n        if len(helper_llm_class_input) > 0:\n            # Rate Limit Sleep f√ºr Gemini Modelle\n            if llm_helper.model_name.startswith(\"gemini-\") & (len(helper_llm_function_input) > 0):\n                update_status(\"üí§ Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n                time.sleep(65)\n            \n            update_status(f\"ü§ñ Helper LLM: Analysiere {len(helper_llm_class_input)} Klassen ({helper_model})...\")\n            \n            logging.info(\"\\n--- Generating documentation for Classes ---\")\n            t_start_class = time.time()\n            class_analysis_results = llm_helper.generate_for_classes(helper_llm_class_input)\n            t_end_class = time.time()\n            net_time_class = calculate_net_time(t_start_class, t_end_class, len(helper_llm_class_input), llm_helper.batch_size, helper_model)\n\n        if len(class_analysis_results) != 0:\n            for doc in class_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"classes\" not in analysis_results:\n                        analysis_results[\"classes\"] = {}\n                    analysis_results[\"classes\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(f\"Failed to generate doc for a class\")\n    except Exception as e:\n        logging.error(f\"Error during Helper LLM class analysis: {e}\")\n        raise\n\n    total_helper_time = net_time_func + net_time_class\n\n    # MainLLM Input Vorbereitung\n    main_llm_input = {\n        \"basic_info\": basic_project_info,\n        \"file_tree\": repo_file_tree,\n        \"ast_schema\": ast_schema,\n        \"analysis_results\": analysis_results\n    }\n\n    # Speichern als JSON (Optional)\n    main_llm_input_json = json.dumps(main_llm_input, indent=2)\n    # with open(\"output.json\", \"w\", encoding=\"utf-8\") as f:\n    #     f.write(main_llm_input_json)\n    #     logging.info(\"JSON-Datei wurde gespeichert.\")\n\n    # Konvertiere Input in Toon Format\n    main_llm_input_toon = encode(main_llm_input)\n\n    #Speichern in TOON Format (Optional)\n    # with open(\"output.toon\", \"w\", encoding=\"utf-8\") as f:\n    #     f.write(main_llm_input_toon)\n    #     logging.info(\"output.toon erfolgreich gespeichert.\")\n    \n    # Token Evaluation\n    savings_data = None\n    try:\n        logging.info(\"--- Evaluierung der Token-Ersparnis ---\")\n        savings_data = estimate_savings(main_llm_input)\n        logging.info(f\"JSON Tokens: {savings_data['json_tokens']}\")\n        logging.info(f\"TOON Tokens: {savings_data['toon_tokens']}\")\n        logging.info(f\"Ersparnis:   {savings_data['savings_percent']:.2f}%\")\n        \n    except Exception as e:\n        logging.warning(f\"Token evaluation could not be performed: {e}\")    \n\n\n\n    prompt_file_mainllm = \"SystemPrompts/SystemPromptMainLLM.txt\"\n    prompt_file_mainllm_toon = \"SystemPrompts/SystemPromptMainLLMToon.txt\"\n    # MainLLM Ausf√ºhrung\n    main_llm = MainLLM(\n        api_key=main_api_key, \n        prompt_file_path=prompt_file_mainllm_toon,\n        model_name=main_model,\n        base_url=base_url,\n    )\n\n\n    # RPM Limit Sleep f√ºr Gemini Modelle\n    if llm_helper.model_name == main_llm.model_name and main_llm.model_name.startswith(\"gemini-\"):\n        time.sleep(65)\n        update_status(\"üí§ Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n\n    # Call MainLLM f√ºr finalen Report\n    update_status(f\"üß† Main LLM: Generiere finalen Report ({main_model})...\")\n    try:\n        total_main_time = 0\n        logging.info(\"\\n--- Generating Final Report ---\")\n        t_start_main = time.time()\n        final_report = main_llm.call_llm(main_llm_input_toon)\n        t_end_main = time.time()\n        total_main_time = t_end_main - t_start_main\n    except Exception as e:\n        logging.error(f\"Error during Main LLM final report generation: {e}\")\n        raise\n\n    \n    # --- Speichern der Ergebnisse ---\n    output_dir = \"result\"\n    stats_dir = \"Statistics\" # Neuer Ordner\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(stats_dir, exist_ok=True) # Stelle sicher, dass Statistics Ordner existiert\n\n    total_active_time = total_helper_time + total_main_time\n    \n    # Dateiname f√ºr Report\n    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M-%S\")\n    report_filename = f\"report_{timestamp}_Helper_{llm_helper.model_name}_MainLLM_{main_llm.model_name}.md\"\n    report_filepath = os.path.join(output_dir, report_filename)\n    \n    if final_report:\n        with open(report_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(final_report)\n        logging.info(f\"Final report saved to '{report_filepath}'.\")\n        \n        if savings_data:\n            try:\n                savings_filename = report_filename.replace(\"report_\", \"savings_\").replace(\".md\", \".png\")\n                savings_filepath = os.path.join(stats_dir, savings_filename)\n                \n                logging.info(f\"Erstelle Token-Chart: {savings_filepath}\")\n                create_savings_chart(\n                    json_tokens=savings_data['json_tokens'], \n                    toon_tokens=savings_data['toon_tokens'], \n                    savings_percent=savings_data['savings_percent'],\n                    output_path=savings_filepath\n                )\n            except Exception as chart_error:\n                logging.error(f\"Konnte Diagramm nicht erstellen: {chart_error}\")\n\n    else:\n        final_report = \"Error: Report generation failed or returned empty.\"\n\n    metrics = {\n        \"helper_time\": round(total_helper_time, 2),\n        \"main_time\": round(total_main_time, 2),\n        \"total_time\": round(total_active_time, 2),\n        \"helper_model\": helper_model,\n        \"main_model\": main_model,\n        \"json_tokens\": savings_data['json_tokens'] if savings_data else None,\n        \"toon_tokens\": savings_data['toon_tokens'] if savings_data else None,\n        \"savings_percent\": round(savings_data['savings_percent'], 2) if savings_data else None\n    \n    }\n\n    return {\n        \"report\": final_report,\n        \"metrics\": metrics\n    }"
            start_line: 75
            end_line: 477
            context:
              calls[22]: backend.AST_Schema.ASTAnalyzer,backend.AST_Schema.ASTAnalyzer.analyze_repository,backend.AST_Schema.ASTAnalyzer.merge_relationship_data,backend.HelperLLM.LLMHelper,backend.HelperLLM.LLMHelper.generate_for_classes,backend.HelperLLM.LLMHelper.generate_for_functions,backend.MainLLM.MainLLM,backend.MainLLM.MainLLM.call_llm,backend.basic_info.ProjektInfoExtractor,backend.basic_info.ProjektInfoExtractor.extrahiere_info,backend.getRepo.GitRepository,backend.main.calculate_net_time,backend.main.create_savings_chart,backend.main.update_status,backend.relationship_analyzer.ProjectAnalyzer,backend.relationship_analyzer.ProjectAnalyzer.analyze,backend.relationship_analyzer.ProjectAnalyzer.get_raw_relationships,schemas.types.ClassAnalysisInput,schemas.types.ClassContextInput,schemas.types.FunctionAnalysisInput,schemas.types.FunctionContextInput,schemas.types.MethodContextInput
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: backend.main.update_status
            name: update_status
            args[1]: msg
            docstring: null
            source_code: "def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)"
            start_line: 77
            end_line: 80
            context:
              calls[0]:
              called_by[2]: backend.main.main_workflow,backend.main.notebook_workflow
          - mode: function_analysis
            identifier: backend.main.notebook_workflow
            name: notebook_workflow
            args[4]: input,api_keys,model,status_callback
            docstring: null
            source_code: "def notebook_workflow(input, api_keys, model, status_callback=None):\n    t_start = time.time()\n    final_report = \"keine Notebooks gefunden\"\n    def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)\n\n    def gemini_payload(basic_info, nb_path, xml_content, images):\n        \n        intro_json = json.dumps({\n            \"basic_info\": basic_info,\n            \"current_notebook_path\": nb_path\n        }, indent=2)\n        \n        payload_content = []\n\n        payload_content.append({\n            \"type\": \"text\",\n            \"text\": f\"Context Information:\\n{intro_json}\\n\\nNotebook XML Structure:\\n\"\n        })\n\n        # Regex to find: <IMAGE_PLACEHOLDER index=\"0\" mime=\"image/png\"/>\n        pattern = r'(<IMAGE_PLACEHOLDER index=\"(\\d+)\" mime=\"([^\"]+)\"/>)'\n        last_pos = 0\n        \n        for match in re.finditer(pattern, xml_content):\n            text_segment = xml_content[last_pos:match.start()]\n            if text_segment.strip():\n                payload_content.append({\n                    \"type\": \"text\",\n                    \"text\": text_segment\n                })\n                \n            image_index = int(match.group(2))\n            mime_type = match.group(3)\n            \n            if image_index < len(images):\n                img_data = images[image_index]\n                b64_string = img_data['data']\n                \n                payload_content.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:{mime_type};base64,{b64_string}\"\n                    }\n                })\n                \n            last_pos = match.end()\n\n        # Add any remaining text after the last image\n        remaining_text = xml_content[last_pos:]\n        if remaining_text.strip():\n            payload_content.append({\n                \"type\": \"text\",\n                \"text\": remaining_text\n            })\n\n        return payload_content\n\n\n    base_url = None\n\n    if model.startswith(\"gpt-\"):\n        input_api_key = api_keys.get(\"gpt\")\n    elif model.startswith(\"gemini-\"):\n        input_api_key = api_keys.get(\"gemini\")\n    elif \"/\" in model or model.startswith(\"alias-\") or any(x in model for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        input_api_key = api_keys.get(\"scadsllm\")\n        base_url = api_keys.get(\"scadsllm_base_url\")\n    else:\n        input_api_key = None\n        base_url = api_keys.get(\"ollama\")\n\n    update_status(\"üîç Analysiere Input...\")\n\n    # URL Extraktion\n    repo_url = None\n    url_pattern = r\"https?://(?:www\\.)?github\\.com/[^\\s]+\"\n    match = re.search(url_pattern, input)\n\n    if match:\n        repo_url = match.group(0)\n        logging.info(f\"Extracted repository URL: {repo_url}\")\n    else:\n        raise ValueError(\"Could not find a valid URL in the provided input.\")\n    \n    update_status(f\"‚¨áÔ∏è Klone Repository: {repo_url} ...\")\n\n    try: \n        with GitRepository(repo_url) as repo:\n            repo_files = repo.get_all_files()\n    except Exception as e:\n        logging.error(f\"Error cloning repository: {e}\")\n        raise\n    \n    update_status(f\"üîó Bereite Input vor...\")\n\n    # convert to XML\n    processed_data = process_repo_notebooks(repo_files)\n\n    \n    # Extrahiere Basic Infos\n    update_status(\"‚ÑπÔ∏è Extrahiere Basis-Informationen...\")\n    try:\n        info_extractor = ProjektInfoExtractor()\n        basic_project_info = info_extractor.extrahiere_info(dateien=repo_files, repo_url=repo_url)\n        logging.info(\"Basic project info extracted\")\n    except Exception as e:\n        logging.error(f\"Error extracting basic project info: {e}\")\n        basic_project_info = \"Could not extract basic info.\"\n\n\n\n    prompt_file_notebook_llm = \"SystemPrompts/SystemPromptNotebookLLM.txt\"\n    notebook_llm = MainLLM(\n        api_key=input_api_key, \n        prompt_file_path=prompt_file_notebook_llm,\n        model_name=model,\n        base_url=base_url,\n    )\n\n    notebook_reports = []\n    total_notebooks = len(processed_data)\n    \n    logging.info(f\"Starting sequential processing of {total_notebooks} notebooks...\")\n\n    # Iterate over each notebook file individually\n    for index, (nb_path, nb_data) in enumerate(processed_data.items(), 1):\n        \n        update_status(f\"üß† Generiere Report ({index}/{total_notebooks}): {os.path.basename(nb_path)}\")\n        \n        nb_xml = nb_data['xml']\n        nb_images = nb_data['images']\n\n        llm_payload = gemini_payload(basic_project_info, nb_path, nb_xml, nb_images)\n\n        try:\n            single_report = notebook_llm.call_llm(llm_payload)\n            if single_report and isinstance(single_report, str):\n                notebook_reports.append(single_report)\n            else:\n                logging.warning(f\"LLM lieferte kein Ergebnis f√ºr {nb_path}\")\n                notebook_reports.append(f\"## Analyse f√ºr {os.path.basename(nb_path)}\\nFehler: Das Modell lieferte keine Antwort.\")\n            \n        except Exception as e:\n            logging.error(f\"Error generating report for {nb_path}: {e}\")\n            notebook_reports.append(f\"# Error processing {nb_path}\\n\\nError: {str(e)}\\n\\n---\\n\")\n\n    # Concatenate all reports\n    final_report = \"\\n\\n<br>\\n<br>\\n<br>\\n\\n\".join([str(r) for r in notebook_reports if r is not None])\n\n    \n    # --- Speichern der Ergebnisse ---\n    output_dir = \"result\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Dateiname f√ºr Report\n    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M-%S\")\n    report_filename = f\"notebook_report_{timestamp}_NotebookLLM_{notebook_llm.model_name}.md\"\n    report_filepath = os.path.join(output_dir, report_filename)\n    \n    if final_report:\n        with open(report_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(final_report)\n        logging.info(f\"Final report saved to '{report_filepath}'.\")\n\n    # Am Ende der Funktion nach dem Speichern:\n    total_time = time.time() - t_start\n    \n    # Metriken f√ºr das Frontend bauen\n    metrics = {\n        \"helper_time\": \"0\", # Notebook-Workflow hat meist keinen separaten Helper\n        \"main_time\": round(total_time, 2),\n        \"total_time\": round(total_time, 2),\n        \"helper_model\": \"None\",\n        \"main_model\": model,\n        \"json_tokens\": 0,\n        \"toon_tokens\": 0,\n        \"savings_percent\": 0\n    }\n\n    return {\n        \"report\": final_report,\n        \"metrics\": metrics\n    }"
            start_line: 483
            end_line: 668
            context:
              calls[8]: backend.MainLLM.MainLLM,backend.MainLLM.MainLLM.call_llm,backend.basic_info.ProjektInfoExtractor,backend.basic_info.ProjektInfoExtractor.extrahiere_info,backend.converter.process_repo_notebooks,backend.getRepo.GitRepository,backend.main.gemini_payload,backend.main.update_status
              called_by[2]: backend.main,frontend.frontend
          - mode: function_analysis
            identifier: backend.main.update_status
            name: update_status
            args[1]: msg
            docstring: null
            source_code: "def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)"
            start_line: 486
            end_line: 489
            context:
              calls[0]:
              called_by[2]: backend.main.main_workflow,backend.main.notebook_workflow
          - mode: function_analysis
            identifier: backend.main.gemini_payload
            name: gemini_payload
            args[4]: basic_info,nb_path,xml_content,images
            docstring: null
            source_code: "def gemini_payload(basic_info, nb_path, xml_content, images):\n        \n        intro_json = json.dumps({\n            \"basic_info\": basic_info,\n            \"current_notebook_path\": nb_path\n        }, indent=2)\n        \n        payload_content = []\n\n        payload_content.append({\n            \"type\": \"text\",\n            \"text\": f\"Context Information:\\n{intro_json}\\n\\nNotebook XML Structure:\\n\"\n        })\n\n        # Regex to find: <IMAGE_PLACEHOLDER index=\"0\" mime=\"image/png\"/>\n        pattern = r'(<IMAGE_PLACEHOLDER index=\"(\\d+)\" mime=\"([^\"]+)\"/>)'\n        last_pos = 0\n        \n        for match in re.finditer(pattern, xml_content):\n            text_segment = xml_content[last_pos:match.start()]\n            if text_segment.strip():\n                payload_content.append({\n                    \"type\": \"text\",\n                    \"text\": text_segment\n                })\n                \n            image_index = int(match.group(2))\n            mime_type = match.group(3)\n            \n            if image_index < len(images):\n                img_data = images[image_index]\n                b64_string = img_data['data']\n                \n                payload_content.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:{mime_type};base64,{b64_string}\"\n                    }\n                })\n                \n            last_pos = match.end()\n\n        # Add any remaining text after the last image\n        remaining_text = xml_content[last_pos:]\n        if remaining_text.strip():\n            payload_content.append({\n                \"type\": \"text\",\n                \"text\": remaining_text\n            })\n\n        return payload_content"
            start_line: 491
            end_line: 541
            context:
              calls[0]:
              called_by[1]: backend.main.notebook_workflow
        classes[0]:
    "backend/relationship_analyzer.py":
      ast_nodes:
        imports[4]: ast,os,logging,collections.defaultdict
        functions[1]:
          - mode: function_analysis
            identifier: backend.relationship_analyzer.path_to_module
            name: path_to_module
            args[2]: filepath,project_root
            docstring: Wandelt einen Dateipfad in einen Python-Modulpfad um.
            source_code: "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    module_path = rel_path.replace(os.path.sep, '.')\n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n    return module_path"
            start_line: 6
            end_line: 18
            context:
              calls[0]:
              called_by[2]: backend.relationship_analyzer.CallResolverVisitor.__init__,backend.relationship_analyzer.ProjectAnalyzer._collect_definitions
        classes[2]:
          - mode: class_analysis
            identifier: backend.relationship_analyzer.ProjectAnalyzer
            name: ProjectAnalyzer
            docstring: null
            source_code: "class ProjectAnalyzer:\n    \n    def __init__(self, project_root):\n        self.project_root = os.path.abspath(project_root)\n        self.definitions = {}\n        self.call_graph = defaultdict(list)\n        self.file_asts = {} \n        self.ignore_dirs = {'.git', '.venv', 'venv', '__pycache__', 'node_modules', 'dist', 'build', 'docs'}\n\n    def analyze(self):\n        py_files = self._find_py_files()\n        \n        for filepath in py_files:\n            self._collect_definitions(filepath)\n            \n        for filepath in py_files:\n            self._resolve_calls(filepath)\n            \n        self.file_asts.clear()\n        \n        return self.call_graph\n\n    def get_raw_relationships(self):\n\n        outgoing = defaultdict(set)\n        incoming = defaultdict(set)\n\n        for callee_id, callers_info_list in self.call_graph.items():\n            for info in callers_info_list:\n                caller_id = info['caller']\n                \n                if caller_id and callee_id:\n                    outgoing[caller_id].add(callee_id)\n                    incoming[callee_id].add(caller_id)\n        \n        return {\n            \"outgoing\": {k: sorted(list(v)) for k, v in outgoing.items()},\n            \"incoming\": {k: sorted(list(v)) for k, v in incoming.items()}\n        }\n\n    def _find_py_files(self):\n        py_files = []\n        for root, dirs, files in os.walk(self.project_root):\n            dirs[:] = [d for d in dirs if d not in self.ignore_dirs]\n            for file in files:\n                if file.endswith(\".py\"):\n                    py_files.append(os.path.join(root, file))\n        return py_files\n\n    def _collect_definitions(self, filepath):\n        try:\n            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n                source = f.read()\n                tree = ast.parse(source, filename=filepath)\n                \n            self.file_asts[filepath] = tree\n            module_path = path_to_module(filepath, self.project_root)\n            \n            for node in ast.walk(tree):\n                if isinstance(node, ast.FunctionDef):\n                    parent = self._get_parent(tree, node)\n                    if isinstance(parent, ast.ClassDef):\n                        path_name = f\"{module_path}.{parent.name}.{node.name}\"\n                        def_type = 'method'\n                    else:\n                        path_name = f\"{module_path}.{node.name}\"\n                        def_type = 'function'\n                    self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': def_type}\n                elif isinstance(node, ast.ClassDef):\n                    path_name = f\"{module_path}.{node.name}\"\n                    self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': 'class'}\n        except Exception as e:\n            logging.error(f\"Error collecting definitions in {filepath}: {e}\")\n            self.file_asts[filepath] = None\n\n    def _get_parent(self, tree, node):\n        for parent in ast.walk(tree):\n            for child in ast.iter_child_nodes(parent):\n                if child is node:\n                    return parent\n        return None\n\n    def _resolve_calls(self, filepath):\n        tree = self.file_asts.get(filepath)\n        if not tree:\n            return\n\n        try:\n            resolver = CallResolverVisitor(filepath, self.project_root, self.definitions)\n            resolver.visit(tree)\n            \n            for callee_pathname, caller_info_list in resolver.calls.items():\n                self.call_graph[callee_pathname].extend(caller_info_list)\n        except Exception as e:\n            logging.error(f\"Error resolving calls in {filepath}: {e}\")"
            start_line: 20
            end_line: 114
            context:
              dependencies[2]: backend.relationship_analyzer.CallResolverVisitor,backend.relationship_analyzer.path_to_module
              instantiated_by[1]: backend.main.main_workflow
              method_context[7]:
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,project_root
                  docstring: null
                  start_line: 22
                  end_line: 27
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.analyze
                  name: analyze
                  calls[0]:
                  called_by[1]: backend.main.main_workflow
                  args[1]: self
                  docstring: null
                  start_line: 29
                  end_line: 40
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.get_raw_relationships
                  name: get_raw_relationships
                  calls[0]:
                  called_by[1]: backend.main.main_workflow
                  args[1]: self
                  docstring: null
                  start_line: 42
                  end_line: 58
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._find_py_files
                  name: _find_py_files
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 60
                  end_line: 67
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._collect_definitions
                  name: _collect_definitions
                  calls[1]: backend.relationship_analyzer.path_to_module
                  called_by[0]:
                  args[2]: self,filepath
                  docstring: null
                  start_line: 69
                  end_line: 93
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._get_parent
                  name: _get_parent
                  calls[0]:
                  called_by[0]:
                  args[3]: self,tree,node
                  docstring: null
                  start_line: 95
                  end_line: 100
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._resolve_calls
                  name: _resolve_calls
                  calls[1]: backend.relationship_analyzer.CallResolverVisitor
                  called_by[0]:
                  args[2]: self,filepath
                  docstring: null
                  start_line: 102
                  end_line: 114
          - mode: class_analysis
            identifier: backend.relationship_analyzer.CallResolverVisitor
            name: CallResolverVisitor
            docstring: null
            source_code: "class CallResolverVisitor(ast.NodeVisitor):\n    def __init__(self, filepath, project_root, definitions):\n        self.filepath = filepath\n        self.module_path = path_to_module(filepath, project_root)\n        self.definitions = definitions\n        self.scope = {}\n        self.instance_types = {}\n        self.current_caller_name = self.module_path\n        self.current_class_name = None\n        self.calls = defaultdict(list)\n\n    def visit_ClassDef(self, node):\n        old_class_name = self.current_class_name\n        self.current_class_name = node.name\n        self.generic_visit(node)\n        self.current_class_name = old_class_name\n\n    def visit_FunctionDef(self, node):\n        old_caller = self.current_caller_name\n        \n        if self.current_class_name:\n            full_identifier = f\"{self.module_path}.{self.current_class_name}.{node.name}\"\n        else:\n            full_identifier = f\"{self.module_path}.{node.name}\"\n            \n        self.current_caller_name = full_identifier\n        self.generic_visit(node)\n        self.current_caller_name = old_caller\n\n    def visit_Call(self, node):\n        callee_pathname = self._resolve_call_qname(node.func)\n        if callee_pathname and callee_pathname in self.definitions:\n            \n            if self.current_caller_name == self.module_path:\n                caller_type = 'module'\n            elif '.<locals>.' in self.current_caller_name:\n                caller_type = 'local_function'\n            elif self.current_class_name:\n                caller_type = 'method'\n            else:\n                caller_type = 'function'\n            \n            caller_info = {\n                'file': os.path.basename(self.filepath),\n                'line': node.lineno,\n                'caller': self.current_caller_name,\n                'caller_type': caller_type \n            }\n            self.calls[callee_pathname].append(caller_info)\n        self.generic_visit(node)\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.scope[alias.asname or alias.name] = alias.name\n        self.generic_visit(node)\n        \n    def visit_ImportFrom(self, node):\n        module = node.module or ''\n        for alias in node.names:\n            name = alias.asname or alias.name\n            if node.level > 0:\n                base = self.module_path.split('.')\n                prefix = '.'.join(base[:-node.level])\n                full_module_path = f\"{prefix}.{module}\" if module else prefix\n            else:\n                full_module_path = module\n            self.scope[name] = f\"{full_module_path}.{alias.name}\"\n        self.generic_visit(node)\n\n    def visit_Assign(self, node):\n        if isinstance(node.value, ast.Call) and isinstance(node.value.func, ast.Name):\n            class_name = node.value.func.id\n            if class_name in self.scope:\n                qualified_class_name = self.scope[class_name]\n                if qualified_class_name in self.definitions:\n                    for target in node.targets:\n                        if isinstance(target, ast.Name):\n                            self.instance_types[target.id] = qualified_class_name\n        self.generic_visit(node)\n\n    def _resolve_call_qname(self, func_node):\n        if isinstance(func_node, ast.Name):\n            name = func_node.id\n            if name in self.scope:\n                return self.scope[name]\n            local_pathname = f\"{self.module_path}.{name}\"\n            if local_pathname in self.definitions:\n                return local_pathname\n        elif isinstance(func_node, ast.Attribute) and isinstance(func_node.value, ast.Name):\n            var_name = func_node.value.id\n            method_name = func_node.attr\n            if var_name in self.instance_types:\n                class_pathname = self.instance_types[var_name]\n                return f\"{class_pathname}.{method_name}\"\n            if var_name in self.scope:\n                module_pathname = self.scope[var_name]\n                return f\"{module_pathname}.{method_name}\"\n        return None"
            start_line: 117
            end_line: 214
            context:
              dependencies[1]: backend.relationship_analyzer.path_to_module
              instantiated_by[1]: backend.relationship_analyzer.ProjectAnalyzer._resolve_calls
              method_context[8]:
                - identifier: backend.relationship_analyzer.CallResolverVisitor.__init__
                  name: __init__
                  calls[1]: backend.relationship_analyzer.path_to_module
                  called_by[0]:
                  args[4]: self,filepath,project_root,definitions
                  docstring: null
                  start_line: 118
                  end_line: 126
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 128
                  end_line: 132
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 134
                  end_line: 144
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Call
                  name: visit_Call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 146
                  end_line: 166
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 168
                  end_line: 171
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 173
                  end_line: 184
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Assign
                  name: visit_Assign
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 186
                  end_line: 195
                - identifier: backend.relationship_analyzer.CallResolverVisitor._resolve_call_qname
                  name: _resolve_call_qname
                  calls[0]:
                  called_by[0]:
                  args[2]: self,func_node
                  docstring: null
                  start_line: 197
                  end_line: 214
    "backend/scads_key_test.py":
      ast_nodes:
        imports[3]: os,dotenv.load_dotenv,openai.OpenAI
        functions[0]:
        classes[0]:
    "database/db.py":
      ast_nodes:
        imports[7]: datetime.datetime,pymongo.MongoClient,dotenv.load_dotenv,streamlit_authenticator,cryptography.fernet.Fernet,uuid,os
        functions[29]:
          - mode: function_analysis
            identifier: database.db.encrypt_text
            name: encrypt_text
            args[1]: text
            docstring: null
            source_code: "def encrypt_text(text: str) -> str:\n    if not text or not cipher_suite: return text\n    return cipher_suite.encrypt(text.strip().encode()).decode()"
            start_line: 28
            end_line: 30
            context:
              calls[0]:
              called_by[3]: database.db.update_gemini_key,database.db.update_gpt_key,database.db.update_opensrc_key
          - mode: function_analysis
            identifier: database.db.decrypt_text
            name: decrypt_text
            args[1]: text
            docstring: null
            source_code: "def decrypt_text(text: str) -> str:\n    if not text or not cipher_suite: return text\n    try:\n        return cipher_suite.decrypt(text.strip().encode()).decode()\n    except Exception:\n        return text"
            start_line: 32
            end_line: 37
            context:
              calls[0]:
              called_by[1]: database.db.get_decrypted_api_keys
          - mode: function_analysis
            identifier: database.db.insert_user
            name: insert_user
            args[3]: username,name,password
            docstring: null
            source_code: "def insert_user(username: str, name: str, password: str):\n    user = {\n        \"_id\": username,\n        \"name\": name, \n        \"hashed_password\": stauth.Hasher.hash(password),\n        \"gemini_api_key\": \"\",\n        \"ollama_base_url\": \"\",\n        \"gpt_api_key\": \"\"\n    }\n    result = dbusers.insert_one(user)\n    return result.inserted_id"
            start_line: 43
            end_line: 53
            context:
              calls[0]:
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: database.db.fetch_all_users
            name: fetch_all_users
            args[0]:
            docstring: null
            source_code: "def fetch_all_users():\n    return list(dbusers.find())"
            start_line: 55
            end_line: 56
            context:
              calls[0]:
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: database.db.fetch_user
            name: fetch_user
            args[1]: username
            docstring: null
            source_code: "def fetch_user(username: str):\n    return dbusers.find_one({\"_id\": username})"
            start_line: 58
            end_line: 59
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_user_name
            name: update_user_name
            args[2]: username,new_name
            docstring: null
            source_code: "def update_user_name(username: str, new_name: str):\n    # Achtung: _id kann in Mongo nicht einfach so ge√§ndert werden. \n    # Hier wird nur das Name-Feld geupdated.\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"name\": new_name}})\n    return result.modified_count"
            start_line: 61
            end_line: 65
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_gemini_key
            name: update_gemini_key
            args[2]: username,gemini_api_key
            docstring: null
            source_code: "def update_gemini_key(username: str, gemini_api_key: str):\n    encrypted_key = encrypt_text(gemini_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"gemini_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 67
            end_line: 70
            context:
              calls[1]: database.db.encrypt_text
              called_by[2]: frontend.frontend,frontend.frontend.save_gemini_cb
          - mode: function_analysis
            identifier: database.db.update_gpt_key
            name: update_gpt_key
            args[2]: username,gpt_api_key
            docstring: null
            source_code: "def update_gpt_key(username: str, gpt_api_key: str):\n    encrypted_key = encrypt_text(gpt_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"gpt_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 72
            end_line: 75
            context:
              calls[1]: database.db.encrypt_text
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: database.db.update_ollama_url
            name: update_ollama_url
            args[2]: username,ollama_base_url
            docstring: null
            source_code: "def update_ollama_url(username: str, ollama_base_url: str):\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"ollama_base_url\": ollama_base_url.strip()}})\n    return result.modified_count"
            start_line: 77
            end_line: 79
            context:
              calls[0]:
              called_by[2]: frontend.frontend,frontend.frontend.save_ollama_cb
          - mode: function_analysis
            identifier: database.db.update_opensrc_key
            name: update_opensrc_key
            args[2]: username,opensrc_api_key
            docstring: null
            source_code: "def update_opensrc_key(username: str, opensrc_api_key: str):\n    encrypted_key = encrypt_text(opensrc_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"opensrc_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 81
            end_line: 84
            context:
              calls[1]: database.db.encrypt_text
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: database.db.update_opensrc_url
            name: update_opensrc_url
            args[2]: username,opensrc_base_url
            docstring: null
            source_code: "def update_opensrc_url(username: str, opensrc_base_url: str):\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"opensrc_base_url\": opensrc_base_url.strip()}})\n    return result.modified_count"
            start_line: 86
            end_line: 88
            context:
              calls[0]:
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: database.db.fetch_gemini_key
            name: fetch_gemini_key
            args[1]: username
            docstring: null
            source_code: "def fetch_gemini_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"gemini_api_key\": 1, \"_id\": 0})\n    return user.get(\"gemini_api_key\") if user else None"
            start_line: 90
            end_line: 92
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_ollama_url
            name: fetch_ollama_url
            args[1]: username
            docstring: null
            source_code: "def fetch_ollama_url(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"ollama_base_url\": 1, \"_id\": 0})\n    return user.get(\"ollama_base_url\") if user else None"
            start_line: 94
            end_line: 96
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_gpt_key
            name: fetch_gpt_key
            args[1]: username
            docstring: null
            source_code: "def fetch_gpt_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"gpt_api_key\": 1, \"_id\": 0})\n    return user.get(\"gpt_api_key\") if user else None"
            start_line: 98
            end_line: 100
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_opensrc_key
            name: fetch_opensrc_key
            args[1]: username
            docstring: null
            source_code: "def fetch_opensrc_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"opensrc_api_key\": 1, \"_id\": 0})\n    return user.get(\"opensrc_api_key\") if user else None"
            start_line: 102
            end_line: 104
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_opensrc_url
            name: fetch_opensrc_url
            args[1]: username
            docstring: null
            source_code: "def fetch_opensrc_url(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"opensrc_base_url\": 1, \"_id\": 0})\n    return user.get(\"opensrc_base_url\") if user else None"
            start_line: 106
            end_line: 108
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.delete_user
            name: delete_user
            args[1]: username
            docstring: null
            source_code: "def delete_user(username: str):\n    return dbusers.delete_one({\"_id\": username}).deleted_count"
            start_line: 110
            end_line: 111
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.get_decrypted_api_keys
            name: get_decrypted_api_keys
            args[1]: username
            docstring: null
            source_code: "def get_decrypted_api_keys(username: str):\n    user = dbusers.find_one({\"_id\": username})\n    if not user: return None, None\n    gemini_plain = decrypt_text(user.get(\"gemini_api_key\", \"\"))\n    ollama_plain = user.get(\"ollama_base_url\", \"\")\n    gpt_plain = decrypt_text(user.get(\"gpt_api_key\", \"\"))\n    opensrc_plain = decrypt_text(user.get(\"opensrc_api_key\", \"\"))\n    opensrc_url = user.get(\"opensrc_base_url\", \"\")\n    return gemini_plain, ollama_plain, gpt_plain, opensrc_plain, opensrc_url"
            start_line: 113
            end_line: 121
            context:
              calls[1]: database.db.decrypt_text
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: database.db.insert_chat
            name: insert_chat
            args[2]: username,chat_name
            docstring: Erstellt einen neuen Chat-Eintrag.
            source_code: "def insert_chat(username: str, chat_name: str):\n    \"\"\"Erstellt einen neuen Chat-Eintrag.\"\"\"\n    chat = {\n        \"_id\": str(uuid.uuid4()),\n        \"username\": username,\n        \"chat_name\": chat_name,\n        \"created_at\": datetime.now()\n    }\n    result = dbchats.insert_one(chat)\n    return result.inserted_id"
            start_line: 127
            end_line: 136
            context:
              calls[0]:
              called_by[3]: frontend.frontend,frontend.frontend.handle_delete_chat,frontend.frontend.load_data_from_db
          - mode: function_analysis
            identifier: database.db.fetch_chats_by_user
            name: fetch_chats_by_user
            args[1]: username
            docstring: Holt alle definierten Chats eines Users.
            source_code: "def fetch_chats_by_user(username: str):\n    \"\"\"Holt alle definierten Chats eines Users.\"\"\"\n    # Sortieren nach Erstellung macht Sinn\n    chats = list(dbchats.find({\"username\": username}).sort(\"created_at\", 1))\n    return chats"
            start_line: 138
            end_line: 142
            context:
              calls[0]:
              called_by[1]: frontend.frontend.load_data_from_db
          - mode: function_analysis
            identifier: database.db.check_chat_exists
            name: check_chat_exists
            args[2]: username,chat_name
            docstring: null
            source_code: "def check_chat_exists(username: str, chat_name: str):\n    return dbchats.find_one({\"username\": username, \"chat_name\": chat_name}) is not None"
            start_line: 144
            end_line: 145
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.rename_chat_fully
            name: rename_chat_fully
            args[3]: username,old_name,new_name
            docstring: Benennt einen Chat und alle zugeh√∂rigen Exchanges um.
            source_code: "def rename_chat_fully(username: str, old_name: str, new_name: str):\n    \"\"\"\n    Benennt einen Chat und alle zugeh√∂rigen Exchanges um.\n    \"\"\"\n    # 1. Chat-Eintrag umbenennen\n    result = dbchats.update_one(\n        {\"username\": username, \"chat_name\": old_name}, \n        {\"$set\": {\"chat_name\": new_name}}\n    )\n    \n    # 2. Alle Messages (Exchanges) umh√§ngen\n    dbexchanges.update_many(\n        {\"username\": username, \"chat_name\": old_name},\n        {\"$set\": {\"chat_name\": new_name}}\n    )\n    return result.modified_count"
            start_line: 148
            end_line: 163
            context:
              calls[0]:
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: database.db.insert_exchange
            name: insert_exchange
            args[13]: question,answer,feedback,username,chat_name,helper_used,main_used,total_time,helper_time,main_time,json_tokens,toon_tokens,savings_percent
            docstring: null
            source_code: "def insert_exchange(question: str, answer: str, feedback: str, username: str, chat_name: str, \n                    helper_used: str=\"\", main_used: str=\"\", total_time: str=\"\", helper_time: str=\"\", main_time: str=\"\",\n                    json_tokens=0, toon_tokens=0, savings_percent=0.0):\n    new_id = str(uuid.uuid4())\n    exchange = {\n        \"_id\": new_id,\n        \"question\": question,\n        \"answer\": answer,\n        \"feedback\": feedback,\n        \"feedback_message\": \"\",\n        \"chat_name\": chat_name,\n        \"username\": username,\n        \"helper_used\": helper_used,\n        \"main_used\": main_used,\n        \"total_time\": total_time,\n        \"helper_time\": helper_time,\n        \"main_time\": main_time,\n        \"json_tokens\": json_tokens,\n        \"toon_tokens\": toon_tokens,\n        \"savings_percent\": savings_percent,\n        \"created_at\": datetime.now()\n    }\n    try:\n        dbexchanges.insert_one(exchange)\n        return new_id\n    except Exception as e:\n        print(f\"DB Error: {e}\")\n        return None"
            start_line: 169
            end_line: 196
            context:
              calls[0]:
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: database.db.fetch_exchanges_by_user
            name: fetch_exchanges_by_user
            args[1]: username
            docstring: null
            source_code: "def fetch_exchanges_by_user(username: str):\n    # Sortieren nach Zeitstempel wichtig f√ºr die Anzeige\n    exchanges = list(dbexchanges.find({\"username\": username}).sort(\"created_at\", 1))\n    return exchanges"
            start_line: 198
            end_line: 201
            context:
              calls[0]:
              called_by[1]: frontend.frontend.load_data_from_db
          - mode: function_analysis
            identifier: database.db.fetch_exchanges_by_chat
            name: fetch_exchanges_by_chat
            args[2]: username,chat_name
            docstring: null
            source_code: "def fetch_exchanges_by_chat(username: str, chat_name: str):\n    exchanges = list(dbexchanges.find({\"username\": username, \"chat_name\": chat_name}).sort(\"created_at\", 1))\n    return exchanges"
            start_line: 203
            end_line: 205
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_exchange_feedback
            name: update_exchange_feedback
            args[2]: exchange_id,feedback
            docstring: null
            source_code: "def update_exchange_feedback(exchange_id, feedback: int):\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback\": feedback}})\n    return result.modified_count"
            start_line: 207
            end_line: 209
            context:
              calls[0]:
              called_by[1]: frontend.frontend.handle_feedback_change
          - mode: function_analysis
            identifier: database.db.update_exchange_feedback_message
            name: update_exchange_feedback_message
            args[2]: exchange_id,feedback_message
            docstring: null
            source_code: "def update_exchange_feedback_message(exchange_id, feedback_message: str):\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback_message\": feedback_message}})\n    return result.modified_count"
            start_line: 211
            end_line: 213
            context:
              calls[0]:
              called_by[1]: frontend.frontend.render_exchange
          - mode: function_analysis
            identifier: database.db.delete_exchange_by_id
            name: delete_exchange_by_id
            args[1]: exchange_id
            docstring: null
            source_code: "def delete_exchange_by_id(exchange_id: str):\n    result = dbexchanges.delete_one({\"_id\": exchange_id})\n    return result.deleted_count"
            start_line: 215
            end_line: 217
            context:
              calls[0]:
              called_by[1]: frontend.frontend.handle_delete_exchange
          - mode: function_analysis
            identifier: database.db.delete_full_chat
            name: delete_full_chat
            args[2]: username,chat_name
            docstring: "L√∂scht den Chat UND alle zugeh√∂rigen Exchanges.\nDas sorgt f√ºr Konsistenz zwischen Frontend und Backend."
            source_code: "def delete_full_chat(username: str, chat_name: str):\n    \"\"\"\n    L√∂scht den Chat UND alle zugeh√∂rigen Exchanges.\n    Das sorgt f√ºr Konsistenz zwischen Frontend und Backend.\n    \"\"\"\n    # 1. Alle Nachrichten in diesem Chat l√∂schen\n    del_exchanges = dbexchanges.delete_many({\"username\": username, \"chat_name\": chat_name})\n    \n    # 2. Den Chat selbst aus der Chat-Liste l√∂schen\n    del_chat = dbchats.delete_one({\"username\": username, \"chat_name\": chat_name})\n    \n    return del_chat.deleted_count"
            start_line: 221
            end_line: 232
            context:
              calls[0]:
              called_by[1]: frontend.frontend.handle_delete_chat
        classes[0]:
    "frontend/frontend.py":
      ast_nodes:
        imports[16]: numpy,datetime.datetime,time,pymongo.MongoClient,dotenv.load_dotenv,os,sys,urllib.parse.urlparse,logging,traceback,re,streamlit_mermaid.st_mermaid,backend.main,database.db,streamlit,streamlit_authenticator
        functions[12]:
          - mode: function_analysis
            identifier: frontend.frontend.clean_names
            name: clean_names
            args[1]: model_list
            docstring: null
            source_code: "def clean_names(model_list):\n    return [m.split(\"/\")[-1] for m in model_list]"
            start_line: 54
            end_line: 55
            context:
              calls[0]:
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: frontend.frontend.get_filtered_models
            name: get_filtered_models
            args[2]: source_list,category_name
            docstring: Filtert eine Liste basierend auf der gew√§hlten Kategorie.
            source_code: "def get_filtered_models(source_list, category_name):\n    \"\"\"Filtert eine Liste basierend auf der gew√§hlten Kategorie.\"\"\"\n    keywords = CATEGORY_KEYWORDS.get(category_name, [\"\"])\n    \n    if \"STANDARD\" in keywords:\n        # Nur Modelle zur√ºckgeben, die auch in der Standard-Liste sind\n        return [m for m in source_list if m in STANDARD_MODELS]\n    \n    filtered = []\n    for model in source_list:\n        # Pr√ºfen ob ein Keyword im Namen steckt\n        if any(k in model.lower() for k in keywords):\n            filtered.append(model)\n            \n    return filtered if filtered else source_list"
            start_line: 86
            end_line: 100
            context:
              calls[0]:
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: frontend.frontend.save_gemini_cb
            name: save_gemini_cb
            args[0]:
            docstring: null
            source_code: "def save_gemini_cb():\n    new_key = st.session_state.get(\"in_gemini_key\", \"\")\n    if new_key:\n        db.update_gemini_key(st.session_state[\"username\"], new_key)\n        st.session_state[\"in_gemini_key\"] = \"\"\n        st.toast(\"Gemini Key erfolgreich gespeichert! ‚úÖ\")"
            start_line: 143
            end_line: 148
            context:
              calls[1]: database.db.update_gemini_key
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.save_ollama_cb
            name: save_ollama_cb
            args[0]:
            docstring: null
            source_code: "def save_ollama_cb():\n    new_url = st.session_state.get(\"in_ollama_url\", \"\")\n    if new_url:\n        db.update_ollama_url(st.session_state[\"username\"], new_url)\n        st.toast(\"Ollama URL gespeichert! ‚úÖ\")"
            start_line: 150
            end_line: 154
            context:
              calls[1]: database.db.update_ollama_url
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.load_data_from_db
            name: load_data_from_db
            args[1]: username
            docstring: L√§dt Chats und Exchanges konsistent aus der DB.
            source_code: "def load_data_from_db(username: str):\n    \"\"\"L√§dt Chats und Exchanges konsistent aus der DB.\"\"\"\n    \n    # Nur laden, wenn neuer User oder noch nicht geladen\n    if \"loaded_user\" not in st.session_state or st.session_state.loaded_user != username:\n        st.session_state.chats = {}\n        \n        # 1. Erst die definierten Chats laden (damit auch leere Chats da sind)\n        db_chats = db.fetch_chats_by_user(username)\n        for c in db_chats:\n            c_name = c.get(\"chat_name\")\n            if c_name:\n                st.session_state.chats[c_name] = {\"exchanges\": []}\n\n        # 2. Dann die Exchanges laden und einsortieren\n        db_exchanges = db.fetch_exchanges_by_user(username)\n        for ex in db_exchanges:\n            c_name = ex.get(\"chat_name\", \"Unbenannt\")\n            \n            # Falls Exchanges existieren f√ºr Chats, die nicht in dbchats sind (Legacy Support)\n            if c_name not in st.session_state.chats:\n                st.session_state.chats[c_name] = {\"exchanges\": []}\n            \n            if \"feedback\" not in ex or ex[\"feedback\"] is None:\n                ex[\"feedback\"] = np.nan\n            \n            st.session_state.chats[c_name][\"exchanges\"].append(ex)\n\n        # 3. Default Chat erstellen, falls gar nichts existiert\n        if not st.session_state.chats:\n            initial_name = \"Chat 1\"\n            # Konsistent in DB anlegen\n            db.insert_chat(username, initial_name)\n            st.session_state.chats[initial_name] = {\"exchanges\": []}\n            st.session_state.active_chat = initial_name\n        else:\n            # Active Chat setzen, falls n√∂tig\n            if \"active_chat\" not in st.session_state or st.session_state.active_chat not in st.session_state.chats:\n                # Nimm den ersten verf√ºgbaren Chat\n                st.session_state.active_chat = list(st.session_state.chats.keys())[0]\n        \n        st.session_state.loaded_user = username"
            start_line: 160
            end_line: 201
            context:
              calls[3]: database.db.fetch_chats_by_user,database.db.fetch_exchanges_by_user,database.db.insert_chat
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: frontend.frontend.handle_feedback_change
            name: handle_feedback_change
            args[2]: ex,val
            docstring: null
            source_code: "def handle_feedback_change(ex, val):\n    ex[\"feedback\"] = val\n    db.update_exchange_feedback(ex[\"_id\"], val)\n    st.rerun()"
            start_line: 207
            end_line: 210
            context:
              calls[1]: database.db.update_exchange_feedback
              called_by[1]: frontend.frontend.render_exchange
          - mode: function_analysis
            identifier: frontend.frontend.handle_delete_exchange
            name: handle_delete_exchange
            args[2]: chat_name,ex
            docstring: null
            source_code: "def handle_delete_exchange(chat_name, ex):\n    db.delete_exchange_by_id(ex[\"_id\"])\n    if chat_name in st.session_state.chats:\n        if ex in st.session_state.chats[chat_name][\"exchanges\"]:\n            st.session_state.chats[chat_name][\"exchanges\"].remove(ex)\n    st.rerun()"
            start_line: 212
            end_line: 217
            context:
              calls[1]: database.db.delete_exchange_by_id
              called_by[1]: frontend.frontend.render_exchange
          - mode: function_analysis
            identifier: frontend.frontend.handle_delete_chat
            name: handle_delete_chat
            args[2]: username,chat_name
            docstring: null
            source_code: "def handle_delete_chat(username, chat_name):\n    # KONSISTENZ: Ruft jetzt delete_full_chat in DB auf\n    db.delete_full_chat(username, chat_name)\n    \n    # State bereinigen\n    if chat_name in st.session_state.chats:\n        del st.session_state.chats[chat_name]\n    \n    # Active Chat neu setzen\n    if len(st.session_state.chats) > 0:\n        st.session_state.active_chat = list(st.session_state.chats.keys())[0]\n    else:\n        # Wenn alles weg ist, neuen leeren Chat anlegen\n        new_name = \"Chat 1\"\n        db.insert_chat(username, new_name)\n        st.session_state.chats[new_name] = {\"exchanges\": []}\n        st.session_state.active_chat = new_name\n        \n    st.rerun()"
            start_line: 219
            end_line: 237
            context:
              calls[2]: database.db.delete_full_chat,database.db.insert_chat
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: frontend.frontend.extract_repo_name
            name: extract_repo_name
            args[1]: text
            docstring: null
            source_code: "def extract_repo_name(text):\n    url_match = re.search(r'(https?://[^\\s]+)', text)\n    if url_match:\n        url = url_match.group(0)\n        parsed = urlparse(url)\n        path = parsed.path.strip(\"/\")\n        if path:\n            repo_name = path.split(\"/\")[-1]\n            if repo_name.endswith(\".git\"):\n                repo_name = repo_name[:-4]\n            return repo_name\n    return None"
            start_line: 243
            end_line: 254
            context:
              calls[0]:
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: frontend.frontend.stream_text_generator
            name: stream_text_generator
            args[1]: text
            docstring: null
            source_code: "def stream_text_generator(text):\n    for word in text.split(\" \"):\n        yield word + \" \"\n        time.sleep(0.01)"
            start_line: 256
            end_line: 259
            context:
              calls[0]:
              called_by[1]: frontend.frontend.render_text_with_mermaid
          - mode: function_analysis
            identifier: frontend.frontend.render_text_with_mermaid
            name: render_text_with_mermaid
            args[2]: markdown_text,should_stream
            docstring: null
            source_code: "def render_text_with_mermaid(markdown_text, should_stream=False):\n    if not markdown_text:\n        return\n\n    parts = re.split(r\"```mermaid\\s+(.*?)\\s+```\", markdown_text, flags=re.DOTALL)\n\n    for i, part in enumerate(parts):\n        if i % 2 == 0:\n            if part.strip():\n                if should_stream:\n                    st.write_stream(stream_text_generator(part))\n                else:\n                    st.markdown(part)\n        else:\n            try:\n                st_mermaid(part, key=f\"mermaid_{hash(part)}_{i}\")\n            except Exception:\n                st.code(part, language=\"mermaid\")"
            start_line: 261
            end_line: 278
            context:
              calls[1]: frontend.frontend.stream_text_generator
              called_by[2]: frontend.frontend,frontend.frontend.render_exchange
          - mode: function_analysis
            identifier: frontend.frontend.render_exchange
            name: render_exchange
            args[2]: ex,current_chat_name
            docstring: null
            source_code: "def render_exchange(ex, current_chat_name):\n    st.chat_message(\"user\").write(ex[\"question\"])\n\n    with st.chat_message(\"assistant\"):\n        answer_text = ex.get(\"answer\", \"\")\n        is_error = answer_text.startswith(\"Fehler\") or answer_text.startswith(\"Error\")\n\n        # --- 1. TOOLBAR CONTAINER ---\n        # Nutzt die neuen Parameter horizontal=True und alignment\n        # border=True rahmt die Toolbar ein\n        # horizontal_alignment=\"right\" schiebt alles nach rechts\n        with st.container(horizontal=True, horizontal_alignment=\"right\"):\n            \n            if not is_error:\n                # 1. Status Text (wird jetzt links von den Buttons angezeigt, aber rechtsb√ºndig im Container)\n                st.write(\"hier die Antwort:\")\n                if ex.get(\"feedback\") == 1:\n                    st.caption(\"‚úÖ Hilfreich\")\n                elif ex.get(\"feedback\") == 0:\n                    st.caption(\"‚ùå Nicht hilfreich\")\n                \n                # 2. Die Buttons (einfach untereinander im Code, erscheinen nebeneinander im UI)\n                \n                # Like\n                type_primary_up = ex.get(\"feedback\") == 1\n                if st.button(\"üëç\", key=f\"up_{ex['_id']}\", type=\"primary\" if type_primary_up else \"secondary\", help=\"Hilfreich\"):\n                    handle_feedback_change(ex, 1)\n\n                # Dislike\n                type_primary_down = ex.get(\"feedback\") == 0\n                if st.button(\"üëé\", key=f\"down_{ex['_id']}\", type=\"primary\" if type_primary_down else \"secondary\", help=\"Nicht hilfreich\"):\n                    handle_feedback_change(ex, 0)\n\n                # Comment Popover\n                with st.popover(\"üí¨\", help=\"Notiz hinzuf√ºgen\"):\n                    msg = st.text_area(\"Notiz:\", value=ex.get(\"feedback_message\", \"\"), key=f\"txt_{ex['_id']}\")\n                    if st.button(\"Speichern\", key=f\"save_{ex['_id']}\"):\n                        ex[\"feedback_message\"] = msg\n                        db.update_exchange_feedback_message(ex[\"_id\"], msg)\n                        st.success(\"Gespeichert!\")\n                        time.sleep(0.5)\n                        st.rerun()\n\n                # Download\n                st.download_button(\n                    label=\"üì•\",\n                    data=ex[\"answer\"],\n                    file_name=f\"response_{ex['_id']}.md\",\n                    mime=\"text/markdown\",\n                    key=f\"dl_{ex['_id']}\",\n                    help=\"Als Markdown herunterladen\"\n                )\n\n                # Delete\n                if st.button(\"üóëÔ∏è\", key=f\"del_{ex['_id']}\", help=\"Nachricht l√∂schen\", type=\"secondary\"):\n                    handle_delete_exchange(current_chat_name, ex)\n\n            else:\n                # Fehlerfall\n                st.error(\"‚ö†Ô∏è Fehler\")\n                if st.button(\"üóëÔ∏è L√∂schen\", key=f\"del_err_{ex['_id']}\"):\n                    handle_delete_exchange(current_chat_name, ex)\n\n        # --- 2. CONTENT (Antwort) ---\n        with st.container(height=500, border=True):\n             render_text_with_mermaid(ex[\"answer\"], should_stream=False)"
            start_line: 284
            end_line: 349
            context:
              calls[4]: database.db.update_exchange_feedback_message,frontend.frontend.handle_delete_exchange,frontend.frontend.handle_feedback_change,frontend.frontend.render_text_with_mermaid
              called_by[1]: frontend.frontend
        classes[0]:
    "schemas/types.py":
      ast_nodes:
        imports[5]: typing.List,typing.Optional,typing.Literal,pydantic.BaseModel,pydantic.ValidationError
        functions[0]:
        classes[15]:
          - mode: class_analysis
            identifier: schemas.types.ParameterDescription
            name: ParameterDescription
            docstring: Describes a single parameter of a function.
            source_code: "class ParameterDescription(BaseModel):\n    \"\"\"Describes a single parameter of a function.\"\"\"\n    name: str\n    type: str\n    description: str"
            start_line: 6
            end_line: 10
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ReturnDescription
            name: ReturnDescription
            docstring: Describes the return value of a function.
            source_code: "class ReturnDescription(BaseModel):\n    \"\"\"Describes the return value of a function.\"\"\"\n    name: str\n    type: str\n    description: str"
            start_line: 12
            end_line: 16
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.UsageContext
            name: UsageContext
            docstring: Describes the calling context of a function.
            source_code: "class UsageContext(BaseModel):\n    \"\"\"Describes the calling context of a function.\"\"\"\n    calls: str\n    called_by: str"
            start_line: 18
            end_line: 21
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionDescription
            name: FunctionDescription
            docstring: Contains the detailed analysis of a function's purpose and signature.
            source_code: "class FunctionDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a function's purpose and signature.\"\"\"\n    overall: str\n    parameters: List[ParameterDescription]\n    returns: List[ReturnDescription]\n    usage_context: UsageContext"
            start_line: 23
            end_line: 28
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionAnalysis
            name: FunctionAnalysis
            docstring: The main model representing the entire JSON schema for a function.
            source_code: "class FunctionAnalysis(BaseModel):\n    \"\"\"The main model representing the entire JSON schema for a function.\"\"\"\n    identifier: str\n    description: FunctionDescription\n    error: Optional[str] = None"
            start_line: 30
            end_line: 34
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ConstructorDescription
            name: ConstructorDescription
            docstring: Describes the __init__ method of a class.
            source_code: "class ConstructorDescription(BaseModel):\n    \"\"\"Describes the __init__ method of a class.\"\"\"\n    description: str\n    parameters: List[ParameterDescription]"
            start_line: 39
            end_line: 42
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassContext
            name: ClassContext
            docstring: Describes the class's external dependencies and primary points of instantiation.
            source_code: "class ClassContext(BaseModel):\n    \"\"\"Describes the class's external dependencies and primary points of instantiation.\"\"\"\n    dependencies: str\n    instantiated_by: str"
            start_line: 44
            end_line: 47
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassDescription
            name: ClassDescription
            docstring: "Contains the detailed analysis of a class's purpose, constructor, and methods."
            source_code: "class ClassDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a class's purpose, constructor, and methods.\"\"\"\n    overall: str\n    init_method: ConstructorDescription\n    methods: List[FunctionAnalysis]\n    usage_context: ClassContext"
            start_line: 49
            end_line: 54
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassAnalysis
            name: ClassAnalysis
            docstring: The main model for the entire JSON schema for a class.
            source_code: "class ClassAnalysis(BaseModel):\n    \"\"\"The main model for the entire JSON schema for a class.\"\"\"\n    identifier: str\n    description: ClassDescription\n    error: Optional[str] = None"
            start_line: 56
            end_line: 60
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.CallInfo
            name: CallInfo
            docstring: "Represents a specific call event from the relationship analyzer.\nUsed in 'called_by' and 'instantiated_by' lists."
            source_code: "class CallInfo(BaseModel):\n    \"\"\"\n    Represents a specific call event from the relationship analyzer.\n    Used in 'called_by' and 'instantiated_by' lists.\n    \"\"\"\n    file: str\n    function: str  # Name des Aufrufers\n    mode: str      # z.B. 'method', 'function', 'module'\n    line: int"
            start_line: 65
            end_line: 73
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionContextInput
            name: FunctionContextInput
            docstring: Structured context for analyzing a function.
            source_code: "class FunctionContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a function.\"\"\"\n    calls: List[str]\n    called_by: List[CallInfo]"
            start_line: 78
            end_line: 81
            context:
              dependencies[0]:
              instantiated_by[1]: backend.main.main_workflow
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionAnalysisInput
            name: FunctionAnalysisInput
            docstring: The required input to generate a FunctionAnalysis object.
            source_code: "class FunctionAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a FunctionAnalysis object.\"\"\"\n    mode: Literal[\"function_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: FunctionContextInput"
            start_line: 83
            end_line: 89
            context:
              dependencies[0]:
              instantiated_by[1]: backend.main.main_workflow
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.MethodContextInput
            name: MethodContextInput
            docstring: Structured context for a classes methods
            source_code: "class MethodContextInput(BaseModel):\n    \"\"\"Structured context for a classes methods\"\"\"\n    identifier: str\n    calls: List[str]\n    called_by: List[CallInfo]\n    args: List[str]\n    docstring: Optional[str]"
            start_line: 94
            end_line: 100
            context:
              dependencies[0]:
              instantiated_by[1]: backend.main.main_workflow
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassContextInput
            name: ClassContextInput
            docstring: Structured context for analyzing a class.
            source_code: "class ClassContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a class.\"\"\"\n    dependencies: List[str]\n    instantiated_by: List[CallInfo]\n    method_context: List[MethodContextInput]"
            start_line: 102
            end_line: 106
            context:
              dependencies[0]:
              instantiated_by[2]: backend.HelperLLM.main_orchestrator,backend.main.main_workflow
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassAnalysisInput
            name: ClassAnalysisInput
            docstring: The required input to generate a ClassAnalysis object.
            source_code: "class ClassAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a ClassAnalysis object.\"\"\"\n    mode: Literal[\"class_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: ClassContextInput"
            start_line: 108
            end_line: 114
            context:
              dependencies[0]:
              instantiated_by[2]: backend.HelperLLM.main_orchestrator,backend.main.main_workflow
              method_context[0]:
analysis_results:
  functions:
    backend.AST_Schema.path_to_module:
      identifier: backend.AST_Schema.path_to_module
      description:
        overall: "This function converts a given file path into a Python module path. It first determines the file's relative path to a specified project root, handling potential `ValueError` by falling back to the base filename. It then removes the '.py' extension if present and replaces path separators with dots. Finally, it adjusts the module path by removing '.__init__' if it appears at the end."
        parameters[2]{name,type,description}:
          filepath,str,The absolute or relative path to the file that needs to be converted.
          project_root,str,"The root directory of the project, used to calculate the relative path of the file."
        returns[1]{name,type,description}:
          module_path,str,"The converted Python module path, suitable for import statements."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.File_Dependency.build_file_dependency_graph:
      identifier: backend.File_Dependency.build_file_dependency_graph
      description:
        overall: "This function constructs a directed graph representing file-level import dependencies within a given Abstract Syntax Tree (AST). It initializes a NetworkX directed graph and then uses a `FileDependencyGraph` visitor to traverse the provided AST, collecting import relationships. The collected dependencies are then added to the graph as nodes and edges, where nodes represent files and edges indicate an import. The function ultimately returns the populated dependency graph."
        parameters[3]{name,type,description}:
          filename,str,The path to the file whose dependencies are being analyzed.
          tree,AST,The Abstract Syntax Tree (AST) of the file to be processed for dependencies.
          repo_root,str,"The root directory of the repository, used for resolving file paths and dependencies."
        returns[1]{name,type,description}:
          graph,nx.DiGraph,A directed graph where nodes are file paths and edges represent import dependencies between them.
        usage_context:
          calls: This function calls backend.File_Dependency.FileDependencyGraph.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.File_Dependency.build_repository_graph:
      identifier: backend.File_Dependency.build_repository_graph
      description:
        overall: "This function constructs a directed graph representing the dependencies within a given Git repository. It first retrieves all files from the repository and filters for Python files. For each Python file, it parses the content to build a file-specific dependency graph using `build_file_dependency_graph`. Finally, it aggregates all these individual file graphs into a single, comprehensive `networkx.DiGraph` that represents the global dependencies across the entire repository."
        parameters[1]{name,type,description}:
          repository,GitRepository,The Git repository object from which to extract and build the dependency graph.
        returns[1]{name,type,description}:
          global_graph,nx.DiGraph,"A directed graph representing the aggregated dependencies between files, functions, and classes found within the repository."
        usage_context:
          calls: This function calls backend.File_Dependency.build_file_dependency_graph.
          called_by: This function is called by no other functions.
      error: null
    backend.File_Dependency.get_all_temp_files:
      identifier: backend.File_Dependency.get_all_temp_files
      description:
        overall: "This function, `get_all_temp_files`, is designed to locate all Python files within a specified directory and its subdirectories. It takes a directory path as a string input and converts it into an absolute `pathlib.Path` object. The function then recursively searches this root path for all files that have a '.py' extension. Finally, it compiles and returns a list of these Python files, with each file's path represented as a `pathlib.Path` object relative to the initial input directory."
        parameters[1]{name,type,description}:
          directory,str,The string path to the root directory from which to start the recursive search for Python files.
        returns[1]{name,type,description}:
          all_files,"list[pathlib.Path]","A list of `pathlib.Path` objects, where each object represents a Python file found within the specified directory, with its path relative to the provided root directory."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.HelperLLM.main_orchestrator:
      identifier: backend.HelperLLM.main_orchestrator
      description:
        overall: "This function serves as a dummy data and processing loop, primarily for testing the LLMHelper class. It defines several example `FunctionAnalysisInput` and `FunctionAnalysis` objects, simulating pre-computed analysis for various functions like `add_item`, `check_stock`, and `generate_report`. It also constructs a `ClassAnalysisInput` for an `InventoryManager` class. Finally, it instantiates an `LLMHelper` and uses it to process the defined function inputs, logging the results and printing the final aggregated documentation."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: "This function calls backend.HelperLLM.LLMHelper, schemas.types.ClassAnalysisInput, and schemas.types.ClassContextInput."
          called_by: This function calls no other functions.
      error: null
    backend.callgraph.make_safe_dot:
      identifier: backend.callgraph.make_safe_dot
      description:
        overall: "This function takes a NetworkX directed graph and an output file path. Its main purpose is to prepare the graph for DOT file generation by ensuring all node names are 'safe' for DOT syntax. It creates a copy of the input graph and relabels all nodes with generic identifiers like 'n0', 'n1', etc. The original node names are then stored as a 'label' attribute on these new nodes. Finally, the function writes this modified graph to the specified output path as a DOT file."
        parameters[2]{name,type,description}:
          graph,nx.DiGraph,The NetworkX directed graph to be processed and written to a DOT file.
          out_path,str,The file path where the 'safe' DOT graph will be saved.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.callgraph.build_filtered_callgraph:
      identifier: backend.callgraph.build_filtered_callgraph
      description:
        overall: "This function constructs a directed call graph for a given Git repository. It first identifies all Python files and extracts a set of 'own functions' by parsing the Abstract Syntax Trees (ASTs) of these files using a CallGraph visitor. Subsequently, it builds a global call graph, adding edges only between functions that are part of the identified 'own functions' set, effectively filtering out external or library calls. The final output is a networkx.DiGraph representing these filtered call relationships."
        parameters[1]{name,type,description}:
          repo,GitRepository,The GitRepository object containing the files to analyze for building the call graph.
        returns[1]{name,type,description}:
          global_graph,nx.DiGraph,A directed graph representing the call relationships exclusively between the 'self-written' functions found within the repository.
        usage_context:
          calls: This function calls backend.callgraph.CallGraph.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.wrap_cdata:
      identifier: backend.converter.wrap_cdata
      description:
        overall: "The `wrap_cdata` function is designed to encapsulate a given string within XML CDATA tags. It takes a single string argument, `content`, and returns a new string where the original content is embedded between `<![CDATA[\\n` and `\\n]]>` markers. This effectively escapes the content for safe inclusion within XML documents, preventing issues with special characters."
        parameters[1]{name,type,description}:
          content,str,The string content to be wrapped within CDATA tags.
        returns[1]{name,type,description}:
          wrapped_content,str,A new string containing the original content enclosed within CDATA tags.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.extract_output_content:
      identifier: backend.converter.extract_output_content
      description:
        overall: "This function processes a list of output objects, typically from a notebook execution, to extract and categorize their content. It iterates through each output, identifying its type to handle text, images, or errors. For image data (PNG or JPEG), it decodes Base64 strings, stores them in an external list, and inserts an XML-like placeholder into the result. Text streams and error messages are directly appended to the output list. The function's primary goal is to consolidate diverse output types into a unified list of strings, including image references."
        parameters[2]{name,type,description}:
          outputs,list,"A list of output objects, presumably from a notebook execution, containing various types of data like text, images, or errors."
          image_list,list,"A list that will be populated with dictionaries, each containing Base64 encoded image data and its MIME type, along with an index."
        returns[1]{name,type,description}:
          extracted_xml_snippets,list of str,"A list of strings, where each string is either plain text content, a formatted error message, or an XML-like placeholder for an image."
        usage_context:
          calls: This function calls backend.converter.process_image.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.process_image:
      identifier: backend.converter.process_image
      description:
        overall: "The process_image function is responsible for extracting and preparing base64-encoded image data. It takes a mime_type as input and attempts to retrieve the corresponding image string from an externally accessible data object. If found, it cleans the base64 string by removing newline characters and appends the image's MIME type and data to an external image_list. The function returns a formatted placeholder string upon successful processing. In cases where the mime_type is not present in data, it returns None, and any exceptions during processing result in an error message string."
        parameters[1]{name,type,description}:
          mime_type,str,"The MIME type of the image to be processed, which serves as a key to locate the corresponding base64 image data."
        returns[3]{name,type,description}:
          image_placeholder_tag,str,"A string formatted as an XML-like tag, <IMAGE_PLACEHOLDER>, containing the assigned index and the MIME type of the processed image."
          error_message_tag,str,"A string formatted as an error tag, <ERROR>, detailing the exception encountered during image decoding."
          no_image_data,None,Returned when no image data corresponding to the provided MIME type is found in the external data object.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.convert_notebook_to_xml:
      identifier: backend.converter.convert_notebook_to_xml
      description:
        overall: "This function takes the raw content of a Jupyter notebook as a string and converts it into an XML representation. It parses the notebook cells, distinguishing between markdown and code cells. For code cells, it also processes their outputs, extracting content and images. If the input content cannot be parsed as a valid Jupyter notebook, it returns an error message."
        parameters[1]{name,type,description}:
          file_content,str,"The raw content of a Jupyter notebook file, expected to be a JSON string."
        returns[2]{name,type,description}:
          xml_output,str,A string containing the XML representation of the notebook cells or an error message if parsing fails.
          extracted_images,list,A list of extracted image data or paths found within the notebook outputs. This list is empty if parsing fails or no images are found.
        usage_context:
          calls: This function calls backend.converter.extract_output_content and backend.converter.wrap_cdata.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.process_repo_notebooks:
      identifier: backend.converter.process_repo_notebooks
      description:
        overall: "This function processes a collection of repository files to identify and convert Jupyter notebooks. It filters the input list to find files ending with '.ipynb'. For each identified notebook, it extracts its content and uses the 'convert_notebook_to_xml' function to generate XML output and associated images. The function then aggregates these results into a dictionary, keyed by the notebook's file path, and returns this collection."
        parameters[1]{name,type,description}:
          repo_files,"List[Any]",A list of file-like objects from a repository. Each object is expected to have 'path' (string) and 'content' (Any) attributes.
        returns[1]{name,type,description}:
          results,"Dict[str, Dict[str, Any]]",A dictionary where keys are the paths of the processed notebook files (str) and values are dictionaries containing the 'xml' output (str) and 'images' (Any) generated from each notebook.
        usage_context:
          calls: This function calls backend.converter.convert_notebook_to_xml.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.create_savings_chart:
      identifier: backend.main.create_savings_chart
      description:
        overall: "This function generates a bar chart to visually compare two token counts, `json_tokens` and `toon_tokens`, highlighting a `savings_percent` in the title. It customizes the plot with labels, colors, and displays the token values above each bar. The generated chart is then saved to a specified file path using `matplotlib.pyplot`."
        parameters[4]{name,type,description}:
          json_tokens,int,The number of tokens representing the JSON format.
          toon_tokens,int,The number of tokens representing the TOON format.
          savings_percent,float,The percentage of savings to be displayed in the chart's title.
          output_path,str,The file path where the generated bar chart will be saved.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.calculate_net_time:
      identifier: backend.main.calculate_net_time
      description:
        overall: "This function calculates the effective processing time by subtracting estimated sleep durations, primarily for rate-limiting, from the total elapsed time. It first determines the total duration between a start and end time. If the `model_name` does not start with 'gemini-', it returns the total duration without adjustments. For 'gemini-' models, it calculates the number of batches and estimates total sleep time based on a fixed sleep duration per batch. The final net time is the total duration minus this estimated sleep time, ensuring the result is never negative."
        parameters[5]{name,type,description}:
          start_time,float,The starting numerical timestamp or time value.
          end_time,float,The ending numerical timestamp or time value.
          total_items,int,The total number of items processed.
          batch_size,int,The number of items processed per batch.
          model_name,str,"The name of the model, used to determine if sleep time adjustments are needed."
        returns[1]{name,type,description}:
          net_time,float,"The calculated net processing time in a numerical format (e.g., seconds), adjusted for estimated rate-limit sleep times if the model is 'gemini-', otherwise the total duration. Returns 0 if total items are 0 or if the calculated net time is negative."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.main_workflow:
      identifier: backend.main.main_workflow
      description:
        overall: "The `main_workflow` function orchestrates a comprehensive analysis pipeline for a given GitHub repository. It begins by extracting API keys and model configurations, then clones the specified repository to access its files. The workflow proceeds to extract basic project information, construct a file tree, analyze code relationships, and build an Abstract Syntax Tree (AST) schema, which is subsequently enriched with relationship data. It then prepares inputs for a Helper LLM to analyze individual functions and classes, and finally uses a Main LLM to generate a comprehensive report based on the aggregated analysis. The function concludes by saving the generated report, along with performance metrics and token savings data, to disk."
        parameters[4]{name,type,description}:
          input,str,"The initial input string, expected to contain a GitHub repository URL for analysis."
          api_keys,dict,"A dictionary containing various API keys (e.g., 'gemini', 'gpt', 'scadsllm') and base URLs required for LLM interactions."
          model_names,dict,A dictionary specifying the names of the helper and main LLM models to be utilized in the workflow.
          status_callback,callable,An optional callback function used to report status updates and progress messages during the workflow execution.
        returns[2]{name,type,description}:
          report,str,"The final generated report from the Main LLM, detailing the repository analysis, or an error message if report generation failed."
          metrics,dict,"A dictionary containing performance metrics such as helper LLM processing time, main LLM processing time, total active time, model names used, and token savings data."
        usage_context:
          calls: "This function calls backend.AST_Schema.ASTAnalyzer, backend.AST_Schema.ASTAnalyzer.analyze_repository, backend.AST_Schema.ASTAnalyzer.merge_relationship_data, backend.HelperLLM.LLMHelper, backend.HelperLLM.LLMHelper.generate_for_classes, backend.HelperLLM.LLMHelper.generate_for_functions, backend.MainLLM.MainLLM, backend.MainLLM.MainLLM.call_llm, backend.basic_info.ProjektInfoExtractor, backend.basic_info.ProjektInfoExtractor.extrahiere_info, backend.getRepo.GitRepository, backend.main.calculate_net_time, backend.main.create_savings_chart, backend.main.update_status, backend.relationship_analyzer.ProjectAnalyzer, backend.relationship_analyzer.ProjectAnalyzer.analyze, backend.relationship_analyzer.ProjectAnalyzer.get_raw_relationships, schemas.types.ClassAnalysisInput, schemas.types.ClassContextInput, schemas.types.FunctionAnalysisInput, schemas.types.FunctionContextInput, and schemas.types.MethodContextInput."
          called_by: This function is called by no other functions.
      error: null
    backend.main.update_status:
      identifier: backend.main.update_status
      description:
        overall: "This function, `update_status`, is designed to handle status updates. It accepts a message string as input. If a `status_callback` function is available in the current scope, it invokes this callback with the provided message. Additionally, it logs the message at the INFO level using the `logging` module, ensuring that the status update is recorded."
        parameters[1]{name,type,description}:
          msg,str,The message string to be used for status updates and logging.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.notebook_workflow:
      identifier: backend.main.notebook_workflow
      description:
        overall: "The notebook_workflow function orchestrates the analysis of Jupyter notebooks found within a specified GitHub repository. It begins by cloning the repository and extracting basic project information. The function then converts the repository's notebooks into an XML-like structure, handling embedded images. It iterates through each processed notebook, constructing a payload for an external Large Language Model (LLM) based on the extracted data and images. Finally, it uses the LLM to generate individual reports for each notebook, concatenates them into a single comprehensive report, saves this report to a file, and returns the report along with execution metrics."
        parameters[4]{name,type,description}:
          input,str,"The input string, expected to contain a GitHub repository URL from which notebooks will be processed."
          api_keys,dict,"A dictionary containing API keys for various LLM services (e.g., 'gpt', 'gemini', 'scadsllm', 'ollama')."
          model,str,"The name of the Large Language Model to be used for generating reports (e.g., 'gpt-4', 'gemini-pro')."
          status_callback,callable or None,"An optional callback function that receives status messages during the workflow execution, allowing for real-time updates."
        returns[2]{name,type,description}:
          report,str,A comprehensive markdown string containing the concatenated analysis reports generated by the LLM for all processed notebooks.
          metrics,dict,"A dictionary providing performance and usage metrics for the workflow, including execution times and model details."
        usage_context:
          calls: "This function calls backend.MainLLM.MainLLM, backend.MainLLM.MainLLM.call_llm, backend.basic_info.ProjektInfoExtractor, backend.basic_info.ProjektInfoExtractor.extrahiere_info, backend.converter.process_repo_notebooks, backend.getRepo.GitRepository, backend.main.gemini_payload, and backend.main.update_status."
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.gemini_payload:
      identifier: backend.main.gemini_payload
      description:
        overall: "This function constructs a multi-part content payload, typically for a large language model API like Google Gemini. It takes basic contextual information, a notebook path, XML content representing the notebook structure, and a list of image data. The function first serializes basic information into a JSON string. It then processes the XML content, identifying and extracting text segments and image placeholders. For each image placeholder, it retrieves the corresponding base64 encoded image data from the provided 'images' list and formats it appropriately. The final output is a list of dictionaries, where each dictionary represents either a text part or an image part, interleaved in the order they appear in the XML content."
        parameters[4]{name,type,description}:
          basic_info,dict,"Contains general information about the project or context, which will be serialized into an introductory JSON block."
          nb_path,str,"The file path to the current notebook being processed, included in the introductory JSON block."
          xml_content,str,"The XML structure of the notebook, which may contain '<IMAGE_PLACEHOLDER>' tags that need to be replaced with actual image data."
          images,list,"A list of dictionaries, where each dictionary is expected to contain at least an 'data' key holding a base64 encoded string of an image, and a 'mime' type."
        returns[1]{name,type,description}:
          payload_content,list,"A list of dictionaries, where each dictionary represents a content part (either text or image_url) formatted according to the Gemini API's multi-part content structure."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.relationship_analyzer.path_to_module:
      identifier: backend.relationship_analyzer.path_to_module
      description:
        overall: "This function converts a given file system path into a Python module path string. It first calculates the relative path of the file from a specified project root, gracefully handling potential `ValueError` by using the file's base name if a relative path cannot be determined. The function then removes the `.py` extension if present and replaces file system path separators with dots. Finally, it specifically processes paths ending with `.__init__` to represent the package itself, returning the resulting module path string."
        parameters[2]{name,type,description}:
          filepath,str,The absolute or relative path to a Python file.
          project_root,str,"The root directory of the project, used as a base for calculating the relative path."
        returns[1]{name,type,description}:
          module_path,str,"The converted Python module path string (e.g., 'my_package.my_module')."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.encrypt_text:
      identifier: database.db.encrypt_text
      description:
        overall: "The `encrypt_text` function is designed to encrypt a given string using a `cipher_suite` object, likely an instance of `cryptography.fernet.Fernet`. It first performs a conditional check: if the input `text` is empty or if the `cipher_suite` is not initialized, it bypasses encryption and returns the original text. Otherwise, it prepares the text by stripping leading/trailing whitespace and encoding it to bytes. The byte-encoded text is then encrypted by the `cipher_suite` and subsequently decoded back into a string before being returned."
        parameters[1]{name,type,description}:
          text,str,The string value to be encrypted.
        returns[1]{name,type,description}:
          encrypted_text,str,"The encrypted version of the input text, or the original text if encryption was skipped due to empty input or uninitialized cipher suite."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.decrypt_text:
      identifier: database.db.decrypt_text
      description:
        overall: "This function attempts to decrypt a given string using an external `cipher_suite` object. It first performs a preliminary check, returning the original text if the input `text` is empty or if `cipher_suite` is not initialized. If decryption proceeds, the function strips whitespace from the input string, encodes it to bytes, performs the decryption, and then decodes the resulting bytes back into a string. A `try-except` block is used to gracefully handle any exceptions that may occur during the decryption process, returning the original text in case of failure."
        parameters[1]{name,type,description}:
          text,str,The string value to be decrypted.
        returns[1]{name,type,description}:
          decrypted_or_original_text,str,"The decrypted string if the operation is successful, or the original string if decryption conditions are not met or an error occurs during decryption."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.insert_user:
      identifier: database.db.insert_user
      description:
        overall: "This function creates a new user document by taking a username, name, and password. It hashes the provided password using `stauth.Hasher.hash` and initializes fields for various API keys as empty strings. The constructed user document is then inserted into the `dbusers` collection. It returns the unique identifier assigned to the newly created user."
        parameters[3]{name,type,description}:
          username,str,"The unique identifier for the user, which will also serve as the document's `_id`."
          name,str,The full name of the user.
          password,str,"The plain-text password for the user, which will be hashed before storage."
        returns[1]{name,type,description}:
          inserted_id,str,"The unique identifier (`_id`) of the newly inserted user document, which corresponds to the provided username."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_all_users:
      identifier: database.db.fetch_all_users
      description:
        overall: "This function, `fetch_all_users`, is responsible for retrieving all user records from a database collection. It executes a find operation on the `dbusers` object, which is presumed to be a database collection or similar data store. The results of this operation, typically a cursor or iterable of user documents, are then converted into a standard Python list before being returned."
        parameters[0]:
        returns[1]{name,type,description}:
          users,list,A list containing all user documents or records retrieved from the 'dbusers' collection.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_user:
      identifier: database.db.fetch_user
      description:
        overall: This function is designed to retrieve a single user record from a database collection named `dbusers`. It takes a username as input and uses it to query the `_id` field within the collection. The function returns the first document that matches the given username.
        parameters[1]{name,type,description}:
          username,str,"The unique identifier for the user to be fetched, which is used to match the `_id` field in the database."
        returns[1]{name,type,description}:
          user_document,dict | None,"A dictionary representing the user document if found, or `None` if no user matches the provided username."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_user_name:
      identifier: database.db.update_user_name
      description:
        overall: This function updates the 'name' field for a user identified by their '_id' in the 'dbusers' collection. It takes the current username (which serves as the document's _id) and the new name to be assigned. The function utilizes the `update_one` method to locate the user by their '_id' and then sets the 'name' attribute to the provided `new_name`. It returns an integer representing the count of documents that were successfully modified by this operation.
        parameters[2]{name,type,description}:
          username,str,"The current username, which is used as the unique identifier (_id) to locate the user document in the database."
          new_name,str,The new name to be set for the identified user.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.update_gemini_key:
      identifier: database.db.update_gemini_key
      description:
        overall: "This function updates a user's Gemini API key in the database. It first encrypts the provided API key using the `encrypt_text` function. Then, it uses the `dbusers` collection to find the user by their username and sets the `gemini_api_key` field to the encrypted value. The function returns the count of modified documents."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose Gemini API key needs to be updated.
          gemini_api_key,str,The new Gemini API key to be stored for the user. This key will be encrypted before storage.
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents modified by the update operation, typically 0 or 1."
        usage_context:
          calls: This function calls database.db.encrypt_text.
          called_by: This function is called by no other functions.
      error: null
    database.db.update_gpt_key:
      identifier: database.db.update_gpt_key
      description:
        overall: "This function updates a user's GPT API key in the database. It takes a username and a new GPT API key as input. The provided API key is first stripped of any leading/trailing whitespace and then encrypted using the `encrypt_text` function. The function then performs an update operation on the `dbusers` collection, setting the `gpt_api_key` field for the specified user. Finally, it returns the count of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose GPT API key is to be updated.
          gpt_api_key,str,The new GPT API key to be stored for the user. It will be stripped of whitespace and encrypted before storage.
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents modified by the update operation, typically 0 or 1."
        usage_context:
          calls: This function calls database.db.encrypt_text.
          called_by: This function is not called by any other functions.
      error: null
    database.db.update_ollama_url:
      identifier: database.db.update_ollama_url
      description:
        overall: This function updates the Ollama base URL for a specific user in the database. It takes a username and a new Ollama base URL as input. The function strips any leading or trailing whitespace from the provided URL before storing it. It then returns the count of documents that were modified by this operation.
        parameters[2]{name,type,description}:
          username,str,The username identifying the user whose Ollama base URL is to be updated.
          ollama_base_url,str,The new Ollama base URL to set for the user. Leading and trailing whitespace will be removed before storage.
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents modified by the update operation. A value of 1 indicates success if the user existed, 0 otherwise."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_opensrc_key:
      identifier: database.db.update_opensrc_key
      description:
        overall: "This function updates the Open Source API key for a specified user in the database. It first encrypts the provided API key, ensuring any leading or trailing whitespace is removed. The encrypted key is then stored in the 'opensrc_api_key' field for the user identified by their username. The function returns the count of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          username,str,The username of the user whose Open Source API key needs to be updated.
          opensrc_api_key,str,The new Open Source API key to be stored. It will be stripped of whitespace and encrypted before storage.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents (users) that were modified by the update operation.
        usage_context:
          calls: This function calls database.db.encrypt_text.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_opensrc_url:
      identifier: database.db.update_opensrc_url
      description:
        overall: This function updates a user's 'opensrc_base_url' in the database. It takes a 'username' and a new 'opensrc_base_url' string. The function uses 'dbusers.update_one' to locate the user by their '_id' (username) and sets the 'opensrc_base_url' field to the provided URL after stripping any leading or trailing whitespace. It returns the count of documents that were modified by this operation.
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose 'opensrc_base_url' is to be updated.
          opensrc_base_url,str,The new base URL for opensource projects to be associated with the user. Leading/trailing whitespace will be removed.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.fetch_gemini_key:
      identifier: database.db.fetch_gemini_key
      description:
        overall: "This function retrieves the Gemini API key associated with a specific username from a database. It queries the 'dbusers' collection, searching for a document where the '_id' matches the provided username. If a user document is found, it extracts and returns the 'gemini_api_key' field. If no user is found or the key is not present, the function returns None."
        parameters[1]{name,type,description}:
          username,str,The username used to identify the user in the database.
        returns[1]{name,type,description}:
          gemini_api_key,str | None,"The Gemini API key string if found for the user, otherwise None."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_ollama_url:
      identifier: database.db.fetch_ollama_url
      description:
        overall: "This function retrieves the Ollama base URL associated with a given username from a database. It queries the `dbusers` collection, using the provided username as the document's `_id`. It specifically projects only the `ollama_base_url` field. The function returns the extracted URL if a user is found, otherwise it returns `None`."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose Ollama base URL is to be fetched.
        returns[1]{name,type,description}:
          ollama_base_url,"Optional[str]","The Ollama base URL for the specified user, or None if the user is not found in the database."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_gpt_key:
      identifier: database.db.fetch_gpt_key
      description:
        overall: "This function, `fetch_gpt_key`, is designed to retrieve a user's GPT API key from a database. It takes a username as input and queries a `dbusers` collection to find a matching user document. The function specifically projects the `gpt_api_key` field, excluding the `_id`. It returns the found API key or `None` if no user is found with the given username."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose GPT API key is to be fetched.
        returns[1]{name,type,description}:
          gpt_api_key,str | None,"The GPT API key associated with the username, or None if no user document is found."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_opensrc_key:
      identifier: database.db.fetch_opensrc_key
      description:
        overall: "This function is designed to retrieve a user's 'opensrc_api_key' from a database. It takes a username as input and queries the 'dbusers' collection. The function specifically looks for a document where the '_id' matches the provided username and projects only the 'opensrc_api_key' field. If a user is found, it returns the associated API key; otherwise, it returns None."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose 'opensrc_api_key' is to be fetched.
        returns[1]{name,type,description}:
          opensrc_api_key,str | None,"The 'opensrc_api_key' associated with the given username, or None if the user is not found."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_opensrc_url:
      identifier: database.db.fetch_opensrc_url
      description:
        overall: "This function retrieves the 'opensrc_base_url' for a specified user from a database. It queries the 'dbusers' collection, searching for a document where the '_id' field matches the provided 'username'. If a user document is found, it extracts the 'opensrc_base_url' field. The function returns this URL if available, or None if the user is not found or the field is absent."
        parameters[1]{name,type,description}:
          username,str,The unique identifier (username) of the user whose Open Source base URL is to be fetched.
        returns[1]{name,type,description}:
          opensrc_base_url,str | None,"The base URL for the user's open source profile if found, otherwise None."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.delete_user:
      identifier: database.db.delete_user
      description:
        overall: "The `delete_user` function is responsible for removing a specific user record from a database collection. It takes a username as input, which serves as the primary key (`_id`) to locate the user document. The function then executes a `delete_one` operation on the `dbusers` collection and returns the count of documents that were successfully deleted."
        parameters[1]{name,type,description}:
          username,str,The unique identifier (username) of the user to be deleted from the database.
        returns[1]{name,type,description}:
          deleted_count,int,"The number of documents deleted by the operation. For `delete_one`, this will typically be 0 or 1."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.get_decrypted_api_keys:
      identifier: database.db.get_decrypted_api_keys
      description:
        overall: "This function retrieves a user's API keys and base URLs from a database based on the provided username. It queries the 'dbusers' collection to find the user. If the user is not found, it returns a tuple of None values. Otherwise, it decrypts specific API keys (Gemini, GPT, open-source) using the 'decrypt_text' function and retrieves other URLs (Ollama, open-source) directly from the user object. Finally, it returns all these decrypted keys and URLs."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose API keys and URLs are to be retrieved.
        returns[5]{name,type,description}:
          gemini_plain,str | None,"The decrypted Gemini API key, or None if the user is not found or the key is absent."
          ollama_plain,str | None,"The Ollama base URL, or None if the user is not found or the URL is absent."
          gpt_plain,str | None,"The decrypted GPT API key, or None if the user is not found or the key is absent."
          opensrc_plain,str | None,"The decrypted open-source API key, or None if the user is not found or the key is absent."
          opensrc_url,str | None,"The open-source base URL, or None if the user is not found or the URL is absent."
        usage_context:
          calls: This function calls database.db.decrypt_text.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.insert_chat:
      identifier: database.db.insert_chat
      description:
        overall: "This function creates a new chat entry within a database. It constructs a dictionary containing a unique identifier generated by `uuid.uuid4()`, the provided username, the chat name, and the current timestamp using `datetime.now()`. This chat dictionary is then inserted into a database collection, likely `dbchats`, using the `insert_one` method. The function's primary purpose is to persist new chat metadata into the database. It returns the unique ID assigned to the newly inserted chat document."
        parameters[2]{name,type,description}:
          username,str,The username associated with the new chat entry.
          chat_name,str,The name of the chat to be created.
        returns[1]{name,type,description}:
          inserted_id,Any,"The unique identifier (e.g., ObjectId or string) of the newly inserted chat document in the database."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions.
      error: null
    database.db.fetch_chats_by_user:
      identifier: database.db.fetch_chats_by_user
      description:
        overall: This function is designed to retrieve all chat records associated with a given username from a database collection named `dbchats`. It performs a query filtering by the provided username and then sorts the results chronologically by their creation timestamp in ascending order. The function converts the database cursor into a list before returning it.
        parameters[1]{name,type,description}:
          username,str,The username for which to fetch chat records.
        returns[1]{name,type,description}:
          chats,list,"A list of chat records associated with the specified username, sorted by creation date."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.check_chat_exists:
      identifier: database.db.check_chat_exists
      description:
        overall: "This function, `check_chat_exists`, is designed to verify the existence of a specific chat within a database collection named `dbchats`. It takes a username and a chat name as input. The function queries the `dbchats` collection to find a document that matches both the provided username and chat name. It returns a boolean value indicating whether such a chat was found."
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to check for existence.
          chat_name,str,The name of the chat to check for existence.
        returns[1]{name,type,description}:
          exists,bool,"True if a chat matching the given username and chat name is found in the database, False otherwise."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.rename_chat_fully:
      identifier: database.db.rename_chat_fully
      description:
        overall: "This function renames a chat and all its associated exchanges (messages) in the database. It first updates the chat entry in the `dbchats` collection by changing its `chat_name` from `old_name` to `new_name` for a given `username`. Subsequently, it updates all related exchanges in the `dbexchanges` collection to reflect the new chat name. The function returns the number of documents modified during the initial chat entry update operation."
        parameters[3]{name,type,description}:
          username,str,The username associated with the chat to be renamed.
          old_name,str,The current name of the chat that needs to be changed.
          new_name,str,The new desired name for the chat.
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents modified by the `dbchats.update_one` operation, indicating if the chat entry was successfully renamed."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.insert_exchange:
      identifier: database.db.insert_exchange
      description:
        overall: "This function inserts a new exchange record into a database collection. It generates a unique identifier for the exchange, constructs a dictionary containing various details such as the question, answer, feedback, user information, and performance metrics. It then attempts to insert this dictionary into the 'dbexchanges' collection. If the insertion is successful, it returns the newly generated ID; otherwise, it catches any exceptions, prints an error, and returns None."
        parameters[13]{name,type,description}:
          question,str,The question string associated with the exchange.
          answer,str,The answer string generated for the exchange.
          feedback,str,The feedback string provided for the exchange.
          username,str,The username associated with the exchange.
          chat_name,str,The name of the chat where the exchange occurred.
          helper_used,str,"Indicates if a helper component was used, defaults to an empty string."
          main_used,str,"Indicates if the main component was used, defaults to an empty string."
          total_time,str,"The total time taken for the exchange, defaults to an empty string."
          helper_time,str,"The time taken by the helper component, defaults to an empty string."
          main_time,str,"The time taken by the main component, defaults to an empty string."
          json_tokens,int,"The number of JSON tokens used, defaults to 0."
          toon_tokens,int,"The number of TOON tokens used, defaults to 0."
          savings_percent,float,"The percentage of savings achieved, defaults to 0.0."
        returns[2]{name,type,description}:
          new_id,str,The unique identifier of the newly inserted exchange if successful.
          None,NoneType,Returns None if an error occurs during the database insertion.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_exchanges_by_user:
      identifier: database.db.fetch_exchanges_by_user
      description:
        overall: This function retrieves a list of exchange records associated with a specific username from the `dbexchanges` collection. It queries the database using the provided username and sorts the results chronologically by their creation timestamp in ascending order. The sorted database documents are then converted into a list and returned.
        parameters[1]{name,type,description}:
          username,str,The username used to filter and fetch relevant exchange records from the database.
        returns[1]{name,type,description}:
          exchanges,list,"A list of exchange records (documents) found for the given username, sorted by their 'created_at' timestamp."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.fetch_exchanges_by_chat:
      identifier: database.db.fetch_exchanges_by_chat
      description:
        overall: This function retrieves a list of exchange documents from the 'dbexchanges' collection. It filters the exchanges by a specified username and chat name. The results are then sorted in ascending order based on their 'created_at' timestamp and returned as a list.
        parameters[2]{name,type,description}:
          username,str,The username used to filter the exchanges.
          chat_name,str,The name of the chat used to filter the exchanges.
        returns[1]{name,type,description}:
          exchanges,list,"A list of exchange documents matching the provided username and chat name, sorted by creation date."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_exchange_feedback:
      identifier: database.db.update_exchange_feedback
      description:
        overall: "This function updates the feedback value for a specific exchange record in a database. It takes an exchange identifier and an integer feedback value as input. The function uses a database client, likely `dbexchanges`, to perform an `update_one` operation, targeting the document by its `_id` and setting the `feedback` field. It then returns the count of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          exchange_id,Any,The unique identifier of the exchange record to be updated in the database.
          feedback,int,The integer feedback value to be set for the specified exchange record.
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents that were modified by the update operation, typically 0 or 1."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_exchange_feedback_message:
      identifier: database.db.update_exchange_feedback_message
      description:
        overall: This function updates a specific exchange record in the database by setting or modifying its 'feedback_message' field. It takes an exchange identifier and the new feedback message as input. The function leverages a database client to perform an update operation on a document matching the provided exchange ID.
        parameters[2]{name,type,description}:
          exchange_id,Any,The unique identifier for the exchange document to be updated. Its specific type is inferred from its usage as a MongoDB '_id'.
          feedback_message,str,The new feedback message to be stored for the specified exchange.
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents modified by the update operation, typically 0 or 1."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.delete_exchange_by_id:
      identifier: database.db.delete_exchange_by_id
      description:
        overall: This function is responsible for deleting a single exchange document from the 'dbexchanges' collection within a database. It takes an exchange ID as input and uses it to locate and remove the corresponding document. The function leverages a database client's 'delete_one' method to perform the operation. It then reports the number of documents that were successfully deleted.
        parameters[1]{name,type,description}:
          exchange_id,str,The unique identifier of the exchange document to be deleted from the database.
        returns[1]{name,type,description}:
          deleted_count,int,The number of documents that were deleted by the operation. This will typically be 0 or 1.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.delete_full_chat:
      identifier: database.db.delete_full_chat
      description:
        overall: "This function is responsible for completely deleting a chat and all associated messages (exchanges) for a given user. It first removes all messages linked to the specified chat name and username using `dbexchanges.delete_many`. Subsequently, it deletes the chat entry itself from the chat list using `dbchats.delete_one`. The function ensures data consistency between frontend and backend by removing both the chat and its contents."
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to be deleted.
          chat_name,str,The name of the chat to be deleted.
        returns[1]{name,type,description}:
          deleted_count,int,"The number of chat documents deleted by the `dbchats.delete_one` operation, typically 1 if the chat existed and was deleted, or 0 otherwise."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    frontend.frontend.clean_names:
      identifier: frontend.frontend.clean_names
      description:
        overall: "This function processes a list of strings, where each string is expected to represent a path or a hierarchical identifier. It iterates through the input list, and for each string, it splits the string by the '/' character. The function then extracts the last segment from the split string and compiles these segments into a new list, which is then returned. This effectively 'cleans' the names by removing any preceding path information."
        parameters[1]{name,type,description}:
          model_list,list,"A list of strings, where each string is typically a path or an identifier containing '/' characters."
        returns[1]{name,type,description}:
          cleaned_names,list,"A new list of strings, where each string is the last segment of the corresponding input string after splitting by '/'."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.get_filtered_models:
      identifier: frontend.frontend.get_filtered_models
      description:
        overall: "This function filters a list of models (`source_list`) based on a specified category (`category_name`). It retrieves keywords associated with the category from a global `CATEGORY_KEYWORDS` mapping. If the category's keywords include \"STANDARD\", it returns only those models from the `source_list` that are also present in `STANDARD_MODELS`. Otherwise, it iterates through the `source_list` and includes models whose names (case-insensitively) contain any of the category's keywords. If no models match the keywords, the original `source_list` is returned."
        parameters[2]{name,type,description}:
          source_list,list,The initial list of models (presumably strings) to be filtered.
          category_name,str,The name of the category used to determine the filtering keywords.
        returns[1]{name,type,description}:
          filtered_models,list,"A list of models filtered based on the category keywords, or the original `source_list` if no matches are found or if the 'STANDARD' category logic is applied."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.save_gemini_cb:
      identifier: frontend.frontend.save_gemini_cb
      description:
        overall: "This function serves as a callback to save a user-provided Gemini API key. It retrieves the potential new key from the Streamlit session state. If a non-empty key is found, it proceeds to update the user's Gemini key in the database using the `db.update_gemini_key` function, associating it with the current username from the session state. After a successful update, the `in_gemini_key` field is cleared from the session state, and a success toast notification is displayed to the user."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: This function calls database.db.update_gemini_key.
          called_by: This function is called by no other functions.
      error: null
    frontend.frontend.save_ollama_cb:
      identifier: frontend.frontend.save_ollama_cb
      description:
        overall: "This function is designed to save a user-provided Ollama URL. It first attempts to retrieve a potential new Ollama URL from the Streamlit session state, specifically from the key 'in_ollama_url'. If a valid URL is found, it proceeds to update this URL in the database using the `db.update_ollama_url` function, associating it with the current user's username also retrieved from the session state. Upon successful update, a confirmation toast message is displayed to the user."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: This function calls database.db.update_ollama_url.
          called_by: This function is called by no other functions.
      error: null
    frontend.frontend.load_data_from_db:
      identifier: frontend.frontend.load_data_from_db
      description:
        overall: "This function loads chat and exchange data from the database for a specified user and populates the Streamlit session state. It first checks if the data for the current user is already loaded; if not, it proceeds to fetch the data. The function initializes the `st.session_state.chats` dictionary, then retrieves defined chats and their corresponding exchanges from the database. It handles cases where exchanges might exist for undefined chats, ensuring data consistency and setting default feedback values. Finally, it ensures at least one chat exists, creating a default 'Chat 1' if necessary, and sets the `active_chat` in the session state."
        parameters[1]{name,type,description}:
          username,str,The username for whom to load chat and exchange data from the database.
        returns[0]:
        usage_context:
          calls: "This function calls database.db.fetch_chats_by_user, database.db.fetch_exchanges_by_user, and database.db.insert_chat."
          called_by: This function is called by no other functions.
      error: null
    frontend.frontend.handle_feedback_change:
      identifier: frontend.frontend.handle_feedback_change
      description:
        overall: "This function is designed to process a change in feedback for a specific exchange object. It first updates the 'feedback' key within the provided 'ex' dictionary with the new 'val'. Subsequently, it persists this feedback change to the database by invoking 'db.update_exchange_feedback', passing the exchange's '_id' and the new feedback value. Finally, it triggers a rerun of the Streamlit application, likely to refresh the UI and reflect the updated feedback."
        parameters[2]{name,type,description}:
          ex,dict,"The exchange object, expected to be a dictionary containing at least an '_id' and a 'feedback' key."
          val,Any,The new feedback value to be assigned to the exchange.
        returns[0]:
        usage_context:
          calls: This function calls database.db.update_exchange_feedback.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.handle_delete_exchange:
      identifier: frontend.frontend.handle_delete_exchange
      description:
        overall: "This function handles the deletion of a specific exchange. It first removes the exchange from the database using its ID. Subsequently, it updates the Streamlit session state by removing the exchange from the relevant chat's exchanges list, if it exists. Finally, it triggers a Streamlit rerun to reflect the changes in the UI."
        parameters[2]{name,type,description}:
          chat_name,str,The name of the chat from which the exchange should be removed in the session state.
          ex,dict,"The exchange object to be deleted, expected to contain an '_id' key for database deletion and to be present in the session state's exchanges list."
        returns[0]:
        usage_context:
          calls: This function calls database.db.delete_exchange_by_id.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.handle_delete_chat:
      identifier: frontend.frontend.handle_delete_chat
      description:
        overall: "This function handles the deletion of a specified chat for a given user. It first calls the database to permanently remove the chat. Subsequently, it updates the Streamlit session state by removing the chat from the active list. If no chats remain after deletion, a new default chat named \"Chat 1\" is created in the database and set as the active chat. Finally, it triggers a Streamlit rerun to refresh the UI."
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to be deleted.
          chat_name,str,The name of the chat to be deleted.
        returns[0]:
        usage_context:
          calls: This function calls database.db.delete_full_chat and database.db.insert_chat.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.extract_repo_name:
      identifier: frontend.frontend.extract_repo_name
      description:
        overall: "This function is designed to extract a repository name from a given text string. It first attempts to find a URL within the text using a regular expression. If a URL is found, it parses the URL to isolate the path component. The last segment of this path is then considered the potential repository name. It includes logic to remove the \".git\" suffix if present. If no URL is found or a repository name cannot be successfully extracted, the function returns None."
        parameters[1]{name,type,description}:
          text,str,"The input string from which to attempt to extract a repository name, potentially containing a URL."
        returns[2]{name,type,description}:
          repo_name,str,The extracted repository name as a string.
          None,None,Indicates that no repository name could be extracted from the input text.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.stream_text_generator:
      identifier: frontend.frontend.stream_text_generator
      description:
        overall: "This function acts as a generator that takes a string of text and yields its words sequentially. It splits the input text by spaces and then iterates through each word. For every word, it yields the word followed by a space, introducing a small delay to simulate a streaming effect. This is useful for displaying text incrementally, such as in a user interface."
        parameters[1]{name,type,description}:
          text,str,The input string that will be split into words and streamed.
        returns[1]{name,type,description}:
          word_chunk,str,"A single word from the input text, followed by a space, yielded sequentially."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    frontend.frontend.render_text_with_mermaid:
      identifier: frontend.frontend.render_text_with_mermaid
      description:
        overall: "This function processes a given markdown text, identifying and rendering embedded Mermaid diagrams separately from standard markdown content. It splits the input text based on '```mermaid' delimiters. Non-Mermaid sections are rendered as standard markdown, with an option to stream the text. Mermaid code blocks are attempted to be rendered using a specialized Streamlit Mermaid component, falling back to displaying the code block if rendering fails."
        parameters[2]{name,type,description}:
          markdown_text,str,"The input text, which may contain standard markdown and embedded Mermaid diagram code blocks."
          should_stream,bool,A boolean flag indicating whether non-Mermaid text parts should be streamed to the output. Defaults to False.
        returns[1]{name,type,description}:
          None,None,This function does not explicitly return a value. It performs side effects by rendering content to a Streamlit application. It returns implicitly if 'markdown_text' is empty.
        usage_context:
          calls: This function calls frontend.frontend.stream_text_generator.
          called_by: This function is called by no other functions.
      error: null
    frontend.frontend.render_exchange:
      identifier: frontend.frontend.render_exchange
      description:
        overall: "This function `render_exchange` is responsible for rendering a single chat exchange (user question and assistant answer) within a Streamlit application. It first displays the user's question using `st.chat_message`. For the assistant's response, it provides an interactive toolbar that includes options for feedback (like/dislike buttons), adding a comment via a popover, downloading the answer as Markdown, and deleting the exchange. It handles error states in the assistant's answer by displaying an error message and offering only a delete option. Finally, it renders the answer content using `render_text_with_mermaid` within a bordered container."
        parameters[2]{name,type,description}:
          ex,dict,"A dictionary-like object representing a chat exchange, containing the user's question, the assistant's answer, feedback status, and other metadata like '_id' and 'feedback_message'."
          current_chat_name,str,"The name of the current chat, used as context when handling the deletion of an exchange."
        returns[0]:
        usage_context:
          calls: "This function calls database.db.update_exchange_feedback_message, frontend.frontend.handle_delete_exchange, frontend.frontend.handle_feedback_change, and frontend.frontend.render_text_with_mermaid."
          called_by: This function is called by no other functions.
      error: null
  classes:
    backend.AST_Schema.ASTVisitor:
      identifier: backend.AST_Schema.ASTVisitor
      description:
        overall: "The ASTVisitor class extends `ast.NodeVisitor` to perform a structured traversal of a Python Abstract Syntax Tree (AST). Its primary purpose is to extract and organize metadata about imports, top-level functions, and class definitions, including their nested methods, into a hierarchical schema. This class is instrumental in building a comprehensive understanding of a Python module's structure and contents."
        init_method:
          description: "The constructor initializes the ASTVisitor instance with the source code of the file being analyzed, its file path, and the project's root directory. It calculates the module path and sets up an empty schema dictionary to store discovered imports, functions, and classes. It also initializes an internal `_current_class` attribute to track the current class being visited during traversal."
          parameters[3]{name,type,description}:
            source_code,str,The raw source code of the file to be analyzed.
            file_path,str,The absolute path to the file being analyzed.
            project_root,str,The root directory of the entire project.
        methods[5]:
          - identifier: visit_Import
            description:
              overall: "This method is part of the `ast.NodeVisitor` pattern and is automatically called when an `ast.Import` node is encountered during AST traversal. It iterates through each alias specified in the import statement and appends the name of the imported module to the `imports` list within the `self.schema` dictionary. After processing the current import node, it calls `self.generic_visit(node)` to ensure that the traversal continues to any child nodes."
              parameters[1]{name,type,description}:
                node,ast.Import,The AST node representing an 'import' statement.
              returns[0]:
              usage_context:
                calls: This method implicitly calls `self.generic_visit` to continue AST traversal.
                called_by: This method is called by the `ast.NodeVisitor` framework when an `ast.Import` node is encountered during AST traversal.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method is invoked by the AST visitor when an `ast.ImportFrom` node is encountered, which represents a 'from ... import ...' statement. It iterates through the names being imported from the specified module and constructs a fully qualified import string (e.g., 'module.name'). This constructed string is then appended to the `imports` list in `self.schema`. Finally, it calls `self.generic_visit(node)` to ensure that the AST traversal continues to any child nodes."
              parameters[1]{name,type,description}:
                node,ast.ImportFrom,The AST node representing a 'from ... import ...' statement.
              returns[0]:
              usage_context:
                calls: This method implicitly calls `self.generic_visit` to continue AST traversal.
                called_by: This method is called by the `ast.NodeVisitor` framework when an `ast.ImportFrom` node is encountered during AST traversal.
            error: null
          - identifier: visit_ClassDef
            description:
              overall: "This method is triggered when an `ast.ClassDef` node is visited, indicating a class definition in the source code. It constructs a unique identifier for the class using the module path and the class name. It then creates a dictionary containing comprehensive metadata about the class, including its identifier, name, docstring, source code segment, and line numbers, and appends this information to the `classes` list within `self.schema`. Crucially, it sets `self._current_class` to this `class_info` before recursively visiting child nodes, which allows nested methods to be correctly associated with this class. After all children have been visited, `self._current_class` is reset to `None`."
              parameters[1]{name,type,description}:
                node,ast.ClassDef,The AST node representing a class definition.
              returns[0]:
              usage_context:
                calls: "This method calls `ast.get_docstring` and `ast.get_source_segment` to extract class details, and `self.generic_visit` to continue AST traversal."
                called_by: This method is called by the `ast.NodeVisitor` framework when an `ast.ClassDef` node is encountered during AST traversal.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method is called when an `ast.FunctionDef` node, representing a regular function definition, is encountered during AST traversal. It first checks if a class is currently being visited by examining `self._current_class`. If a class is active, the function is treated as a method, and its metadata (identifier, name, arguments, docstring, line numbers) is appended to the `method_context` list of the current class. If no class is active, it's treated as a top-level function, and its metadata is appended to the `functions` list in `self.schema`. In both scenarios, `self.generic_visit(node)` is called to continue the AST traversal."
              parameters[1]{name,type,description}:
                node,ast.FunctionDef,The AST node representing a function definition.
              returns[0]:
              usage_context:
                calls: "This method calls `ast.get_docstring` and `ast.get_source_segment` to extract function details, and `self.generic_visit` to continue AST traversal."
                called_by: This method is called by the `ast.NodeVisitor` framework when an `ast.FunctionDef` node is encountered during AST traversal.
            error: null
          - identifier: visit_AsyncFunctionDef
            description:
              overall: "This method is invoked when an `ast.AsyncFunctionDef` node is encountered, which represents an asynchronous function definition. Its implementation simply delegates the processing of the asynchronous function node to the `visit_FunctionDef` method. This design choice means that asynchronous functions are treated identically to regular functions in terms of how their metadata is extracted and stored within the overall schema."
              parameters[1]{name,type,description}:
                node,ast.AsyncFunctionDef,The AST node representing an asynchronous function definition.
              returns[0]:
              usage_context:
                calls: This method explicitly calls `self.visit_FunctionDef` to process the async function node.
                called_by: This method is called by the `ast.NodeVisitor` framework when an `ast.AsyncFunctionDef` node is encountered during AST traversal.
            error: null
        usage_context:
          dependencies: This class depends on `backend.AST_Schema.path_to_module` for converting file paths to module paths.
          instantiated_by: "This class is not explicitly instantiated in the provided context, suggesting it might be instantiated by a higher-level AST processing orchestrator."
      error: null
    backend.AST_Schema.ASTAnalyzer:
      identifier: backend.AST_Schema.ASTAnalyzer
      description:
        overall: "The ASTAnalyzer class is responsible for processing a collection of Python source files within a repository to generate a structured Abstract Syntax Tree (AST) schema. It can parse individual files to extract their structural components like functions, classes, and imports, and then integrate dynamic relationship data (like calls and instantiations) into this schema. This class serves as a central component for static code analysis, providing a detailed, machine-readable representation of a codebase's structure and interdependencies."
        init_method:
          description: "Initializes an instance of the ASTAnalyzer class. This constructor does not perform any specific setup or attribute assignments, serving as a placeholder."
          parameters[0]:
        methods[2]:
          - identifier: merge_relationship_data
            description:
              overall: "This method integrates relationship data, specifically incoming and outgoing calls and instantiations, into a pre-existing full AST schema. It iterates through all functions and classes identified within the schema, updating their respective 'context' fields with the provided relationship information. For classes, it also computes and stores a list of external dependencies based on the methods' outgoing calls, excluding calls to methods within the same class."
              parameters[2]{name,type,description}:
                full_schema,dict,"The comprehensive AST schema of the repository, containing parsed file, function, and class nodes."
                raw_relationships,dict,"A dictionary containing raw 'outgoing' and 'incoming' call/instantiation relationships, typically derived from dynamic analysis or a separate static analysis pass."
              returns[1]{name,type,description}:
                full_schema,dict,The updated full schema dictionary with the relationship data integrated into the context of functions and classes.
              usage_context:
                calls: "This method primarily accesses dictionary methods like get and items, and list methods like add and append. It does not call other custom functions or classes."
                called_by: This method is not explicitly called by any other functions or methods within the provided context.
            error: null
          - identifier: analyze_repository
            description:
              overall: "This method processes a list of file objects from a Git repository to construct a comprehensive AST schema. It filters for Python files, reads their content, and then uses the ASTVisitor to parse each file's Abstract Syntax Tree, extracting structural components like functions, classes, and imports. The collected data is organized into a 'full_schema' dictionary, with robust error handling for parsing failures."
              parameters[2]{name,type,description}:
                files,list,"A list of file objects, where each object is expected to have 'path' and 'content' attributes representing a source file."
                repo,GitRepository,"An object representing the Git repository, used to provide context for file paths, though not directly used for content retrieval in this method."
              returns[1]{name,type,description}:
                full_schema,dict,"A dictionary representing the AST schema of the analyzed repository, structured by file paths and containing parsed AST nodes."
              usage_context:
                calls: "This method calls os.path.commonpath, os.path.isfile, os.path.dirname, ast.parse, and instantiates backend.AST_Schema.ASTVisitor."
                called_by: This method is not explicitly called by any other functions or methods within the provided context.
            error: null
        usage_context:
          dependencies: This class depends on backend.AST_Schema.ASTVisitor for parsing Abstract Syntax Trees.
          instantiated_by: This class is not explicitly instantiated by any known components within the provided context.
      error: null
    backend.File_Dependency.FileDependencyGraph:
      identifier: backend.File_Dependency.FileDependencyGraph
      description:
        overall: "The FileDependencyGraph class, inheriting from ast.NodeVisitor, is designed to analyze Python source code files to build a graph of import dependencies. It utilizes a class-level dictionary, import_dependencies, to store the mapping of files to their imported modules. The class traverses the Abstract Syntax Tree (AST) of a given file, identifying both absolute and relative import statements. Its primary responsibility is to accurately resolve module names, especially for complex relative imports, and populate this import_dependencies dictionary, thereby providing a structured representation of file-to-module dependencies within a repository."
        init_method:
          description: This constructor initializes an instance of the FileDependencyGraph by setting the filename attribute to the path of the file currently being analyzed and repo_root to the root directory of the repository. These instance attributes are essential for resolving file paths and module dependencies during the AST traversal.
          parameters[2]{name,type,description}:
            filename,str,The path to the file for which dependencies are being analyzed.
            repo_root,Any,"The root directory of the repository, used for resolving absolute paths."
        methods[3]:
          - identifier: _resolve_module_name
            description:
              overall: "This method is responsible for resolving relative import statements (e.g., from .. import name). It calculates the correct base directory based on the import level and the current file's location within the repository. It then checks if the imported names correspond to existing module files or symbols exported via __init__.py files. If successful, it returns a list of resolved module/symbol names; otherwise, it raises an ImportError."
              parameters[1]{name,type,description}:
                node,ImportFrom,An AST ImportFrom node representing the relative import statement to be resolved.
              returns[1]{name,type,description}:
                resolved,"list[str]",A list of resolved module or symbol names.
              usage_context:
                calls: "This method calls get_all_temp_files to retrieve all files in the repository, Path for path manipulation, iskeyword to check if a name is a Python keyword, module_file_exists to verify file existence, and init_exports_symbol to check __init__.py exports."
                called_by: This method is called by visit_ImportFrom when processing relative import statements.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method processes Import and ImportFrom AST nodes to record file dependencies. It iterates through the imported names and adds them to the import_dependencies dictionary, mapping the current self.filename to a set of imported module names. If a base_name is provided (typically from visit_ImportFrom for specific module parts), it uses that; otherwise, it uses the alias name directly. After processing, it calls generic_visit to continue traversing the AST."
              parameters[2]{name,type,description}:
                node,Import | ImportFrom,An AST node representing either an import or from ... import ... statement.
                base_name,str | None,"An optional base name for the imported module, used primarily for from ... import ... statements where only the module part is relevant."
              returns[0]:
              usage_context:
                calls: This method calls self.generic_visit to continue AST traversal.
                called_by: This method is called by visit_ImportFrom after determining the base module name.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method specifically handles ImportFrom AST nodes. It first attempts to extract the module name from the node. If a direct module name exists (e.g., from a.b.c import d), it extracts the last part (c) and passes it to visit_Import. If the import is relative (no direct module name, indicated by node.level > 0), it calls _resolve_module_name to resolve the actual module paths. Any resolved base names are then passed to visit_Import. Errors during relative import resolution are caught and printed. Finally, it calls generic_visit to continue AST traversal."
              parameters[1]{name,type,description}:
                node,ImportFrom,An AST ImportFrom node representing a from ... import ... statement.
              returns[0]:
              usage_context:
                calls: This method calls self._resolve_module_name to handle relative imports and self.visit_Import to record the dependencies. It also calls self.generic_visit for AST traversal.
                called_by: This method is part of the NodeVisitor pattern and is implicitly called by the AST traversal mechanism when an ImportFrom node is encountered.
            error: null
        usage_context:
          dependencies: "This class depends on get_all_temp_files for repository file discovery, init_exports_symbol for checking __init__.py exports, and module_file_exists for verifying module file existence."
          instantiated_by: This class is not explicitly instantiated by any known components in the provided context.
      error: null
    backend.HelperLLM.LLMHelper:
      identifier: backend.HelperLLM.LLMHelper
      description:
        overall: "The LLMHelper class serves as a robust interface for interacting with various large language models (LLMs) to generate structured documentation for Python functions and classes. It centralizes the configuration and interaction with LLM providers like Google Gemini, OpenAI, or Ollama, ensuring that outputs adhere to specific Pydantic schemas. The class manages system prompt loading, dynamic LLM client initialization, batch processing, and rate limiting to efficiently handle documentation generation tasks."
        init_method:
          description: "This constructor initializes the LLMHelper by setting up API keys, loading system prompts from specified file paths, and configuring the underlying Language Model. It dynamically selects and initializes an LLM client (Gemini, OpenAI, or Ollama) based on the provided model name and base URL. It also configures batch processing settings and prepares structured output LLM instances for function and class analysis."
          parameters[5]{name,type,description}:
            api_key,str,The API key required for authenticating with the chosen LLM service.
            function_prompt_path,str,The file path to the system prompt used for generating function documentation.
            class_prompt_path,str,The file path to the system prompt used for generating class documentation.
            model_name,str,"The name of the LLM model to be used, defaulting to 'gemini-2.0-flash-lite'."
            base_url,str,An optional base URL for custom or Ollama LLM endpoints.
        methods[3]:
          - identifier: _configure_batch_settings
            description:
              overall: "This private method configures the batch size for LLM API calls based on the specified `model_name`. It assigns a specific integer value to `self.batch_size` to optimize performance and respect rate limits for different LLM providers and models. The method includes specific batch sizes for various Gemini, Llama, and GPT models, as well as custom/alias models. If the model name is not recognized, it logs a warning and applies a conservative default batch size of 2."
              parameters[1]{name,type,description}:
                model_name,str,The name of the LLM model for which to configure batch settings.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other functions or methods.
                called_by: This method is called by backend.HelperLLM.LLMHelper.__init__.
            error: null
          - identifier: generate_for_functions
            description:
              overall: "This method generates structured documentation for a list of functions by interacting with the configured LLM. It processes `FunctionAnalysisInput` objects in batches, converting them into JSON payloads for the LLM. The method uses `self.function_llm` to perform batch API calls, handles potential errors by returning `None` for failed items, and incorporates a waiting period between batches to manage API rate limits effectively. The output is a list of validated `FunctionAnalysis` objects, or `None` for any failed analyses."
              parameters[1]{name,type,description}:
                function_inputs,"List[FunctionAnalysisInput]",A list of input objects containing function data for analysis and documentation generation.
              returns[1]{name,type,description}:
                all_validated_functions,"List[Optional[FunctionAnalysis]]","A list of FunctionAnalysis objects, where each object represents the structured documentation for a function, or None if an error occurred during its generation."
              usage_context:
                calls: "This method calls json.dumps, function_input.model_dump, SystemMessage, HumanMessage, self.function_llm.batch, logging.info, logging.error, and time.sleep."
                called_by: This method is not explicitly called by other functions or methods in the provided context.
            error: null
          - identifier: generate_for_classes
            description:
              overall: "This method is responsible for generating structured documentation for a batch of classes using the configured LLM. It takes a list of `ClassAnalysisInput` objects, converts them into JSON, and sends them to `self.class_llm` in batches. The method includes error handling to manage API call failures, extending the results with `None` for any failed items. It also implements a waiting period between batches to comply with API rate limits, ultimately returning a list of validated `ClassAnalysis` objects."
              parameters[1]{name,type,description}:
                class_inputs,"List[ClassAnalysisInput]",A list of input objects containing class data for analysis and documentation generation.
              returns[1]{name,type,description}:
                all_validated_classes,"List[Optional[ClassAnalysis]]","A list of ClassAnalysis objects, where each object represents the structured documentation for a class, or None if an error occurred during its generation."
              usage_context:
                calls: "This method calls json.dumps, class_input.model_dump, SystemMessage, HumanMessage, self.class_llm.batch, logging.info, logging.error, and time.sleep."
                called_by: This method is not explicitly called by other functions or methods in the provided context.
            error: null
        usage_context:
          dependencies: "The class depends on logging for output, time for rate limiting, json for payload serialization, langchain_google_genai.ChatGoogleGenerativeAI, langchain_ollama.ChatOllama, langchain_openai.ChatOpenAI for LLM integration, langchain.messages.HumanMessage, langchain.messages.SystemMessage for conversation construction, and schemas.types.FunctionAnalysis, schemas.types.ClassAnalysis, schemas.types.FunctionAnalysisInput, schemas.types.ClassAnalysisInput for structured data handling. It also relies on SCADSLLM_URL and OLLAMA_BASE_URL environment variables for certain model configurations."
          instantiated_by: This class is not explicitly instantiated by other functions or methods in the provided context.
      error: null
    backend.MainLLM.MainLLM:
      identifier: backend.MainLLM.MainLLM
      description:
        overall: "The MainLLM class serves as a versatile interface for interacting with various large language models (LLMs), abstracting away the specifics of different providers. It handles the initialization of LLM clients (Gemini, OpenAI-compatible, Ollama) based on configuration, loads a system prompt from a file, and provides methods for both synchronous (`call_llm`) and streaming (`stream_llm`) interactions. This class centralizes LLM communication, making it easier to switch between models and manage prompts."
        init_method:
          description: "This constructor initializes the MainLLM class by setting up the API key, loading a system prompt from a specified file path, and configuring the appropriate Language Model (LLM) client based on the `model_name`. It supports various LLM providers like Google Gemini, OpenAI-compatible APIs (e.g., SCADSLLM), and Ollama, raising `ValueError` if essential configurations like API keys or URLs are missing."
          parameters[4]{name,type,description}:
            api_key,str,The API key required for authenticating with the chosen LLM provider.
            prompt_file_path,str,The file path to a text file containing the system prompt for the LLM.
            model_name,str,"The name of the LLM model to use (default: \"gemini-2.5-pro\")."
            base_url,str | None,"An optional base URL for custom LLM endpoints, particularly for Ollama or OpenAI-compatible models."
        methods[2]:
          - identifier: call_llm
            description:
              overall: "This method sends a user input to the configured LLM for a single, synchronous response. It constructs a message list including the system prompt and the user's input, then invokes the LLM client. The method logs the call status and returns the content of the LLM's response or `None` if an error occurs during the invocation."
              parameters[1]{name,type,description}:
                user_input,str,The user's query or message to be sent to the LLM.
              returns[2]{name,type,description}:
                content,str,The text content of the LLM's response.
                None,None,Indicates an error occurred during the LLM call.
              usage_context:
                calls: "This method calls `SystemMessage`, `HumanMessage`, `self.llm.invoke`, `logging.info`, and `logging.error`."
                called_by: This method is not explicitly called by any other methods in the provided context.
            error: null
          - identifier: stream_llm
            description:
              overall: "This method initiates a streaming interaction with the configured LLM, allowing for real-time reception of response chunks. It prepares the messages with the system prompt and user input, then uses the LLM client's `stream` method. It yields each content chunk as it arrives or yields an error message if an exception occurs during the streaming process."
              parameters[1]{name,type,description}:
                user_input,str,The user's query or message for which a streaming response is desired.
              returns[2]{name,type,description}:
                chunk.content,str,A generator yielding individual content chunks from the LLM's streaming response.
                error_message,str,An error message string yielded if an exception occurs during the streaming call.
              usage_context:
                calls: "This method calls `SystemMessage`, `HumanMessage`, `self.llm.stream`, `logging.info`, and `logging.error`."
                called_by: This method is not explicitly called by any other methods in the provided context.
            error: null
        usage_context:
          dependencies: "The class depends on external libraries for LLM integration, specifically `langchain_google_genai`, `langchain_ollama`, `langchain_openai`, and `langchain.messages`. It also relies on the `logging` module for operational insights."
          instantiated_by: This class is not explicitly instantiated by any other components in the provided context.
      error: null
    backend.basic_info.ProjektInfoExtractor:
      identifier: backend.basic_info.ProjektInfoExtractor
      description:
        overall: "The ProjektInfoExtractor class is designed to extract fundamental project information from various common project files such as README, pyproject.toml, and requirements.txt. It initializes a structured dictionary to store details like project overview, installation instructions, and dependencies. Through a series of private parsing methods, it processes the content of these files, prioritizing information from `pyproject.toml` and then `requirements.txt` over `README.md` where overlaps exist. The class provides a public method to orchestrate this extraction, consolidate the findings, and offer fallback mechanisms for missing information, such as deriving a project title from a repository URL."
        init_method:
          description: "The constructor initializes the class by setting a constant `INFO_NICHT_GEFUNDEN` for placeholder values and creating a nested dictionary `self.info`. This dictionary is pre-populated with default 'Information not found' strings for various project details, including title, description, features, tech stack, status, dependencies, setup instructions, and quick start guide. This structure ensures a consistent output format even when specific information cannot be extracted."
          parameters[0]:
        methods[7]:
          - identifier: _clean_content
            description:
              overall: "This private helper method is responsible for sanitizing input string content by removing null bytes (`\\x00`). Null bytes can occasionally appear in text files due to encoding mismatches, such as reading a UTF-16 encoded file as UTF-8, and can cause issues in subsequent string processing. The method first checks if the input content is empty to avoid unnecessary operations, returning an empty string if so, otherwise it performs the replacement."
              parameters[1]{name,type,description}:
                content,str,The string content to be cleaned.
              returns[1]{name,type,description}:
                "",str,"The cleaned string with all null bytes removed, or an empty string if the input was empty."
              usage_context:
                calls: This method primarily uses Python's built-in string manipulation methods like `replace`.
                called_by: "This method is called by `_parse_readme`, `_parse_toml`, and `_parse_requirements` to preprocess file contents."
            error: null
          - identifier: _finde_datei
            description:
              overall: "This private method searches through a list of file-like objects to find one whose path matches any of the provided patterns. The search is case-insensitive, ensuring flexibility in file naming conventions. It iterates through each file and then through each pattern, returning the first file object that satisfies any pattern. If no matching file is found after checking all possibilities, the method returns `None`."
              parameters[2]{name,type,description}:
                patterns,"List[str]","A list of string patterns (e.g., file extensions or names) to match against file paths."
                dateien,"List[Any]","A list of file-like objects, where each object is expected to have a `path` attribute."
              returns[1]{name,type,description}:
                "","Optional[Any]","The first file object found that matches one of the patterns, or `None` if no match is made."
              usage_context:
                calls: This method uses string methods such as `lower()` and `endswith()` for case-insensitive path matching.
                called_by: "This method is called by `extrahiere_info` to locate specific project files like README, pyproject.toml, and requirements.txt."
            error: null
          - identifier: _extrahiere_sektion_aus_markdown
            description:
              overall: This private method is designed to extract specific text sections from a Markdown content string. It identifies sections based on Markdown level 2 headings (##) that contain any of the specified keywords. A regular expression is dynamically built to match these headings and capture all content following them until another level 2 heading or the end of the document. This allows for structured extraction of information like 'Features' or 'Installation' from README files.
              parameters[2]{name,type,description}:
                inhalt,str,The Markdown content string from which to extract a section.
                keywords,"List[str]","A list of keywords to match within the Markdown heading (e.g., 'Features', 'Installation')."
              returns[1]{name,type,description}:
                "","Optional[str]","The extracted section content as a string, with leading/trailing whitespace removed, or `None` if no matching section is found."
              usage_context:
                calls: "This method utilizes the `re` module for regular expression operations, specifically `re.escape`, `re.compile`, and `re.search`."
                called_by: "This method is called by `_parse_readme` to extract various sections like 'Key Features', 'Tech Stack', and 'Installation'."
            error: null
          - identifier: _parse_readme
            description:
              overall: "This method parses the content of a README file to extract various project details and populate the `self.info` dictionary. It begins by cleaning the content using `_clean_content`. It then attempts to find the project title and a general description using regular expressions. Subsequently, it leverages `_extrahiere_sektion_aus_markdown` to extract specific sections like 'Key Features', 'Tech Stack', 'Current Status', 'Installation', and 'Quick Start Guide', updating the `self.info` dictionary only if the information hasn't been found already."
              parameters[1]{name,type,description}:
                inhalt,str,The raw content of the README file.
              returns[0]:
              usage_context:
                calls: "This method calls `_clean_content` for preprocessing, `re.search` for title and description, and `_extrahiere_sektion_aus_markdown` for structured section extraction."
                called_by: This method is called by `extrahiere_info` after identifying a README file.
            error: null
          - identifier: _parse_toml
            description:
              overall: "This method parses the content of a `pyproject.toml` file to extract project-level metadata. It first cleans the input content using `_clean_content`. It then attempts to load the TOML content using the `tomllib` module. If `tomllib` is not installed or a `TOMLDecodeError` occurs during parsing, a warning is printed and the method returns. Upon successful parsing, it extracts the project `name`, `description`, and `dependencies` from the 'project' table and updates the `self.info` dictionary accordingly."
              parameters[1]{name,type,description}:
                inhalt,str,The raw content of the `pyproject.toml` file.
              returns[0]:
              usage_context:
                calls: This method calls `_clean_content` for preprocessing and `tomllib.loads` to parse the TOML content. It also uses dictionary `get` method.
                called_by: This method is called by `extrahiere_info` if a `pyproject.toml` file is found.
            error: null
          - identifier: _parse_requirements
            description:
              overall: "This method parses the content of a `requirements.txt` file to extract project dependencies. It starts by cleaning the input content using `_clean_content`. It then splits the content into individual lines, filtering out any empty lines or lines that begin with a '#' (comments). The extracted dependencies are then used to update the `self.info` dictionary's 'dependencies' field, but only if this information has not already been populated from a `pyproject.toml` file, ensuring `pyproject.toml` takes precedence for dependency information."
              parameters[1]{name,type,description}:
                inhalt,str,The raw content of the `requirements.txt` file.
              returns[0]:
              usage_context:
                calls: This method calls `_clean_content` for preprocessing.
                called_by: This method is called by `extrahiere_info` if a `requirements.txt` file is found and dependencies haven't been set by `pyproject.toml`.
            error: null
          - identifier: extrahiere_info
            description:
              overall: "This is the primary public method that orchestrates the entire project information extraction process. It first identifies relevant project files (README, pyproject.toml, requirements.txt) using `_finde_datei`. It then parses these files in a specific order of precedence: `pyproject.toml` first, then `requirements.txt`, and finally `README.md`. After parsing, it formats the extracted dependencies into a human-readable list. As a fallback, if no project title is found from the files, it attempts to derive one from the provided repository URL. The method returns a comprehensive dictionary containing all gathered project information."
              parameters[2]{name,type,description}:
                dateien,"List[Any]","A list of file-like objects, each expected to have `path` and `content` attributes, representing files in the repository."
                repo_url,str,"The URL of the repository, used as a fallback to generate a project title if none is found in the files."
              returns[1]{name,type,description}:
                "","Dict[str, Any]","A dictionary containing all extracted project information, structured into 'projekt_uebersicht' and 'installation' categories."
              usage_context:
                calls: "This method calls `_finde_datei` to locate files, `_parse_toml`, `_parse_requirements`, and `_parse_readme` to process file contents. It also uses `os.path.basename` and string methods like `removesuffix`."
                called_by: This is a public method intended to be called externally to initiate the information extraction for a given set of files and repository URL.
            error: null
        usage_context:
          dependencies: "The class depends on the `re` module for regular expression operations, the `os` module for path manipulation, and conditionally on the `tomllib` module for parsing TOML files."
          instantiated_by: The class is not explicitly shown to be instantiated by any other components in the provided context.
      error: null
    backend.callgraph.CallGraph:
      identifier: backend.callgraph.CallGraph
      description:
        overall: "The CallGraph class is an ast.NodeVisitor subclass designed to construct a directed call graph for a given Python source file. It traverses the Abstract Syntax Tree (AST) of a file, identifying function definitions, class definitions, import statements, and function calls. By maintaining context such as the current class and function, and resolving names through local definitions and import mappings, it accurately maps callers to callees and builds a comprehensive representation of function dependencies within the file, storing them in a NetworkX graph."
        init_method:
          description: "The constructor initializes the CallGraph instance by setting the filename of the source code being analyzed. It sets up internal state variables to track the current function and class context during AST traversal. Additionally, it initializes several data structures: `local_defs` for local name resolution, `graph` as a NetworkX directed graph to store the call graph, `import_mapping` for resolving imported names, `function_set` to keep track of defined functions, and `edges` to store raw caller-callee relationships."
          parameters[1]{name,type,description}:
            filename,str,The path or identifier of the source file being analyzed.
        methods[11]:
          - identifier: _recursive_call
            description:
              overall: "This private helper method recursively extracts the components of a function or method call from an AST node. It processes `ast.Call` nodes to get to the function being called, `ast.Name` nodes for simple identifiers, and `ast.Attribute` nodes to build a list representing the fully qualified name components (e.g., ['pkg', 'mod', 'Class', 'method']). This function is crucial for deconstructing call expressions into their constituent parts for later resolution."
              parameters[1]{name,type,description}:
                node,ast.AST,"The AST node representing a call, name, or attribute expression."
              returns[1]{name,type,description}:
                parts,"list[str]",A list of string components representing the dotted name of the called entity.
              usage_context:
                calls: This method does not explicitly call other functions or methods within its own source code.
                called_by: This method is called by _resolve_all_callee_names and visit_Call.
            error: null
          - identifier: _resolve_all_callee_names
            description:
              overall: "This private helper method takes a list of potential callee name components (e.g., `[['module', 'function'], ['Class', 'method']]`) and attempts to resolve them to their full, unique identifiers within the context of the current file. It prioritizes resolution by checking local definitions first, then the `import_mapping`, and finally constructs a full name based on the current filename and class context. This resolution is critical for accurately mapping call targets to their canonical names in the call graph."
              parameters[1]{name,type,description}:
                callee_nodes,"list[list[str]]",A list where each inner list represents the name components of a potential callee.
              returns[1]{name,type,description}:
                resolved,"list[str]",A list of fully resolved string identifiers for the callees.
              usage_context:
                calls: This method does not explicitly call other functions or methods within its own source code.
                called_by: This method is called by visit_Call.
            error: null
          - identifier: _make_full_name
            description:
              overall: "This private helper method constructs a fully qualified name for a function or method, incorporating the filename and, optionally, the class name. It formats the name as `filename::[class_name::]basename`, providing a unique and unambiguous identifier for functions within the call graph. This ensures that functions with the same base name but different contexts (e.g., in different classes or files) are uniquely identified."
              parameters[2]{name,type,description}:
                basename,str,The base name of the function or method.
                class_name,str | None,"The name of the class if the function is a method, otherwise None."
              returns[1]{name,type,description}:
                full_name,str,The fully qualified name of the function or method.
              usage_context:
                calls: This method does not explicitly call other functions or methods within its own source code.
                called_by: This method is called by visit_FunctionDef.
            error: null
          - identifier: _current_caller
            description:
              overall: "This private helper method determines the identifier of the current calling context. If `self.current_function` is set, it returns that value, indicating that the AST traversal is currently inside a defined function. Otherwise, it returns a placeholder indicating the global scope within the current file (`<filename>`) or a generic global scope (`<global-scope>`) if the filename is not available. This provides the 'source' for any detected calls."
              parameters[0]:
              returns[1]{name,type,description}:
                caller_identifier,str,The identifier of the current caller (function name or scope indicator).
              usage_context:
                calls: This method does not explicitly call other functions or methods within its own source code.
                called_by: This method is called by visit_Call.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method, overriding `ast.NodeVisitor.visit_Import`, processes `import` statements. It iterates through the imported modules and their aliases, storing the mapping from the alias (or original name) to the module's actual name in `self.import_mapping`. This mapping is crucial for resolving imported names to their original module paths when analyzing function calls. After processing, it calls `self.generic_visit(node)` to continue AST traversal."
              parameters[1]{name,type,description}:
                node,ast.Import,The AST node representing an import statement.
              returns[0]:
              usage_context:
                calls: This method calls self.generic_visit.
                called_by: This method is implicitly called by the ast.NodeVisitor traversal mechanism when an ast.Import node is encountered.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method, overriding `ast.NodeVisitor.visit_ImportFrom`, processes `from ... import ...` statements. It extracts the module name (handling dotted imports by taking the last component) and any aliases for the imported names. It then stores these mappings in `self.import_mapping`, which is essential for correctly resolving names imported from specific modules. Finally, it calls `self.generic_visit(node)` to continue the AST traversal."
              parameters[1]{name,type,description}:
                node,ast.ImportFrom,The AST node representing a 'from ... import ...' statement.
              returns[0]:
              usage_context:
                calls: This method calls self.generic_visit.
                called_by: This method is implicitly called by the ast.NodeVisitor traversal mechanism when an ast.ImportFrom node is encountered.
            error: null
          - identifier: visit_ClassDef
            description:
              overall: "This method, overriding `ast.NodeVisitor.visit_ClassDef`, handles class definitions during AST traversal. It temporarily updates `self.current_class` to the name of the class being visited, allowing any nested methods or functions to correctly identify their enclosing class context. After traversing the class body using `self.generic_visit(node)`, it restores the previous `self.current_class` to maintain the correct scope for subsequent nodes."
              parameters[1]{name,type,description}:
                node,ast.ClassDef,The AST node representing a class definition.
              returns[0]:
              usage_context:
                calls: This method calls self.generic_visit.
                called_by: This method is implicitly called by the ast.NodeVisitor traversal mechanism when an ast.ClassDef node is encountered.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method, overriding `ast.NodeVisitor.visit_FunctionDef`, processes synchronous function definitions. It constructs a full, unique name for the function using `_make_full_name`, stores this name in `self.local_defs` for local resolution, and updates `self.current_function` to track the current scope. The function is added as a node to the `self.graph` and its full name is added to `self.function_set`. After traversing the function's body using `self.generic_visit(node)`, it restores the previous function context."
              parameters[1]{name,type,description}:
                node,ast.FunctionDef,The AST node representing a function definition.
              returns[0]:
              usage_context:
                calls: This method calls _make_full_name and self.generic_visit.
                called_by: "This method is implicitly called by the ast.NodeVisitor traversal mechanism when an ast.FunctionDef node is encountered, and explicitly by visit_AsyncFunctionDef."
            error: null
          - identifier: visit_AsyncFunctionDef
            description:
              overall: "This method, overriding `ast.NodeVisitor.visit_AsyncFunctionDef`, processes asynchronous function definitions. It delegates the actual processing of the function definition to `visit_FunctionDef`. This approach ensures that both synchronous and asynchronous functions are handled consistently for the purpose of building the call graph, treating them similarly in terms of name resolution and graph node creation."
              parameters[1]{name,type,description}:
                node,ast.AsyncFunctionDef,The AST node representing an asynchronous function definition.
              returns[0]:
              usage_context:
                calls: This method calls self.visit_FunctionDef.
                called_by: This method is implicitly called by the ast.NodeVisitor traversal mechanism when an ast.AsyncFunctionDef node is encountered.
            error: null
          - identifier: visit_Call
            description:
              overall: "This method, overriding `ast.NodeVisitor.visit_Call`, is central to building the call graph. It first identifies the current calling context using `_current_caller`. Then, it uses `_recursive_call` to extract the components of the called entity and `_resolve_all_callee_names` to resolve its full, canonical identifier. Finally, it records the call by adding an edge from the `caller` to the `callee` in the `self.edges` dictionary, effectively mapping the dependency. It then continues AST traversal with `self.generic_visit(node)`."
              parameters[1]{name,type,description}:
                node,ast.Call,The AST node representing a function call.
              returns[0]:
              usage_context:
                calls: "This method calls _current_caller, _recursive_call, _resolve_all_callee_names, and self.generic_visit."
                called_by: This method is implicitly called by the ast.NodeVisitor traversal mechanism when an ast.Call node is encountered.
            error: null
          - identifier: visit_If
            description:
              overall: "This method, overriding `ast.NodeVisitor.visit_If`, processes `if` statements. It includes special logic to detect the `if __name__ == \"__main__\":` block. When this specific condition is met, it temporarily sets `self.current_function` to \"<main_block>\" to correctly attribute calls within this block to a special main execution scope. For all other `if` statements, it simply continues the generic AST traversal without altering the current function context."
              parameters[1]{name,type,description}:
                node,ast.If,The AST node representing an 'if' statement.
              returns[0]:
              usage_context:
                calls: This method calls self.generic_visit.
                called_by: This method is implicitly called by the ast.NodeVisitor traversal mechanism when an ast.If node is encountered.
            error: null
        usage_context:
          dependencies: This class depends on the 'ast' module for parsing Python code and the 'networkx' library for graph representation.
          instantiated_by: The input context does not specify where this class is instantiated.
      error: null
    backend.getRepo.RepoFile:
      identifier: backend.getRepo.RepoFile
      description:
        overall: "The RepoFile class represents a single file within a Git repository, designed for efficient, lazy loading of its content and metadata. It encapsulates the file's path and its associated Git Tree object, providing properties to access the Git blob, decoded content, and size only when needed. This approach optimizes resource usage by deferring heavy operations until data is actually requested. The class also offers utility methods for converting its data to a dictionary and performing basic content analysis."
        init_method:
          description: "This method initializes a RepoFile object by storing the file path and the Git Tree object from which the file originates. It sets up internal attributes (`_blob`, `_content`, `_size`) to `None`, indicating that the Git blob, file content, and size are to be loaded lazily upon first access."
          parameters[2]{name,type,description}:
            file_path,str,The path to the file within the repository.
            commit_tree,git.Tree,The Tree object of the commit from which the file originates.
        methods[6]:
          - identifier: blob
            description:
              overall: "This property provides lazy loading for the Git blob object associated with the file. It first checks if the `_blob` attribute is already set; if not, it attempts to retrieve the blob from the `_tree` using the stored `path`. If the file cannot be found within the commit's tree, a `FileNotFoundError` is raised to indicate the absence of the specified file."
              parameters[0]:
              returns[1]{name,type,description}:
                blob,git.Blob,The Git blob object representing the file.
              usage_context:
                calls: This method does not explicitly call other functions or methods.
                called_by: This method is not explicitly called by other methods in the provided context.
            error: null
          - identifier: content
            description:
              overall: "This property provides lazy loading for the decoded content of the file. It checks if the `_content` attribute is already loaded; if not, it accesses the Git blob object (via the `blob` property), reads its data stream, and decodes it using UTF-8 encoding, ignoring any decoding errors. This ensures the file content is only processed when explicitly requested."
              parameters[0]:
              returns[1]{name,type,description}:
                content,str,The decoded string content of the file.
              usage_context:
                calls: This method does not explicitly call other functions or methods.
                called_by: This method is not explicitly called by other methods in the provided context.
            error: null
          - identifier: size
            description:
              overall: "This property provides lazy loading for the size of the file in bytes. It checks if the `_size` attribute is already loaded; if not, it accesses the Git blob object (via the `blob` property) and retrieves its `size` attribute. This ensures the file size is only determined and stored when needed."
              parameters[0]:
              returns[1]{name,type,description}:
                size,int,The size of the file in bytes.
              usage_context:
                calls: This method does not explicitly call other functions or methods.
                called_by: This method is not explicitly called by other methods in the provided context.
            error: null
          - identifier: analyze_word_count
            description:
              overall: "This method serves as an example analysis function, demonstrating how to process the file's content. It calculates and returns the total number of words present in the file's content. This is achieved by accessing the `content` property, splitting the string by whitespace, and then counting the resulting elements."
              parameters[0]:
              returns[1]{name,type,description}:
                word_count,int,The total number of words in the file content.
              usage_context:
                calls: This method does not explicitly call other functions or methods.
                called_by: This method is not explicitly called by other methods in the provided context.
            error: null
          - identifier: __repr__
            description:
              overall: "This special method provides a developer-friendly string representation of the `RepoFile` object. It returns a string formatted to show the class name and the file's path, which is particularly useful for debugging, logging, and interactive console sessions, offering a concise summary of the object's identity."
              parameters[0]:
              returns[1]{name,type,description}:
                representation,str,"A string representation of the RepoFile object, including its path."
              usage_context:
                calls: This method does not explicitly call other functions or methods.
                called_by: This method is not explicitly called by other methods in the provided context.
            error: null
          - identifier: to_dict
            description:
              overall: "This method converts the `RepoFile` object into a dictionary representation, providing a structured way to access its properties. It includes the file's path, its base name, its size, and a type indicator. Optionally, it can also include the full file content if the `include_content` parameter is set to `True`, allowing for flexible data export."
              parameters[1]{name,type,description}:
                include_content,bool,A flag indicating whether the file's content should be included in the dictionary. Defaults to False.
              returns[1]{name,type,description}:
                file_data,dict,A dictionary containing the file's metadata and optionally its content.
              usage_context:
                calls: This method does not explicitly call other functions or methods.
                called_by: This method is not explicitly called by other methods in the provided context.
            error: null
        usage_context:
          dependencies: This class does not explicitly depend on other components in the provided context.
          instantiated_by: This class is not explicitly instantiated by other components in the provided context.
      error: null
    backend.getRepo.GitRepository:
      identifier: backend.getRepo.GitRepository
      description:
        overall: "This class provides a robust mechanism for interacting with a Git repository. It handles the cloning of a remote repository into a temporary local directory, offers methods to retrieve all files within the repository as structured objects, and can generate a hierarchical representation of the file tree. Furthermore, it implements the context manager protocol, ensuring proper cleanup of the temporary directory upon exiting a 'with' block."
        init_method:
          description: "Initializes the GitRepository object by cloning a remote Git repository into a temporary directory. It sets up attributes for the repository URL, the temporary directory path, the GitPython Repo object, and an empty list for files. It also captures the latest commit and its tree. Error handling is included for cloning failures."
          parameters[1]{name,type,description}:
            repo_url,str,The URL of the Git repository to be cloned.
        methods[5]:
          - identifier: get_all_files
            description:
              overall: "This method retrieves a list of all files present in the cloned Git repository. It uses Git's 'ls-files' command to get file paths and then creates RepoFile objects for each path, storing them in the 'self.files' attribute. Finally, it returns this list of RepoFile instances."
              parameters[0]:
              returns[1]{name,type,description}:
                files,"list[RepoFile]",A list of RepoFile instances representing all files in the repository.
              usage_context:
                calls: This method calls backend.getRepo.RepoFile.
                called_by: This method is not explicitly called by other methods in the provided context.
            error: null
          - identifier: close
            description:
              overall: This method is responsible for cleaning up resources by deleting the temporary directory where the Git repository was cloned. It checks if 'self.temp_dir' is set before attempting to delete it and then sets 'self.temp_dir' to 'None'.
              parameters[0]:
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other methods or functions.
                called_by: This method is not explicitly called by other methods in the provided context.
            error: null
          - identifier: __enter__
            description:
              overall: "This special method allows the GitRepository class to be used as a context manager. When entering a 'with' statement, this method is invoked and simply returns the instance of the GitRepository itself, making it available as the 'as' target."
              parameters[0]:
              returns[1]{name,type,description}:
                self,GitRepository,The instance of the GitRepository class.
              usage_context:
                calls: This method does not explicitly call other methods or functions.
                called_by: This method is not explicitly called by other methods in the provided context.
            error: null
          - identifier: __exit__
            description:
              overall: "This special method is part of the context manager protocol. It is automatically called when exiting a 'with' statement, regardless of whether an exception occurred. Its primary responsibility is to ensure that the 'close' method is invoked to clean up the temporary repository directory."
              parameters[3]{name,type,description}:
                exc_type,type or None,"The type of the exception that caused the context to be exited, or None if no exception occurred."
                exc_val,Exception or None,"The exception instance that caused the context to be exited, or None."
                exc_tb,traceback or None,"The traceback object associated with the exception, or None."
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other methods or functions.
                called_by: This method is not explicitly called by other methods in the provided context.
            error: null
          - identifier: get_file_tree
            description:
              overall: "This method constructs a hierarchical dictionary representation of the repository's file structure, similar to a file system tree. If 'self.files' is empty, it first calls 'get_all_files' to populate it. It then iterates through the files, splitting their paths to build nested dictionaries representing directories and appending file objects at the leaf nodes. The 'include_content' parameter determines if file content is included in the file objects."
              parameters[1]{name,type,description}:
                include_content,bool,A flag indicating whether the content of the files should be included in the generated file tree. Defaults to False.
              returns[1]{name,type,description}:
                tree,dict,A dictionary representing the hierarchical file tree of the repository.
              usage_context:
                calls: This method does not explicitly call other methods or functions.
                called_by: This method is not explicitly called by other methods in the provided context.
            error: null
        usage_context:
          dependencies: This class depends on backend.getRepo.RepoFile for representing individual files within the repository.
          instantiated_by: This class is not explicitly instantiated by other components in the provided context.
      error: null
    backend.relationship_analyzer.ProjectAnalyzer:
      identifier: backend.relationship_analyzer.ProjectAnalyzer
      description:
        overall: "The ProjectAnalyzer class is designed to perform a static analysis of a Python project to build a comprehensive call graph. It identifies all Python files, collects definitions of classes, functions, and methods, and then resolves calls between these entities. The class provides methods to initiate the analysis and retrieve the raw relationships in a structured format, making it a core component for understanding code dependencies and structure."
        init_method:
          description: "This constructor initializes the ProjectAnalyzer instance by setting the project's root directory and establishing several internal data structures. It sets up dictionaries for definitions, a defaultdict for the call graph, and a dictionary for file ASTs. Additionally, it defines a set of common directories to be ignored during file system traversal."
          parameters[1]{name,type,description}:
            project_root,str,The absolute path to the root directory of the Python project to be analyzed.
        methods[6]:
          - identifier: analyze
            description:
              overall: "This method orchestrates the entire analysis process for the project. It begins by identifying all Python files within the specified project root, excluding ignored directories. It then iterates through these files twice: first to collect all function, method, and class definitions, and second to resolve all calls made between these defined entities. After resolving calls, it clears the stored ASTs to free up memory and returns the generated call graph."
              parameters[0]:
              returns[1]{name,type,description}:
                call_graph,defaultdict(list),"A dictionary representing the call graph, where keys are callee identifiers and values are lists of caller information."
              usage_context:
                calls: "This method calls _find_py_files to locate Python files, _collect_definitions to gather entity definitions, and _resolve_calls to establish call relationships."
                called_by: This method is not explicitly called by other methods within the provided context.
            error: null
          - identifier: get_raw_relationships
            description:
              overall: "This method processes the internal call graph to generate a structured representation of outgoing and incoming relationships between code entities. It iterates through the call graph, extracting caller and callee identifiers, and populates two defaultdict(set) objects: one for outgoing calls (caller to callee) and one for incoming calls (callee from caller). The collected sets are then converted to sorted lists for a consistent and ordered output format."
              parameters[0]:
              returns[1]{name,type,description}:
                relationships,dict,"A dictionary containing 'outgoing' and 'incoming' keys, each mapping to a dictionary where keys are entity identifiers and values are sorted lists of related entity identifiers."
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions within the provided context."
                called_by: This method is not explicitly called by other methods within the provided context.
            error: null
          - identifier: _find_py_files
            description:
              overall: "This private helper method is responsible for recursively traversing the project's root directory to locate all Python files. It utilizes `os.walk` to navigate the file system, ensuring that directories specified in `self.ignore_dirs` are skipped. For each file encountered, it checks if the file's extension is '.py' and, if so, appends its full absolute path to a list of Python files."
              parameters[0]:
              returns[1]{name,type,description}:
                py_files,"list[str]","A list of absolute file paths to all Python files found in the project, excluding ignored directories."
              usage_context:
                calls: This method calls `os.walk` to traverse directories and `os.path.join` to construct file paths.
                called_by: This method is called by the `analyze` method.
            error: null
          - identifier: _collect_definitions
            description:
              overall: "This private method parses a given Python file to identify and store definitions of functions, methods, and classes. It reads the file's source code, parses it into an Abstract Syntax Tree (AST) using `ast.parse`, and stores this AST in `self.file_asts`. The method then walks the AST to find `ast.FunctionDef` and `ast.ClassDef` nodes, determining their fully qualified path names and types (function, method, or class), and stores this information in `self.definitions`. Error handling is included to log any parsing exceptions."
              parameters[1]{name,type,description}:
                filepath,str,The path to the Python file from which to collect definitions.
              returns[0]:
              usage_context:
                calls: "This method calls `path_to_module` to convert file paths to module paths and `_get_parent` to determine the parent of a node in the AST. It also uses `ast.parse`, `ast.walk`, `ast.FunctionDef`, `ast.ClassDef`, `logging.error`, and `open` for file operations."
                called_by: This method is called by the `analyze` method.
            error: null
          - identifier: _get_parent
            description:
              overall: "This private helper method traverses an Abstract Syntax Tree (AST) to find the immediate parent node of a given child node. It iterates through all nodes in the provided AST and, for each potential parent, checks its direct children. If the target child node is found among the children of a potential parent, that parent node is returned. If no parent is found after traversing the entire tree, the method returns `None`."
              parameters[2]{name,type,description}:
                tree,ast.AST,The root of the Abstract Syntax Tree to search within.
                node,ast.AST,The child node for which to find the parent.
              returns[1]{name,type,description}:
                parent_node,ast.AST | None,"The parent AST node if found, otherwise None."
              usage_context:
                calls: This method calls `ast.walk` to traverse the AST and `ast.iter_child_nodes` to iterate over a node's children.
                called_by: This method is called by the `_collect_definitions` method.
            error: null
          - identifier: _resolve_calls
            description:
              overall: "This private method processes a given Python file's AST to identify and resolve function and method calls within it. It first retrieves the AST for the specified `filepath` from `self.file_asts`. It then instantiates a `CallResolverVisitor` with the file's context and known definitions, and uses this visitor to traverse the AST, populating its internal `calls` attribute. Finally, it merges the resolved calls from the `resolver` into the class's `self.call_graph`. Error handling is included to log any exceptions encountered during call resolution."
              parameters[1]{name,type,description}:
                filepath,str,The path to the Python file whose calls need to be resolved.
              returns[0]:
              usage_context:
                calls: This method instantiates and uses `CallResolverVisitor` to visit the AST and resolve calls. It also uses `logging.error` for error reporting.
                called_by: This method is called by the `analyze` method.
            error: null
        usage_context:
          dependencies: "This class depends on `backend.relationship_analyzer.CallResolverVisitor` for resolving calls and `backend.relationship_analyzer.path_to_module` for converting file paths to module paths. It also utilizes standard library modules such as `ast`, `os`, `logging`, and `collections.defaultdict`."
          instantiated_by: This class is not explicitly instantiated by other components within the provided context.
      error: null
    backend.relationship_analyzer.CallResolverVisitor:
      identifier: backend.relationship_analyzer.CallResolverVisitor
      description:
        overall: "The CallResolverVisitor class is an `ast.NodeVisitor` designed to traverse an Abstract Syntax Tree (AST) of a Python file and identify all function and method calls. It resolves the fully qualified names of both callers and callees, tracking imports, class definitions, function definitions, and instance types to accurately map call relationships. The visitor collects this information into a `calls` dictionary, providing a comprehensive call graph for the analyzed module."
        init_method:
          description: "The constructor initializes the visitor with the file path, project root, and a dictionary of known definitions. It sets up internal state variables like `module_path`, `scope` for local name resolution, `instance_types` for tracking object types, and `calls` to store detected call relationships, using `defaultdict` for convenience."
          parameters[3]{name,type,description}:
            filepath,str,The path to the source file being analyzed.
            project_root,str,"The root directory of the project, used to determine the module path."
            definitions,dict,"A dictionary containing known definitions (e.g., functions, classes) for resolution."
        methods[7]:
          - identifier: visit_ClassDef
            description:
              overall: "This method is part of the `ast.NodeVisitor` pattern and is called when an `ast.ClassDef` node is encountered. It updates the `current_class_name` attribute to reflect the class being visited, allowing nested methods to correctly form their full identifiers. After processing the class's children by calling `generic_visit`, it restores the previous `current_class_name` to maintain correct scope."
              parameters[1]{name,type,description}:
                node,ast.ClassDef,The AST node representing a class definition.
              returns[0]:
              usage_context:
                calls: This method implicitly calls `self.generic_visit` to continue AST traversal.
                called_by: This method is called by the `ast.NodeVisitor` framework when traversing an AST and encountering a class definition.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method handles `ast.FunctionDef` nodes, which represent function or method definitions. It constructs a `full_identifier` for the function, incorporating the module path and class name if applicable, and updates `current_caller_name`. This ensures that calls made within this function are correctly attributed to its full qualified name. It then traverses the function's body using `generic_visit` and restores the `current_caller_name` upon exit."
              parameters[1]{name,type,description}:
                node,ast.FunctionDef,The AST node representing a function definition.
              returns[0]:
              usage_context:
                calls: This method implicitly calls `self.generic_visit` to continue AST traversal.
                called_by: This method is called by the `ast.NodeVisitor` framework when traversing an AST and encountering a function definition.
            error: null
          - identifier: visit_Call
            description:
              overall: "This method is invoked for `ast.Call` nodes, representing function or method calls. It attempts to resolve the qualified name of the called function using the private helper method `_resolve_call_qname`. If the callee is successfully resolved and found within the known `definitions`, it records the call, including the caller's file, line number, full identifier, and type (module, local function, method, or function). This method is central to collecting call graph information."
              parameters[1]{name,type,description}:
                node,ast.Call,The AST node representing a function or method call.
              returns[0]:
              usage_context:
                calls: This method calls `_resolve_call_qname` to determine the qualified name of the called function.
                called_by: This method is called by the `ast.NodeVisitor` framework when traversing an AST and encountering a function or method call.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method processes `ast.Import` nodes, which represent `import module` statements. It populates the `self.scope` dictionary, mapping the imported module's alias (or its original name) to its full name. This scope is later used by `_resolve_call_qname` to resolve calls to imported modules or their members. After processing, it calls `generic_visit` to continue traversal."
              parameters[1]{name,type,description}:
                node,ast.Import,The AST node representing an import statement.
              returns[0]:
              usage_context:
                calls: This method implicitly calls `self.generic_visit` to continue AST traversal.
                called_by: This method is called by the `ast.NodeVisitor` framework when traversing an AST and encountering an import statement.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method handles `ast.ImportFrom` nodes, representing `from module import name` statements. It calculates the full qualified path for each imported name, carefully considering relative imports indicated by `node.level`. It then adds these resolved names to the `self.scope` dictionary, enabling subsequent resolution of calls to these imported entities. It then continues AST traversal with `generic_visit`."
              parameters[1]{name,type,description}:
                node,ast.ImportFrom,The AST node representing a `from ... import ...` statement.
              returns[0]:
              usage_context:
                calls: This method implicitly calls `self.generic_visit` to continue AST traversal.
                called_by: This method is called by the `ast.NodeVisitor` framework when traversing an AST and encountering a `from ... import ...` statement.
            error: null
          - identifier: visit_Assign
            description:
              overall: "This method processes `ast.Assign` nodes, specifically looking for assignments where the right-hand side is a call to a class constructor (e.g., `x = MyClass()`). If such an assignment is found and the class name can be resolved via `self.scope` and `self.definitions`, it records the type of the assigned variable in `self.instance_types`. This allows for resolving method calls on instances later in the AST traversal. It then calls `generic_visit`."
              parameters[1]{name,type,description}:
                node,ast.Assign,The AST node representing an assignment statement.
              returns[0]:
              usage_context:
                calls: This method implicitly calls `self.generic_visit` to continue AST traversal.
                called_by: This method is called by the `ast.NodeVisitor` framework when traversing an AST and encountering an assignment statement.
            error: null
          - identifier: _resolve_call_qname
            description:
              overall: "This private helper method attempts to determine the fully qualified name (QName) of a function or method call. It handles two main cases: direct name calls (`ast.Name`) and attribute calls (`ast.Attribute`, e.g., `obj.method`). It first checks `self.scope` for imported names, then local definitions, and for attribute calls, it tries to resolve the instance type from `self.instance_types` or the module from `self.scope`. If a resolution is found, the QName is returned; otherwise, `None` is returned."
              parameters[1]{name,type,description}:
                func_node,ast.expr,"The AST node representing the function or method being called (e.g., `ast.Name` or `ast.Attribute`)."
              returns[1]{name,type,description}:
                name,str | None,"The fully qualified name of the called entity if resolved, otherwise `None`."
              usage_context:
                calls: This method does not explicitly call other methods or functions.
                called_by: This method is called by `visit_Call` to resolve the qualified name of a function or method being invoked.
            error: null
        usage_context:
          dependencies: This class depends on `backend.relationship_analyzer.path_to_module` for converting file paths to module paths.
          instantiated_by: This class is not explicitly instantiated by any known components in the provided context.
      error: null
    schemas.types.ParameterDescription:
      identifier: schemas.types.ParameterDescription
      description:
        overall: "The ParameterDescription class is a Pydantic BaseModel designed to provide a structured representation for a single parameter within a function's signature. It serves as a data model to encapsulate essential information about a parameter, including its name, its data type, and a descriptive explanation of its role. This class is fundamental for generating comprehensive documentation or for programmatic analysis of function interfaces."
        init_method:
          description: "This class, inheriting from Pydantic's BaseModel, automatically generates an __init__ method. This constructor initializes an instance of ParameterDescription by validating and assigning the 'name', 'type', and 'description' fields based on the provided arguments."
          parameters[3]{name,type,description}:
            name,str,The name of the parameter.
            type,str,The type hint or inferred type of the parameter.
            description,str,A brief explanation of the parameter's purpose.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in the provided context.
          instantiated_by: The instantiation points for this class are not specified in the provided context.
      error: null
    schemas.types.ReturnDescription:
      identifier: schemas.types.ReturnDescription
      description:
        overall: "The ReturnDescription class is a Pydantic BaseModel designed to standardize the representation of a function's return value. It serves as a data structure to clearly define the name, data type, and a descriptive explanation of what a function returns. This class is crucial for generating structured documentation or for systems that require a programmatic understanding of function outputs."
        init_method:
          description: "As a Pydantic BaseModel, the `__init__` method for ReturnDescription is automatically generated. It initializes an instance of the class by accepting `name`, `type`, and `description` as keyword arguments, validating them against their string type hints."
          parameters[3]{name,type,description}:
            name,str,The name or identifier of the return value.
            type,str,"The data type of the return value, e.g., 'str', 'int', 'List[str]'."
            description,str,"A detailed explanation of the return value's purpose, content, or behavior."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies.
          instantiated_by: This class is not explicitly shown to be instantiated by any other components in the provided context.
      error: null
    schemas.types.UsageContext:
      identifier: schemas.types.UsageContext
      description:
        overall: "The UsageContext class is a Pydantic BaseModel designed to encapsulate information about the calling context of a function or method. It serves as a structured data container, defining two essential string attributes: 'calls', which describes the entities invoked by the function, and 'called_by', which indicates where the function itself is utilized. This class provides a clear and validated structure for representing functional dependencies and interactions within a system."
        init_method:
          description: "As a Pydantic BaseModel, the `__init__` method is implicitly generated. It initializes instances of `UsageContext` by accepting `calls` and `called_by` as keyword arguments, performing type validation according to their annotations."
          parameters[2]{name,type,description}:
            calls,str,"A string summarizing the functions, methods, or classes that this entity calls."
            called_by,str,"A string summarizing the functions, methods, or classes that call this entity."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies.
          instantiated_by: This class's instantiation points are not explicitly provided in the context.
      error: null
    schemas.types.FunctionDescription:
      identifier: schemas.types.FunctionDescription
      description:
        overall: "The `FunctionDescription` class is a Pydantic BaseModel designed to encapsulate a comprehensive analysis of a Python function. It serves as a structured data container for detailing a function's high-level purpose, its input parameters, its return values, and its operational context within a larger system. This class is fundamental for generating structured documentation or for AI systems that need to understand and process function metadata."
        init_method:
          description: "As a Pydantic BaseModel, the `__init__` method for `FunctionDescription` is implicitly generated by Pydantic. It handles the initialization of an instance by validating and assigning values to its defined fields: `overall`, `parameters`, `returns`, and `usage_context` upon object creation."
          parameters[4]{name,type,description}:
            overall,str,"A concise, high-level summary describing the function's primary purpose and what it achieves."
            parameters,"List[ParameterDescription]","A list of `ParameterDescription` objects, each detailing an input parameter of the function, including its name, type, and description."
            returns,"List[ReturnDescription]","A list of `ReturnDescription` objects, each describing a value or type that the function is expected to return."
            usage_context,UsageContext,"An object providing contextual information about the function's interactions, such as other functions it calls or where it is called from."
        methods[0]:
        usage_context:
          dependencies: "This class relies on `BaseModel` from Pydantic for its structure and validation, and `List` from `typing` for type hinting. It also depends on `ParameterDescription`, `ReturnDescription`, and `UsageContext` for its field types."
          instantiated_by: The instantiation points for this class are not specified in the provided context.
      error: null
    schemas.types.FunctionAnalysis:
      identifier: schemas.types.FunctionAnalysis
      description:
        overall: "The FunctionAnalysis class is a Pydantic BaseModel designed to encapsulate the comprehensive analysis of a single function. It serves as a structured data container, holding the function's unique identifier, a detailed description object, and an optional error message. This model is crucial for standardizing the output of automated function analysis processes within a larger system."
        init_method:
          description: "The FunctionAnalysis class does not explicitly define an __init__ method. As a pydantic.BaseModel, its constructor is implicitly generated, allowing instantiation by providing keyword arguments corresponding to its defined fields: identifier, description, and an optional error."
          parameters[3]{name,type,description}:
            identifier,str,The unique name or identifier of the function being analyzed.
            description,FunctionDescription,"An object containing the detailed analysis of the function's purpose, parameters, returns, and usage context."
            error,"Optional[str]","An optional field to store an error message if the function analysis encounters issues, otherwise it is None."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in the provided context.
          instantiated_by: The instantiation points for this class are not specified in the provided context.
      error: null
    schemas.types.ConstructorDescription:
      identifier: schemas.types.ConstructorDescription
      description:
        overall: "The `ConstructorDescription` class is a Pydantic BaseModel designed to structure and validate information about a Python class's `__init__` method. It serves as a data model to encapsulate a textual summary of the constructor's behavior along with a detailed list of its parameters. This class is crucial for standardizing the representation of constructor metadata within a larger system, likely for automated documentation, code analysis, or API schema generation."
        init_method:
          description: "As a Pydantic BaseModel, `ConstructorDescription` has an implicitly generated `__init__` method. This constructor is responsible for validating and assigning values to its `description` (a string) and `parameters` (a list of `ParameterDescription` objects) fields upon instantiation. It ensures that any data used to create an instance of this class adheres to the specified types and structure."
          parameters[2]{name,type,description}:
            description,str,A string providing a high-level summary of the constructor's purpose and functionality.
            parameters,"List[ParameterDescription]","A list of `ParameterDescription` objects, each detailing a specific parameter of the constructor, including its name, type, and individual description."
        methods[0]:
        usage_context:
          dependencies: This class depends on `pydantic.BaseModel` for its data modeling capabilities and `typing.List` for type hinting the list of parameters.
          instantiated_by: The specific points where this class is instantiated are not provided in the current context.
      error: null
    schemas.types.ClassContext:
      identifier: schemas.types.ClassContext
      description:
        overall: The ClassContext class is a Pydantic BaseModel designed to encapsulate contextual information about a Python class. It specifically tracks the external dependencies that a class relies on and the locations or modules where the class is instantiated. This model provides a structured way to represent and validate these two critical pieces of contextual metadata.
        init_method:
          description: "The `__init__` method for `ClassContext` is implicitly generated by Pydantic's `BaseModel`. It initializes instances of `ClassContext` by accepting values for `dependencies` and `instantiated_by` as keyword arguments, performing validation according to their type hints."
          parameters[2]{name,type,description}:
            dependencies,str,A string summarizing the external dependencies of the class.
            instantiated_by,str,A string summarizing where the class is instantiated.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in the provided context.
          instantiated_by: The provided context does not specify where this class is instantiated.
      error: null
    schemas.types.ClassDescription:
      identifier: schemas.types.ClassDescription
      description:
        overall: "The ClassDescription class is a Pydantic BaseModel designed to store a comprehensive, structured analysis of a Python class. It serves as a data container, encapsulating various aspects of a class, including its high-level purpose, details about its constructor, a list of all its methods with their individual analyses, and contextual information regarding its usage and dependencies. This model provides a standardized format for representing class analysis results, facilitating machine-readable documentation and further automated processing."
        init_method:
          description: "The `__init__` method for ClassDescription is implicitly generated by Pydantic's BaseModel. It initializes an instance of ClassDescription by validating and assigning values to its defined fields: `overall`, `init_method`, `methods`, and `usage_context`. This constructor ensures that all required components for a class analysis are present and correctly typed upon object creation."
          parameters[4]{name,type,description}:
            overall,str,A high-level summary of the class's purpose.
            init_method,ConstructorDescription,A detailed description of the class's constructor.
            methods,"List[FunctionAnalysis]",A list containing detailed analyses of each method within the class.
            usage_context,ClassContext,Contextual information about the class's dependencies and where it is instantiated.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list external functional dependencies in the provided context.
          instantiated_by: The specific points of instantiation for this class are not provided in the current context.
      error: null
    schemas.types.ClassAnalysis:
      identifier: schemas.types.ClassAnalysis
      description:
        overall: "The ClassAnalysis class serves as a Pydantic model to structure the comprehensive analysis of a Python class. It encapsulates the class's unique identifier, a detailed ClassDescription object containing its constructor, methods, and usage context, and an optional field to report any analysis errors. This model is designed to provide a standardized, machine-readable representation of a class's structure and behavior."
        init_method:
          description: "The __init__ method for this Pydantic model initializes an instance of ClassAnalysis by accepting values for `identifier`, `description`, and an optional `error` string. It leverages Pydantic's validation to ensure the provided data conforms to the defined types and structure."
          parameters[3]{name,type,description}:
            identifier,str,A unique string identifying the class being analyzed.
            description,ClassDescription,"An object containing the detailed analysis of the class, including its constructor, methods, and usage context."
            error,"Optional[str]","An optional string indicating an error message if the class analysis failed, defaulting to None."
        methods[0]:
        usage_context:
          dependencies: This class has no explicit external functional dependencies listed in the provided context.
          instantiated_by: The specific locations where this class is instantiated are not provided in the current context.
      error: null
    schemas.types.CallInfo:
      identifier: schemas.types.CallInfo
      description:
        overall: "The CallInfo class is a Pydantic BaseModel designed to represent a specific call event within a relationship analyzer. It encapsulates details about where a call originates, including the file path, the name of the calling function, the mode of the call (e.g., 'method', 'function', 'module'), and the specific line number. This model serves as a structured data container for tracking and analyzing call relationships, particularly in contexts like 'called_by' and 'instantiated_by' lists."
        init_method:
          description: "The CallInfo class, as a Pydantic BaseModel, automatically generates its constructor. It initializes an instance by accepting keyword arguments that correspond to its defined fields: file, function, mode, and line. These parameters are validated upon instantiation to ensure they conform to their specified types, providing robust data integrity for call event information."
          parameters[4]{name,type,description}:
            file,str,The path to the file where the call event occurred.
            function,str,The name of the function or method that is making the call.
            mode,str,"The type or context of the call, such as 'method', 'function', or 'module'."
            line,int,The line number in the file where the call event is located.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly depend on other components within the provided context.
          instantiated_by: This class is not explicitly shown to be instantiated by other components within the provided context.
      error: null
    schemas.types.FunctionContextInput:
      identifier: schemas.types.FunctionContextInput
      description:
        overall: "The `FunctionContextInput` class is a Pydantic BaseModel designed to structure the contextual information required for analyzing a function. It serves as a data container, defining the expected format for inputs related to a function's interactions within a larger system. This class explicitly tracks what other entities a function calls and by which entities it is called, providing a clear, machine-readable representation of its dependencies and usage."
        init_method:
          description: "This class is a Pydantic BaseModel, meaning its constructor is implicitly generated by Pydantic. It initializes instances by validating and assigning values to its `calls` and `called_by` fields based on the provided arguments, ensuring data integrity according to the defined types."
          parameters[2]{name,type,description}:
            calls,"List[str]","A list of identifiers (strings) representing other methods, classes, or functions that the function being analyzed calls."
            called_by,"List[CallInfo]","A list of `CallInfo` objects, each detailing an entity that calls the function being analyzed, providing context on its callers."
        methods[0]:
        usage_context:
          dependencies: "This class implicitly depends on `pydantic.BaseModel` for its core functionality and `typing.List` for type hinting. It also relies on the `CallInfo` type, which is not defined within this source code but is expected to be available."
          instantiated_by: The provided context does not specify where this class is instantiated. It is typically instantiated when parsing or creating structured data related to function analysis.
      error: null
    schemas.types.FunctionAnalysisInput:
      identifier: schemas.types.FunctionAnalysisInput
      description:
        overall: "The FunctionAnalysisInput class is a Pydantic BaseModel designed to define the structured input required for generating a FunctionAnalysis object. It serves as a data contract, ensuring that all necessary components‚Äîsuch as the analysis mode, function identifier, source code, relevant imports, and contextual information‚Äîare provided and correctly typed before a function analysis can proceed. This class facilitates robust data validation and clear communication of analysis requirements within the system."
        init_method:
          description: "This class does not explicitly define an __init__ method. As a Pydantic BaseModel, its constructor is implicitly generated, allowing instantiation by passing keyword arguments corresponding to its defined fields."
          parameters[5]{name,type,description}:
            mode,"Literal[\"function_analysis\"]","Specifies the analysis mode, which must be 'function_analysis'."
            identifier,str,The unique name or identifier of the function to be analyzed.
            source_code,str,The raw source code of the function.
            imports,"List[str]",A list of import statements relevant to the function's source file.
            context,FunctionContextInput,Additional contextual information required for the function analysis.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list external functional dependencies within the provided context.
          instantiated_by: The instantiation points for this class are not specified in the provided context.
      error: null
    schemas.types.MethodContextInput:
      identifier: schemas.types.MethodContextInput
      description:
        overall: "The MethodContextInput class is a Pydantic BaseModel designed to encapsulate structured contextual information about a specific method. It serves as a data transfer object, holding details such as the method's unique identifier, a list of other functions or methods it calls, a list of entities that call it, its arguments, and its docstring. This model is crucial for providing a standardized format for method-level context within a larger system."
        init_method:
          description: "This class, being a Pydantic BaseModel, is initialized by providing values for its defined fields. These fields represent structured context information about a method, such as its identifier, calls it makes, where it is called from, its arguments, and its docstring."
          parameters[5]{name,type,description}:
            identifier,str,A unique string identifier for the method.
            calls,"List[str]","A list of identifiers for other methods, classes, or functions called by this method."
            called_by,"List[CallInfo]",A list of CallInfo objects indicating where this method is called from.
            args,"List[str]",A list of argument names for the method.
            docstring,"Optional[str]","The docstring of the method, if available."
        methods[0]:
        usage_context:
          dependencies: This class depends on pydantic.BaseModel for its core functionality and typing.List and typing.Optional for type hinting its fields.
          instantiated_by: The instantiation points for this class are not explicitly provided in the context.
      error: null
    schemas.types.ClassContextInput:
      identifier: schemas.types.ClassContextInput
      description:
        overall: "The ClassContextInput class is a Pydantic BaseModel designed to encapsulate structured context information necessary for analyzing a Python class. It serves as a data container, defining the expected format for dependencies, instantiation points, and detailed method contexts, facilitating a holistic understanding of a class's role and interactions within a larger system."
        init_method:
          description: "This class, being a Pydantic BaseModel, automatically generates an __init__ method. This constructor is responsible for validating and assigning the provided `dependencies`, `instantiated_by`, and `method_context` lists to the instance attributes upon object creation, ensuring data integrity according to the defined types."
          parameters[3]{name,type,description}:
            dependencies,"List[str]",A list of strings representing external dependencies of the class.
            instantiated_by,"List[CallInfo]",A list of CallInfo objects indicating where this class is instantiated.
            method_context,"List[MethodContextInput]","A list of MethodContextInput objects, each providing context for a specific method within the class."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external dependencies within the provided context.
          instantiated_by: This class is not explicitly shown to be instantiated by any other components within the provided context.
      error: null
    schemas.types.ClassAnalysisInput:
      identifier: schemas.types.ClassAnalysisInput
      description:
        overall: "The `ClassAnalysisInput` class is a Pydantic BaseModel designed to define the structured input required for performing a class analysis. It acts as a data contract, ensuring that all necessary information‚Äîsuch as the class identifier, its source code, relevant imports, and contextual details‚Äîis provided in a standardized format. This class is fundamental for systems that automate code analysis, providing a clear schema for data exchange."
        init_method:
          description: "This class does not explicitly define an `__init__` method. As a `pydantic.BaseModel`, its constructor is implicitly generated by Pydantic based on the defined fields, allowing instantiation by passing keyword arguments corresponding to its attributes."
          parameters[5]{name,type,description}:
            mode,"Literal[\"class_analysis\"]","The operational mode, which is fixed to 'class_analysis' for this specific input type, indicating the type of analysis to be performed."
            identifier,str,The unique identifier or fully qualified name of the class that is to be analyzed.
            source_code,str,"The complete raw source code string of the entire class definition, including its methods and docstrings."
            imports,"List[str]","A list of strings, each representing an import statement relevant to the class or its containing module, providing necessary context for type resolution."
            context,ClassContextInput,"An object containing additional contextual information pertinent to the class, such as its dependencies and where it is instantiated within the larger codebase."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies within the provided context.
          instantiated_by: The specific points of instantiation for this class are not provided in the current context.
      error: null