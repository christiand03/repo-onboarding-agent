basic_info:
  projekt_uebersicht:
    titel: Repo Onboarding Agent üöÄ
    beschreibung: ```
    aktueller_status: Information not found
    key_features: Information not found
    tech_stack: Information not found
  installation:
    dependencies: "- altair==4.2.2\n- annotated-types==0.7.0\n- anyio==4.11.0\n- attrs==25.4.0\n- bcrypt==5.0.0\n- blinker==1.9.0\n- cachetools==6.2.2\n- captcha==0.7.1\n- certifi==2025.11.12\n- cffi==2.0.0\n- charset-normalizer==3.4.4\n- click==8.3.1\n- colorama==0.4.6\n- contourpy==1.3.3\n- cryptography==46.0.3\n- cycler==0.12.1\n- distro==1.9.0\n- dnspython==2.8.0\n- dotenv==0.9.9\n- entrypoints==0.4\n- extra-streamlit-components==0.1.81\n- filetype==1.2.0\n- fonttools==4.61.0\n- gitdb==4.0.12\n- GitPython==3.1.45\n- google-ai-generativelanguage==0.9.0\n- google-api-core==2.28.1\n- google-auth==2.43.0\n- googleapis-common-protos==1.72.0\n- grpcio==1.76.0\n- grpcio-status==1.76.0\n- h11==0.16.0\n- httpcore==1.0.9\n- httpx==0.28.1\n- idna==3.11\n- Jinja2==3.1.6\n- jiter==0.12.0\n- jsonpatch==1.33\n- jsonpointer==3.0.0\n- jsonschema==4.25.1\n- jsonschema-specifications==2025.9.1\n- kiwisolver==1.4.9\n- langchain==1.0.8\n- langchain-core==1.1.0\n- langchain-google-genai==3.1.0\n- langchain-ollama==1.0.0\n- langchain-openai==1.1.0\n- langgraph==1.0.3\n- langgraph-checkpoint==3.0.1\n- langgraph-prebuilt==1.0.5\n- langgraph-sdk==0.2.9\n- langsmith==0.4.46\n- MarkupSafe==3.0.3\n- matplotlib==3.10.7\n- narwhals==2.12.0\n- networkx==3.6\n- numpy==2.3.5\n- ollama==0.6.1\n- openai==2.8.1\n- orjson==3.11.4\n- ormsgpack==1.12.0\n- packaging==25.0\n- pandas==2.3.3\n- pillow==12.0.0\n- proto-plus==1.26.1\n- protobuf==6.33.1\n- pyarrow==21.0.0\n- pyasn1==0.6.1\n- pyasn1_modules==0.4.2\n- pycparser==2.23\n- pydantic==2.12.4\n- pydantic_core==2.41.5\n- pydeck==0.9.1\n- PyJWT==2.10.1\n- pymongo==4.15.4\n- pyparsing==3.2.5\n- python-dateutil==2.9.0.post0\n- python-dotenv==1.2.1\n- pytz==2025.2\n- PyYAML==6.0.3\n- referencing==0.37.0\n- regex==2025.11.3\n- requests==2.32.5\n- requests-toolbelt==1.0.0\n- rpds-py==0.29.0\n- rsa==4.9.1\n- setuptools==75.9.1\n- six==1.17.0\n- smmap==5.0.2\n- sniffio==1.3.1\n- streamlit==1.51.0\n- streamlit-authenticator==0.4.2\n- streamlit-mermaid==0.3.0\n- tenacity==9.1.2\n- tiktoken==0.12.0\n- toml==0.10.2\n- toolz==1.1.0\n- toon_format @ git+https://github.com/toon-format/toon-python.git@9c4f0c0c24f2a0b0b376315f4b8707f8c9006de6\n- tornado==6.5.2\n- tqdm==4.67.1\n- typing-inspection==0.4.2\n- typing_extensions==4.15.0\n- tzdata==2025.2\n- urllib3==2.5.0\n- watchdog==6.0.0\n- xxhash==3.6.0\n- zstandard==0.25.0\n- nbformat"
    setup_anleitung: Information not found
    quick_start_guide: Information not found
file_tree:
  name: root
  type: directory
  children[16]:
    - path: .env.example
      name: .env.example
      size: 48
      type: file
    - path: .gitignore
      name: .gitignore
      size: 184
      type: file
    - name: SystemPrompts
      type: directory
      children[6]{path,name,size,type}:
        SystemPrompts/SystemPromptClassHelperLLM.txt,SystemPromptClassHelperLLM.txt,6645,file
        SystemPrompts/SystemPromptFunctionHelperLLM.txt,SystemPromptFunctionHelperLLM.txt,5546,file
        SystemPrompts/SystemPromptHelperLLM.txt,SystemPromptHelperLLM.txt,9350,file
        SystemPrompts/SystemPromptMainLLM.txt,SystemPromptMainLLM.txt,8380,file
        SystemPrompts/SystemPromptMainLLMToon.txt,SystemPromptMainLLMToon.txt,8456,file
        SystemPrompts/SystemPromptNotebookLLM.txt,SystemPromptNotebookLLM.txt,6921,file
    - path: analysis_output.json
      name: analysis_output.json
      size: 569396
      type: file
    - name: backend
      type: directory
      children[12]{path,name,size,type}:
        backend/AST_Schema.py,AST_Schema.py,6622,file
        backend/File_Dependency.py,File_Dependency.py,7384,file
        backend/HelperLLM.py,HelperLLM.py,17280,file
        backend/MainLLM.py,MainLLM.py,4281,file
        backend/__init__.py,__init__.py,0,file
        backend/basic_info.py,basic_info.py,7284,file
        backend/callgraph.py,callgraph.py,9020,file
        backend/converter.py,converter.py,3594,file
        backend/getRepo.py,getRepo.py,4739,file
        backend/main.py,main.py,25591,file
        backend/relationship_analyzer.py,relationship_analyzer.py,8279,file
        backend/scads_key_test.py,scads_key_test.py,663,file
    - name: database
      type: directory
      children[1]{path,name,size,type}:
        database/db.py,db.py,8222,file
    - name: frontend
      type: directory
      children[4]:
        - name: .streamlit
          type: directory
          children[1]{path,name,size,type}:
            frontend/.streamlit/config.toml,config.toml,165,file
        - path: frontend/__init__.py
          name: __init__.py
          size: 0
          type: file
        - path: frontend/frontend.py
          name: frontend.py
          size: 31176
          type: file
        - name: gifs
          type: directory
          children[1]{path,name,size,type}:
            frontend/gifs/4j.gif,4j.gif,3306723,file
    - name: notizen
      type: directory
      children[8]:
        - path: notizen/Report Agenda.txt
          name: Report Agenda.txt
          size: 3492
          type: file
        - path: notizen/Zwischenpraesentation Agenda.txt
          name: Zwischenpraesentation Agenda.txt
          size: 809
          type: file
        - path: notizen/doc_bestandteile.md
          name: doc_bestandteile.md
          size: 847
          type: file
        - name: grafiken
          type: directory
          children[4]:
            - name: "1"
              type: directory
              children[5]{path,name,size,type}:
                notizen/grafiken/1/File_Dependency_Graph_Repo.dot,File_Dependency_Graph_Repo.dot,1227,file
                notizen/grafiken/1/global_callgraph.png,global_callgraph.png,940354,file
                notizen/grafiken/1/global_graph.png,global_graph.png,4756519,file
                notizen/grafiken/1/global_graph_2.png,global_graph_2.png,242251,file
                notizen/grafiken/1/repo.dot,repo.dot,13870,file
            - name: "2"
              type: directory
              children[12]{path,name,size,type}:
                notizen/grafiken/2/FDG_repo.dot,FDG_repo.dot,9150,file
                notizen/grafiken/2/fdg_graph.png,fdg_graph.png,425374,file
                notizen/grafiken/2/fdg_graph_2.png,fdg_graph_2.png,4744718,file
                notizen/grafiken/2/filtered_callgraph_flask.png,filtered_callgraph_flask.png,614631,file
                notizen/grafiken/2/filtered_callgraph_repo-agent.png,filtered_callgraph_repo-agent.png,280428,file
                notizen/grafiken/2/filtered_callgraph_repo-agent_3.png,filtered_callgraph_repo-agent_3.png,1685838,file
                notizen/grafiken/2/filtered_repo_callgraph_flask.dot,filtered_repo_callgraph_flask.dot,19984,file
                notizen/grafiken/2/filtered_repo_callgraph_repo-agent-3.dot,filtered_repo_callgraph_repo-agent-3.dot,20120,file
                notizen/grafiken/2/filtered_repo_callgraph_repo-agent.dot,filtered_repo_callgraph_repo-agent.dot,3118,file
                notizen/grafiken/2/global_callgraph.png,global_callgraph.png,608807,file
                notizen/grafiken/2/graph_flask.md,graph_flask.md,17398,file
                notizen/grafiken/2/repo.dot,repo.dot,19984,file
            - name: Flask-Repo
              type: directory
              children[65]{path,name,size,type}:
                notizen/grafiken/Flask-Repo/__init__.dot,__init__.dot,89,file
                notizen/grafiken/Flask-Repo/__main__.dot,__main__.dot,87,file
                notizen/grafiken/Flask-Repo/app.dot,app.dot,78,file
                notizen/grafiken/Flask-Repo/auth.dot,auth.dot,1126,file
                notizen/grafiken/Flask-Repo/blog.dot,blog.dot,939,file
                notizen/grafiken/Flask-Repo/blueprints.dot,blueprints.dot,4601,file
                notizen/grafiken/Flask-Repo/cli.dot,cli.dot,8269,file
                notizen/grafiken/Flask-Repo/conf.dot,conf.dot,489,file
                notizen/grafiken/Flask-Repo/config.dot,config.dot,2130,file
                notizen/grafiken/Flask-Repo/conftest.dot,conftest.dot,1756,file
                notizen/grafiken/Flask-Repo/ctx.dot,ctx.dot,3009,file
                notizen/grafiken/Flask-Repo/db.dot,db.dot,782,file
                notizen/grafiken/Flask-Repo/debughelpers.dot,debughelpers.dot,1903,file
                notizen/grafiken/Flask-Repo/factory.dot,factory.dot,220,file
                notizen/grafiken/Flask-Repo/flask.dot,flask.dot,82,file
                notizen/grafiken/Flask-Repo/globals.dot,globals.dot,370,file
                notizen/grafiken/Flask-Repo/hello.dot,hello.dot,152,file
                notizen/grafiken/Flask-Repo/helpers.dot,helpers.dot,2510,file
                notizen/grafiken/Flask-Repo/importerrorapp.dot,importerrorapp.dot,155,file
                notizen/grafiken/Flask-Repo/logging.dot,logging.dot,564,file
                notizen/grafiken/Flask-Repo/make_celery.dot,make_celery.dot,99,file
                notizen/grafiken/Flask-Repo/multiapp.dot,multiapp.dot,88,file
                notizen/grafiken/Flask-Repo/provider.dot,provider.dot,1769,file
                notizen/grafiken/Flask-Repo/scaffold.dot,scaffold.dot,3700,file
                notizen/grafiken/Flask-Repo/sessions.dot,sessions.dot,4178,file
                notizen/grafiken/Flask-Repo/signals.dot,signals.dot,133,file
                notizen/grafiken/Flask-Repo/tag.dot,tag.dot,3750,file
                notizen/grafiken/Flask-Repo/tasks.dot,tasks.dot,312,file
                notizen/grafiken/Flask-Repo/templating.dot,templating.dot,2277,file
                notizen/grafiken/Flask-Repo/test_appctx.dot,test_appctx.dot,2470,file
                notizen/grafiken/Flask-Repo/test_async.dot,test_async.dot,1919,file
                notizen/grafiken/Flask-Repo/test_auth.dot,test_auth.dot,725,file
                notizen/grafiken/Flask-Repo/test_basic.dot,test_basic.dot,17383,file
                notizen/grafiken/Flask-Repo/test_blog.dot,test_blog.dot,1105,file
                notizen/grafiken/Flask-Repo/test_blueprints.dot,test_blueprints.dot,11515,file
                notizen/grafiken/Flask-Repo/test_cli.dot,test_cli.dot,6435,file
                notizen/grafiken/Flask-Repo/test_config.dot,test_config.dot,2854,file
                notizen/grafiken/Flask-Repo/test_config.png,test_config.png,288693,file
                notizen/grafiken/Flask-Repo/test_converters.dot,test_converters.dot,937,file
                notizen/grafiken/Flask-Repo/test_db.dot,test_db.dot,481,file
                notizen/grafiken/Flask-Repo/test_factory.dot,test_factory.dot,201,file
                notizen/grafiken/Flask-Repo/test_helpers.dot,test_helpers.dot,5857,file
                notizen/grafiken/Flask-Repo/test_instance_config.dot,test_instance_config.dot,1249,file
                notizen/grafiken/Flask-Repo/test_js_example.dot,test_js_example.dot,453,file
                notizen/grafiken/Flask-Repo/test_json.dot,test_json.dot,4021,file
                notizen/grafiken/Flask-Repo/test_json_tag.dot,test_json_tag.dot,1452,file
                notizen/grafiken/Flask-Repo/test_logging.dot,test_logging.dot,1348,file
                notizen/grafiken/Flask-Repo/test_regression.dot,test_regression.dot,763,file
                notizen/grafiken/Flask-Repo/test_reqctx.dot,test_reqctx.dot,3809,file
                notizen/grafiken/Flask-Repo/test_request.dot,test_request.dot,668,file
                notizen/grafiken/Flask-Repo/test_session_interface.dot,test_session_interface.dot,667,file
                notizen/grafiken/Flask-Repo/test_signals.dot,test_signals.dot,1671,file
                notizen/grafiken/Flask-Repo/test_subclassing.dot,test_subclassing.dot,612,file
                notizen/grafiken/Flask-Repo/test_templating.dot,test_templating.dot,5129,file
                notizen/grafiken/Flask-Repo/test_testing.dot,test_testing.dot,4081,file
                notizen/grafiken/Flask-Repo/test_user_error_handler.dot,test_user_error_handler.dot,5984,file
                notizen/grafiken/Flask-Repo/test_views.dot,test_views.dot,2641,file
                notizen/grafiken/Flask-Repo/testing.dot,testing.dot,2785,file
                notizen/grafiken/Flask-Repo/typing.dot,typing.dot,86,file
                notizen/grafiken/Flask-Repo/typing_app_decorators.dot,typing_app_decorators.dot,500,file
                notizen/grafiken/Flask-Repo/typing_error_handler.dot,typing_error_handler.dot,420,file
                notizen/grafiken/Flask-Repo/typing_route.dot,typing_route.dot,1717,file
                notizen/grafiken/Flask-Repo/views.dot,views.dot,1125,file
                notizen/grafiken/Flask-Repo/wrappers.dot,wrappers.dot,911,file
                notizen/grafiken/Flask-Repo/wsgi.dot,wsgi.dot,19,file
            - name: Repo-onboarding
              type: directory
              children[15]{path,name,size,type}:
                notizen/grafiken/Repo-onboarding/AST.dot,AST.dot,1361,file
                notizen/grafiken/Repo-onboarding/Frontend.dot,Frontend.dot,538,file
                notizen/grafiken/Repo-onboarding/HelperLLM.dot,HelperLLM.dot,1999,file
                notizen/grafiken/Repo-onboarding/HelperLLM.png,HelperLLM.png,234861,file
                notizen/grafiken/Repo-onboarding/MainLLM.dot,MainLLM.dot,2547,file
                notizen/grafiken/Repo-onboarding/agent.dot,agent.dot,2107,file
                notizen/grafiken/Repo-onboarding/basic_info.dot,basic_info.dot,1793,file
                notizen/grafiken/Repo-onboarding/callgraph.dot,callgraph.dot,1478,file
                notizen/grafiken/Repo-onboarding/getRepo.dot,getRepo.dot,1431,file
                notizen/grafiken/Repo-onboarding/graph_AST.png,graph_AST.png,132743,file
                notizen/grafiken/Repo-onboarding/graph_AST2.png,graph_AST2.png,12826,file
                notizen/grafiken/Repo-onboarding/graph_AST3.png,graph_AST3.png,136016,file
                notizen/grafiken/Repo-onboarding/main.dot,main.dot,1091,file
                notizen/grafiken/Repo-onboarding/tools.dot,tools.dot,1328,file
                notizen/grafiken/Repo-onboarding/types.dot,types.dot,133,file
        - path: notizen/notizen.md
          name: notizen.md
          size: 4937
          type: file
        - path: notizen/paul_notizen.md
          name: paul_notizen.md
          size: 2033
          type: file
        - path: notizen/praesentation_notizen.md
          name: praesentation_notizen.md
          size: 2738
          type: file
        - path: notizen/technische_notizen.md
          name: technische_notizen.md
          size: 3616
          type: file
    - path: output.json
      name: output.json
      size: 454008
      type: file
    - path: output.toon
      name: output.toon
      size: 341854
      type: file
    - path: readme.md
      name: readme.md
      size: 1905
      type: file
    - path: requirements.txt
      name: requirements.txt
      size: 4286
      type: file
    - name: result
      type: directory
      children[28]{path,name,size,type}:
        result/ast_schema_01_12_2025_11-49-24.json,ast_schema_01_12_2025_11-49-24.json,201215,file
        result/notebook_report_23_12_2025_12-56-24_NotebookLLM_gemini-2.5-flash.md,notebook_report_23_12_2025_12-56-24_NotebookLLM_gemini-2.5-flash.md,15776,file
        result/notebook_report_27_12_2025_15-06-09_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-06-09_NotebookLLM_gemini-2.5-flash.md,14804,file
        result/notebook_report_27_12_2025_15-09-29_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-09-29_NotebookLLM_gemini-2.5-flash.md,17598,file
        result/notebook_report_27_12_2025_15-26-34_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-26-34_NotebookLLM_gemini-2.5-flash.md,20458,file
        result/notebook_report_27_12_2025_15-33-06_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-33-06_NotebookLLM_gemini-2.5-flash.md,15268,file
        result/notebook_report_29_12_2025_15-03-21_NotebookLLM_gemini-2.5-flash.md,notebook_report_29_12_2025_15-03-21_NotebookLLM_gemini-2.5-flash.md,16111,file
        result/report_01_12_2025_12-26-46_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_12-26-46_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,79523,file
        result/report_01_12_2025_12-55-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_12-55-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,143883,file
        result/report_01_12_2025_13-37-30_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_13-37-30_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,141924,file
        result/report_01_12_2025_14-15-04_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_14-15-04_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,129176,file
        result/report_01_12_2025_14-42-38_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_14-42-38_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,38933,file
        result/report_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,138015,file
        result/report_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.md,report_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.md,127747,file
        result/report_03_12_2025_22-46-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_03_12_2025_22-46-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,80305,file
        result/report_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,163921,file
        result/report_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.md,report_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.md,128520,file
        result/report_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.md,report_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.md,7245,file
        result/report_14_11_2025_14-52-36.md,report_14_11_2025_14-52-36.md,16393,file
        result/report_14_11_2025_15-21-53.md,report_14_11_2025_15-21-53.md,108585,file
        result/report_14_11_2025_15-26-24.md,report_14_11_2025_15-26-24.md,11659,file
        result/report_21_11_2025_15-43-30.md,report_21_11_2025_15-43-30.md,57940,file
        result/report_21_11_2025_16-06-12.md,report_21_11_2025_16-06-12.md,76423,file
        result/report_22_11_2025_14-01-50_Helper_llama3_Main_geminipro.md,report_22_11_2025_14-01-50_Helper_llama3_Main_geminipro.md,39595,file
        result/report_22_11_2025_14-39-55_Helper_llama3_MainLLM_llama3.md,report_22_11_2025_14-39-55_Helper_llama3_MainLLM_llama3.md,2013,file
        result/result_2025-11-11_12-30-53.md,result_2025-11-11_12-30-53.md,1232,file
        result/result_2025-11-11_12-43-51.md,result_2025-11-11_12-43-51.md,33683,file
        result/result_2025-11-11_12-45-37.md,result_2025-11-11_12-45-37.md,1193,file
    - name: schemas
      type: directory
      children[1]{path,name,size,type}:
        schemas/types.py,types.py,7559,file
    - name: statistics
      type: directory
      children[5]{path,name,size,type}:
        statistics/savings_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.png,savings_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.png,23279,file
        statistics/savings_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.png,savings_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.png,23650,file
        statistics/savings_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.png,savings_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.png,23805,file
        statistics/savings_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.png,savings_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.png,22698,file
        statistics/savings_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.png,savings_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.png,22539,file
    - path: test.json
      name: test.json
      size: 241669
      type: file
ast_schema:
  files:
    "backend/AST_Schema.py":
      ast_nodes:
        imports[3]: ast,os,getRepo.GitRepository
        functions[1]:
          - mode: function_analysis
            identifier: backend.AST_Schema.path_to_module
            name: path_to_module
            args[2]: filepath,project_root
            docstring: Wandelt einen Dateipfad in einen Python-Modulpfad um.
            source_code: "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    \n    module_path = rel_path.replace(os.path.sep, '.')\n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n    return module_path"
            start_line: 5
            end_line: 18
            context:
              calls[0]:
              called_by[1]: backend.AST_Schema.ASTVisitor.__init__
        classes[2]:
          - mode: class_analysis
            identifier: backend.AST_Schema.ASTVisitor
            name: ASTVisitor
            docstring: null
            source_code: "class ASTVisitor(ast.NodeVisitor):\n    def __init__(self, source_code: str, file_path: str, project_root: str):\n        self.source_code = source_code\n        self.file_path = file_path\n        self.project_root = project_root\n        self.module_path = path_to_module(self.file_path, self.project_root)\n        self.schema = {\"imports\": [], \"functions\": [], \"classes\": []}\n        self._current_class = None\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(f\"{node.module}.{alias.name}\")\n        self.generic_visit(node)\n\n    def visit_ClassDef(self, node):\n        class_identifier = f\"{self.module_path}.{node.name}\"\n        class_info = {\n            \"mode\": \"class_analysis\",\n            \"identifier\": class_identifier,\n            \"name\": node.name,\n            \"docstring\": ast.get_docstring(node),\n            \"source_code\": ast.get_source_segment(self.source_code, node),\n            \"start_line\": node.lineno,\n            \"end_line\": node.end_lineno,\n            \"context\": {\n                \"dependencies\": [],\n                \"instantiated_by\": [],\n                \"method_context\": []\n            },   \n        }\n        self.schema[\"classes\"].append(class_info)\n        self._current_class = class_info \n        self.generic_visit(node)\n        self._current_class = None\n\n    def visit_FunctionDef(self, node):\n        if self._current_class:\n            method_identifier = f\"{self._current_class['identifier']}.{node.name}\"\n            method_context_info = {\n                \"identifier\": method_identifier,\n                \"name\": node.name,\n                \"calls\": [],\n                \"called_by\": [],\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n            }\n            self._current_class[\"context\"][\"method_context\"].append(method_context_info)\n        else:\n            func_identifier = f\"{self.module_path}.{node.name}\"\n            func_info = {\n                \"mode\": \"function_analysis\",\n                \"identifier\": func_identifier,\n                \"name\": node.name,\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"source_code\": ast.get_source_segment(self.source_code, node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n                \"context\": {\n                    \"calls\": [],\n                    \"called_by\": []\n                }\n            }\n            self.schema[\"functions\"].append(func_info)\n        self.generic_visit(node)\n    \n    def visit_AsyncFunctionDef(self, node):\n        self.visit_FunctionDef(node)"
            start_line: 20
            end_line: 94
            context:
              dependencies[1]: backend.AST_Schema.path_to_module
              instantiated_by[1]: backend.AST_Schema.ASTAnalyzer.analyze_repository
              method_context[6]:
                - identifier: backend.AST_Schema.ASTVisitor.__init__
                  name: __init__
                  calls[1]: backend.AST_Schema.path_to_module
                  called_by[0]:
                  args[4]: self,source_code,file_path,project_root
                  docstring: null
                  start_line: 21
                  end_line: 27
                - identifier: backend.AST_Schema.ASTVisitor.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 29
                  end_line: 32
                - identifier: backend.AST_Schema.ASTVisitor.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 34
                  end_line: 37
                - identifier: backend.AST_Schema.ASTVisitor.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 39
                  end_line: 58
                - identifier: backend.AST_Schema.ASTVisitor.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 60
                  end_line: 91
                - identifier: backend.AST_Schema.ASTVisitor.visit_AsyncFunctionDef
                  name: visit_AsyncFunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 93
                  end_line: 94
          - mode: class_analysis
            identifier: backend.AST_Schema.ASTAnalyzer
            name: ASTAnalyzer
            docstring: null
            source_code: "class ASTAnalyzer:\n\n    def __init__(self):\n        pass\n\n    def merge_relationship_data(self, full_schema: dict, raw_relationships: dict) -> dict:\n\n        outgoing_calls = raw_relationships.get(\"outgoing\", {})\n        incoming_calls = raw_relationships.get(\"incoming\", {})\n\n        for file_path, file_data in full_schema.get(\"files\", {}).items():\n            ast_nodes = file_data.get(\"ast_nodes\", {})\n            \n            for func in ast_nodes.get(\"functions\", []):\n                func_id = func.get(\"identifier\")\n                if func_id:\n                    func[\"context\"][\"calls\"] = outgoing_calls.get(func_id, [])\n                    func[\"context\"][\"called_by\"] = incoming_calls.get(func_id, [])\n\n            for cls in ast_nodes.get(\"classes\", []):\n                cls_id = cls.get(\"identifier\")\n                \n                if cls_id:\n                    cls[\"context\"][\"instantiated_by\"] = incoming_calls.get(cls_id, [])\n\n                class_dependencies = set()\n                \n                for method in cls[\"context\"].get(\"method_context\", []):\n                    method_id = method.get(\"identifier\")\n                    if method_id:\n                        m_calls = outgoing_calls.get(method_id, [])\n                        method[\"calls\"] = m_calls\n                        method[\"called_by\"] = incoming_calls.get(method_id, [])\n                        \n                        for callee in m_calls:\n                            if cls_id and not callee.startswith(f\"{cls_id}.\"):\n                                class_dependencies.add(callee)\n                \n                cls[\"context\"][\"dependencies\"] = sorted(list(class_dependencies))\n        \n        return full_schema\n\n    def analyze_repository(self, files: list, repo: GitRepository) -> dict:\n        full_schema = {\n            \"files\": {}\n        }\n\n        all_paths = [file_obj.path for file_obj in files]\n        if not all_paths:\n            return full_schema\n        \n        project_root = os.path.commonpath(all_paths)\n        if os.path.isfile(project_root):\n            project_root = os.path.dirname(project_root)\n\n        for file_obj in files:\n            if not file_obj.path.endswith('.py'):\n                continue\n            \n            file_content = file_obj.content\n            if not file_content.strip():\n                continue\n\n            try:\n                tree = ast.parse(file_content)\n                visitor = ASTVisitor(\n                    source_code=file_content, \n                    file_path=file_obj.path, \n                    project_root=project_root\n                )\n                visitor.visit(tree)\n                file_schema_nodes = visitor.schema\n\n                if file_schema_nodes[\"imports\"] or file_schema_nodes[\"functions\"] or file_schema_nodes[\"classes\"]:\n                    full_schema[\"files\"][file_obj.path] = {\n                        \"ast_nodes\": file_schema_nodes\n                    }\n\n            except (SyntaxError, ValueError) as e:\n                print(f\"Warnung: Konnte Datei '{file_obj.path}' nicht parsen. Fehler: {e}\")\n\n        return full_schema"
            start_line: 97
            end_line: 178
            context:
              dependencies[1]: backend.AST_Schema.ASTVisitor
              instantiated_by[1]: backend.main.main_workflow
              method_context[3]:
                - identifier: backend.AST_Schema.ASTAnalyzer.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 99
                  end_line: 100
                - identifier: backend.AST_Schema.ASTAnalyzer.merge_relationship_data
                  name: merge_relationship_data
                  calls[0]:
                  called_by[1]: backend.main.main_workflow
                  args[3]: self,full_schema,raw_relationships
                  docstring: null
                  start_line: 102
                  end_line: 137
                - identifier: backend.AST_Schema.ASTAnalyzer.analyze_repository
                  name: analyze_repository
                  calls[1]: backend.AST_Schema.ASTVisitor
                  called_by[1]: backend.main.main_workflow
                  args[3]: self,files,repo
                  docstring: null
                  start_line: 139
                  end_line: 178
    "backend/File_Dependency.py":
      ast_nodes:
        imports[17]: networkx,os,ast.Assign,ast.AST,ast.ClassDef,ast.FunctionDef,ast.Import,ast.ImportFrom,ast.Name,ast.NodeVisitor,ast.literal_eval,ast.parse,ast.walk,keyword.iskeyword,pathlib.Path,getRepo.GitRepository,callgraph.make_safe_dot
        functions[3]:
          - mode: function_analysis
            identifier: backend.File_Dependency.build_file_dependency_graph
            name: build_file_dependency_graph
            args[3]: filename,tree,repo_root
            docstring: null
            source_code: "def build_file_dependency_graph(filename: str, tree: AST, repo_root: str) -> nx.DiGraph:\n    graph = nx.DiGraph()\n\n    tree_visitor = FileDependencyGraph(filename, repo_root)\n    tree_visitor.visit(tree)\n\n    for caller, callees in tree_visitor.import_dependencies.items():\n        graph.add_node(caller)\n        graph.add_nodes_from(callees)\n        for callee in callees:\n            graph.add_edge(caller, callee)\n    \n    return graph"
            start_line: 153
            end_line: 165
            context:
              calls[1]: backend.File_Dependency.FileDependencyGraph
              called_by[1]: backend.File_Dependency.build_repository_graph
          - mode: function_analysis
            identifier: backend.File_Dependency.build_repository_graph
            name: build_repository_graph
            args[1]: repository
            docstring: null
            source_code: "def build_repository_graph(repository: GitRepository) -> nx.DiGraph:\n    all_files = repository.get_all_files()\n    repo_root = repository.temp_dir\n    global_graph = nx.DiGraph()\n\n    for file in all_files: \n        if not file.path.endswith(\".py\"):\n            continue\n        filename = str(os.path.basename(file.path)).removesuffix(\".py\")\n        tree = parse(file.content)\n        graph = build_file_dependency_graph(filename, tree, repo_root)\n        \n        for node in graph.nodes:\n            global_graph.add_node(node)\n\n        for caller, callee in graph.edges:\n            if callee:\n                global_graph.add_node(callee)\n                global_graph.add_edge(caller, callee)\n    \n    return global_graph"
            start_line: 167
            end_line: 187
            context:
              calls[1]: backend.File_Dependency.build_file_dependency_graph
              called_by[1]: backend.File_Dependency
          - mode: function_analysis
            identifier: backend.File_Dependency.get_all_temp_files
            name: get_all_temp_files
            args[1]: directory
            docstring: null
            source_code: "def get_all_temp_files(directory: str) -> list[Path]:\n    root_path = Path(directory).resolve()\n    all_files = [file.relative_to(root_path) for file in root_path.rglob(\"*.py\")]\n\n    return all_files"
            start_line: 189
            end_line: 193
            context:
              calls[0]:
              called_by[1]: backend.File_Dependency.FileDependencyGraph._resolve_module_name
        classes[1]:
          - mode: class_analysis
            identifier: backend.File_Dependency.FileDependencyGraph
            name: FileDependencyGraph
            docstring: null
            source_code: "class FileDependencyGraph(NodeVisitor):\n\n    import_dependencies: dict[str, set[str]] = {}\n    def __init__(self, filename: str, repo_root):\n        \"\"\"\n        Initialisiert den File Dependency Graphen\n\n        Args:\n\n        \"\"\"\n        self.filename = filename\n        self.repo_root = repo_root\n\n    def _resolve_module_name(self, node: ImportFrom) -> list[str]:\n        \"\"\"\n        L√∂st relative Imports der Form `from .. import name1, name2` auf.\n        Liefert die Liste der tats√§chlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\n        Wirft ImportError, wenn nichts aufgel√∂st werden konnte.\n        \"\"\"\n        level_depth = node.level\n        names = [alias.name for alias in node.names]  \n        all_files = get_all_temp_files(self.repo_root)  \n\n        candidates = [p for p in all_files if p.stem == Path(self.filename).stem or p.name == f\"{self.filename}.py\"]\n\n        if not candidates:\n            raise ImportError(f\"Kann aktuelle Datei '{self.filename}' im Repo nicht finden.\")\n\n\n        candidates.sort(key=lambda p: len(p.parts))\n        current_rel_path = candidates[0]\n        base_dir = current_rel_path.parent\n        if level_depth <= 0:\n            raise ImportError(\"Erwarteter relativer Import (level >= 1).\")\n        for _ in range(level_depth - 1):\n            if base_dir == base_dir.parent and len(base_dir.parts) == 0:\n                raise ImportError(f\"Relative Import-Ebene ({level_depth}) zu gro√ü f√ºr Datei '{current_rel_path}'.\")\n            base_dir = base_dir.parent\n\n        repo_root_path = Path(self.repo_root).resolve()\n        resolved: list[str] = []\n\n        def module_file_exists(rel_base: Path, name: str) -> bool:\n            file_path = repo_root_path / rel_base / f\"{name}.py\"\n            pkg_init = repo_root_path / rel_base / name / \"__init__.py\"\n            return file_path.exists() or pkg_init.exists()\n\n        def init_exports_symbol(rel_base: Path, symbol: str) -> bool:\n            \"\"\"\n            Pr√ºft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\n            oder als Name (funktion/klasse/assign) definiert ist.\n            \"\"\"\n            init_path = repo_root_path / rel_base / \"__init__.py\"\n            if not init_path.exists():\n                return False\n            try:\n                src = init_path.read_text(encoding=\"utf-8\")\n                mod = parse(src, filename=str(init_path))\n            except Exception:\n                return False\n\n            for node_ in walk(mod):\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == \"__all__\":\n                            try:\n                                value = literal_eval(node_.value)\n                                if isinstance(value, (list, tuple)) and symbol in value:\n                                    return True\n                            except Exception:\n                                pass\n                if isinstance(node_, (FunctionDef, ClassDef)) and node_.name == symbol:\n                    return True\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == symbol:\n                            return True\n            return False\n\n        for name in names:\n            if not name.isidentifier() or iskeyword(name):\n                continue\n\n            if module_file_exists(base_dir, name):\n                resolved.append(name)\n                continue\n            if init_exports_symbol(base_dir, name):\n                resolved.append(name)\n                continue\n\n        resolved = sorted(set(resolved))\n\n        if not resolved:\n            raise ImportError(\n                f\"Kein passendes Modul/Symbol f√ºr relative Import-Aufl√∂sung gefunden \"\n                f\"(level={level_depth}, names={names}, base_dir={base_dir})\"\n            )\n\n        return resolved\n    \n    def visit_Import(self, node: Import | ImportFrom, base_name: str | None = None):\n        for alias in node.names:\n        \n            if self.filename not in self.import_dependencies:\n                self.import_dependencies[self.filename] = set()\n\n            if base_name:\n                self.import_dependencies[self.filename].add(base_name)\n            else:\n                self.import_dependencies[self.filename].add(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node: ImportFrom):\n        \"\"\"\n        Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\n        also c, und dieser wird als callee f√ºr den caller, das File, gesetzt.\n        \"\"\"\n        module_name = node.module\n        if module_name:\n            module_base = module_name.split(\".\")[-1]\n            self.visit_Import(node, module_base)\n        else:\n            try:\n                resolved = self._resolve_module_name(node)\n                for base in resolved:\n                    self.visit_Import(node, base)\n            except ImportError as e:\n                print(f\"Aufl√∂sung eines relativen Imports fehlgeschlagen: {e}\")\n\n        self.generic_visit(node)"
            start_line: 22
            end_line: 151
            context:
              dependencies[3]: backend.File_Dependency.get_all_temp_files,backend.File_Dependency.init_exports_symbol,backend.File_Dependency.module_file_exists
              instantiated_by[1]: backend.File_Dependency.build_file_dependency_graph
              method_context[6]:
                - identifier: backend.File_Dependency.FileDependencyGraph.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[3]: self,filename,repo_root
                  docstring: "Initialisiert den File Dependency Graphen\n\nArgs:"
                  start_line: 25
                  end_line: 33
                - identifier: backend.File_Dependency.FileDependencyGraph._resolve_module_name
                  name: _resolve_module_name
                  calls[3]: backend.File_Dependency.get_all_temp_files,backend.File_Dependency.init_exports_symbol,backend.File_Dependency.module_file_exists
                  called_by[0]:
                  args[2]: self,node
                  docstring: "L√∂st relative Imports der Form `from .. import name1, name2` auf.\nLiefert die Liste der tats√§chlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\nWirft ImportError, wenn nichts aufgel√∂st werden konnte."
                  start_line: 35
                  end_line: 120
                - identifier: backend.File_Dependency.FileDependencyGraph.module_file_exists
                  name: module_file_exists
                  calls[0]:
                  called_by[0]:
                  args[2]: rel_base,name
                  docstring: null
                  start_line: 64
                  end_line: 67
                - identifier: backend.File_Dependency.FileDependencyGraph.init_exports_symbol
                  name: init_exports_symbol
                  calls[0]:
                  called_by[0]:
                  args[2]: rel_base,symbol
                  docstring: "Pr√ºft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\noder als Name (funktion/klasse/assign) definiert ist."
                  start_line: 69
                  end_line: 99
                - identifier: backend.File_Dependency.FileDependencyGraph.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[3]: self,node,base_name
                  docstring: null
                  start_line: 122
                  end_line: 132
                - identifier: backend.File_Dependency.FileDependencyGraph.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: "Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\nalso c, und dieser wird als callee f√ºr den caller, das File, gesetzt."
                  start_line: 134
                  end_line: 151
    "backend/HelperLLM.py":
      ast_nodes:
        imports[23]: os,json,logging,time,typing.List,typing.Dict,typing.Any,typing.Optional,typing.Union,dotenv.load_dotenv,langchain_google_genai.ChatGoogleGenerativeAI,langchain_ollama.ChatOllama,langchain_openai.ChatOpenAI,langchain.messages.HumanMessage,langchain.messages.SystemMessage,langchain.messages.AIMessage,pydantic.ValidationError,schemas.types.FunctionAnalysis,schemas.types.ClassAnalysis,schemas.types.FunctionAnalysisInput,schemas.types.FunctionContextInput,schemas.types.ClassAnalysisInput,schemas.types.ClassContextInput
        functions[1]:
          - mode: function_analysis
            identifier: backend.HelperLLM.main_orchestrator
            name: main_orchestrator
            args[0]:
            docstring: "Dummy Data and processing loop for testing the LLMHelper class.\nThis version is syntactically correct and logically matches the Pydantic models."
            source_code: "def main_orchestrator():\n    \"\"\"\n    Dummy Data and processing loop for testing the LLMHelper class.\n    This version is syntactically correct and logically matches the Pydantic models.\n    \"\"\"\n    \n    # --- Step 1: Define the pre-computed analysis for each method ---\n    \n    # Example Input 1: For the 'add_item' function\n    add_item_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"source_code\": \"\"\"def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\\\"\\\"\\\"\n        if not isinstance(quantity, int) or quantity <= 0:\n            raise ValueError(\"Quantity must be a positive integer.\")\n    \n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n        print(f\"Added {quantity} of {item_name}. New total: {self.inventory[item_name]}\")\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"process_shipment\", \"restock_api_endpoint\"]\n        }\n    })\n\n    # Example Input 2: For the 'check_stock' function\n    check_stock_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"source_code\": \"\"\"def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Retrieves the current stock quantity for a given item.\\\"\\\"\\\"\n        stock_level = self.inventory.get(item_name, 0)\n        return stock_level\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"fulfill_order\", \"ui_display_handler\"]\n        }\n    })\n\n    # Example Input 3: For the 'generate_report' function\n    generate_report_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"source_code\": \"\"\"def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a timestamped string summary of the current inventory state.\\\"\\\"\\\"\n        timestamp = datetime.now()\n        report_header = \"Inventory Report as of: {ts}\\\\n\".format(ts=timestamp)\n        report_lines = [report_header]\n        \n        for item, quantity in self.inventory.items():\n            line = \"- {name}: {qty}\".format(name=item, qty=quantity)\n            report_lines.append(line)\n            \n        return \"\\\\n\".join(report_lines)\"\"\",\n        \"imports\": [\"from datetime import datetime\"],\n        \"context\": {\n            \"calls\": [\"datetime.now\", \"str.format\"],\n            \"called_by\": [\"daily_cron_job\", \"admin_dashboard_export\"]\n        }\n    })\n\n    add_item_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"description\": {\n            \"overall\": \"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to add.\"},\n                {\"name\": \"quantity\", \"type\": \"int\", \"description\": \"The number of units to add. Must be a positive integer.\"}\n            ],\n            \"returns\": [],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"process_shipment and restock_api_endpoint\" }\n        },\n        \"error\": None\n    })\n\n    check_stock_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"description\": {\n            \"overall\": \"Retrieves the current stock quantity for a given item.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to check.\"}\n            ],\n            \"returns\": [\n                {\"name\": \"stock_level\", \"type\": \"int\", \"description\": \"The quantity of the item in stock. Returns 0 if the item is not found.\"}\n            ],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"fulfill_order and ui_display_handler\" }\n        },\n        \"error\": None\n    })\n\n    generate_report_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"description\": {\n            \"overall\": \"Generates a timestamped string summary of the current inventory state.\",\n            \"parameters\": [ {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"} ],\n            \"returns\": [\n                {\"name\": \"report\", \"type\": \"str\", \"description\": \"A formatted string detailing all items and their quantities.\"}\n            ],\n            \"usage_context\": { \"calls\": \"datetime.now and str.format\", \"called_by\": \"daily_cron_job and admin_dashboard_export\" }\n        },\n        \"error\": None\n    })\n\n    class_input = ClassAnalysisInput(\n        mode=\"class_analysis\",\n        identifier=\"InventoryManager\",\n        # NOTE: The indentation of this string is now fixed. It starts at column 0.\n        source_code=\"\"\"class InventoryManager:\n    \\\"\\\"\\\"Manages stock levels for products in a warehouse.\\\"\\\"\\\"\n    def __init__(self, warehouse_id: str):\n        self.warehouse_id = warehouse_id\n        self.inventory = {}  # item_name: quantity\n\n    def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds an item to the inventory.\\\"\\\"\\\"\n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n\n    def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Checks the stock of a specific item.\\\"\\\"\\\"\n        return self.inventory.get(item_name, 0)\n\n    def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a summary report of the inventory.\\\"\\\"\\\"\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        report_lines = [f\"Inventory Report for {self.warehouse_id} at {timestamp}\"]\n        if not self.inventory:\n            report_lines.append(\"Inventory is empty.\")\n        else:\n            for item, quantity in self.inventory.items():\n                report_lines.append(f\"- {item}: {quantity}\")\n        return \"\\\\n\".join(report_lines)\n\"\"\",\n        imports=[\"from datetime import datetime\"],\n        context=ClassContextInput(\n            dependencies=[\"datetime\"],\n            instantiated_by=[\"main_app_startup\", \"warehouse_worker_script\"],\n            methods_analysis=[\n                add_item_analysis,\n                check_stock_analysis,\n                generate_report_analysis\n            ]\n        )\n    )\n\n    \n    # The helper methods expect a LIST of inputs, so we wrap our single object in a list.\n    input = [add_item_input, check_stock_input, generate_report_input]\n    analysis = [add_item_analysis, check_stock_analysis, generate_report_analysis] \n\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    llm_helper = LLMHelper(api_key=GEMINI_API_KEY, function_prompt_path=function_prompt_file, class_prompt_path=class_prompt_file)\n\n    # This will be the final JSON containing all documentation\n    final_documentation = {}\n\n    # The analysis of the methods is already provided in the context.\n    logging.info(\"\\n--- Generating documentation for classes ---\")\n    \n    # The `generate_for_classes` method returns a list of results.\n    analysis_results = llm_helper.generate_for_functions(input)\n\n    # --- Step 4: Process the results ---\n    \n    # We loop through the list of results (even though there's only one in this case).\n    for doc in analysis_results:\n        if doc:\n            logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n            if \"classes\" not in final_documentation:\n                final_documentation[\"classes\"] = {}\n            # Use .model_dump() to convert the Pydantic object back to a dict for JSON serialization\n            final_documentation[\"classes\"][doc.identifier] = doc.model_dump() \n        else:\n            logging.warning(f\"Failed to generate doc for a class\")\n\n    # --- Step 5: Display the final aggregated result ---\n    logging.info(\"\\n--- Final Generated Documentation ---\")\n    print(json.dumps(final_documentation, indent=2))"
            start_line: 227
            end_line: 413
            context:
              calls[3]: backend.HelperLLM.LLMHelper,schemas.types.ClassAnalysisInput,schemas.types.ClassContextInput
              called_by[1]: backend.HelperLLM
        classes[1]:
          - mode: class_analysis
            identifier: backend.HelperLLM.LLMHelper
            name: LLMHelper
            docstring: "A class to interact with Google Gemini for generating code snippet documentation.\nIt centralizes API interaction, error handling, and validates I/O using Pydantic."
            source_code: "class LLMHelper:\n    \"\"\"\n    A class to interact with Google Gemini for generating code snippet documentation.\n    It centralizes API interaction, error handling, and validates I/O using Pydantic.\n    \"\"\"\n    def __init__(self, api_key: str, function_prompt_path: str, class_prompt_path: str, model_name: str = \"gemini-2.0-flash-lite\", base_url: str = None):\n        if not api_key:\n            raise ValueError(\"Gemini API Key must be set.\")\n        \n        try:\n            with open(function_prompt_path, 'r', encoding='utf-8') as f:\n                self.function_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Function system prompt file not found at: {function_prompt_path}\")\n            raise\n\n        # Handle the second file\n        try:\n            with open(class_prompt_path, 'r', encoding='utf-8') as f:\n                self.class_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Class system prompt file not found at: {class_prompt_path}\")\n            raise\n        \n        # Batch-Size config\n        self.model_name = model_name\n        self._configure_batch_settings(model_name)\n\n\n        if model_name.startswith(\"gemini-\"):\n            base_llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=0.3, \n            )\n        \n        elif model_name.startswith(\"gpt-\") and \"openGPT\" not in model_name:\n            base_llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=0.3,\n            )\n\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            if not SCADSLLM_URL:\n                raise ValueError(f\"SCADSLLM_URL environment variable is required for model {model_name}\")\n            \n            logging.info(f\"Connecting to Custom API at {SCADSLLM_URL} for model {model_name}\")\n            \n            base_llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                base_url=SCADSLLM_URL,\n                temperature=0.3,\n            )\n\n        else:\n            target_url = base_url if base_url else OLLAMA_BASE_URL\n            logging.info(f\"Using Ollama at {target_url} for model {model_name}\")\n            base_llm = ChatOllama(\n                model=model_name,\n                temperature=0.3,\n                base_url=target_url,\n            )\n\n        self.function_llm = base_llm.with_structured_output(FunctionAnalysis, method=\"json_schema\")\n        self.class_llm = base_llm.with_structured_output(ClassAnalysis, method=\"json_schema\")\n\n        self.raw_llm = base_llm\n\n        logging.info(f\"LLMHelper initialized with model '{model_name}'. Batch Size: {self.batch_size}\")\n\n    def _configure_batch_settings(self, model_name: str):\n\n        if model_name == \"gemini-2.0-flash-lite\":\n            self.batch_size = 30\n        \n        elif model_name == \"gemini-flash-latest\":\n            self.batch_size = 10\n\n        elif model_name == \"gemini-2.5-flash\":\n            self.batch_size = 10\n            \n        elif model_name == \"gemini-2.5-pro\":\n            self.batch_size = 2\n        \n        elif model_name == \"gemini-2.0-flash\":\n            self.batch_size = 15\n\n        elif model_name == \"gemini-2.5-flash-lite\":\n            self.batch_size = 15\n\n        elif model_name == \"llama3\":\n            self.batch_size = 50\n\n        elif model_name == \"gpt-5.1\":\n            self.batch_size = 500\n\n        elif model_name == \"gpt-5-mini\":\n            self.batch_size = 500\n\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            self.batch_size = 500\n            \n        else:\n            logging.warning(f\"Unknown model '{model_name}', using conservative defaults.\")\n            self.batch_size = 2\n\n    def generate_for_functions(self, function_inputs: List[FunctionAnalysisInput]) -> List[Optional[FunctionAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of functions.\"\"\"\n\n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 62\n\n        if not function_inputs:\n            return []\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(function_input.model_dump(), indent=2) \n            for function_input in function_inputs\n        ]\n       \n        conversations = [[SystemMessage(content=self.function_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_functions = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n        \n            try:\n                batch_results = self.function_llm.batch(batch_conversations, config={\"max_concurrency\": self.batch_size})\n                all_validated_functions.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                all_validated_functions.extend([None] * len(batch_conversations))\n            \n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_functions\n        \n            \n    \n\n    def generate_for_classes(self, class_inputs: List[ClassAnalysisInput]) -> List[Optional[ClassAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of classes.\"\"\"\n        if not class_inputs:\n            return []\n        \n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 62\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(class_input.model_dump(), indent=2)\n            for class_input in class_inputs\n        ]\n\n        conversations = [[SystemMessage(content=self.class_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_classes = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n\n            try:\n                batch_results = self.class_llm.batch(batch_conversations , config={\"max_concurrency\": self.batch_size})\n                all_validated_classes.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                # Falls ein Fehler auftritt, f√ºllen wir die Liste mit None auf, um die Reihenfolge zu wahren\n                all_validated_classes.extend([None] * len(batch_conversations))\n\n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_classes"
            start_line: 31
            end_line: 221
            context:
              dependencies[0]:
              instantiated_by[2]: backend.HelperLLM.main_orchestrator,backend.main.main_workflow
              method_context[4]:
                - identifier: backend.HelperLLM.LLMHelper.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[6]: self,api_key,function_prompt_path,class_prompt_path,model_name,base_url
                  docstring: null
                  start_line: 36
                  end_line: 101
                - identifier: backend.HelperLLM.LLMHelper._configure_batch_settings
                  name: _configure_batch_settings
                  calls[0]:
                  called_by[0]:
                  args[2]: self,model_name
                  docstring: null
                  start_line: 103
                  end_line: 137
                - identifier: backend.HelperLLM.LLMHelper.generate_for_functions
                  name: generate_for_functions
                  calls[0]:
                  called_by[1]: backend.main.main_workflow
                  args[2]: self,function_inputs
                  docstring: Generates and validates documentation for a batch of functions.
                  start_line: 139
                  end_line: 178
                - identifier: backend.HelperLLM.LLMHelper.generate_for_classes
                  name: generate_for_classes
                  calls[0]:
                  called_by[1]: backend.main.main_workflow
                  args[2]: self,class_inputs
                  docstring: Generates and validates documentation for a batch of classes.
                  start_line: 183
                  end_line: 221
    "backend/MainLLM.py":
      ast_nodes:
        imports[9]: os,logging,sys,dotenv.load_dotenv,langchain_google_genai.ChatGoogleGenerativeAI,langchain_ollama.ChatOllama,langchain_openai.ChatOpenAI,langchain.messages.HumanMessage,langchain.messages.SystemMessage
        functions[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.MainLLM.MainLLM
            name: MainLLM
            docstring: Hauptklasse f√ºr die Interaktion mit dem LLM.
            source_code: "class MainLLM:\n    \"\"\"\n    Hauptklasse f√ºr die Interaktion mit dem LLM.\n    \"\"\"\n    def __init__(self, api_key: str, prompt_file_path: str, model_name: str = \"gemini-2.5-pro\", base_url: str = None):\n        if not api_key:\n            raise ValueError(\"Gemini API Key must be set.\")\n        \n        try:\n            with open(prompt_file_path, 'r', encoding='utf-8') as f:\n                self.system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"System prompt file not found at: {prompt_file_path}\")\n            raise\n        \n        self.model_name = model_name\n\n        if model_name.startswith(\"gemini-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n\n        elif model_name.startswith(\"gpt-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            if not SCADSLLM_URL:\n                raise ValueError(f\"SCADSLLM_URL environment variable is required for model {model_name}\")\n            \n            logging.info(f\"Connecting to Custom API at {SCADSLLM_URL} for model {model_name}\")\n            \n            self.llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                base_url=SCADSLLM_URL,\n                temperature=1.0,\n            )\n\n        else:\n            target_url = base_url if base_url else OLLAMA_BASE_URL\n            self.llm = ChatOllama(\n                model=model_name,\n                temperature=1.0,\n                base_url=target_url,\n            )\n\n        logging.info(f\"Main LLM initialized with model '{model_name}'.\")\n    \n    def call_llm(self, user_input: str):\n\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with HelperLLM input...\")\n\n        try:\n            response = self.llm.invoke(messages)\n            logging.info(\"LLM call successful.\")\n            return response.content\n        except Exception as e:\n            logging.error(f\"Error during LLM call: {e}\")\n            return None\n\n    def stream_llm(self, user_input: str):\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with 'stream'...\")\n\n        try:\n            stream_iterator = self.llm.stream(messages)\n            \n            for chunk in stream_iterator:\n                yield chunk.content\n        except Exception as e:\n            error_message = f\"\\n--- Error during LLM stream call: {e} ---\"\n            logging.error(error_message)\n            yield error_message"
            start_line: 22
            end_line: 106
            context:
              dependencies[0]:
              instantiated_by[2]: backend.main.main_workflow,backend.main.notebook_workflow
              method_context[3]:
                - identifier: backend.MainLLM.MainLLM.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[5]: self,api_key,prompt_file_path,model_name,base_url
                  docstring: null
                  start_line: 26
                  end_line: 73
                - identifier: backend.MainLLM.MainLLM.call_llm
                  name: call_llm
                  calls[0]:
                  called_by[2]: backend.main.main_workflow,backend.main.notebook_workflow
                  args[2]: self,user_input
                  docstring: null
                  start_line: 75
                  end_line: 89
                - identifier: backend.MainLLM.MainLLM.stream_llm
                  name: stream_llm
                  calls[0]:
                  called_by[0]:
                  args[2]: self,user_input
                  docstring: null
                  start_line: 91
                  end_line: 106
    "backend/basic_info.py":
      ast_nodes:
        imports[7]: re,os,tomllib,typing.List,typing.Dict,typing.Any,typing.Optional
        functions[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.basic_info.ProjektInfoExtractor
            name: ProjektInfoExtractor
            docstring: "Extrahiert grundlegende Projektinformationen aus g√§ngigen Projektdateien\nwie README, pyproject.toml und requirements.txt."
            source_code: "class ProjektInfoExtractor:\n    \"\"\"\n    Extrahiert grundlegende Projektinformationen aus g√§ngigen Projektdateien\n    wie README, pyproject.toml und requirements.txt.\n    \"\"\"\n    def __init__(self):\n        self.INFO_NICHT_GEFUNDEN = \"Information not found\"\n        # Initialisiert die Struktur mit Platzhaltern\n        self.info = {\n            \"projekt_uebersicht\": {\n                \"titel\": self.INFO_NICHT_GEFUNDEN,\n                \"beschreibung\": self.INFO_NICHT_GEFUNDEN,\n                \"aktueller_status\": self.INFO_NICHT_GEFUNDEN,\n                \"key_features\": self.INFO_NICHT_GEFUNDEN,\n                \"tech_stack\": self.INFO_NICHT_GEFUNDEN,\n            },\n            \"installation\": {\n                \"dependencies\": self.INFO_NICHT_GEFUNDEN,\n                \"setup_anleitung\": self.INFO_NICHT_GEFUNDEN,\n                \"quick_start_guide\": self.INFO_NICHT_GEFUNDEN,\n            }\n        }\n\n    def _clean_content(self, content: str) -> str:\n        \"\"\"Entfernt Null-Bytes, die durch Encoding-Fehler (UTF-16 als UTF-8 gelesen) entstehen.\"\"\"\n        if not content:\n            return \"\"\n        return content.replace('\\x00', '')\n\n    def _finde_datei(self, patterns: List[str], dateien: List[Any]) -> Optional[Any]:\n        \"\"\"Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht.\"\"\"\n        for datei in dateien:\n            for pattern in patterns:\n                if datei.path.lower().endswith(pattern.lower()):\n                    return datei\n        return None\n\n    def _extrahiere_sektion_aus_markdown(self, inhalt: str, keywords: List[str]) -> Optional[str]:\n        \"\"\"\n        Extrahiert den Text unter einer Markdown-√úberschrift (##).\n        \"\"\"\n        # Erstellt ein Regex-Pattern, das auf jedes der Schl√ºsselw√∂rter reagiert\n        keyword_pattern = \"|\".join(re.escape(k) for k in keywords)\n        \n        # Sucht nach \"## Schl√ºsselwort\" und erfasst alles bis zur n√§chsten \"##\" oder dem Dateiende\n        pattern = re.compile(\n            rf\"##\\s*({keyword_pattern})\\s*\\n(.*?)(?=\\n##|\\Z)\",\n            re.IGNORECASE | re.DOTALL\n        )\n        match = pattern.search(inhalt)\n        if match:\n            return match.group(2).strip()\n        return None\n\n    def _parse_readme(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer README-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        if self.info[\"projekt_uebersicht\"][\"titel\"] == self.INFO_NICHT_GEFUNDEN:\n            title_match = re.search(r\"^\\s*#\\s*(.*)\", inhalt)\n            if title_match:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = title_match.group(1).strip()\n\n        # Beschreibung (Fallback)\n        if self.info[\"projekt_uebersicht\"][\"beschreibung\"] == self.INFO_NICHT_GEFUNDEN:\n            desc_match = re.search(r\"^\\s*#\\s*.*\\n+([^#\\n].*)\", inhalt, re.DOTALL)\n            if desc_match:\n                 self.info[\"projekt_uebersicht\"][\"beschreibung\"] = desc_match.group(1).strip().split('\\n\\n')[0]\n\n        # Key Features\n        features = self._extrahiere_sektion_aus_markdown(inhalt, [\"Features\", \"Key Features\", \"Merkmale\"])\n        if features:\n            self.info[\"projekt_uebersicht\"][\"key_features\"] = features\n            \n        # Tech Stack\n        tech_stack = self._extrahiere_sektion_aus_markdown(inhalt, [\"Tech Stack\", \"Technology\", \"Technologien\"])\n        if tech_stack:\n            self.info[\"projekt_uebersicht\"][\"tech_stack\"] = tech_stack\n            \n        # Status\n        status = self._extrahiere_sektion_aus_markdown(inhalt, [\"Status\", \"Current Status\"])\n        if status:\n            self.info[\"projekt_uebersicht\"][\"aktueller_status\"] = status\n\n        # Setup-Anleitung\n        setup = self._extrahiere_sektion_aus_markdown(inhalt, [\"Installation\", \"Setup\", \"Getting Started\"])\n        if setup:\n            self.info[\"installation\"][\"setup_anleitung\"] = setup\n            \n        # Quick Start\n        quick_start = self._extrahiere_sektion_aus_markdown(inhalt, [\"Quick Start\", \"Schnellstart\"])\n        if quick_start:\n            self.info[\"installation\"][\"quick_start_guide\"] = quick_start\n\n    def _parse_toml(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer pyproject.toml-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        if not tomllib:\n            print(\"Warnung: 'tomli' ist nicht installiert. pyproject.toml kann nicht analysiert werden.\")\n            return\n            \n        try:\n            data = tomllib.loads(inhalt)\n            project_data = data.get(\"project\", {})\n            \n            if \"name\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = project_data[\"name\"]\n            if \"description\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"beschreibung\"] = project_data[\"description\"]\n            if \"dependencies\" in project_data:\n                self.info[\"installation\"][\"dependencies\"] = project_data[\"dependencies\"]\n        except tomllib.TOMLDecodeError as e:\n            print(f\"Warnung: Fehler beim Parsen der pyproject.toml: {e}\")\n\n    def _parse_requirements(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer requirements.txt-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        # Nur f√ºllen, wenn noch keine Dependencies aus toml gefunden wurden\n        if self.info[\"installation\"][\"dependencies\"] == self.INFO_NICHT_GEFUNDEN:\n            lines = inhalt.splitlines()\n            dependencies = [\n                line.strip() for line in lines \n                if line.strip() and not line.strip().startswith('#')\n            ]\n            if dependencies:\n                self.info[\"installation\"][\"dependencies\"] = dependencies\n\n    def extrahiere_info(self, dateien: List[Any], repo_url: str) -> Dict[str, Any]:\n        \"\"\"\n        Orchestriert die Extraktion von Informationen.\n        \"\"\"\n        # Relevante Dateien finden\n        readme_datei = self._finde_datei([\"readme.md\", \"readme.rst\", \"readme.txt\", \"readme\"], dateien)\n        toml_datei = self._finde_datei([\"pyproject.toml\"], dateien)\n        req_datei = self._finde_datei([\"requirements.txt\"], dateien)\n\n        # Dateien parsen (mit Priorisierung)\n        if toml_datei:\n            self._parse_toml(toml_datei.content)\n\n        if req_datei:\n            self._parse_requirements(req_datei.content)\n            \n        if readme_datei:\n            self._parse_readme(readme_datei.content)\n            \n        # Finale Formatierung der Dependencies\n        deps = self.info[\"installation\"][\"dependencies\"]\n        if isinstance(deps, list):\n            if not deps:\n                self.info[\"installation\"][\"dependencies\"] = self.INFO_NICHT_GEFUNDEN\n            else:\n                self.info[\"installation\"][\"dependencies\"] = \"\\n\".join(f\"- {dep}\" for dep in deps)\n        \n        # Titel aus URL ableiten, falls nichts gefunden wurde (oder als Zusatzinfo)\n        if repo_url:\n            repo_name = os.path.basename(repo_url.removesuffix('.git'))\n            # Wenn noch kein Titel da ist oder er generisch war\n            if self.info[\"projekt_uebersicht\"][\"titel\"] == self.INFO_NICHT_GEFUNDEN:\n                 self.info[\"projekt_uebersicht\"][\"titel\"] = f\"{repo_name} documentation\"\n\n        return self.info"
            start_line: 9
            end_line: 172
            context:
              dependencies[0]:
              instantiated_by[2]: backend.main.main_workflow,backend.main.notebook_workflow
              method_context[8]:
                - identifier: backend.basic_info.ProjektInfoExtractor.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 14
                  end_line: 30
                - identifier: backend.basic_info.ProjektInfoExtractor._clean_content
                  name: _clean_content
                  calls[0]:
                  called_by[0]:
                  args[2]: self,content
                  docstring: "Entfernt Null-Bytes, die durch Encoding-Fehler (UTF-16 als UTF-8 gelesen) entstehen."
                  start_line: 32
                  end_line: 36
                - identifier: backend.basic_info.ProjektInfoExtractor._finde_datei
                  name: _finde_datei
                  calls[0]:
                  called_by[0]:
                  args[3]: self,patterns,dateien
                  docstring: "Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht."
                  start_line: 38
                  end_line: 44
                - identifier: backend.basic_info.ProjektInfoExtractor._extrahiere_sektion_aus_markdown
                  name: _extrahiere_sektion_aus_markdown
                  calls[0]:
                  called_by[0]:
                  args[3]: self,inhalt,keywords
                  docstring: Extrahiert den Text unter einer Markdown-√úberschrift (##).
                  start_line: 46
                  end_line: 61
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_readme
                  name: _parse_readme
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer README-Datei.
                  start_line: 63
                  end_line: 101
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_toml
                  name: _parse_toml
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer pyproject.toml-Datei.
                  start_line: 103
                  end_line: 122
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_requirements
                  name: _parse_requirements
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer requirements.txt-Datei.
                  start_line: 124
                  end_line: 136
                - identifier: backend.basic_info.ProjektInfoExtractor.extrahiere_info
                  name: extrahiere_info
                  calls[0]:
                  called_by[2]: backend.main.main_workflow,backend.main.notebook_workflow
                  args[3]: self,dateien,repo_url
                  docstring: Orchestriert die Extraktion von Informationen.
                  start_line: 138
                  end_line: 172
    "backend/callgraph.py":
      ast_nodes:
        imports[9]: ast,networkx,os,pathlib.Path,typing.Dict,getRepo.GitRepository,getRepo.GitRepository,basic_info.ProjektInfoExtractor,os
        functions[2]:
          - mode: function_analysis
            identifier: backend.callgraph.make_safe_dot
            name: make_safe_dot
            args[2]: graph,out_path
            docstring: null
            source_code: "def make_safe_dot(graph: nx.DiGraph, out_path: str):\n    mapping = {}\n    H = graph.copy()\n    for i, n in enumerate(list(graph.nodes())):\n        safe = f\"n{i}\"           \n        mapping[n] = safe\n    H = nx.relabel_nodes(H, mapping)\n    for orig, safe in mapping.items():\n        H.nodes[safe][\"label\"] = orig\n    nx.drawing.nx_pydot.write_dot(H, out_path)"
            start_line: 173
            end_line: 182
            context:
              calls[0]:
              called_by[1]: backend.callgraph
          - mode: function_analysis
            identifier: backend.callgraph.build_filtered_callgraph
            name: build_filtered_callgraph
            args[1]: repo
            docstring: Baut den globalen Call-Graphen und filtert ihn auf selbst geschriebene Funktionen.
            source_code: "def build_filtered_callgraph(repo: GitRepository) -> nx.DiGraph:\n    \"\"\"\n    Baut den globalen Call-Graphen und filtert ihn auf selbst geschriebene Funktionen.\n    \"\"\"\n    all_file_objects = repo.get_all_files()\n    own_functions = set()\n    file_trees = {}\n\n    for file in all_file_objects:\n        if not file.path.endswith(\".py\"):\n            continue\n        filename = Path(file.path).stem\n        tree = ast.parse(file.content)\n        file_trees[filename] = tree\n        visitor = CallGraph(filename)\n        visitor.visit(tree)\n        own_functions.update(visitor.function_set)\n\n    # Globalen Call-Graph bauen\n    global_graph = nx.DiGraph()\n    for filename, tree in file_trees.items():\n        visitor = CallGraph(filename)\n        visitor.visit(tree)\n        for caller, callees in visitor.edges.items():\n            if caller not in own_functions:\n                continue \n            for callee in callees:\n                if callee in own_functions:\n                    global_graph.add_edge(caller, callee)\n                    global_graph.add_node(caller)\n                    global_graph.add_node(callee)\n    return global_graph"
            start_line: 185
            end_line: 216
            context:
              calls[1]: backend.callgraph.CallGraph
              called_by[1]: backend.callgraph
        classes[1]:
          - mode: class_analysis
            identifier: backend.callgraph.CallGraph
            name: CallGraph
            docstring: null
            source_code: "class CallGraph(ast.NodeVisitor):\n    def __init__(self, filename: str):\n        self.filename = filename\n        self.current_function = None\n        self.current_class = None\n        \n        self.local_defs: dict[str, str] = {}\n        self.graph = nx.DiGraph()\n        self.import_mapping: dict[str, str] = {}\n        self.function_set: set[str] = set()\n        self.edges: Dict[str, set[str]] = {}\n\n    def _recursive_call(self, node):\n        \"\"\"\n        Liefert eine Liste der Namenskomponenten als dotted string z.B. ['pkg', 'mod', 'Class', 'method']\n        \"\"\"\n        if isinstance(node, ast.Call):\n            return self._recursive_call(node.func)\n        if isinstance(node, ast.Name):\n            return [node.id]\n        if isinstance(node, ast.Attribute):\n            parts = self._recursive_call(node.value)\n            parts.append(node.attr)\n            return parts\n        return []\n\n    def _resolve_all_callee_names(self, callee_nodes: list[list[str]]) -> list[str]:\n        \"\"\"\n        callee_nodes ist jetzt eine Liste von Listen (Name-Steps).\n        Wir pr√ºfen zuerst lokale Definitionen, dann import_mapping.\n        \"\"\"\n        resolved = []\n        for parts in callee_nodes:\n            if not parts:\n                continue\n            simple = parts[-1]\n            dotted = \".\".join(parts[-2:]) if len(parts) >= 2 else simple\n\n            if simple in self.local_defs:\n                resolved.append(self.local_defs[simple])\n                continue\n            if dotted in self.local_defs:\n                resolved.append(self.local_defs[dotted])\n                continue\n\n            first = parts[0]\n            if first in self.import_mapping:\n                mod = self.import_mapping[first]\n                rest = \"::\".join(parts[1:]) if len(parts) > 1 else simple\n                resolved.append(f\"{mod}::{rest}\")\n                continue\n\n            if self.current_class:\n                resolved.append(f\"{self.filename}::{parts[0]}::{simple}\" if len(parts)==1 else f\"{self.filename}::{'::'.join(parts)}\")\n            else:\n                resolved.append(f\"{self.filename}::{ '::'.join(parts)}\")\n        return resolved\n\n    def _make_full_name(self, basename: str, class_name: str | None = None) -> str:\n        if class_name:\n            return f\"{self.filename}::{class_name}::{basename}\"\n        return f\"{self.filename}::{basename}\"\n    \n    def _current_caller(self) -> str:\n        if self.current_function:\n            return self.current_function\n        return f\"<{self.filename}>\" if self.filename else \"<global-scope>\"\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            module_name = alias.name\n            module_asname = alias.asname if alias.asname else module_name\n            self.import_mapping[module_asname] = module_name\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        module_name = node.module.split(\".\")[-1] if node.module else \"\"\n        for alias in node.names:\n            self.import_mapping[alias.asname or alias.name] = f\"{module_name}\" if module_name else alias.name\n\n    def visit_ClassDef(self, node: ast.ClassDef):\n        prev_class = self.current_class\n        self.current_class = node.name\n        self.generic_visit(node)\n        self.current_class = prev_class\n        \n    def visit_FunctionDef(self, node):\n        prev_function = self.current_function\n        full_name = self._make_full_name(node.name, self.current_class)\n        self.local_defs[node.name] = full_name\n        if self.current_class:\n            self.local_defs[f\"{self.current_class}.{node.name}\"] = full_name\n\n        self.current_function = full_name\n        self.graph.add_node(self.current_function)\n        self.generic_visit(node)\n        self.function_set.add(self.current_function)\n        self.current_function = prev_function\n\n    def visit_AsyncFunctionDef(self, node):\n        self.visit_FunctionDef(node)\n\n    def visit_Call(self, node):\n        caller = self._current_caller()\n        parts = self._recursive_call(node)\n        resolved_callees = self._resolve_all_callee_names([parts])\n\n        if caller not in self.edges:\n            self.edges[caller] = set()\n\n        for callee in resolved_callees:\n            if callee:\n                self.edges[caller].add(callee)\n        self.generic_visit(node)\n\n    def visit_If(self, node):\n        if (isinstance(node.test, ast.Compare) and\n            isinstance(node.test.left, ast.Name) and \n            node.test.left.id == \"__name__\"):\n            caller_backup = self.current_function\n            self.current_function = \"<main_block>\"\n            self.generic_visit(node)\n            self.current_function = caller_backup\n        else:\n            self.generic_visit(node)"
            start_line: 10
            end_line: 134
            context:
              dependencies[0]:
              instantiated_by[1]: backend.callgraph.build_filtered_callgraph
              method_context[12]:
                - identifier: backend.callgraph.CallGraph.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,filename
                  docstring: null
                  start_line: 11
                  end_line: 20
                - identifier: backend.callgraph.CallGraph._recursive_call
                  name: _recursive_call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: "Liefert eine Liste der Namenskomponenten als dotted string z.B. ['pkg', 'mod', 'Class', 'method']"
                  start_line: 22
                  end_line: 34
                - identifier: backend.callgraph.CallGraph._resolve_all_callee_names
                  name: _resolve_all_callee_names
                  calls[0]:
                  called_by[0]:
                  args[2]: self,callee_nodes
                  docstring: "callee_nodes ist jetzt eine Liste von Listen (Name-Steps).\nWir pr√ºfen zuerst lokale Definitionen, dann import_mapping."
                  start_line: 36
                  end_line: 66
                - identifier: backend.callgraph.CallGraph._make_full_name
                  name: _make_full_name
                  calls[0]:
                  called_by[0]:
                  args[3]: self,basename,class_name
                  docstring: null
                  start_line: 68
                  end_line: 71
                - identifier: backend.callgraph.CallGraph._current_caller
                  name: _current_caller
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 73
                  end_line: 76
                - identifier: backend.callgraph.CallGraph.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 78
                  end_line: 83
                - identifier: backend.callgraph.CallGraph.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 85
                  end_line: 88
                - identifier: backend.callgraph.CallGraph.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 90
                  end_line: 94
                - identifier: backend.callgraph.CallGraph.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 96
                  end_line: 107
                - identifier: backend.callgraph.CallGraph.visit_AsyncFunctionDef
                  name: visit_AsyncFunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 109
                  end_line: 110
                - identifier: backend.callgraph.CallGraph.visit_Call
                  name: visit_Call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 112
                  end_line: 123
                - identifier: backend.callgraph.CallGraph.visit_If
                  name: visit_If
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 125
                  end_line: 134
    "backend/converter.py":
      ast_nodes:
        imports[4]: logging,nbformat,base64,nbformat.reader.NotJSONError
        functions[5]:
          - mode: function_analysis
            identifier: backend.converter.wrap_cdata
            name: wrap_cdata
            args[1]: content
            docstring: Wraps content in CDATA tags.
            source_code: "def wrap_cdata(content):\n    \"\"\"Wraps content in CDATA tags.\"\"\"\n    return f\"<![CDATA[\\n{content}\\n]]>\""
            start_line: 8
            end_line: 10
            context:
              calls[0]:
              called_by[1]: backend.converter.convert_notebook_to_xml
          - mode: function_analysis
            identifier: backend.converter.extract_output_content
            name: extract_output_content
            args[2]: outputs,image_list
            docstring: "Extracts text and handles images by decoding Base64 to bytes.\nReturns: A list of text strings or placeholders."
            source_code: "def extract_output_content(outputs, image_list):\n    \"\"\"\n    Extracts text and handles images by decoding Base64 to bytes.\n    Returns: A list of text strings or placeholders.\n    \"\"\"\n    extracted_xml_snippets = []\n    \n    for output in outputs:\n\n        if output.output_type in ('display_data', 'execute_result') and hasattr(output, 'data'):\n            data = output.data\n            \n            # Helper to process image\n            def process_image(mime_type):\n                if mime_type in data:\n                    try:\n                        b64_str = data[mime_type]\n                        b64_str = b64_str.replace('\\n', '')\n\n                        image_index = len(image_list)\n                        image_list.append({\n                            \"mime_type\": mime_type,\n                            \"data\": b64_str\n                        })\n\n                        return f'\\n<IMAGE_PLACEHOLDER index=\"{image_index}\" mime=\"{mime_type}\"/>\\n'\n                    except Exception as e:\n                        return f\"<ERROR>Could not decode image: {e}</ERROR>\"\n                return None\n\n            # Ignore jpeg if png is found\n            img_xml = process_image('image/png')\n            if not img_xml:\n                img_xml = process_image('image/jpeg')\n            \n            if img_xml:\n                extracted_xml_snippets.append(img_xml)\n            elif 'text/plain' in data:\n                extracted_xml_snippets.append(data['text/plain'])\n\n        elif output.output_type == 'stream':\n            extracted_xml_snippets.append(output.text)\n            \n        elif output.output_type == 'error':\n            extracted_xml_snippets.append(f\"{output.ename}: {output.evalue}\")\n\n    return extracted_xml_snippets"
            start_line: 12
            end_line: 58
            context:
              calls[1]: backend.converter.process_image
              called_by[1]: backend.converter.convert_notebook_to_xml
          - mode: function_analysis
            identifier: backend.converter.process_image
            name: process_image
            args[1]: mime_type
            docstring: null
            source_code: "def process_image(mime_type):\n                if mime_type in data:\n                    try:\n                        b64_str = data[mime_type]\n                        b64_str = b64_str.replace('\\n', '')\n\n                        image_index = len(image_list)\n                        image_list.append({\n                            \"mime_type\": mime_type,\n                            \"data\": b64_str\n                        })\n\n                        return f'\\n<IMAGE_PLACEHOLDER index=\"{image_index}\" mime=\"{mime_type}\"/>\\n'\n                    except Exception as e:\n                        return f\"<ERROR>Could not decode image: {e}</ERROR>\"\n                return None"
            start_line: 25
            end_line: 40
            context:
              calls[0]:
              called_by[1]: backend.converter.extract_output_content
          - mode: function_analysis
            identifier: backend.converter.convert_notebook_to_xml
            name: convert_notebook_to_xml
            args[1]: file_content
            docstring: null
            source_code: "def convert_notebook_to_xml(file_content):\n    try:\n        nb = nbformat.reads(file_content, as_version=4)\n    except NotJSONError:\n        return \"<ERROR>Could not parse file as JSON/Notebook</ERROR>\", []\n\n    xml_parts = []\n    extracted_images = [] \n\n    for cell in nb.cells:\n        if cell.cell_type == 'markdown':\n            cell_xml = f'<CELL type=\"markdown\">\\n{cell.source}\\n</CELL>'\n            xml_parts.append(cell_xml)\n\n        elif cell.cell_type == 'code':\n            source_code = wrap_cdata(cell.source)\n            xml_parts.append(f'<CELL type=\"code\">\\n{source_code}\\n</CELL>')\n\n            if hasattr(cell, 'outputs') and cell.outputs:\n                snippets = extract_output_content(cell.outputs, extracted_images)\n                \n                content = \"\\n\".join(snippets)\n                \n                if content.strip():\n                    wrapped_output = wrap_cdata(content)\n                    xml_parts.append(f'<CELL type=\"output\">\\n{wrapped_output}\\n</CELL>')\n\n    return \"\\n\\n\".join(xml_parts), extracted_images"
            start_line: 60
            end_line: 87
            context:
              calls[2]: backend.converter.extract_output_content,backend.converter.wrap_cdata
              called_by[1]: backend.converter.process_repo_notebooks
          - mode: function_analysis
            identifier: backend.converter.process_repo_notebooks
            name: process_repo_notebooks
            args[1]: repo_files
            docstring: null
            source_code: "def process_repo_notebooks(repo_files):\n    notebook_files = [f for f in repo_files if f.path.endswith('.ipynb')]\n    logging.info(f\"Found {len(notebook_files)} notebooks.\")\n        \n    results = {}\n\n    for nb_file in notebook_files:\n        logging.info(f\"Processing: {nb_file.path}\")\n\n        xml_output, images = convert_notebook_to_xml(nb_file.content)\n        results[nb_file.path] = {\n            \"xml\": xml_output,\n            \"images\": images\n        }\n            \n    return results"
            start_line: 90
            end_line: 105
            context:
              calls[1]: backend.converter.convert_notebook_to_xml
              called_by[1]: backend.main.notebook_workflow
        classes[0]:
    "backend/getRepo.py":
      ast_nodes:
        imports[5]: tempfile,git.Repo,git.GitCommandError,logging,os
        functions[0]:
        classes[2]:
          - mode: class_analysis
            identifier: backend.getRepo.RepoFile
            name: RepoFile
            docstring: "Repr√§sentiert eine einzelne Datei in einem Git-Repository.\n\nDer Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tats√§chlichem Zugriff."
            source_code: "class RepoFile:\n    \"\"\"\n    Repr√§sentiert eine einzelne Datei in einem Git-Repository.\n    \n    Der Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tats√§chlichem Zugriff.\n    \"\"\"\n    def __init__(self, file_path, commit_tree):\n        \"\"\"\n        Initialisiert das RepoFile-Objekt.\n\n        Args:\n            file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n            commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt.\n        \"\"\"\n        self.path = file_path\n        self._tree = commit_tree\n        \n\n        self._blob = None\n        self._content = None\n        self._size = None\n\n    @property\n    def blob(self):\n        \"\"\"Lazy-l√§dt das Git-Blob-Objekt.\"\"\"\n        if self._blob is None:\n            try:\n                self._blob = self._tree[self.path]\n            except KeyError:\n                raise FileNotFoundError(f\"Datei '{self.path}' konnte im Commit-Tree nicht gefunden werden.\")\n        return self._blob\n\n    @property\n    def content(self):\n        \"\"\"Lazy-l√§dt und gibt den dekodierten Inhalt der Datei zur√ºck.\"\"\"\n        if self._content is None:\n            self._content = self.blob.data_stream.read().decode('utf-8', errors='ignore')\n        return self._content\n\n    @property\n    def size(self):\n        \"\"\"Lazy-l√§dt und gibt die Gr√∂√üe der Datei in Bytes zur√ºck.\"\"\"\n        if self._size is None:\n            self._size = self.blob.size\n        return self._size\n        \n    def analyze_word_count(self):\n        \"\"\"\n        Eine Beispiel-Analyse-Methode. Z√§hlt die W√∂rter im Dateiinhalt.\n        \"\"\"\n        return len(self.content.split())\n\n    def __repr__(self):\n        \"\"\"Gibt eine n√ºtzliche String-Repr√§sentation des Objekts zur√ºck.\"\"\"\n        return f\"<RepoFile(path='{self.path}')>\"\n    \n    def to_dict(self, include_content=False):\n        data = {\n            \"path\": self.path,\n            \"name\": os.path.basename(self.path),\n            \"size\": self.size,\n            \"type\": \"file\"\n        }\n        if include_content:\n            data[\"content\"] = self.content\n        return data"
            start_line: 7
            end_line: 72
            context:
              dependencies[0]:
              instantiated_by[1]: backend.getRepo.GitRepository.get_all_files
              method_context[7]:
                - identifier: backend.getRepo.RepoFile.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[3]: self,file_path,commit_tree
                  docstring: "Initialisiert das RepoFile-Objekt.\n\nArgs:\n    file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n    commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt."
                  start_line: 13
                  end_line: 27
                - identifier: backend.getRepo.RepoFile.blob
                  name: blob
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt das Git-Blob-Objekt.
                  start_line: 30
                  end_line: 37
                - identifier: backend.getRepo.RepoFile.content
                  name: content
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt und gibt den dekodierten Inhalt der Datei zur√ºck.
                  start_line: 40
                  end_line: 44
                - identifier: backend.getRepo.RepoFile.size
                  name: size
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt und gibt die Gr√∂√üe der Datei in Bytes zur√ºck.
                  start_line: 47
                  end_line: 51
                - identifier: backend.getRepo.RepoFile.analyze_word_count
                  name: analyze_word_count
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Eine Beispiel-Analyse-Methode. Z√§hlt die W√∂rter im Dateiinhalt.
                  start_line: 53
                  end_line: 57
                - identifier: backend.getRepo.RepoFile.__repr__
                  name: __repr__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Gibt eine n√ºtzliche String-Repr√§sentation des Objekts zur√ºck.
                  start_line: 59
                  end_line: 61
                - identifier: backend.getRepo.RepoFile.to_dict
                  name: to_dict
                  calls[0]:
                  called_by[0]:
                  args[2]: self,include_content
                  docstring: null
                  start_line: 63
                  end_line: 72
          - mode: class_analysis
            identifier: backend.getRepo.GitRepository
            name: GitRepository
            docstring: "Verwaltet ein Git-Repository, einschlie√ülich Klonen in ein tempor√§res\nVerzeichnis und Bereitstellung von RepoFile-Objekten."
            source_code: "class GitRepository:\n    \"\"\"\n    Verwaltet ein Git-Repository, einschlie√ülich Klonen in ein tempor√§res\n    Verzeichnis und Bereitstellung von RepoFile-Objekten.\n    \"\"\"\n    def __init__(self, repo_url):\n        self.repo_url = repo_url\n        self.temp_dir = tempfile.mkdtemp()\n        self.repo = None\n\n        self.files = []\n        \n        try:\n            logging.info(f\"Clone Repository {self.repo_url}...\")\n            self.repo = Repo.clone_from(self.repo_url, self.temp_dir)\n            self.latest_commit = self.repo.head.commit\n            self.commit_tree = self.latest_commit.tree\n            logging.info(\"Cloning successful.\")\n        except GitCommandError as e:\n            self.close()\n            raise RuntimeError(f\"Error cloning repository: {e}\") from e\n\n    def get_all_files(self):\n        \"\"\"\n        Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zur√ºck.\n\n        Returns:\n            list[RepoFile]: Eine Liste von RepoFile-Instanzen.\n        \"\"\"\n        file_paths = self.repo.git.ls_files().split('\\n')\n        self.files = [RepoFile(path, self.commit_tree) for path in file_paths if path]\n        return self.files\n\n    def close(self):\n        \"\"\"L√∂scht das tempor√§re Verzeichnis und dessen Inhalt.\"\"\"\n        if self.temp_dir:\n            print(f\"\\nL√∂sche tempor√§res Verzeichnis: {self.temp_dir}\")\n            self.temp_dir = None\n            \n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def get_file_tree(self, include_content=False):\n\n        if not self.files:\n            self.get_all_files()\n\n        tree = {\"name\": \"root\", \"type\": \"directory\", \"children\": []}\n\n        for file_obj in self.files:\n            parts = file_obj.path.split('/')\n            current_level = tree[\"children\"]\n            \n            # Iteriere durch die Ordnerstruktur\n            for part in parts[:-1]:\n                # Suche, ob Ordner schon existiert\n                found = next((item for item in current_level if item[\"name\"] == part and item[\"type\"] == \"directory\"), None)\n                if not found:\n                    new_dir = {\"name\": part, \"type\": \"directory\", \"children\": []}\n                    current_level.append(new_dir)\n                    current_level = new_dir[\"children\"]\n                else:\n                    current_level = found[\"children\"]\n            \n            # Datei am Ende hinzuf√ºgen\n            current_level.append(file_obj.to_dict(include_content=include_content))\n\n        return tree"
            start_line: 78
            end_line: 148
            context:
              dependencies[1]: backend.getRepo.RepoFile
              instantiated_by[2]: backend.main.main_workflow,backend.main.notebook_workflow
              method_context[6]:
                - identifier: backend.getRepo.GitRepository.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,repo_url
                  docstring: null
                  start_line: 83
                  end_line: 98
                - identifier: backend.getRepo.GitRepository.get_all_files
                  name: get_all_files
                  calls[1]: backend.getRepo.RepoFile
                  called_by[0]:
                  args[1]: self
                  docstring: "Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zur√ºck.\n\nReturns:\n    list[RepoFile]: Eine Liste von RepoFile-Instanzen."
                  start_line: 100
                  end_line: 109
                - identifier: backend.getRepo.GitRepository.close
                  name: close
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: L√∂scht das tempor√§re Verzeichnis und dessen Inhalt.
                  start_line: 111
                  end_line: 115
                - identifier: backend.getRepo.GitRepository.__enter__
                  name: __enter__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 117
                  end_line: 118
                - identifier: backend.getRepo.GitRepository.__exit__
                  name: __exit__
                  calls[0]:
                  called_by[0]:
                  args[4]: self,exc_type,exc_val,exc_tb
                  docstring: null
                  start_line: 120
                  end_line: 121
                - identifier: backend.getRepo.GitRepository.get_file_tree
                  name: get_file_tree
                  calls[0]:
                  called_by[0]:
                  args[2]: self,include_content
                  docstring: null
                  start_line: 123
                  end_line: 148
    "backend/main.py":
      ast_nodes:
        imports[28]: json,math,logging,os,re,time,math,datetime.datetime,matplotlib.pyplot,datetime.datetime,pathlib.Path,dotenv.load_dotenv,getRepo.GitRepository,AST_Schema.ASTAnalyzer,MainLLM.MainLLM,basic_info.ProjektInfoExtractor,HelperLLM.LLMHelper,relationship_analyzer.ProjectAnalyzer,schemas.types.FunctionContextInput,schemas.types.FunctionAnalysisInput,schemas.types.ClassContextInput,schemas.types.ClassAnalysisInput,schemas.types.MethodContextInput,toon_format.encode,toon_format.count_tokens,toon_format.estimate_savings,toon_format.compare_formats,converter.process_repo_notebooks
        functions[7]:
          - mode: function_analysis
            identifier: backend.main.create_savings_chart
            name: create_savings_chart
            args[4]: json_tokens,toon_tokens,savings_percent,output_path
            docstring: Erstellt ein Balkendiagramm f√ºr den Token-Vergleich und speichert es.
            source_code: "def create_savings_chart(json_tokens, toon_tokens, savings_percent, output_path):\n    \"\"\"Erstellt ein Balkendiagramm f√ºr den Token-Vergleich und speichert es.\"\"\"\n    labels = ['JSON', 'TOON']\n    values = [json_tokens, toon_tokens]\n    colors = ['#ff9999', '#66b3ff']\n\n    plt.figure(figsize=(8, 6))\n    bars = plt.bar(labels, values, color=colors, width=0.5)\n\n    # Titel und Beschriftungen\n    plt.title(f'Token-Vergleich: {savings_percent:.2f}% Einsparung', fontsize=14)\n    plt.ylabel('Anzahl Token')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n\n    # Werte √ºber den Balken anzeigen\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width() / 2.0, height, \n                 f'{int(height):,}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n\n    # Speichern\n    plt.savefig(output_path)\n    plt.close()"
            start_line: 33
            end_line: 55
            context:
              calls[0]:
              called_by[1]: backend.main.main_workflow
          - mode: function_analysis
            identifier: backend.main.calculate_net_time
            name: calculate_net_time
            args[5]: start_time,end_time,total_items,batch_size,model_name
            docstring: Berechnet die Dauer abz√ºglich der Sleep-Zeiten f√ºr Rate-Limits.
            source_code: "def calculate_net_time(start_time, end_time, total_items, batch_size, model_name):\n    \"\"\"Berechnet die Dauer abz√ºglich der Sleep-Zeiten f√ºr Rate-Limits.\"\"\"\n    total_duration = end_time - start_time\n    \n    if not model_name.startswith(\"gemini-\"):\n        return total_duration\n\n    if total_items == 0:\n        return 0\n\n    num_batches = math.ceil(total_items / batch_size)\n    sleep_count = max(0, num_batches - 1)\n    total_sleep_time = sleep_count * 61\n    \n    net_time = total_duration - total_sleep_time\n    return max(0, net_time)"
            start_line: 57
            end_line: 72
            context:
              calls[0]:
              called_by[1]: backend.main.main_workflow
          - mode: function_analysis
            identifier: backend.main.main_workflow
            name: main_workflow
            args[4]: input,api_keys,model_names,status_callback
            docstring: null
            source_code: "def main_workflow(input, api_keys: dict, model_names: dict, status_callback=None):\n    \n    def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)\n\n    update_status(\"üîç Analysiere Input...\")\n    \n    user_input = input\n    \n    # API Key & Ollama Base URL aus Frontend holen\n    gemini_api_key = api_keys.get(\"gemini\")\n    openai_api_key = api_keys.get(\"gpt\")\n    scadsllm_api_key = api_keys.get(\"scadsllm\")\n    scadsllm_base_url = api_keys.get(\"scadsllm_base_url\")\n    ollama_base_url = api_keys.get(\"ollama\")\n    base_url = None\n\n    if model_names[\"helper\"].startswith(\"gpt-\"):\n        helper_api_key = openai_api_key\n    elif model_names[\"helper\"].startswith(\"gemini-\"):\n        helper_api_key = gemini_api_key\n    elif \"/\" in model_names[\"helper\"] or model_names[\"helper\"].startswith(\"alias-\") or any(x in model_names[\"helper\"] for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        helper_api_key = scadsllm_api_key\n        base_url = scadsllm_base_url\n    else:\n        helper_api_key = None\n        base_url = ollama_base_url\n    if model_names[\"main\"].startswith(\"gpt-\"):\n        main_api_key = openai_api_key\n    elif model_names[\"main\"].startswith(\"gemini-\"):\n        main_api_key = gemini_api_key\n    elif \"/\" in model_names[\"main\"] or model_names[\"main\"].startswith(\"alias-\") or any(x in model_names[\"main\"] for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        main_api_key = scadsllm_api_key\n        base_url = scadsllm_base_url\n    else:\n        main_api_key = None\n        base_url = ollama_base_url\n\n    # Standardeinstellungen f√ºr Modelle\n    helper_model = model_names.get(\"helper\", \"gpt-5-mini\")\n    main_model = model_names.get(\"main\", \"gpt-5.1\")\n\n    # Error Handling f√ºr fehlende API Keys\n    if not gemini_api_key and \"gemini\" in helper_model:\n        raise ValueError(\"Gemini API Key was not provided in api_keys dictionary.\")\n\n    # URL Extraktion\n    repo_url = None\n    url_pattern = r\"https?://(?:www\\.)?github\\.com/[^\\s]+\"\n    match = re.search(url_pattern, user_input)\n\n    if match:\n        repo_url = match.group(0)\n        logging.info(f\"Extracted repository URL: {repo_url}\")\n    else:\n        raise ValueError(\"Could not find a valid URL in the provided input.\")\n    \n    # Repo klonen und Dateien extrahieren\n    update_status(f\"‚¨áÔ∏è Klone Repository: {repo_url} ...\")\n    \n    repo_files = []\n    local_repo_path = \"\" \n\n    try: \n\n        with GitRepository(repo_url) as repo:\n            repo_files = repo.get_all_files()\n            local_repo_path = repo.temp_dir\n\n            logging.info(f\"Total files retrieved: {len(repo_files)}\")\n\n    except Exception as e:\n        logging.error(f\"Error cloning repository: {e}\")\n        raise \n\n\n    # Extrahiere Basic Infos\n    update_status(\"‚ÑπÔ∏è Extrahiere Basis-Informationen...\")\n    try:\n        info_extractor = ProjektInfoExtractor()\n        basic_project_info = info_extractor.extrahiere_info(dateien=repo_files, repo_url=repo_url)\n        logging.info(\"Basic project info extracted\")\n    except Exception as e:\n        logging.error(f\"Error extracting basic project info: {e}\")\n        basic_project_info = \"Could not extract basic info.\"\n\n    # Erstelle Repository Dateibaum\n    update_status(\"üå≤ Erstelle Repository Dateibaum...\")\n    try:\n        repo_file_tree = repo.get_file_tree()\n        logging.info(\"Repository file tree constructed\")\n    except Exception as e:\n        logging.error(f\"Error constructing repository file tree: {e}\")\n        repo_file_tree = \"Could not create file tree.\"\n\n    # Relationship Analyse durchf√ºhren\n    update_status(\"üîó Analysiere Beziehungen (Calls & Instanziierungen)...\")\n    try:\n        rel_analyzer = ProjectAnalyzer(project_root=local_repo_path)\n        rel_analyzer.analyze()\n        \n        raw_relationships = rel_analyzer.get_raw_relationships()\n        \n        logging.info(f\"Relationships analyzed.\")\n    except Exception as e:\n        logging.error(f\"Error in relationship analyzer: {e}\")\n        raw_relationships = {\"outgoing\": {}, \"incoming\": {}}\n\n    # Erstelle AST Schema\n    update_status(\"üå≥ Erstelle Abstract Syntax Tree (AST)...\")\n    try:        \n        ast_analyzer = ASTAnalyzer()   \n        ast_schema = ast_analyzer.analyze_repository(files=repo_files, repo=repo)\n        logging.info(\"AST schema created\")\n    except Exception as e:\n        logging.error(f\"Error retrieving repository files: {e}\")\n        raise\n\n    # Anreichern des AST Schemas mit Relationship Daten\n    update_status(\"‚ûï Reiche AST mit Beziehungsdaten an...\")            \n    try:   \n        ast_schema = ast_analyzer.merge_relationship_data(ast_schema, raw_relationships)\n        logging.info(\"AST schema created and enriched\")\n\n    except Exception as e:\n        logging.error(f\"Error processing repository: {e}\")\n        raise\n\n    # Vorbereitung der HelperLLM Eingaben\n    update_status(\"‚öôÔ∏è Bereite Daten f√ºr Helper LLM vor...\")\n    \n    helper_llm_function_input = []\n    helper_llm_class_input = []\n\n    try:\n        for filename, file_data in ast_schema['files'].items():\n            ast_nodes = file_data.get('ast_nodes', {})\n            imports = ast_nodes.get('imports', [])\n            functions = ast_nodes.get('functions', [])\n            classes = ast_nodes.get('classes', [])\n\n            for function in functions:\n                context = function.get('context', {})\n                \n                raw_called_by = context.get('called_by', [])\n                clean_called_by = [cb for cb in raw_called_by if isinstance(cb, dict)]\n\n                function_context = FunctionContextInput(\n                    calls = context.get('calls', []),\n                    called_by = clean_called_by \n                )\n                \n                function_input = FunctionAnalysisInput(\n                    mode = function.get('mode', 'function_analysis'),\n                    identifier = function.get('identifier'),\n                    source_code = function.get('source_code'),\n                    imports = imports,\n                    context = function_context\n                )\n                \n                helper_llm_function_input.append(function_input)\n\n            for _class in classes:\n                context = _class.get('context', {})\n                \n                method_context_inputs = []\n                for method in context.get('method_context', []):\n                    \n                    raw_method_called_by = method.get('called_by', [])\n                    clean_method_called_by = [cb for cb in raw_method_called_by if isinstance(cb, dict)]\n\n                    method_context_inputs.append(\n                        MethodContextInput(\n                            identifier=method.get('identifier'),\n                            calls=method.get('calls', []),\n                            called_by=clean_method_called_by, \n                            args=method.get('args', []),\n                            docstring=method.get('docstring')\n                        )\n                    )\n\n                raw_instantiated_by = context.get('instantiated_by', [])\n                clean_instantiated_by = [ib for ib in raw_instantiated_by if isinstance(ib, dict)]\n\n                class_context = ClassContextInput(\n                    dependencies = context.get('dependencies', []),\n                    instantiated_by = clean_instantiated_by, \n                    method_context = method_context_inputs\n                )\n\n                class_input = ClassAnalysisInput(\n                    mode = _class.get('mode', 'class_analysis'),\n                    identifier =_class.get('identifier'),\n                    source_code = _class.get('source_code'), \n                    imports = imports, \n                    context = class_context\n                )\n                \n                helper_llm_class_input.append(class_input)\n\n    except Exception as e:\n        logging.error(f\"Error preparing inputs for Helper LLM: {e}\")\n        raise\n    \n    # Initialisiere HelperLLM\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    \n    llm_helper = LLMHelper(\n        api_key=helper_api_key, \n        function_prompt_path=function_prompt_file, \n        class_prompt_path=class_prompt_file,\n        model_name=helper_model,\n        base_url=base_url,\n    )\n    \n    if ollama_base_url:\n        os.environ[\"OLLAMA_BASE_URL\"] = ollama_base_url\n\n    # Initialisiere Ergebniscontainer\n    analysis_results = {}\n    function_analysis_results = []\n    class_analysis_results = []\n\n    # Call HelperLLM f√ºr Funktionen\n    update_status(f\"ü§ñ Helper LLM: Analysiere {len(helper_llm_function_input)} Funktionen ({helper_model})...\")\n\n    try:\n        net_time_func = 0\n        if len(helper_llm_function_input) > 0:\n\n            logging.info(\"\\n--- Generating documentation for Functions ---\")\n            t_start_func = time.time()    \n            function_analysis_results = llm_helper.generate_for_functions(helper_llm_function_input)    \n            t_end_func = time.time()\n            net_time_func = calculate_net_time(t_start_func, t_end_func, len(helper_llm_function_input), llm_helper.batch_size, helper_model)\n\n        if len(function_analysis_results) != 0:\n            for doc in function_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"functions\" not in analysis_results:\n                        analysis_results[\"functions\"] = {}\n                    analysis_results[\"functions\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(f\"Failed to generate doc for a function\") \n    except Exception as e:\n        logging.error(f\"Error during Helper LLM function analysis: {e}\")\n        raise\n    \n\n    # Call HelperLLM f√ºr Klassen\n    try:\n        net_time_class = 0\n        if len(helper_llm_class_input) > 0:\n            # Rate Limit Sleep f√ºr Gemini Modelle\n            if llm_helper.model_name.startswith(\"gemini-\") & (len(helper_llm_function_input) > 0):\n                update_status(\"üí§ Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n                time.sleep(65)\n            \n            update_status(f\"ü§ñ Helper LLM: Analysiere {len(helper_llm_class_input)} Klassen ({helper_model})...\")\n            \n            logging.info(\"\\n--- Generating documentation for Classes ---\")\n            t_start_class = time.time()\n            class_analysis_results = llm_helper.generate_for_classes(helper_llm_class_input)\n            t_end_class = time.time()\n            net_time_class = calculate_net_time(t_start_class, t_end_class, len(helper_llm_class_input), llm_helper.batch_size, helper_model)\n\n        if len(class_analysis_results) != 0:\n            for doc in class_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"classes\" not in analysis_results:\n                        analysis_results[\"classes\"] = {}\n                    analysis_results[\"classes\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(f\"Failed to generate doc for a class\")\n    except Exception as e:\n        logging.error(f\"Error during Helper LLM class analysis: {e}\")\n        raise\n\n    total_helper_time = net_time_func + net_time_class\n\n    # MainLLM Input Vorbereitung\n    main_llm_input = {\n        \"basic_info\": basic_project_info,\n        \"file_tree\": repo_file_tree,\n        \"ast_schema\": ast_schema,\n        \"analysis_results\": analysis_results\n    }\n\n    # Speichern als JSON (Optional)\n    main_llm_input_json = json.dumps(main_llm_input, indent=2)\n    # with open(\"output.json\", \"w\", encoding=\"utf-8\") as f:\n    #     f.write(main_llm_input_json)\n    #     logging.info(\"JSON-Datei wurde gespeichert.\")\n\n    # Konvertiere Input in Toon Format\n    main_llm_input_toon = encode(main_llm_input)\n\n    #Speichern in TOON Format (Optional)\n    # with open(\"output.toon\", \"w\", encoding=\"utf-8\") as f:\n    #     f.write(main_llm_input_toon)\n    #     logging.info(\"output.toon erfolgreich gespeichert.\")\n    \n    # Token Evaluation\n    savings_data = None\n    try:\n        logging.info(\"--- Evaluierung der Token-Ersparnis ---\")\n        savings_data = estimate_savings(main_llm_input)\n        logging.info(f\"JSON Tokens: {savings_data['json_tokens']}\")\n        logging.info(f\"TOON Tokens: {savings_data['toon_tokens']}\")\n        logging.info(f\"Ersparnis:   {savings_data['savings_percent']:.2f}%\")\n        \n    except Exception as e:\n        logging.warning(f\"Token evaluation could not be performed: {e}\")    \n\n\n\n    prompt_file_mainllm = \"SystemPrompts/SystemPromptMainLLM.txt\"\n    prompt_file_mainllm_toon = \"SystemPrompts/SystemPromptMainLLMToon.txt\"\n    # MainLLM Ausf√ºhrung\n    main_llm = MainLLM(\n        api_key=main_api_key, \n        prompt_file_path=prompt_file_mainllm_toon,\n        model_name=main_model,\n        base_url=base_url,\n    )\n\n\n    # RPM Limit Sleep f√ºr Gemini Modelle\n    if llm_helper.model_name == main_llm.model_name and main_llm.model_name.startswith(\"gemini-\"):\n        time.sleep(65)\n        update_status(\"üí§ Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n\n    # Call MainLLM f√ºr finalen Report\n    update_status(f\"üß† Main LLM: Generiere finalen Report ({main_model})...\")\n    try:\n        total_main_time = 0\n        logging.info(\"\\n--- Generating Final Report ---\")\n        t_start_main = time.time()\n        final_report = main_llm.call_llm(main_llm_input_toon)\n        t_end_main = time.time()\n        total_main_time = t_end_main - t_start_main\n    except Exception as e:\n        logging.error(f\"Error during Main LLM final report generation: {e}\")\n        raise\n\n    \n    # --- Speichern der Ergebnisse ---\n    output_dir = \"result\"\n    stats_dir = \"Statistics\" # Neuer Ordner\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(stats_dir, exist_ok=True) # Stelle sicher, dass Statistics Ordner existiert\n\n    total_active_time = total_helper_time + total_main_time\n    \n    # Dateiname f√ºr Report\n    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M-%S\")\n    report_filename = f\"report_{timestamp}_Helper_{llm_helper.model_name}_MainLLM_{main_llm.model_name}.md\"\n    report_filepath = os.path.join(output_dir, report_filename)\n    \n    if final_report:\n        with open(report_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(final_report)\n        logging.info(f\"Final report saved to '{report_filepath}'.\")\n        \n        if savings_data:\n            try:\n                savings_filename = report_filename.replace(\"report_\", \"savings_\").replace(\".md\", \".png\")\n                savings_filepath = os.path.join(stats_dir, savings_filename)\n                \n                logging.info(f\"Erstelle Token-Chart: {savings_filepath}\")\n                create_savings_chart(\n                    json_tokens=savings_data['json_tokens'], \n                    toon_tokens=savings_data['toon_tokens'], \n                    savings_percent=savings_data['savings_percent'],\n                    output_path=savings_filepath\n                )\n            except Exception as chart_error:\n                logging.error(f\"Konnte Diagramm nicht erstellen: {chart_error}\")\n\n    else:\n        final_report = \"Error: Report generation failed or returned empty.\"\n\n    metrics = {\n        \"helper_time\": round(total_helper_time, 2),\n        \"main_time\": round(total_main_time, 2),\n        \"total_time\": round(total_active_time, 2),\n        \"helper_model\": helper_model,\n        \"main_model\": main_model,\n        \"json_tokens\": savings_data['json_tokens'] if savings_data else None,\n        \"toon_tokens\": savings_data['toon_tokens'] if savings_data else None,\n        \"savings_percent\": round(savings_data['savings_percent'], 2) if savings_data else None\n    \n    }\n\n    return {\n        \"report\": final_report,\n        \"metrics\": metrics\n    }"
            start_line: 75
            end_line: 477
            context:
              calls[22]: backend.AST_Schema.ASTAnalyzer,backend.AST_Schema.ASTAnalyzer.analyze_repository,backend.AST_Schema.ASTAnalyzer.merge_relationship_data,backend.HelperLLM.LLMHelper,backend.HelperLLM.LLMHelper.generate_for_classes,backend.HelperLLM.LLMHelper.generate_for_functions,backend.MainLLM.MainLLM,backend.MainLLM.MainLLM.call_llm,backend.basic_info.ProjektInfoExtractor,backend.basic_info.ProjektInfoExtractor.extrahiere_info,backend.getRepo.GitRepository,backend.main.calculate_net_time,backend.main.create_savings_chart,backend.main.update_status,backend.relationship_analyzer.ProjectAnalyzer,backend.relationship_analyzer.ProjectAnalyzer.analyze,backend.relationship_analyzer.ProjectAnalyzer.get_raw_relationships,schemas.types.ClassAnalysisInput,schemas.types.ClassContextInput,schemas.types.FunctionAnalysisInput,schemas.types.FunctionContextInput,schemas.types.MethodContextInput
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: backend.main.update_status
            name: update_status
            args[1]: msg
            docstring: null
            source_code: "def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)"
            start_line: 77
            end_line: 80
            context:
              calls[0]:
              called_by[2]: backend.main.main_workflow,backend.main.notebook_workflow
          - mode: function_analysis
            identifier: backend.main.notebook_workflow
            name: notebook_workflow
            args[4]: input,api_keys,model,status_callback
            docstring: null
            source_code: "def notebook_workflow(input, api_keys, model, status_callback=None):\n    t_start = time.time()\n    final_report = \"keine Notebooks gefunden\"\n    def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)\n\n    def gemini_payload(basic_info, nb_path, xml_content, images):\n        \n        intro_json = json.dumps({\n            \"basic_info\": basic_info,\n            \"current_notebook_path\": nb_path\n        }, indent=2)\n        \n        payload_content = []\n\n        payload_content.append({\n            \"type\": \"text\",\n            \"text\": f\"Context Information:\\n{intro_json}\\n\\nNotebook XML Structure:\\n\"\n        })\n\n        # Regex to find: <IMAGE_PLACEHOLDER index=\"0\" mime=\"image/png\"/>\n        pattern = r'(<IMAGE_PLACEHOLDER index=\"(\\d+)\" mime=\"([^\"]+)\"/>)'\n        last_pos = 0\n        \n        for match in re.finditer(pattern, xml_content):\n            text_segment = xml_content[last_pos:match.start()]\n            if text_segment.strip():\n                payload_content.append({\n                    \"type\": \"text\",\n                    \"text\": text_segment\n                })\n                \n            image_index = int(match.group(2))\n            mime_type = match.group(3)\n            \n            if image_index < len(images):\n                img_data = images[image_index]\n                b64_string = img_data['data']\n                \n                payload_content.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:{mime_type};base64,{b64_string}\"\n                    }\n                })\n                \n            last_pos = match.end()\n\n        # Add any remaining text after the last image\n        remaining_text = xml_content[last_pos:]\n        if remaining_text.strip():\n            payload_content.append({\n                \"type\": \"text\",\n                \"text\": remaining_text\n            })\n\n        return payload_content\n\n\n    base_url = None\n\n    if model.startswith(\"gpt-\"):\n        input_api_key = api_keys.get(\"gpt\")\n    elif model.startswith(\"gemini-\"):\n        input_api_key = api_keys.get(\"gemini\")\n    elif \"/\" in model or model.startswith(\"alias-\") or any(x in model for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        input_api_key = api_keys.get(\"scadsllm\")\n        base_url = api_keys.get(\"scadsllm_base_url\")\n    else:\n        input_api_key = None\n        base_url = api_keys.get(\"ollama\")\n\n    update_status(\"üîç Analysiere Input...\")\n\n    # URL Extraktion\n    repo_url = None\n    url_pattern = r\"https?://(?:www\\.)?github\\.com/[^\\s]+\"\n    match = re.search(url_pattern, input)\n\n    if match:\n        repo_url = match.group(0)\n        logging.info(f\"Extracted repository URL: {repo_url}\")\n    else:\n        raise ValueError(\"Could not find a valid URL in the provided input.\")\n    \n    update_status(f\"‚¨áÔ∏è Klone Repository: {repo_url} ...\")\n\n    try: \n        with GitRepository(repo_url) as repo:\n            repo_files = repo.get_all_files()\n    except Exception as e:\n        logging.error(f\"Error cloning repository: {e}\")\n        raise\n    \n    update_status(f\"üîó Bereite Input vor...\")\n\n    # convert to XML\n    processed_data = process_repo_notebooks(repo_files)\n\n    \n    # Extrahiere Basic Infos\n    update_status(\"‚ÑπÔ∏è Extrahiere Basis-Informationen...\")\n    try:\n        info_extractor = ProjektInfoExtractor()\n        basic_project_info = info_extractor.extrahiere_info(dateien=repo_files, repo_url=repo_url)\n        logging.info(\"Basic project info extracted\")\n    except Exception as e:\n        logging.error(f\"Error extracting basic project info: {e}\")\n        basic_project_info = \"Could not extract basic info.\"\n\n\n\n    prompt_file_notebook_llm = \"SystemPrompts/SystemPromptNotebookLLM.txt\"\n    notebook_llm = MainLLM(\n        api_key=input_api_key, \n        prompt_file_path=prompt_file_notebook_llm,\n        model_name=model,\n        base_url=base_url,\n    )\n\n    notebook_reports = []\n    total_notebooks = len(processed_data)\n    \n    logging.info(f\"Starting sequential processing of {total_notebooks} notebooks...\")\n\n    # Iterate over each notebook file individually\n    for index, (nb_path, nb_data) in enumerate(processed_data.items(), 1):\n        \n        update_status(f\"üß† Generiere Report ({index}/{total_notebooks}): {os.path.basename(nb_path)}\")\n        \n        nb_xml = nb_data['xml']\n        nb_images = nb_data['images']\n\n        llm_payload = gemini_payload(basic_project_info, nb_path, nb_xml, nb_images)\n\n        try:\n            single_report = notebook_llm.call_llm(llm_payload)\n            if single_report and isinstance(single_report, str):\n                notebook_reports.append(single_report)\n            else:\n                logging.warning(f\"LLM lieferte kein Ergebnis f√ºr {nb_path}\")\n                notebook_reports.append(f\"## Analyse f√ºr {os.path.basename(nb_path)}\\nFehler: Das Modell lieferte keine Antwort.\")\n            \n        except Exception as e:\n            logging.error(f\"Error generating report for {nb_path}: {e}\")\n            notebook_reports.append(f\"# Error processing {nb_path}\\n\\nError: {str(e)}\\n\\n---\\n\")\n\n    # Concatenate all reports\n    final_report = \"\\n\\n<br>\\n<br>\\n<br>\\n\\n\".join([str(r) for r in notebook_reports if r is not None])\n\n    \n    # --- Speichern der Ergebnisse ---\n    output_dir = \"result\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Dateiname f√ºr Report\n    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M-%S\")\n    report_filename = f\"notebook_report_{timestamp}_NotebookLLM_{notebook_llm.model_name}.md\"\n    report_filepath = os.path.join(output_dir, report_filename)\n    \n    if final_report:\n        with open(report_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(final_report)\n        logging.info(f\"Final report saved to '{report_filepath}'.\")\n\n    # Am Ende der Funktion nach dem Speichern:\n    total_time = time.time() - t_start\n    \n    # Metriken f√ºr das Frontend bauen\n    metrics = {\n        \"helper_time\": \"0\", # Notebook-Workflow hat meist keinen separaten Helper\n        \"main_time\": round(total_time, 2),\n        \"total_time\": round(total_time, 2),\n        \"helper_model\": \"None\",\n        \"main_model\": model,\n        \"json_tokens\": 0,\n        \"toon_tokens\": 0,\n        \"savings_percent\": 0\n    }\n\n    return {\n        \"report\": final_report,\n        \"metrics\": metrics\n    }"
            start_line: 483
            end_line: 668
            context:
              calls[8]: backend.MainLLM.MainLLM,backend.MainLLM.MainLLM.call_llm,backend.basic_info.ProjektInfoExtractor,backend.basic_info.ProjektInfoExtractor.extrahiere_info,backend.converter.process_repo_notebooks,backend.getRepo.GitRepository,backend.main.gemini_payload,backend.main.update_status
              called_by[2]: backend.main,frontend.frontend
          - mode: function_analysis
            identifier: backend.main.update_status
            name: update_status
            args[1]: msg
            docstring: null
            source_code: "def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)"
            start_line: 486
            end_line: 489
            context:
              calls[0]:
              called_by[2]: backend.main.main_workflow,backend.main.notebook_workflow
          - mode: function_analysis
            identifier: backend.main.gemini_payload
            name: gemini_payload
            args[4]: basic_info,nb_path,xml_content,images
            docstring: null
            source_code: "def gemini_payload(basic_info, nb_path, xml_content, images):\n        \n        intro_json = json.dumps({\n            \"basic_info\": basic_info,\n            \"current_notebook_path\": nb_path\n        }, indent=2)\n        \n        payload_content = []\n\n        payload_content.append({\n            \"type\": \"text\",\n            \"text\": f\"Context Information:\\n{intro_json}\\n\\nNotebook XML Structure:\\n\"\n        })\n\n        # Regex to find: <IMAGE_PLACEHOLDER index=\"0\" mime=\"image/png\"/>\n        pattern = r'(<IMAGE_PLACEHOLDER index=\"(\\d+)\" mime=\"([^\"]+)\"/>)'\n        last_pos = 0\n        \n        for match in re.finditer(pattern, xml_content):\n            text_segment = xml_content[last_pos:match.start()]\n            if text_segment.strip():\n                payload_content.append({\n                    \"type\": \"text\",\n                    \"text\": text_segment\n                })\n                \n            image_index = int(match.group(2))\n            mime_type = match.group(3)\n            \n            if image_index < len(images):\n                img_data = images[image_index]\n                b64_string = img_data['data']\n                \n                payload_content.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:{mime_type};base64,{b64_string}\"\n                    }\n                })\n                \n            last_pos = match.end()\n\n        # Add any remaining text after the last image\n        remaining_text = xml_content[last_pos:]\n        if remaining_text.strip():\n            payload_content.append({\n                \"type\": \"text\",\n                \"text\": remaining_text\n            })\n\n        return payload_content"
            start_line: 491
            end_line: 541
            context:
              calls[0]:
              called_by[1]: backend.main.notebook_workflow
        classes[0]:
    "backend/relationship_analyzer.py":
      ast_nodes:
        imports[4]: ast,os,logging,collections.defaultdict
        functions[1]:
          - mode: function_analysis
            identifier: backend.relationship_analyzer.path_to_module
            name: path_to_module
            args[2]: filepath,project_root
            docstring: Wandelt einen Dateipfad in einen Python-Modulpfad um.
            source_code: "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    module_path = rel_path.replace(os.path.sep, '.')\n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n    return module_path"
            start_line: 6
            end_line: 18
            context:
              calls[0]:
              called_by[2]: backend.relationship_analyzer.CallResolverVisitor.__init__,backend.relationship_analyzer.ProjectAnalyzer._collect_definitions
        classes[2]:
          - mode: class_analysis
            identifier: backend.relationship_analyzer.ProjectAnalyzer
            name: ProjectAnalyzer
            docstring: null
            source_code: "class ProjectAnalyzer:\n    \n    def __init__(self, project_root):\n        self.project_root = os.path.abspath(project_root)\n        self.definitions = {}\n        self.call_graph = defaultdict(list)\n        self.file_asts = {} \n        self.ignore_dirs = {'.git', '.venv', 'venv', '__pycache__', 'node_modules', 'dist', 'build', 'docs'}\n\n    def analyze(self):\n        py_files = self._find_py_files()\n        \n        for filepath in py_files:\n            self._collect_definitions(filepath)\n            \n        for filepath in py_files:\n            self._resolve_calls(filepath)\n            \n        self.file_asts.clear()\n        \n        return self.call_graph\n\n    def get_raw_relationships(self):\n\n        outgoing = defaultdict(set)\n        incoming = defaultdict(set)\n\n        for callee_id, callers_info_list in self.call_graph.items():\n            for info in callers_info_list:\n                caller_id = info['caller']\n                \n                if caller_id and callee_id:\n                    outgoing[caller_id].add(callee_id)\n                    incoming[callee_id].add(caller_id)\n        \n        return {\n            \"outgoing\": {k: sorted(list(v)) for k, v in outgoing.items()},\n            \"incoming\": {k: sorted(list(v)) for k, v in incoming.items()}\n        }\n\n    def _find_py_files(self):\n        py_files = []\n        for root, dirs, files in os.walk(self.project_root):\n            dirs[:] = [d for d in dirs if d not in self.ignore_dirs]\n            for file in files:\n                if file.endswith(\".py\"):\n                    py_files.append(os.path.join(root, file))\n        return py_files\n\n    def _collect_definitions(self, filepath):\n        try:\n            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n                source = f.read()\n                tree = ast.parse(source, filename=filepath)\n                \n            self.file_asts[filepath] = tree\n            module_path = path_to_module(filepath, self.project_root)\n            \n            for node in ast.walk(tree):\n                if isinstance(node, ast.FunctionDef):\n                    parent = self._get_parent(tree, node)\n                    if isinstance(parent, ast.ClassDef):\n                        path_name = f\"{module_path}.{parent.name}.{node.name}\"\n                        def_type = 'method'\n                    else:\n                        path_name = f\"{module_path}.{node.name}\"\n                        def_type = 'function'\n                    self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': def_type}\n                elif isinstance(node, ast.ClassDef):\n                    path_name = f\"{module_path}.{node.name}\"\n                    self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': 'class'}\n        except Exception as e:\n            logging.error(f\"Error collecting definitions in {filepath}: {e}\")\n            self.file_asts[filepath] = None\n\n    def _get_parent(self, tree, node):\n        for parent in ast.walk(tree):\n            for child in ast.iter_child_nodes(parent):\n                if child is node:\n                    return parent\n        return None\n\n    def _resolve_calls(self, filepath):\n        tree = self.file_asts.get(filepath)\n        if not tree:\n            return\n\n        try:\n            resolver = CallResolverVisitor(filepath, self.project_root, self.definitions)\n            resolver.visit(tree)\n            \n            for callee_pathname, caller_info_list in resolver.calls.items():\n                self.call_graph[callee_pathname].extend(caller_info_list)\n        except Exception as e:\n            logging.error(f\"Error resolving calls in {filepath}: {e}\")"
            start_line: 20
            end_line: 114
            context:
              dependencies[2]: backend.relationship_analyzer.CallResolverVisitor,backend.relationship_analyzer.path_to_module
              instantiated_by[1]: backend.main.main_workflow
              method_context[7]:
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,project_root
                  docstring: null
                  start_line: 22
                  end_line: 27
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.analyze
                  name: analyze
                  calls[0]:
                  called_by[1]: backend.main.main_workflow
                  args[1]: self
                  docstring: null
                  start_line: 29
                  end_line: 40
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.get_raw_relationships
                  name: get_raw_relationships
                  calls[0]:
                  called_by[1]: backend.main.main_workflow
                  args[1]: self
                  docstring: null
                  start_line: 42
                  end_line: 58
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._find_py_files
                  name: _find_py_files
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 60
                  end_line: 67
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._collect_definitions
                  name: _collect_definitions
                  calls[1]: backend.relationship_analyzer.path_to_module
                  called_by[0]:
                  args[2]: self,filepath
                  docstring: null
                  start_line: 69
                  end_line: 93
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._get_parent
                  name: _get_parent
                  calls[0]:
                  called_by[0]:
                  args[3]: self,tree,node
                  docstring: null
                  start_line: 95
                  end_line: 100
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._resolve_calls
                  name: _resolve_calls
                  calls[1]: backend.relationship_analyzer.CallResolverVisitor
                  called_by[0]:
                  args[2]: self,filepath
                  docstring: null
                  start_line: 102
                  end_line: 114
          - mode: class_analysis
            identifier: backend.relationship_analyzer.CallResolverVisitor
            name: CallResolverVisitor
            docstring: null
            source_code: "class CallResolverVisitor(ast.NodeVisitor):\n    def __init__(self, filepath, project_root, definitions):\n        self.filepath = filepath\n        self.module_path = path_to_module(filepath, project_root)\n        self.definitions = definitions\n        self.scope = {}\n        self.instance_types = {}\n        self.current_caller_name = self.module_path\n        self.current_class_name = None\n        self.calls = defaultdict(list)\n\n    def visit_ClassDef(self, node):\n        old_class_name = self.current_class_name\n        self.current_class_name = node.name\n        self.generic_visit(node)\n        self.current_class_name = old_class_name\n\n    def visit_FunctionDef(self, node):\n        old_caller = self.current_caller_name\n        \n        if self.current_class_name:\n            full_identifier = f\"{self.module_path}.{self.current_class_name}.{node.name}\"\n        else:\n            full_identifier = f\"{self.module_path}.{node.name}\"\n            \n        self.current_caller_name = full_identifier\n        self.generic_visit(node)\n        self.current_caller_name = old_caller\n\n    def visit_Call(self, node):\n        callee_pathname = self._resolve_call_qname(node.func)\n        if callee_pathname and callee_pathname in self.definitions:\n            \n            if self.current_caller_name == self.module_path:\n                caller_type = 'module'\n            elif '.<locals>.' in self.current_caller_name:\n                caller_type = 'local_function'\n            elif self.current_class_name:\n                caller_type = 'method'\n            else:\n                caller_type = 'function'\n            \n            caller_info = {\n                'file': os.path.basename(self.filepath),\n                'line': node.lineno,\n                'caller': self.current_caller_name,\n                'caller_type': caller_type \n            }\n            self.calls[callee_pathname].append(caller_info)\n        self.generic_visit(node)\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.scope[alias.asname or alias.name] = alias.name\n        self.generic_visit(node)\n        \n    def visit_ImportFrom(self, node):\n        module = node.module or ''\n        for alias in node.names:\n            name = alias.asname or alias.name\n            if node.level > 0:\n                base = self.module_path.split('.')\n                prefix = '.'.join(base[:-node.level])\n                full_module_path = f\"{prefix}.{module}\" if module else prefix\n            else:\n                full_module_path = module\n            self.scope[name] = f\"{full_module_path}.{alias.name}\"\n        self.generic_visit(node)\n\n    def visit_Assign(self, node):\n        if isinstance(node.value, ast.Call) and isinstance(node.value.func, ast.Name):\n            class_name = node.value.func.id\n            if class_name in self.scope:\n                qualified_class_name = self.scope[class_name]\n                if qualified_class_name in self.definitions:\n                    for target in node.targets:\n                        if isinstance(target, ast.Name):\n                            self.instance_types[target.id] = qualified_class_name\n        self.generic_visit(node)\n\n    def _resolve_call_qname(self, func_node):\n        if isinstance(func_node, ast.Name):\n            name = func_node.id\n            if name in self.scope:\n                return self.scope[name]\n            local_pathname = f\"{self.module_path}.{name}\"\n            if local_pathname in self.definitions:\n                return local_pathname\n        elif isinstance(func_node, ast.Attribute) and isinstance(func_node.value, ast.Name):\n            var_name = func_node.value.id\n            method_name = func_node.attr\n            if var_name in self.instance_types:\n                class_pathname = self.instance_types[var_name]\n                return f\"{class_pathname}.{method_name}\"\n            if var_name in self.scope:\n                module_pathname = self.scope[var_name]\n                return f\"{module_pathname}.{method_name}\"\n        return None"
            start_line: 117
            end_line: 214
            context:
              dependencies[1]: backend.relationship_analyzer.path_to_module
              instantiated_by[1]: backend.relationship_analyzer.ProjectAnalyzer._resolve_calls
              method_context[8]:
                - identifier: backend.relationship_analyzer.CallResolverVisitor.__init__
                  name: __init__
                  calls[1]: backend.relationship_analyzer.path_to_module
                  called_by[0]:
                  args[4]: self,filepath,project_root,definitions
                  docstring: null
                  start_line: 118
                  end_line: 126
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 128
                  end_line: 132
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 134
                  end_line: 144
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Call
                  name: visit_Call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 146
                  end_line: 166
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 168
                  end_line: 171
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 173
                  end_line: 184
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Assign
                  name: visit_Assign
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 186
                  end_line: 195
                - identifier: backend.relationship_analyzer.CallResolverVisitor._resolve_call_qname
                  name: _resolve_call_qname
                  calls[0]:
                  called_by[0]:
                  args[2]: self,func_node
                  docstring: null
                  start_line: 197
                  end_line: 214
    "backend/scads_key_test.py":
      ast_nodes:
        imports[3]: os,dotenv.load_dotenv,openai.OpenAI
        functions[0]:
        classes[0]:
    "database/db.py":
      ast_nodes:
        imports[7]: datetime.datetime,pymongo.MongoClient,dotenv.load_dotenv,streamlit_authenticator,cryptography.fernet.Fernet,uuid,os
        functions[29]:
          - mode: function_analysis
            identifier: database.db.encrypt_text
            name: encrypt_text
            args[1]: text
            docstring: null
            source_code: "def encrypt_text(text: str) -> str:\n    if not text or not cipher_suite: return text\n    return cipher_suite.encrypt(text.strip().encode()).decode()"
            start_line: 28
            end_line: 30
            context:
              calls[0]:
              called_by[3]: database.db.update_gemini_key,database.db.update_gpt_key,database.db.update_opensrc_key
          - mode: function_analysis
            identifier: database.db.decrypt_text
            name: decrypt_text
            args[1]: text
            docstring: null
            source_code: "def decrypt_text(text: str) -> str:\n    if not text or not cipher_suite: return text\n    try:\n        return cipher_suite.decrypt(text.strip().encode()).decode()\n    except Exception:\n        return text"
            start_line: 32
            end_line: 37
            context:
              calls[0]:
              called_by[1]: database.db.get_decrypted_api_keys
          - mode: function_analysis
            identifier: database.db.insert_user
            name: insert_user
            args[3]: username,name,password
            docstring: null
            source_code: "def insert_user(username: str, name: str, password: str):\n    user = {\n        \"_id\": username,\n        \"name\": name, \n        \"hashed_password\": stauth.Hasher.hash(password),\n        \"gemini_api_key\": \"\",\n        \"ollama_base_url\": \"\",\n        \"gpt_api_key\": \"\"\n    }\n    result = dbusers.insert_one(user)\n    return result.inserted_id"
            start_line: 43
            end_line: 53
            context:
              calls[0]:
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: database.db.fetch_all_users
            name: fetch_all_users
            args[0]:
            docstring: null
            source_code: "def fetch_all_users():\n    return list(dbusers.find())"
            start_line: 55
            end_line: 56
            context:
              calls[0]:
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: database.db.fetch_user
            name: fetch_user
            args[1]: username
            docstring: null
            source_code: "def fetch_user(username: str):\n    return dbusers.find_one({\"_id\": username})"
            start_line: 58
            end_line: 59
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_user_name
            name: update_user_name
            args[2]: username,new_name
            docstring: null
            source_code: "def update_user_name(username: str, new_name: str):\n    # Achtung: _id kann in Mongo nicht einfach so ge√§ndert werden. \n    # Hier wird nur das Name-Feld geupdated.\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"name\": new_name}})\n    return result.modified_count"
            start_line: 61
            end_line: 65
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_gemini_key
            name: update_gemini_key
            args[2]: username,gemini_api_key
            docstring: null
            source_code: "def update_gemini_key(username: str, gemini_api_key: str):\n    encrypted_key = encrypt_text(gemini_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"gemini_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 67
            end_line: 70
            context:
              calls[1]: database.db.encrypt_text
              called_by[2]: frontend.frontend,frontend.frontend.save_gemini_cb
          - mode: function_analysis
            identifier: database.db.update_gpt_key
            name: update_gpt_key
            args[2]: username,gpt_api_key
            docstring: null
            source_code: "def update_gpt_key(username: str, gpt_api_key: str):\n    encrypted_key = encrypt_text(gpt_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"gpt_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 72
            end_line: 75
            context:
              calls[1]: database.db.encrypt_text
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: database.db.update_ollama_url
            name: update_ollama_url
            args[2]: username,ollama_base_url
            docstring: null
            source_code: "def update_ollama_url(username: str, ollama_base_url: str):\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"ollama_base_url\": ollama_base_url.strip()}})\n    return result.modified_count"
            start_line: 77
            end_line: 79
            context:
              calls[0]:
              called_by[2]: frontend.frontend,frontend.frontend.save_ollama_cb
          - mode: function_analysis
            identifier: database.db.update_opensrc_key
            name: update_opensrc_key
            args[2]: username,opensrc_api_key
            docstring: null
            source_code: "def update_opensrc_key(username: str, opensrc_api_key: str):\n    encrypted_key = encrypt_text(opensrc_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"opensrc_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 81
            end_line: 84
            context:
              calls[1]: database.db.encrypt_text
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: database.db.update_opensrc_url
            name: update_opensrc_url
            args[2]: username,opensrc_base_url
            docstring: null
            source_code: "def update_opensrc_url(username: str, opensrc_base_url: str):\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"opensrc_base_url\": opensrc_base_url.strip()}})\n    return result.modified_count"
            start_line: 86
            end_line: 88
            context:
              calls[0]:
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: database.db.fetch_gemini_key
            name: fetch_gemini_key
            args[1]: username
            docstring: null
            source_code: "def fetch_gemini_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"gemini_api_key\": 1, \"_id\": 0})\n    return user.get(\"gemini_api_key\") if user else None"
            start_line: 90
            end_line: 92
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_ollama_url
            name: fetch_ollama_url
            args[1]: username
            docstring: null
            source_code: "def fetch_ollama_url(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"ollama_base_url\": 1, \"_id\": 0})\n    return user.get(\"ollama_base_url\") if user else None"
            start_line: 94
            end_line: 96
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_gpt_key
            name: fetch_gpt_key
            args[1]: username
            docstring: null
            source_code: "def fetch_gpt_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"gpt_api_key\": 1, \"_id\": 0})\n    return user.get(\"gpt_api_key\") if user else None"
            start_line: 98
            end_line: 100
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_opensrc_key
            name: fetch_opensrc_key
            args[1]: username
            docstring: null
            source_code: "def fetch_opensrc_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"opensrc_api_key\": 1, \"_id\": 0})\n    return user.get(\"opensrc_api_key\") if user else None"
            start_line: 102
            end_line: 104
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_opensrc_url
            name: fetch_opensrc_url
            args[1]: username
            docstring: null
            source_code: "def fetch_opensrc_url(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"opensrc_base_url\": 1, \"_id\": 0})\n    return user.get(\"opensrc_base_url\") if user else None"
            start_line: 106
            end_line: 108
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.delete_user
            name: delete_user
            args[1]: username
            docstring: null
            source_code: "def delete_user(username: str):\n    return dbusers.delete_one({\"_id\": username}).deleted_count"
            start_line: 110
            end_line: 111
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.get_decrypted_api_keys
            name: get_decrypted_api_keys
            args[1]: username
            docstring: null
            source_code: "def get_decrypted_api_keys(username: str):\n    user = dbusers.find_one({\"_id\": username})\n    if not user: return None, None\n    gemini_plain = decrypt_text(user.get(\"gemini_api_key\", \"\"))\n    ollama_plain = user.get(\"ollama_base_url\", \"\")\n    gpt_plain = decrypt_text(user.get(\"gpt_api_key\", \"\"))\n    opensrc_plain = decrypt_text(user.get(\"opensrc_api_key\", \"\"))\n    opensrc_url = user.get(\"opensrc_base_url\", \"\")\n    return gemini_plain, ollama_plain, gpt_plain, opensrc_plain, opensrc_url"
            start_line: 113
            end_line: 121
            context:
              calls[1]: database.db.decrypt_text
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: database.db.insert_chat
            name: insert_chat
            args[2]: username,chat_name
            docstring: Erstellt einen neuen Chat-Eintrag.
            source_code: "def insert_chat(username: str, chat_name: str):\n    \"\"\"Erstellt einen neuen Chat-Eintrag.\"\"\"\n    chat = {\n        \"_id\": str(uuid.uuid4()),\n        \"username\": username,\n        \"chat_name\": chat_name,\n        \"created_at\": datetime.now()\n    }\n    result = dbchats.insert_one(chat)\n    return result.inserted_id"
            start_line: 127
            end_line: 136
            context:
              calls[0]:
              called_by[3]: frontend.frontend,frontend.frontend.handle_delete_chat,frontend.frontend.load_data_from_db
          - mode: function_analysis
            identifier: database.db.fetch_chats_by_user
            name: fetch_chats_by_user
            args[1]: username
            docstring: Holt alle definierten Chats eines Users.
            source_code: "def fetch_chats_by_user(username: str):\n    \"\"\"Holt alle definierten Chats eines Users.\"\"\"\n    # Sortieren nach Erstellung macht Sinn\n    chats = list(dbchats.find({\"username\": username}).sort(\"created_at\", 1))\n    return chats"
            start_line: 138
            end_line: 142
            context:
              calls[0]:
              called_by[1]: frontend.frontend.load_data_from_db
          - mode: function_analysis
            identifier: database.db.check_chat_exists
            name: check_chat_exists
            args[2]: username,chat_name
            docstring: null
            source_code: "def check_chat_exists(username: str, chat_name: str):\n    return dbchats.find_one({\"username\": username, \"chat_name\": chat_name}) is not None"
            start_line: 144
            end_line: 145
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.rename_chat_fully
            name: rename_chat_fully
            args[3]: username,old_name,new_name
            docstring: Benennt einen Chat und alle zugeh√∂rigen Exchanges um.
            source_code: "def rename_chat_fully(username: str, old_name: str, new_name: str):\n    \"\"\"\n    Benennt einen Chat und alle zugeh√∂rigen Exchanges um.\n    \"\"\"\n    # 1. Chat-Eintrag umbenennen\n    result = dbchats.update_one(\n        {\"username\": username, \"chat_name\": old_name}, \n        {\"$set\": {\"chat_name\": new_name}}\n    )\n    \n    # 2. Alle Messages (Exchanges) umh√§ngen\n    dbexchanges.update_many(\n        {\"username\": username, \"chat_name\": old_name},\n        {\"$set\": {\"chat_name\": new_name}}\n    )\n    return result.modified_count"
            start_line: 148
            end_line: 163
            context:
              calls[0]:
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: database.db.insert_exchange
            name: insert_exchange
            args[13]: question,answer,feedback,username,chat_name,helper_used,main_used,total_time,helper_time,main_time,json_tokens,toon_tokens,savings_percent
            docstring: null
            source_code: "def insert_exchange(question: str, answer: str, feedback: str, username: str, chat_name: str, \n                    helper_used: str=\"\", main_used: str=\"\", total_time: str=\"\", helper_time: str=\"\", main_time: str=\"\",\n                    json_tokens=0, toon_tokens=0, savings_percent=0.0):\n    new_id = str(uuid.uuid4())\n    exchange = {\n        \"_id\": new_id,\n        \"question\": question,\n        \"answer\": answer,\n        \"feedback\": feedback,\n        \"feedback_message\": \"\",\n        \"chat_name\": chat_name,\n        \"username\": username,\n        \"helper_used\": helper_used,\n        \"main_used\": main_used,\n        \"total_time\": total_time,\n        \"helper_time\": helper_time,\n        \"main_time\": main_time,\n        \"json_tokens\": json_tokens,\n        \"toon_tokens\": toon_tokens,\n        \"savings_percent\": savings_percent,\n        \"created_at\": datetime.now()\n    }\n    try:\n        dbexchanges.insert_one(exchange)\n        return new_id\n    except Exception as e:\n        print(f\"DB Error: {e}\")\n        return None"
            start_line: 169
            end_line: 196
            context:
              calls[0]:
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: database.db.fetch_exchanges_by_user
            name: fetch_exchanges_by_user
            args[1]: username
            docstring: null
            source_code: "def fetch_exchanges_by_user(username: str):\n    # Sortieren nach Zeitstempel wichtig f√ºr die Anzeige\n    exchanges = list(dbexchanges.find({\"username\": username}).sort(\"created_at\", 1))\n    return exchanges"
            start_line: 198
            end_line: 201
            context:
              calls[0]:
              called_by[1]: frontend.frontend.load_data_from_db
          - mode: function_analysis
            identifier: database.db.fetch_exchanges_by_chat
            name: fetch_exchanges_by_chat
            args[2]: username,chat_name
            docstring: null
            source_code: "def fetch_exchanges_by_chat(username: str, chat_name: str):\n    exchanges = list(dbexchanges.find({\"username\": username, \"chat_name\": chat_name}).sort(\"created_at\", 1))\n    return exchanges"
            start_line: 203
            end_line: 205
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_exchange_feedback
            name: update_exchange_feedback
            args[2]: exchange_id,feedback
            docstring: null
            source_code: "def update_exchange_feedback(exchange_id, feedback: int):\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback\": feedback}})\n    return result.modified_count"
            start_line: 207
            end_line: 209
            context:
              calls[0]:
              called_by[1]: frontend.frontend.handle_feedback_change
          - mode: function_analysis
            identifier: database.db.update_exchange_feedback_message
            name: update_exchange_feedback_message
            args[2]: exchange_id,feedback_message
            docstring: null
            source_code: "def update_exchange_feedback_message(exchange_id, feedback_message: str):\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback_message\": feedback_message}})\n    return result.modified_count"
            start_line: 211
            end_line: 213
            context:
              calls[0]:
              called_by[1]: frontend.frontend.render_exchange
          - mode: function_analysis
            identifier: database.db.delete_exchange_by_id
            name: delete_exchange_by_id
            args[1]: exchange_id
            docstring: null
            source_code: "def delete_exchange_by_id(exchange_id: str):\n    result = dbexchanges.delete_one({\"_id\": exchange_id})\n    return result.deleted_count"
            start_line: 215
            end_line: 217
            context:
              calls[0]:
              called_by[1]: frontend.frontend.handle_delete_exchange
          - mode: function_analysis
            identifier: database.db.delete_full_chat
            name: delete_full_chat
            args[2]: username,chat_name
            docstring: "L√∂scht den Chat UND alle zugeh√∂rigen Exchanges.\nDas sorgt f√ºr Konsistenz zwischen Frontend und Backend."
            source_code: "def delete_full_chat(username: str, chat_name: str):\n    \"\"\"\n    L√∂scht den Chat UND alle zugeh√∂rigen Exchanges.\n    Das sorgt f√ºr Konsistenz zwischen Frontend und Backend.\n    \"\"\"\n    # 1. Alle Nachrichten in diesem Chat l√∂schen\n    del_exchanges = dbexchanges.delete_many({\"username\": username, \"chat_name\": chat_name})\n    \n    # 2. Den Chat selbst aus der Chat-Liste l√∂schen\n    del_chat = dbchats.delete_one({\"username\": username, \"chat_name\": chat_name})\n    \n    return del_chat.deleted_count"
            start_line: 221
            end_line: 232
            context:
              calls[0]:
              called_by[1]: frontend.frontend.handle_delete_chat
        classes[0]:
    "frontend/frontend.py":
      ast_nodes:
        imports[16]: numpy,datetime.datetime,time,pymongo.MongoClient,dotenv.load_dotenv,os,sys,urllib.parse.urlparse,logging,traceback,re,streamlit_mermaid.st_mermaid,backend.main,database.db,streamlit,streamlit_authenticator
        functions[12]:
          - mode: function_analysis
            identifier: frontend.frontend.clean_names
            name: clean_names
            args[1]: model_list
            docstring: null
            source_code: "def clean_names(model_list):\n    return [m.split(\"/\")[-1] for m in model_list]"
            start_line: 54
            end_line: 55
            context:
              calls[0]:
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: frontend.frontend.get_filtered_models
            name: get_filtered_models
            args[2]: source_list,category_name
            docstring: Filtert eine Liste basierend auf der gew√§hlten Kategorie.
            source_code: "def get_filtered_models(source_list, category_name):\n    \"\"\"Filtert eine Liste basierend auf der gew√§hlten Kategorie.\"\"\"\n    keywords = CATEGORY_KEYWORDS.get(category_name, [\"\"])\n    \n    if \"STANDARD\" in keywords:\n        # Nur Modelle zur√ºckgeben, die auch in der Standard-Liste sind\n        return [m for m in source_list if m in STANDARD_MODELS]\n    \n    filtered = []\n    for model in source_list:\n        # Pr√ºfen ob ein Keyword im Namen steckt\n        if any(k in model.lower() for k in keywords):\n            filtered.append(model)\n            \n    return filtered if filtered else source_list"
            start_line: 86
            end_line: 100
            context:
              calls[0]:
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: frontend.frontend.save_gemini_cb
            name: save_gemini_cb
            args[0]:
            docstring: null
            source_code: "def save_gemini_cb():\n    new_key = st.session_state.get(\"in_gemini_key\", \"\")\n    if new_key:\n        db.update_gemini_key(st.session_state[\"username\"], new_key)\n        st.session_state[\"in_gemini_key\"] = \"\"\n        st.toast(\"Gemini Key erfolgreich gespeichert! ‚úÖ\")"
            start_line: 143
            end_line: 148
            context:
              calls[1]: database.db.update_gemini_key
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.save_ollama_cb
            name: save_ollama_cb
            args[0]:
            docstring: null
            source_code: "def save_ollama_cb():\n    new_url = st.session_state.get(\"in_ollama_url\", \"\")\n    if new_url:\n        db.update_ollama_url(st.session_state[\"username\"], new_url)\n        st.toast(\"Ollama URL gespeichert! ‚úÖ\")"
            start_line: 150
            end_line: 154
            context:
              calls[1]: database.db.update_ollama_url
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.load_data_from_db
            name: load_data_from_db
            args[1]: username
            docstring: L√§dt Chats und Exchanges konsistent aus der DB.
            source_code: "def load_data_from_db(username: str):\n    \"\"\"L√§dt Chats und Exchanges konsistent aus der DB.\"\"\"\n    \n    # Nur laden, wenn neuer User oder noch nicht geladen\n    if \"loaded_user\" not in st.session_state or st.session_state.loaded_user != username:\n        st.session_state.chats = {}\n        \n        # 1. Erst die definierten Chats laden (damit auch leere Chats da sind)\n        db_chats = db.fetch_chats_by_user(username)\n        for c in db_chats:\n            c_name = c.get(\"chat_name\")\n            if c_name:\n                st.session_state.chats[c_name] = {\"exchanges\": []}\n\n        # 2. Dann die Exchanges laden und einsortieren\n        db_exchanges = db.fetch_exchanges_by_user(username)\n        for ex in db_exchanges:\n            c_name = ex.get(\"chat_name\", \"Unbenannt\")\n            \n            # Falls Exchanges existieren f√ºr Chats, die nicht in dbchats sind (Legacy Support)\n            if c_name not in st.session_state.chats:\n                st.session_state.chats[c_name] = {\"exchanges\": []}\n            \n            if \"feedback\" not in ex or ex[\"feedback\"] is None:\n                ex[\"feedback\"] = np.nan\n            \n            st.session_state.chats[c_name][\"exchanges\"].append(ex)\n\n        # 3. Default Chat erstellen, falls gar nichts existiert\n        if not st.session_state.chats:\n            initial_name = \"Chat 1\"\n            # Konsistent in DB anlegen\n            db.insert_chat(username, initial_name)\n            st.session_state.chats[initial_name] = {\"exchanges\": []}\n            st.session_state.active_chat = initial_name\n        else:\n            # Active Chat setzen, falls n√∂tig\n            if \"active_chat\" not in st.session_state or st.session_state.active_chat not in st.session_state.chats:\n                # Nimm den ersten verf√ºgbaren Chat\n                st.session_state.active_chat = list(st.session_state.chats.keys())[0]\n        \n        st.session_state.loaded_user = username"
            start_line: 160
            end_line: 201
            context:
              calls[3]: database.db.fetch_chats_by_user,database.db.fetch_exchanges_by_user,database.db.insert_chat
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: frontend.frontend.handle_feedback_change
            name: handle_feedback_change
            args[2]: ex,val
            docstring: null
            source_code: "def handle_feedback_change(ex, val):\n    ex[\"feedback\"] = val\n    db.update_exchange_feedback(ex[\"_id\"], val)\n    st.rerun()"
            start_line: 207
            end_line: 210
            context:
              calls[1]: database.db.update_exchange_feedback
              called_by[1]: frontend.frontend.render_exchange
          - mode: function_analysis
            identifier: frontend.frontend.handle_delete_exchange
            name: handle_delete_exchange
            args[2]: chat_name,ex
            docstring: null
            source_code: "def handle_delete_exchange(chat_name, ex):\n    db.delete_exchange_by_id(ex[\"_id\"])\n    if chat_name in st.session_state.chats:\n        if ex in st.session_state.chats[chat_name][\"exchanges\"]:\n            st.session_state.chats[chat_name][\"exchanges\"].remove(ex)\n    st.rerun()"
            start_line: 212
            end_line: 217
            context:
              calls[1]: database.db.delete_exchange_by_id
              called_by[1]: frontend.frontend.render_exchange
          - mode: function_analysis
            identifier: frontend.frontend.handle_delete_chat
            name: handle_delete_chat
            args[2]: username,chat_name
            docstring: null
            source_code: "def handle_delete_chat(username, chat_name):\n    # KONSISTENZ: Ruft jetzt delete_full_chat in DB auf\n    db.delete_full_chat(username, chat_name)\n    \n    # State bereinigen\n    if chat_name in st.session_state.chats:\n        del st.session_state.chats[chat_name]\n    \n    # Active Chat neu setzen\n    if len(st.session_state.chats) > 0:\n        st.session_state.active_chat = list(st.session_state.chats.keys())[0]\n    else:\n        # Wenn alles weg ist, neuen leeren Chat anlegen\n        new_name = \"Chat 1\"\n        db.insert_chat(username, new_name)\n        st.session_state.chats[new_name] = {\"exchanges\": []}\n        st.session_state.active_chat = new_name\n        \n    st.rerun()"
            start_line: 219
            end_line: 237
            context:
              calls[2]: database.db.delete_full_chat,database.db.insert_chat
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: frontend.frontend.extract_repo_name
            name: extract_repo_name
            args[1]: text
            docstring: null
            source_code: "def extract_repo_name(text):\n    url_match = re.search(r'(https?://[^\\s]+)', text)\n    if url_match:\n        url = url_match.group(0)\n        parsed = urlparse(url)\n        path = parsed.path.strip(\"/\")\n        if path:\n            repo_name = path.split(\"/\")[-1]\n            if repo_name.endswith(\".git\"):\n                repo_name = repo_name[:-4]\n            return repo_name\n    return None"
            start_line: 243
            end_line: 254
            context:
              calls[0]:
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: frontend.frontend.stream_text_generator
            name: stream_text_generator
            args[1]: text
            docstring: null
            source_code: "def stream_text_generator(text):\n    for word in text.split(\" \"):\n        yield word + \" \"\n        time.sleep(0.01)"
            start_line: 256
            end_line: 259
            context:
              calls[0]:
              called_by[1]: frontend.frontend.render_text_with_mermaid
          - mode: function_analysis
            identifier: frontend.frontend.render_text_with_mermaid
            name: render_text_with_mermaid
            args[2]: markdown_text,should_stream
            docstring: null
            source_code: "def render_text_with_mermaid(markdown_text, should_stream=False):\n    if not markdown_text:\n        return\n\n    parts = re.split(r\"```mermaid\\s+(.*?)\\s+```\", markdown_text, flags=re.DOTALL)\n\n    for i, part in enumerate(parts):\n        if i % 2 == 0:\n            if part.strip():\n                if should_stream:\n                    st.write_stream(stream_text_generator(part))\n                else:\n                    st.markdown(part)\n        else:\n            try:\n                st_mermaid(part, key=f\"mermaid_{hash(part)}_{i}\")\n            except Exception:\n                st.code(part, language=\"mermaid\")"
            start_line: 261
            end_line: 278
            context:
              calls[1]: frontend.frontend.stream_text_generator
              called_by[2]: frontend.frontend,frontend.frontend.render_exchange
          - mode: function_analysis
            identifier: frontend.frontend.render_exchange
            name: render_exchange
            args[2]: ex,current_chat_name
            docstring: null
            source_code: "def render_exchange(ex, current_chat_name):\n    st.chat_message(\"user\").write(ex[\"question\"])\n\n    with st.chat_message(\"assistant\"):\n        answer_text = ex.get(\"answer\", \"\")\n        is_error = answer_text.startswith(\"Fehler\") or answer_text.startswith(\"Error\")\n\n        # --- 1. TOOLBAR CONTAINER ---\n        # Nutzt die neuen Parameter horizontal=True und alignment\n        # border=True rahmt die Toolbar ein\n        # horizontal_alignment=\"right\" schiebt alles nach rechts\n        with st.container(horizontal=True, horizontal_alignment=\"right\"):\n            \n            if not is_error:\n                # 1. Status Text (wird jetzt links von den Buttons angezeigt, aber rechtsb√ºndig im Container)\n                st.write(\"hier die Antwort:\")\n                if ex.get(\"feedback\") == 1:\n                    st.caption(\"‚úÖ Hilfreich\")\n                elif ex.get(\"feedback\") == 0:\n                    st.caption(\"‚ùå Nicht hilfreich\")\n                \n                # 2. Die Buttons (einfach untereinander im Code, erscheinen nebeneinander im UI)\n                \n                # Like\n                type_primary_up = ex.get(\"feedback\") == 1\n                if st.button(\"üëç\", key=f\"up_{ex['_id']}\", type=\"primary\" if type_primary_up else \"secondary\", help=\"Hilfreich\"):\n                    handle_feedback_change(ex, 1)\n\n                # Dislike\n                type_primary_down = ex.get(\"feedback\") == 0\n                if st.button(\"üëé\", key=f\"down_{ex['_id']}\", type=\"primary\" if type_primary_down else \"secondary\", help=\"Nicht hilfreich\"):\n                    handle_feedback_change(ex, 0)\n\n                # Comment Popover\n                with st.popover(\"üí¨\", help=\"Notiz hinzuf√ºgen\"):\n                    msg = st.text_area(\"Notiz:\", value=ex.get(\"feedback_message\", \"\"), key=f\"txt_{ex['_id']}\")\n                    if st.button(\"Speichern\", key=f\"save_{ex['_id']}\"):\n                        ex[\"feedback_message\"] = msg\n                        db.update_exchange_feedback_message(ex[\"_id\"], msg)\n                        st.success(\"Gespeichert!\")\n                        time.sleep(0.5)\n                        st.rerun()\n\n                # Download\n                st.download_button(\n                    label=\"üì•\",\n                    data=ex[\"answer\"],\n                    file_name=f\"response_{ex['_id']}.md\",\n                    mime=\"text/markdown\",\n                    key=f\"dl_{ex['_id']}\",\n                    help=\"Als Markdown herunterladen\"\n                )\n\n                # Delete\n                if st.button(\"üóëÔ∏è\", key=f\"del_{ex['_id']}\", help=\"Nachricht l√∂schen\", type=\"secondary\"):\n                    handle_delete_exchange(current_chat_name, ex)\n\n            else:\n                # Fehlerfall\n                st.error(\"‚ö†Ô∏è Fehler\")\n                if st.button(\"üóëÔ∏è L√∂schen\", key=f\"del_err_{ex['_id']}\"):\n                    handle_delete_exchange(current_chat_name, ex)\n\n        # --- 2. CONTENT (Antwort) ---\n        with st.container(height=500, border=True):\n             render_text_with_mermaid(ex[\"answer\"], should_stream=False)"
            start_line: 284
            end_line: 349
            context:
              calls[4]: database.db.update_exchange_feedback_message,frontend.frontend.handle_delete_exchange,frontend.frontend.handle_feedback_change,frontend.frontend.render_text_with_mermaid
              called_by[1]: frontend.frontend
        classes[0]:
    "schemas/types.py":
      ast_nodes:
        imports[5]: typing.List,typing.Optional,typing.Literal,pydantic.BaseModel,pydantic.ValidationError
        functions[0]:
        classes[15]:
          - mode: class_analysis
            identifier: schemas.types.ParameterDescription
            name: ParameterDescription
            docstring: Describes a single parameter of a function.
            source_code: "class ParameterDescription(BaseModel):\n    \"\"\"Describes a single parameter of a function.\"\"\"\n    name: str\n    type: str\n    description: str"
            start_line: 6
            end_line: 10
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ReturnDescription
            name: ReturnDescription
            docstring: Describes the return value of a function.
            source_code: "class ReturnDescription(BaseModel):\n    \"\"\"Describes the return value of a function.\"\"\"\n    name: str\n    type: str\n    description: str"
            start_line: 12
            end_line: 16
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.UsageContext
            name: UsageContext
            docstring: Describes the calling context of a function.
            source_code: "class UsageContext(BaseModel):\n    \"\"\"Describes the calling context of a function.\"\"\"\n    calls: str\n    called_by: str"
            start_line: 18
            end_line: 21
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionDescription
            name: FunctionDescription
            docstring: Contains the detailed analysis of a function's purpose and signature.
            source_code: "class FunctionDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a function's purpose and signature.\"\"\"\n    overall: str\n    parameters: List[ParameterDescription]\n    returns: List[ReturnDescription]\n    usage_context: UsageContext"
            start_line: 23
            end_line: 28
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionAnalysis
            name: FunctionAnalysis
            docstring: The main model representing the entire JSON schema for a function.
            source_code: "class FunctionAnalysis(BaseModel):\n    \"\"\"The main model representing the entire JSON schema for a function.\"\"\"\n    identifier: str\n    description: FunctionDescription\n    error: Optional[str] = None"
            start_line: 30
            end_line: 34
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ConstructorDescription
            name: ConstructorDescription
            docstring: Describes the __init__ method of a class.
            source_code: "class ConstructorDescription(BaseModel):\n    \"\"\"Describes the __init__ method of a class.\"\"\"\n    description: str\n    parameters: List[ParameterDescription]"
            start_line: 39
            end_line: 42
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassContext
            name: ClassContext
            docstring: Describes the class's external dependencies and primary points of instantiation.
            source_code: "class ClassContext(BaseModel):\n    \"\"\"Describes the class's external dependencies and primary points of instantiation.\"\"\"\n    dependencies: str\n    instantiated_by: str"
            start_line: 44
            end_line: 47
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassDescription
            name: ClassDescription
            docstring: "Contains the detailed analysis of a class's purpose, constructor, and methods."
            source_code: "class ClassDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a class's purpose, constructor, and methods.\"\"\"\n    overall: str\n    init_method: ConstructorDescription\n    methods: List[FunctionAnalysis]\n    usage_context: ClassContext"
            start_line: 49
            end_line: 54
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassAnalysis
            name: ClassAnalysis
            docstring: The main model for the entire JSON schema for a class.
            source_code: "class ClassAnalysis(BaseModel):\n    \"\"\"The main model for the entire JSON schema for a class.\"\"\"\n    identifier: str\n    description: ClassDescription\n    error: Optional[str] = None"
            start_line: 56
            end_line: 60
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.CallInfo
            name: CallInfo
            docstring: "Represents a specific call event from the relationship analyzer.\nUsed in 'called_by' and 'instantiated_by' lists."
            source_code: "class CallInfo(BaseModel):\n    \"\"\"\n    Represents a specific call event from the relationship analyzer.\n    Used in 'called_by' and 'instantiated_by' lists.\n    \"\"\"\n    file: str\n    function: str  # Name des Aufrufers\n    mode: str      # z.B. 'method', 'function', 'module'\n    line: int"
            start_line: 65
            end_line: 73
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionContextInput
            name: FunctionContextInput
            docstring: Structured context for analyzing a function.
            source_code: "class FunctionContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a function.\"\"\"\n    calls: List[str]\n    called_by: List[CallInfo]"
            start_line: 78
            end_line: 81
            context:
              dependencies[0]:
              instantiated_by[1]: backend.main.main_workflow
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionAnalysisInput
            name: FunctionAnalysisInput
            docstring: The required input to generate a FunctionAnalysis object.
            source_code: "class FunctionAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a FunctionAnalysis object.\"\"\"\n    mode: Literal[\"function_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: FunctionContextInput"
            start_line: 83
            end_line: 89
            context:
              dependencies[0]:
              instantiated_by[1]: backend.main.main_workflow
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.MethodContextInput
            name: MethodContextInput
            docstring: Structured context for a classes methods
            source_code: "class MethodContextInput(BaseModel):\n    \"\"\"Structured context for a classes methods\"\"\"\n    identifier: str\n    calls: List[str]\n    called_by: List[CallInfo]\n    args: List[str]\n    docstring: Optional[str]"
            start_line: 94
            end_line: 100
            context:
              dependencies[0]:
              instantiated_by[1]: backend.main.main_workflow
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassContextInput
            name: ClassContextInput
            docstring: Structured context for analyzing a class.
            source_code: "class ClassContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a class.\"\"\"\n    dependencies: List[str]\n    instantiated_by: List[CallInfo]\n    method_context: List[MethodContextInput]"
            start_line: 102
            end_line: 106
            context:
              dependencies[0]:
              instantiated_by[2]: backend.HelperLLM.main_orchestrator,backend.main.main_workflow
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassAnalysisInput
            name: ClassAnalysisInput
            docstring: The required input to generate a ClassAnalysis object.
            source_code: "class ClassAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a ClassAnalysis object.\"\"\"\n    mode: Literal[\"class_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: ClassContextInput"
            start_line: 108
            end_line: 114
            context:
              dependencies[0]:
              instantiated_by[2]: backend.HelperLLM.main_orchestrator,backend.main.main_workflow
              method_context[0]:
analysis_results:
  functions:
    backend.AST_Schema.path_to_module:
      identifier: backend.AST_Schema.path_to_module
      description:
        overall: "This function converts a given file path into a Python module path. It first determines the relative path of the file with respect to the project root. It then removes the '.py' extension if present and replaces operating system path separators with dots. Finally, it handles special cases for `__init__.py` files by removing the `.__init__` suffix to yield the correct package module path."
        parameters[2]{name,type,description}:
          filepath,str,The absolute path to the Python file to be converted.
          project_root,str,The absolute path to the root directory of the project.
        returns[1]{name,type,description}:
          module_path,str,The Python module path derived from the input filepath.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    backend.File_Dependency.build_file_dependency_graph:
      identifier: backend.File_Dependency.build_file_dependency_graph
      description:
        overall: "This function constructs a file dependency graph for a given Python file using NetworkX. It initializes a `FileDependencyGraph` visitor with the specified filename and repository root, then processes the provided Abstract Syntax Tree (AST) to identify import dependencies. The function iterates through the collected import dependencies, adding nodes for callers and callees, and edges representing the import relationships to a NetworkX directed graph. Finally, the populated dependency graph is returned."
        parameters[3]{name,type,description}:
          filename,str,The path to the file for which the dependency graph is being built.
          tree,AST,The Abstract Syntax Tree (AST) of the file to be analyzed for dependencies.
          repo_root,str,"The root directory of the repository, used for resolving relative import paths."
        returns[1]{name,type,description}:
          graph,nx.DiGraph,A NetworkX directed graph representing the file-level import dependencies.
        usage_context:
          calls: This function calls backend.File_Dependency.FileDependencyGraph.
          called_by: This function is called by no other functions.
      error: null
    backend.File_Dependency.build_repository_graph:
      identifier: backend.File_Dependency.build_repository_graph
      description:
        overall: "This function constructs a global directed graph representing the dependencies between Python files within a given Git repository. It iterates through all files in the repository, filtering for Python files. For each Python file, it parses its content and uses a helper function to build a file-specific dependency graph. The nodes and edges from these individual file graphs are then integrated into a single, comprehensive repository-wide dependency graph, which is ultimately returned."
        parameters[1]{name,type,description}:
          repository,GitRepository,The GitRepository object representing the repository whose file dependencies are to be analyzed.
        returns[1]{name,type,description}:
          global_graph,nx.DiGraph,"A NetworkX directed graph where nodes represent files or entities within files, and edges indicate dependencies between them across the repository."
        usage_context:
          calls: This function calls backend.File_Dependency.build_file_dependency_graph.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.File_Dependency.get_all_temp_files:
      identifier: backend.File_Dependency.get_all_temp_files
      description:
        overall: "This function, `get_all_temp_files`, is designed to locate all Python files within a specified directory and its subdirectories. It takes a directory path as input, resolves it to an absolute path, and then recursively searches for all files with a '.py' extension. The function returns a list of these Python files, with each file's path represented as a `pathlib.Path` object relative to the initial input directory."
        parameters[1]{name,type,description}:
          directory,str,The path to the root directory from which to start searching for Python files.
        returns[1]{name,type,description}:
          all_files,"list[Path]","A list of pathlib.Path objects, where each object represents a Python file found within the specified directory and its subdirectories. The paths are relative to the input 'directory'."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    backend.HelperLLM.main_orchestrator:
      identifier: backend.HelperLLM.main_orchestrator
      description:
        overall: "This function serves as a dummy data and processing loop designed for testing the LLMHelper class. It defines pre-computed analysis inputs and outputs for several example functions (add_item, check_stock, generate_report) and a class (InventoryManager) using Pydantic models. The orchestrator then initializes an LLMHelper instance with API key and prompt paths, calls the helper to generate documentation for the defined functions, and processes the results by logging and storing them in a final documentation structure. Finally, it prints the aggregated documentation."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: "This function calls backend.HelperLLM.LLMHelper, schemas.types.ClassAnalysisInput, and schemas.types.ClassContextInput."
          called_by: This function calls no other functions.
      error: null
    backend.callgraph.make_safe_dot:
      identifier: backend.callgraph.make_safe_dot
      description:
        overall: "This function takes a NetworkX directed graph and a file path as input. It creates a copy of the input graph and relabels its nodes with safe, generic identifiers (e.g., \"n0\", \"n1\") to ensure compatibility with DOT file format. The original node names are preserved by assigning them as a 'label' attribute to the new nodes. Finally, the modified graph is written to the specified output path as a DOT file."
        parameters[2]{name,type,description}:
          graph,nx.DiGraph,The NetworkX directed graph to be processed and written to a DOT file.
          out_path,str,The file path where the DOT representation of the graph will be saved.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.callgraph.build_filtered_callgraph:
      identifier: backend.callgraph.build_filtered_callgraph
      description:
        overall: "This function constructs a directed call graph for a given Git repository. It first identifies all Python files within the repository and parses their Abstract Syntax Trees (ASTs) to determine a set of 'own functions'. Subsequently, it iterates through the parsed files again, building a global call graph where edges represent calls between functions. Only calls between functions identified as 'own functions' are included in the final graph, which is returned as a NetworkX DiGraph."
        parameters[1]{name,type,description}:
          repo,GitRepository,The GitRepository object from which to extract files and build the call graph.
        returns[1]{name,type,description}:
          global_graph,nx.DiGraph,"A NetworkX directed graph representing the filtered call graph, containing only 'self-written' functions and their inter-dependencies."
        usage_context:
          calls: This function calls backend.callgraph.CallGraph.
          called_by: This function calls no other functions.
      error: null
    backend.converter.wrap_cdata:
      identifier: backend.converter.wrap_cdata
      description:
        overall: "This function takes a string `content` as input and wraps it within XML CDATA tags. It constructs an f-string that includes the literal `<![CDATA[`, a newline character, the provided content, another newline character, and finally the literal `]]>`. The primary purpose is to escape content for XML output, ensuring that special characters within the content are treated as literal data rather than markup."
        parameters[1]{name,type,description}:
          content,str,The string data to be enclosed within CDATA tags.
        returns[1]{name,type,description}:
          wrapped_content,str,"A string containing the original content wrapped in CDATA tags, including leading and trailing newlines."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.extract_output_content:
      identifier: backend.converter.extract_output_content
      description:
        overall: "This function processes a list of notebook output objects to extract various forms of content, including text, images, and error messages. It iterates through each output, identifying its type. For display data or execution results, it prioritizes extracting PNG images, then JPEG, encoding them as Base64 and storing them in the provided `image_list` while generating an XML placeholder. If no image is found, it extracts plain text. Stream outputs are appended directly, and error outputs are formatted as strings. The function returns a list of these extracted content snippets."
        parameters[2]{name,type,description}:
          outputs,iterable,"An iterable of output objects, typically from a notebook execution, which can contain display data, execution results, stream data, or error information."
          image_list,"list[dict]","A mutable list that will be populated with dictionaries, each containing the 'mime_type' and Base64 encoded 'data' for any extracted images. This list is modified in-place."
        returns[1]{name,type,description}:
          extracted_xml_snippets,"list[str]","A list of strings. Each string represents either extracted plain text, an XML-like placeholder for an image (referencing its index in `image_list`), or a formatted error message."
        usage_context:
          calls: This function calls backend.converter.process_image.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.process_image:
      identifier: backend.converter.process_image
      description:
        overall: "The process_image function is designed to extract and format base64-encoded image data based on a given MIME type. It expects to find the image data within an external 'data' dictionary and to append processed image information to an external 'image_list'. The function first checks for the 'mime_type' in 'data', then retrieves and cleans the base64 string by removing newline characters. It then constructs a dictionary containing the 'mime_type' and cleaned data, appending it to 'image_list', and returns a unique placeholder string. If any error occurs during the decoding or processing, an error message is returned. If the 'mime_type' is not present in 'data', the function returns None."
        parameters[1]{name,type,description}:
          mime_type,str,"The MIME type of the image to be processed (e.g., 'image/png', 'image/jpeg')."
        returns[3]{name,type,description}:
          image_placeholder_tag,str,"A string representing an image placeholder tag, including the index of the image in 'image_list' and its MIME type, if processing is successful."
          error_message,str,"An error message string indicating that the image could not be decoded, along with the exception details, if an error occurs during processing."
          None,NoneType,Returns None if the specified 'mime_type' is not found in the 'data' dictionary.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.convert_notebook_to_xml:
      identifier: backend.converter.convert_notebook_to_xml
      description:
        overall: "This function `convert_notebook_to_xml` takes the raw content of a Jupyter notebook file as a string and transforms it into an XML representation. It first attempts to parse the input string as a notebook using `nbformat`. If parsing fails due to `NotJSONError`, it returns an error XML string. Otherwise, it iterates through each cell of the notebook, formatting markdown and code cells into XML `<CELL>` tags. Code cell outputs are processed to generate snippets and extract images, which are also included as `<CELL type=\"output\">` tags. Finally, it returns the concatenated XML parts and a list of any extracted images."
        parameters[1]{name,type,description}:
          file_content,str,The raw string content of the Jupyter notebook file to be converted.
        returns[2]{name,type,description}:
          xml_output,str,"A string containing the XML representation of the notebook cells. In case of a parsing error, it returns an error message string."
          extracted_images,"list[str]","A list of strings, where each string represents extracted image data or identifiers from the code cell outputs."
        usage_context:
          calls: This function calls backend.converter.extract_output_content and backend.converter.wrap_cdata.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.process_repo_notebooks:
      identifier: backend.converter.process_repo_notebooks
      description:
        overall: "This function processes a list of repository files to identify and convert Jupyter notebooks. It filters the input list to find files with the '.ipynb' extension. For each identified notebook, it extracts its content and then calls 'convert_notebook_to_xml' to transform the notebook into XML format and extract associated images. The function aggregates these conversion results, storing the XML output and images for each notebook, keyed by its file path, and returns them as a dictionary."
        parameters[1]{name,type,description}:
          repo_files,"List[object]","A list of objects, where each object represents a file from a repository and is expected to have 'path' (string) and 'content' (any) attributes."
        returns[1]{name,type,description}:
          results,"Dict[str, Dict[str, Any]]",A dictionary where keys are the paths of processed notebooks (string) and values are dictionaries containing the 'xml' output (string) and 'images' (list of image data) generated from the notebook conversion.
        usage_context:
          calls: This function calls backend.converter.convert_notebook_to_xml.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.create_savings_chart:
      identifier: backend.main.create_savings_chart
      description:
        overall: "This function generates a bar chart to visually compare two token counts, specifically for JSON and TOON formats, and highlights a calculated savings percentage. It utilizes the matplotlib.pyplot library to create the visualization, setting a title, axis labels, and displaying the numerical token values directly above each bar. The resulting chart is then saved to a specified file path and the plot is closed."
        parameters[4]{name,type,description}:
          json_tokens,int,The number of tokens associated with the JSON format.
          toon_tokens,int,The number of tokens associated with the TOON format.
          savings_percent,float,The calculated percentage representing the token savings.
          output_path,str,The file path where the generated bar chart image will be saved.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    backend.main.calculate_net_time:
      identifier: backend.main.calculate_net_time
      description:
        overall: "This function calculates the net processing time by subtracting estimated sleep durations, primarily for models prefixed with 'gemini-'. It first determines the total elapsed time between a start and end time. If the model is not a 'gemini-' model or if there are no items, it returns the total duration or zero, respectively. For 'gemini-' models, it estimates the total sleep time based on the number of batches and a fixed sleep duration per batch, then subtracts this from the total duration to yield the net time."
        parameters[5]{name,type,description}:
          start_time,datetime.datetime | float,The starting timestamp or numeric time value of the process.
          end_time,datetime.datetime | float,The ending timestamp or numeric time value of the process.
          total_items,int,The total number of items processed.
          batch_size,int,The number of items processed per batch.
          model_name,str,"The name of the model being used, which determines if sleep times are factored in."
        returns[1]{name,type,description}:
          net_time,float,"The calculated net duration, after accounting for potential rate-limit sleep times, or the total duration if no sleep times are applicable. Returns 0 if total_items is 0."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.main_workflow:
      identifier: backend.main.main_workflow
      description:
        overall: "The main_workflow function orchestrates a comprehensive analysis of a GitHub repository. It begins by extracting API keys and LLM model configurations from the input, then clones the specified repository. The workflow proceeds to extract basic project information, construct a file tree, analyze code relationships, and build an Abstract Syntax Tree (AST) schema, which is then enriched with relationship data. Subsequently, it prepares and dispatches analysis tasks for individual functions and classes to a Helper LLM. Finally, a Main LLM synthesizes all gathered data into a comprehensive final report, which is then saved along with performance metrics and token savings analysis."
        parameters[4]{name,type,description}:
          input,str,"The initial user input, expected to contain a GitHub repository URL for analysis."
          api_keys,dict,"A dictionary containing various API keys (e.g., 'gemini', 'gpt', 'scadsllm') and base URLs ('scadsllm_base_url', 'ollama') required for LLM interactions."
          model_names,dict,"A dictionary specifying the names of the LLM models to be used for helper and main tasks (e.g., 'helper', 'main')."
          status_callback,callable | None,An optional callback function used to provide real-time status updates during the workflow execution.
        returns[1]{name,type,description}:
          result,dict,A dictionary containing the final generated report under the 'report' key and various performance and token usage metrics under the 'metrics' key.
        usage_context:
          calls: "This function calls backend.AST_Schema.ASTAnalyzer, backend.AST_Schema.ASTAnalyzer.analyze_repository, backend.AST_Schema.ASTAnalyzer.merge_relationship_data, backend.HelperLLM.LLMHelper, backend.HelperLLM.LLMHelper.generate_for_classes, backend.HelperLLM.LLMHelper.generate_for_functions, backend.MainLLM.MainLLM, backend.MainLLM.MainLLM.call_llm, backend.basic_info.ProjektInfoExtractor, backend.basic_info.ProjektInfoExtractor.extrahiere_info, backend.getRepo.GitRepository, backend.main.calculate_net_time, backend.main.create_savings_chart, backend.main.update_status, backend.relationship_analyzer.ProjectAnalyzer, backend.relationship_analyzer.ProjectAnalyzer.analyze, backend.relationship_analyzer.ProjectAnalyzer.get_raw_relationships, schemas.types.ClassAnalysisInput, schemas.types.ClassContextInput, schemas.types.FunctionAnalysisInput, schemas.types.FunctionContextInput, and schemas.types.MethodContextInput."
          called_by: This function is called by no other functions.
      error: null
    backend.main.update_status:
      identifier: backend.main.update_status
      description:
        overall: "This function, `update_status`, is designed to handle status messages. It takes a single message as input. If a `status_callback` function is available and truthy, it invokes this callback with the provided message. Additionally, it logs the message at the INFO level using the `logging` module, ensuring that all status updates are recorded."
        parameters[1]{name,type,description}:
          msg,str,The status message to be processed and logged.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.notebook_workflow:
      identifier: backend.main.notebook_workflow
      description:
        overall: "This function orchestrates a workflow to analyze Jupyter notebooks from a given GitHub repository using a Large Language Model (LLM). It begins by extracting a repository URL from the input, cloning the repository, and processing its notebook files into an XML-like structure. It then extracts basic project information and, for each notebook, constructs a specific payload for the chosen LLM, which can be GPT, Gemini, or a custom LLM based on the model name. The function iterates through all notebooks, calls the LLM to generate individual reports, and finally concatenates these reports into a single markdown file, saving it to disk. It also tracks and returns performance metrics for the entire process."
        parameters[4]{name,type,description}:
          input,str,"The input string, expected to contain a GitHub repository URL from which notebooks will be analyzed."
          api_keys,dict,"A dictionary containing API keys for various LLM services (e.g., 'gpt', 'gemini', 'scadsllm', 'ollama') used for authentication."
          model,str,"The name of the Large Language Model to be used for analysis, which dictates which API key and base URL to retrieve."
          status_callback,callable or None,"An optional callback function that receives status messages during the workflow execution, allowing for external progress updates."
        returns[2]{name,type,description}:
          report,str,"The final concatenated markdown report generated by the LLM, summarizing the analysis of all processed notebooks."
          metrics,dict,"A dictionary containing performance metrics for the workflow, including execution times and model information."
        usage_context:
          calls: "This function calls backend.MainLLM.MainLLM, backend.MainLLM.MainLLM.call_llm, backend.basic_info.ProjektInfoExtractor, backend.basic_info.ProjektInfoExtractor.extrahiere_info, backend.converter.process_repo_notebooks, backend.getRepo.GitRepository, backend.main.gemini_payload, and backend.main.update_status."
          called_by: This function is called by no other functions.
      error: null
    backend.main.gemini_payload:
      identifier: backend.main.gemini_payload
      description:
        overall: "This function constructs a multi-part content payload suitable for a Gemini model. It begins by serializing `basic_info` and `nb_path` into an initial JSON context string. The core logic involves parsing `xml_content` to identify and process image placeholders. It extracts text segments appearing before and after these placeholders, adding them as 'text' entries to the payload. For each image placeholder, it retrieves the corresponding base64-encoded image data from the `images` list and embeds it as an 'image_url' entry. The function returns a list of dictionaries, representing the complete structured payload."
        parameters[4]{name,type,description}:
          basic_info,object,"A dictionary or object containing basic information to be included in the payload context, serialized as JSON."
          nb_path,str,"The file path of the current notebook, included in the JSON context."
          xml_content,str,"A string containing the XML structure of the notebook, which may include `<IMAGE_PLACEHOLDER>` tags."
          images,list,"A list of dictionaries, where each dictionary is expected to contain image data, typically with a 'data' key holding a base64 encoded string."
        returns[1]{name,type,description}:
          content_payload,list,"A list of dictionaries, where each dictionary represents a content part (text or image) for the Gemini payload."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.relationship_analyzer.path_to_module:
      identifier: backend.relationship_analyzer.path_to_module
      description:
        overall: "This function converts a given file system path into a Python module path string. It first determines the relative path from a specified project root, handling cases where the file is not within the root by falling back to the base filename. It then removes the '.py' extension if present and replaces directory separators with dots. Finally, it adjusts the module path if it represents an '__init__.py' file by removing the '.__init__' suffix."
        parameters[2]{name,type,description}:
          filepath,str,The absolute or relative path to a Python file.
          project_root,str,"The root directory of the project, used to calculate the relative path."
        returns[1]{name,type,description}:
          module_path,str,The converted Python module path string.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.encrypt_text:
      identifier: database.db.encrypt_text
      description:
        overall: "This function encrypts a given text string using a `cipher_suite` object. It first checks if the input text or the `cipher_suite` itself is empty; if either is true, it returns the original text without encryption. Otherwise, it prepares the text by stripping leading/trailing whitespace and encoding it to bytes, then encrypts it and decodes the resulting bytes back into a string."
        parameters[1]{name,type,description}:
          text,str,The string value to be encrypted.
        returns[1]{name,type,description}:
          encrypted_text,str,"The encrypted string, or the original text if encryption was skipped due to empty input or missing cipher suite."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.decrypt_text:
      identifier: database.db.decrypt_text
      description:
        overall: "This function attempts to decrypt a given string using a pre-initialized `cipher_suite` object. It first checks if the input `text` is empty or if the `cipher_suite` is not available; if either condition is true, it returns the original text without any processing. Otherwise, it prepares the text by stripping whitespace and encoding it to bytes before attempting decryption. The decrypted bytes are then decoded back into a string. If any exception occurs during the decryption process, the function catches it and returns the original, unencrypted text."
        parameters[1]{name,type,description}:
          text,str,"The string value that needs to be decrypted. If this string is empty or if the decryption suite is not configured, the original string will be returned."
        returns[1]{name,type,description}:
          decrypted_string,str,"The successfully decrypted string, or the original input string if decryption fails or is skipped due to initial conditions."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.insert_user:
      identifier: database.db.insert_user
      description:
        overall: "This function creates a new user document using the provided username, name, and password. It hashes the password for secure storage and initializes fields for various API keys (Gemini, Ollama, GPT) as empty strings. The constructed user document is then inserted into a database collection named `dbusers`. The function concludes by returning the unique identifier assigned to the newly inserted user document."
        parameters[3]{name,type,description}:
          username,str,"The unique identifier for the user, which also serves as the document's primary key."
          name,str,The full name of the user.
          password,str,"The plain-text password for the user, which will be hashed before being stored in the database."
        returns[1]{name,type,description}:
          inserted_id,str,"The unique identifier (e.g., ObjectId or username) of the newly inserted user document in the database."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.fetch_all_users:
      identifier: database.db.fetch_all_users
      description:
        overall: "This function, `fetch_all_users`, is designed to retrieve all user records from a database collection. It accesses a collection named `dbusers` and invokes its `find()` method, which typically returns a cursor or an iterable of all documents within that collection. The results are then immediately converted into a standard Python list before being returned by the function. This provides a complete snapshot of all user data stored in `dbusers`."
        parameters[0]:
        returns[1]{name,type,description}:
          users,list,A list containing all user documents retrieved from the 'dbusers' collection.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_user:
      identifier: database.db.fetch_user
      description:
        overall: "The `fetch_user` function is designed to retrieve a single user record from a database collection named `dbusers`. It queries the collection by matching the provided `username` against the `_id` field of the documents. The function returns the first document that satisfies this query, effectively fetching a specific user's data."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user to be fetched from the database.
        returns[1]{name,type,description}:
          user_document,dict,"A dictionary representing the user document if found, otherwise `None`."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_user_name:
      identifier: database.db.update_user_name
      description:
        overall: "This function is designed to update the 'name' field for a specific user in a database collection, identified by their username. It takes the current username, which serves as the document's `_id`, and the new name to be assigned. The function utilizes a database `update_one` operation to modify the user's record. It then returns the count of documents that were successfully modified by this operation."
        parameters[2]{name,type,description}:
          username,str,"The unique identifier of the user whose name is to be updated, corresponding to the `_id` field in the database."
          new_name,str,The new name to be set for the specified user.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_gemini_key:
      identifier: database.db.update_gemini_key
      description:
        overall: "This function is responsible for updating a user's Gemini API key within the database. It takes a username and the new API key as input. The provided Gemini API key is first stripped of leading/trailing whitespace and then encrypted. Finally, the encrypted key is stored in the database for the specified user, and the function returns the count of modified documents."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose Gemini API key is to be updated.
          gemini_api_key,str,"The new Gemini API key to be stored, which will be encrypted before saving."
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation.
        usage_context:
          calls: This function calls database.db.encrypt_text.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_gpt_key:
      identifier: database.db.update_gpt_key
      description:
        overall: "This function is responsible for updating a user's GPT API key within a database. It takes a username and a new API key as input. The provided API key is first stripped of whitespace and then encrypted before being stored. The function then performs an update operation on the 'dbusers' collection, setting the 'gpt_api_key' field for the specified user. It returns the count of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose GPT API key is to be updated.
          gpt_api_key,str,The new GPT API key to be stored for the user. It will be encrypted before storage.
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents modified by the update operation, typically 0 or 1."
        usage_context:
          calls: This function calls database.db.encrypt_text.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_ollama_url:
      identifier: database.db.update_ollama_url
      description:
        overall: "This function updates the Ollama base URL for a specific user in a database. It takes a username and a new Ollama base URL as input. The function uses the provided username to locate the user's record and then updates the 'ollama_base_url' field with the new URL, ensuring any leading/trailing whitespace is removed. It returns the count of modified documents."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose Ollama URL is to be updated.
          ollama_base_url,str,"The new base URL for Ollama, which will be stored after stripping whitespace."
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_opensrc_key:
      identifier: database.db.update_opensrc_key
      description:
        overall: "This function is responsible for updating a user's Open Source API key within a database. It takes a username and the new raw API key as input. The API key is first processed by removing any leading or trailing whitespace, then encrypted using a helper function. Finally, the function attempts to locate the user by their username and update their 'opensrc_api_key' field with the newly encrypted value. It returns an integer indicating how many documents were modified by the update operation."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose Open Source API key is to be updated.
          opensrc_api_key,str,The raw Open Source API key to be encrypted and stored for the user.
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents modified by the update operation, typically 0 or 1."
        usage_context:
          calls: This function calls database.db.encrypt_text.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_opensrc_url:
      identifier: database.db.update_opensrc_url
      description:
        overall: "This function updates the 'opensrc_base_url' field for a specific user in a database collection. It identifies the user by their 'username', which is used as the document's '_id'. The provided 'opensrc_base_url' is stripped of leading/trailing whitespace before being stored. The function then returns the count of documents that were successfully modified by this operation."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose 'opensrc_base_url' is to be updated. This is used to match the document's '_id'.
          opensrc_base_url,str,The new base URL for the open-source repository. This string will have leading/trailing whitespace removed before being saved.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation. Typically 0 or 1 for a unique user identifier.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_gemini_key:
      identifier: database.db.fetch_gemini_key
      description:
        overall: "This function is designed to retrieve a user's Gemini API key from a database. It takes a username as input and queries a 'dbusers' collection to find the corresponding user record. If a user is found, it extracts and returns the 'gemini_api_key' field; otherwise, it returns None. This ensures secure retrieval of API keys based on user identity."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose Gemini API key is to be fetched.
        returns[1]{name,type,description}:
          gemini_api_key,str | None,"The Gemini API key associated with the specified username, or None if the user is not found or the key does not exist."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_ollama_url:
      identifier: database.db.fetch_ollama_url
      description:
        overall: "This function retrieves the Ollama base URL associated with a specific user from a database. It takes a username as input and performs a database query to find a matching user record. If a user is found, it extracts and returns the 'ollama_base_url' field. If no user is found or the 'ollama_base_url' field is absent, the function returns None."
        parameters[1]{name,type,description}:
          username,str,The unique identifier (username) of the user whose Ollama base URL is to be fetched.
        returns[1]{name,type,description}:
          ollama_base_url,str | None,"The Ollama base URL as a string if found, otherwise None."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_gpt_key:
      identifier: database.db.fetch_gpt_key
      description:
        overall: "This function is designed to retrieve a user's GPT API key from a database. It takes a username as input and queries a 'dbusers' collection to locate the corresponding user document. If a user is found, it extracts the 'gpt_api_key' field. The function returns the API key if present, or None if the user is not found or the key is missing."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose GPT API key is to be fetched.
        returns[1]{name,type,description}:
          gpt_api_key,str | None,"The GPT API key associated with the user, or None if the user is not found or the key is not set."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_opensrc_key:
      identifier: database.db.fetch_opensrc_key
      description:
        overall: "The `fetch_opensrc_key` function is designed to retrieve an Open Source API key associated with a specific user from a database. It takes a username as input and queries the `dbusers` collection to locate the corresponding user document. If a user is found, the function extracts the `opensrc_api_key` field from that document. If the user is not found or the key is absent, it returns `None`."
        parameters[1]{name,type,description}:
          username,str,The unique identifier (username) of the user whose Open Source API key is to be fetched.
        returns[1]{name,type,description}:
          opensrc_api_key,str | None,"The Open Source API key for the specified user, or None if the user or key is not found."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.fetch_opensrc_url:
      identifier: database.db.fetch_opensrc_url
      description:
        overall: "The `fetch_opensrc_url` function is designed to retrieve a user's Open Source base URL from a database. It takes a username as input and queries the `dbusers` collection to locate the corresponding user document. The function specifically projects and extracts the `opensrc_base_url` field. If a user is found and the URL exists, it returns that URL; otherwise, it returns `None`."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose Open Source base URL is to be fetched.
        returns[1]{name,type,description}:
          opensrc_base_url,str | None,"The Open Source base URL associated with the user, or `None` if the user is not found or the URL field is missing."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.delete_user:
      identifier: database.db.delete_user
      description:
        overall: The `delete_user` function is designed to remove a specific user record from the `dbusers` collection in a database. It takes a username as input and uses it to locate the corresponding document by matching it against the `_id` field. The function then performs a delete operation and returns the count of documents that were successfully removed.
        parameters[1]{name,type,description}:
          username,str,The unique identifier (username) of the user to be deleted from the database.
        returns[1]{name,type,description}:
          deleted_count,int,"The number of documents deleted from the `dbusers` collection, typically 0 or 1."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.get_decrypted_api_keys:
      identifier: database.db.get_decrypted_api_keys
      description:
        overall: "This function retrieves and decrypts various API keys and base URLs for a specified user from a database. It first attempts to locate the user document using the provided username. If the user is not found, the function immediately returns a tuple of None values. Otherwise, it extracts specific API keys (Gemini, GPT, Opensource) and decrypts them using the `decrypt_text` utility. It also retrieves the Ollama base URL and Opensource base URL directly without decryption. Finally, it returns a tuple containing all the processed API keys and URLs."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose API keys and URLs are to be retrieved.
        returns[5]{name,type,description}:
          gemini_plain,str | None,"The decrypted Gemini API key, or None if the user is not found or the key is absent."
          ollama_plain,str | None,"The Ollama base URL, or None if the user is not found or the URL is absent."
          gpt_plain,str | None,"The decrypted GPT API key, or None if the user is not found or the key is absent."
          opensrc_plain,str | None,"The decrypted Opensource API key, or None if the user is not found or the key is absent."
          opensrc_url,str | None,"The Opensource base URL, or None if the user is not found or the URL is absent."
        usage_context:
          calls: This function calls database.db.decrypt_text.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.insert_chat:
      identifier: database.db.insert_chat
      description:
        overall: "This function creates a new chat entry in the database. It generates a unique identifier using UUID, records the provided username and chat name, and captures the current timestamp. The constructed chat dictionary is then inserted into the 'dbchats' collection, and the unique ID of the newly inserted document is returned."
        parameters[2]{name,type,description}:
          username,str,The username associated with the new chat entry.
          chat_name,str,The name of the chat to be created.
        returns[1]{name,type,description}:
          inserted_id,str,The unique identifier of the newly created chat entry in the database.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_chats_by_user:
      identifier: database.db.fetch_chats_by_user
      description:
        overall: "This function, `fetch_chats_by_user`, is designed to retrieve all chat records associated with a specific user from a database. It takes a username as input and queries the `dbchats` collection. The retrieved chat documents are then sorted by their `created_at` timestamp in ascending order. Finally, the function returns these sorted chat records as a list."
        parameters[1]{name,type,description}:
          username,str,The username for which to fetch the associated chat records.
        returns[1]{name,type,description}:
          chats,list,"A list of chat documents (dictionaries) belonging to the specified user, sorted by their creation timestamp."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.check_chat_exists:
      identifier: database.db.check_chat_exists
      description:
        overall: The `check_chat_exists` function verifies the existence of a chat session within the `dbchats` collection. It takes a username and a chat name as input parameters. The function performs a database query to find a document that matches both the provided username and chat name. It returns a boolean indicating whether such a chat entry was found.
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to be checked.
          chat_name,str,The specific name of the chat to verify.
        returns[1]{name,type,description}:
          chat_exists,bool,"True if a chat matching the username and chat_name exists, False otherwise."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.rename_chat_fully:
      identifier: database.db.rename_chat_fully
      description:
        overall: "The `rename_chat_fully` function is responsible for renaming a chat and all its associated messages or 'exchanges' within a database. It performs two distinct database update operations. First, it updates a single chat entry in the `dbchats` collection, changing the `chat_name` from `old_name` to `new_name` for a specific `username`. Second, it updates all related exchange entries in the `dbexchanges` collection, similarly updating their `chat_name` to reflect the new name for the same `username`. The function returns the count of modified chat entries from the initial update operation."
        parameters[3]{name,type,description}:
          username,str,The username associated with the chat to be renamed.
          old_name,str,The current name of the chat that needs to be changed.
          new_name,str,The new desired name for the chat.
        returns[1]{name,type,description}:
          modified_count,int,The number of chat entries that were modified in the `dbchats` collection.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.insert_exchange:
      identifier: database.db.insert_exchange
      description:
        overall: "This function inserts a new exchange record into a database collection named `dbexchanges`. It generates a unique identifier for the exchange using `uuid.uuid4()` and constructs a dictionary containing various details such as the question, answer, feedback, user information, and performance metrics like helper/main components used, time taken, and token counts. The record also includes a `created_at` timestamp. If the insertion is successful, it returns the new unique ID; otherwise, it catches any exceptions, prints an error message, and returns `None`."
        parameters[13]{name,type,description}:
          question,str,The user's question in the exchange.
          answer,str,The generated answer for the question.
          feedback,str,The feedback provided for the exchange.
          username,str,The username associated with this exchange.
          chat_name,str,The name of the chat session where the exchange occurred.
          helper_used,str,"An optional identifier for the helper component used, defaults to an empty string."
          main_used,str,"An optional identifier for the main component used, defaults to an empty string."
          total_time,str,"An optional string representing the total time taken for the exchange, defaults to an empty string."
          helper_time,str,"An optional string representing the time taken by the helper component, defaults to an empty string."
          main_time,str,"An optional string representing the time taken by the main component, defaults to an empty string."
          json_tokens,int,"An optional integer representing the number of JSON tokens used, defaults to 0."
          toon_tokens,int,"An optional integer representing the number of Toon tokens used, defaults to 0."
          savings_percent,float,"An optional float representing the percentage of savings, defaults to 0.0."
        returns[2]{name,type,description}:
          new_id,str,The unique identifier of the newly inserted exchange if the operation is successful.
          None,None,Returns None if an exception occurs during the database insertion.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_exchanges_by_user:
      identifier: database.db.fetch_exchanges_by_user
      description:
        overall: This function retrieves all exchange records associated with a specific username from a database collection. It queries the database for documents matching the provided username and sorts the results by the 'created_at' timestamp in ascending order. The function then converts these results into a list and returns it.
        parameters[1]{name,type,description}:
          username,str,The username for which to fetch exchange records.
        returns[1]{name,type,description}:
          exchanges,list,"A list of exchange records (likely dictionaries or database documents) associated with the specified username, sorted by their creation timestamp."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_exchanges_by_chat:
      identifier: database.db.fetch_exchanges_by_chat
      description:
        overall: "This function is designed to retrieve a collection of chat exchanges from a database. It queries the `dbexchanges` collection, filtering documents based on a specific username and chat name. The retrieved exchanges are then sorted by their `created_at` timestamp in ascending order. Finally, the function converts the database query result into a list of exchange documents and returns it."
        parameters[2]{name,type,description}:
          username,str,The username used as a criterion to filter the chat exchanges from the database.
          chat_name,str,The name of the chat used as a criterion to filter the chat exchanges from the database.
        returns[1]{name,type,description}:
          exchanges,list,"A list of chat exchange documents that match the provided username and chat name, sorted by their creation date."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.update_exchange_feedback:
      identifier: database.db.update_exchange_feedback
      description:
        overall: "This function updates the 'feedback' field for a specific exchange record in a database. It takes an exchange identifier and an integer feedback value. The function uses a database client, likely `dbexchanges`, to perform an `update_one` operation, targeting the record by its `_id`. It then returns the count of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          exchange_id,Any,"The unique identifier of the exchange record to be updated. Its specific type is inferred from database usage, often a string or ObjectId."
          feedback,int,The integer value representing the feedback to be set for the specified exchange record.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_exchange_feedback_message:
      identifier: database.db.update_exchange_feedback_message
      description:
        overall: This function updates an existing exchange record in the database. It specifically targets a document identified by `exchange_id` and sets its `feedback_message` field to the provided string. The function then returns the number of documents that were modified by this operation.
        parameters[2]{name,type,description}:
          exchange_id,Any,The unique identifier for the exchange document to be updated. This is used to locate the specific record in the database.
          feedback_message,str,The new feedback message string to be set for the identified exchange.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.delete_exchange_by_id:
      identifier: database.db.delete_exchange_by_id
      description:
        overall: This function is responsible for deleting a single exchange record from a database collection. It takes an exchange ID as input and uses it to locate and remove the corresponding document. The function leverages a database client's `delete_one` method to perform the deletion operation. It then reports the number of documents that were successfully deleted.
        parameters[1]{name,type,description}:
          exchange_id,str,The unique identifier (ID) of the exchange record to be deleted from the database.
        returns[1]{name,type,description}:
          deleted_count,int,The number of documents that were deleted by the operation. This will typically be 0 or 1.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.delete_full_chat:
      identifier: database.db.delete_full_chat
      description:
        overall: "This function is designed to completely remove a specific chat and all its associated message exchanges from the database. It operates by first deleting all exchanges linked to the given username and chat name using `dbexchanges.delete_many`. Subsequently, it deletes the chat entry itself from the `dbchats` collection using `dbchats.delete_one`. This two-step process ensures data consistency between chat entries and their related messages. The function returns the count of chat documents that were successfully deleted."
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to be deleted.
          chat_name,str,The name of the chat to be deleted.
        returns[1]{name,type,description}:
          deleted_count,int,The number of chat documents successfully deleted from the database.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.clean_names:
      identifier: frontend.frontend.clean_names
      description:
        overall: "This function processes a list of strings, treating each string as a path or URL. For every item in the input list, it splits the string by the \"/\" character and extracts the last segment. The function then returns a new list containing these cleaned, base names. It is designed to extract the final component of a path or identifier."
        parameters[1]{name,type,description}:
          model_list,list,"A list of strings, where each string is expected to be a path or identifier that may contain '/' characters."
        returns[1]{name,type,description}:
          cleaned_names,"List[str]",A new list containing the last segment of each input string after splitting by '/'.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.get_filtered_models:
      identifier: frontend.frontend.get_filtered_models
      description:
        overall: "This function filters a given list of models (`source_list`) based on a specified `category_name`. It retrieves relevant keywords from a global `CATEGORY_KEYWORDS` dictionary. If the category's keywords include 'STANDARD', it returns only those models from the `source_list` that are also present in a `STANDARD_MODELS` list. Otherwise, it iterates through the `source_list` and collects models where any of the category's keywords are found within the model's name (case-insensitive). The function returns the filtered list if any matches are found; otherwise, it returns the original `source_list`."
        parameters[2]{name,type,description}:
          source_list,list,The list of models (likely strings) to be filtered.
          category_name,str,The name of the category to use for filtering the models.
        returns[1]{name,type,description}:
          filtered_models,list,"A list of models filtered according to the specified category, or the original list if no models match the filter criteria."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.save_gemini_cb:
      identifier: frontend.frontend.save_gemini_cb
      description:
        overall: "This function, `save_gemini_cb`, serves as a callback to handle the saving of a Gemini API key. It retrieves the potential new key from the Streamlit session state. If a non-empty key is found, it proceeds to update the user's Gemini key in the database via `db.update_gemini_key`. After a successful update, it clears the session state variable associated with the input key and displays a success toast message to the user."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: This function calls database.db.update_gemini_key.
          called_by: This function is called by no other functions.
      error: null
    frontend.frontend.save_ollama_cb:
      identifier: frontend.frontend.save_ollama_cb
      description:
        overall: "This function, `save_ollama_cb`, is designed to handle the saving of a new Ollama URL. It retrieves the potential new URL from the Streamlit session state. If a valid URL is found, it proceeds to update this URL in the database, associating it with the current user's username, also obtained from the session state. Upon successful update, a confirmation toast message is displayed to the user. This function primarily performs side effects related to user configuration and database interaction."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: This function calls database.db.update_ollama_url.
          called_by: This function is called by no other functions.
      error: null
    frontend.frontend.load_data_from_db:
      identifier: frontend.frontend.load_data_from_db
      description:
        overall: "This function, `load_data_from_db`, is designed to consistently load chat and exchange data from the database into the Streamlit session state for a specified user. It first checks if the data for the given username is already loaded; if not, it proceeds to initialize the session state. The function fetches predefined chats and then individual exchanges from the database, organizing them into the `st.session_state.chats` dictionary. It includes logic to handle legacy exchanges that might not have a predefined chat and ensures a default chat is created if no chats exist for the user. Finally, it sets the `active_chat` in the session state."
        parameters[1]{name,type,description}:
          username,str,The username for whom to load chat and exchange data from the database.
        returns[0]:
        usage_context:
          calls: "This function calls database.db.fetch_chats_by_user, database.db.fetch_exchanges_by_user, and database.db.insert_chat."
          called_by: This function is called by no other functions.
      error: null
    frontend.frontend.handle_feedback_change:
      identifier: frontend.frontend.handle_feedback_change
      description:
        overall: "This function processes a feedback change for a given exchange record. It updates the 'feedback' key within the provided 'ex' dictionary-like object with the new 'val'. Concurrently, it calls a database utility to persist this feedback change, using the '_id' from the 'ex' object. Finally, it triggers a full rerun of the Streamlit application, likely to refresh the user interface and display the updated feedback."
        parameters[2]{name,type,description}:
          ex,dict,"A dictionary-like object representing an exchange record, expected to contain 'feedback' and '_id' keys."
          val,Any,The new feedback value to be assigned to the exchange record and updated in the database.
        returns[1]{name,type,description}:
          None,None,"This function does not return any value; it performs updates to an object and a database, and triggers a Streamlit rerun as side effects."
        usage_context:
          calls: This function calls database.db.update_exchange_feedback.
          called_by: This function is called by no other functions.
      error: null
    frontend.frontend.handle_delete_exchange:
      identifier: frontend.frontend.handle_delete_exchange
      description:
        overall: "This function handles the deletion of a specific exchange, removing it from both the database and the active Streamlit session state. It takes a chat name and the exchange object as input. The function first deletes the exchange from the database using its unique identifier. It then checks if the chat and the specific exchange exist within the Streamlit session state and removes the exchange if found. Finally, it triggers a Streamlit rerun to update the UI."
        parameters[2]{name,type,description}:
          chat_name,str,The name of the chat from which the exchange should be removed from the session state.
          ex,dict,"The exchange object to be deleted, which is expected to contain an '_id' field for database identification."
        returns[0]:
        usage_context:
          calls: This function calls database.db.delete_exchange_by_id.
          called_by: This function is called by no other functions.
      error: null
    frontend.frontend.handle_delete_chat:
      identifier: frontend.frontend.handle_delete_chat
      description:
        overall: "This function handles the deletion of a specific chat for a given user. It first removes the chat from the database using `db.delete_full_chat`. Subsequently, it cleans up the Streamlit session state by removing the chat from `st.session_state.chats`. If other chats exist, the first one becomes the active chat; otherwise, a new default chat named 'Chat 1' is created and set as active. Finally, it triggers a Streamlit rerun to update the UI."
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to be deleted.
          chat_name,str,The name of the chat to be deleted.
        returns[0]:
        usage_context:
          calls: This function calls database.db.delete_full_chat and database.db.insert_chat.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.extract_repo_name:
      identifier: frontend.frontend.extract_repo_name
      description:
        overall: "This function aims to extract a repository name from a given text string. It first attempts to find a URL within the text using a regular expression. If a URL is successfully matched, it then parses this URL to isolate the path component. The last segment of the URL path is considered the potential repository name, and any '.git' suffix is removed for cleanliness. If no URL is found or no valid repository name can be derived, the function returns None."
        parameters[1]{name,type,description}:
          text,str,The input string that may contain a URL from which a repository name needs to be extracted.
        returns[1]{name,type,description}:
          repo_name,str | None,"The extracted repository name as a string, or None if no URL is found or a repository name cannot be determined."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.stream_text_generator:
      identifier: frontend.frontend.stream_text_generator
      description:
        overall: "This function is a generator that takes a string of text and yields its words one by one. It splits the input text by spaces and then iterates through each word. After yielding a word followed by a space, it introduces a small delay of 0.01 seconds using `time.sleep()` before processing the next word. This creates a streaming effect for the text."
        parameters[1]{name,type,description}:
          text,str,The input string of text to be streamed word by word.
        returns[1]{name,type,description}:
          word_with_space,str,"A single word from the input text, followed by a space, yielded sequentially."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.render_text_with_mermaid:
      identifier: frontend.frontend.render_text_with_mermaid
      description:
        overall: "This function processes a given markdown text, identifying and rendering embedded Mermaid diagrams. It splits the input text into parts based on '```mermaid``` delimiters. Non-Mermaid sections are rendered as standard markdown, with an option to stream the text. Mermaid code blocks are first attempted to be rendered using `st_mermaid`; if this fails, the Mermaid code is displayed as a code block. The function handles empty input by returning early."
        parameters[2]{name,type,description}:
          markdown_text,str,"The input text, potentially containing markdown and embedded Mermaid diagrams."
          should_stream,bool,A flag indicating whether non-Mermaid text parts should be streamed using `st.write_stream` or rendered directly with `st.markdown`. Defaults to False.
        returns[0]:
        usage_context:
          calls: This function calls frontend.frontend.stream_text_generator.
          called_by: This function is called by no other functions.
      error: null
    frontend.frontend.render_exchange:
      identifier: frontend.frontend.render_exchange
      description:
        overall: "This function renders a single chat exchange, comprising a user's question and an assistant's answer, within a Streamlit application. It first displays the user's question. For the assistant's response, it constructs a dynamic toolbar featuring feedback mechanisms (like/dislike, comment popover), a download option, and a delete button. If the answer indicates an error, a simplified error message with a delete option is shown. Finally, the function renders the main content of the assistant's answer, potentially including Mermaid diagrams."
        parameters[2]{name,type,description}:
          ex,dict,"A dictionary representing a single chat exchange, expected to contain keys such as 'question', 'answer', '_id', 'feedback', and 'feedback_message'."
          current_chat_name,str,"The name of the current chat session, used to provide context for operations like deleting an exchange."
        returns[0]:
        usage_context:
          calls: "This function calls database.db.update_exchange_feedback_message, frontend.frontend.handle_delete_exchange, frontend.frontend.handle_feedback_change, and frontend.frontend.render_text_with_mermaid."
          called_by: This function is called by no other functions.
      error: null
  classes:
    backend.AST_Schema.ASTVisitor:
      identifier: backend.AST_Schema.ASTVisitor
      description:
        overall: "The ASTVisitor class extends `ast.NodeVisitor` to systematically traverse the Abstract Syntax Tree (AST) of Python source code. Its primary purpose is to extract and organize structured metadata about imports, class definitions, and function definitions (including asynchronous ones) found within a specified file. It populates an internal `schema` dictionary with this information, ensuring that nested methods are correctly associated with their parent classes."
        init_method:
          description: "The constructor initializes the ASTVisitor instance by storing the raw source code, file path, and project root. It then calculates the module's full path and sets up an empty `schema` dictionary to accumulate extracted AST information. Additionally, it initializes `_current_class` to `None` to track the context of nested definitions."
          parameters[3]{name,type,description}:
            source_code,str,The raw Python source code string of the file being analyzed.
            file_path,str,The absolute path to the Python file being visited.
            project_root,str,"The root directory of the project, used for calculating module paths."
        methods[5]:
          - identifier: visit_Import
            description:
              overall: "This method is invoked by the `ast.NodeVisitor` framework when an `ast.Import` node is encountered during AST traversal. It iterates through each alias in the import statement and appends the full name of the imported module to the `imports` list within the `self.schema` dictionary. After processing the current node, it ensures continued traversal of its children by calling `self.generic_visit(node)`."
              parameters[1]{name,type,description}:
                node,ast.Import,The AST node representing an 'import' statement.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other functions or methods beyond its own `generic_visit` which is part of the AST traversal mechanism.
                called_by: This method is called by the `ast.NodeVisitor` framework when an `ast.Import` node is encountered during AST traversal.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method handles `ast.ImportFrom` nodes, which correspond to 'from ... import ...' statements in Python. For each alias within the import statement, it constructs a fully qualified import string by combining the module name and the alias name, then appends this string to the `imports` list in `self.schema`. It concludes by calling `self.generic_visit(node)` to ensure recursive traversal of the AST."
              parameters[1]{name,type,description}:
                node,ast.ImportFrom,The AST node representing a 'from ... import ...' statement.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other functions or methods beyond its own `generic_visit` which is part of the AST traversal mechanism.
                called_by: This method is called by the `ast.NodeVisitor` framework when an `ast.ImportFrom` node is encountered during AST traversal.
            error: null
          - identifier: visit_ClassDef
            description:
              overall: "This method processes `ast.ClassDef` nodes, which represent class definitions in the source code. It generates a unique identifier for the class, extracts its name, docstring, and the exact source code segment. This information, along with line numbers, is stored in a dictionary and appended to the `classes` list within `self.schema`. It temporarily sets `_current_class` to the newly created class info to correctly associate nested methods, then performs a generic visit, and finally resets `_current_class` to `None`."
              parameters[1]{name,type,description}:
                node,ast.ClassDef,The AST node representing a class definition.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other functions or methods beyond its own `generic_visit` which is part of the AST traversal mechanism.
                called_by: This method is called by the `ast.NodeVisitor` framework when an `ast.ClassDef` node is encountered during AST traversal.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method handles `ast.FunctionDef` nodes, distinguishing between methods defined within a class and top-level functions. If `_current_class` is set, it extracts method details like identifier, name, arguments, docstring, and line numbers, appending this as `method_context_info` to the current class's context. Otherwise, for top-level functions, it creates `func_info` with similar details and adds it to the `functions` list in `self.schema`. It ensures continued AST traversal by calling `self.generic_visit(node)`."
              parameters[1]{name,type,description}:
                node,ast.FunctionDef,The AST node representing a function definition.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other functions or methods beyond its own `generic_visit` which is part of the AST traversal mechanism.
                called_by: This method is called by the `ast.NodeVisitor` framework when an `ast.FunctionDef` node is encountered during AST traversal.
            error: null
          - identifier: visit_AsyncFunctionDef
            description:
              overall: "This method is specifically designed to handle `ast.AsyncFunctionDef` nodes, which represent asynchronous function definitions. Its implementation is straightforward: it simply delegates the entire processing logic to the `visit_FunctionDef` method. This ensures that asynchronous functions are treated identically to regular functions in terms of how their metadata is extracted and stored within the schema, maintaining consistency in the collected data."
              parameters[1]{name,type,description}:
                node,ast.AsyncFunctionDef,The AST node representing an asynchronous function definition.
              returns[0]:
              usage_context:
                calls: This method calls the `visit_FunctionDef` method to process asynchronous function definitions.
                called_by: This method is called by the `ast.NodeVisitor` framework when an `ast.AsyncFunctionDef` node is encountered during AST traversal.
            error: null
        usage_context:
          dependencies: This class depends on `backend.AST_Schema.path_to_module` for resolving module paths.
          instantiated_by: This class is not explicitly instantiated by any known callers in the provided context.
      error: null
    backend.AST_Schema.ASTAnalyzer:
      identifier: backend.AST_Schema.ASTAnalyzer
      description:
        overall: "The ASTAnalyzer class is responsible for analyzing a codebase to construct a detailed Abstract Syntax Tree (AST) schema and integrate relationship data. It processes individual Python files within a repository, parses their ASTs using an ASTVisitor, and then merges call and instantiation relationships into the generated schema. This class provides a structured representation of the code's components and their interconnections."
        init_method:
          description: The constructor for ASTAnalyzer does not perform any explicit initialization of attributes. It simply defines the class without setting up any initial state.
          parameters[0]:
        methods[2]:
          - identifier: merge_relationship_data
            description:
              overall: "This method integrates raw relationship data (incoming and outgoing calls) into a structured schema of AST nodes. It iterates through files, functions, and classes within the full_schema. For each function, it populates its calls and called_by context. For each class, it populates instantiated_by and then iterates through its methods to populate their calls and called_by, also identifying external class dependencies."
              parameters[2]{name,type,description}:
                full_schema,dict,"The complete AST schema containing file, function, and class definitions."
                raw_relationships,dict,A dictionary containing raw 'outgoing' and 'incoming' call relationships.
              returns[1]{name,type,description}:
                full_schema,dict,The updated full_schema with relationship data integrated.
              usage_context:
                calls: This method does not explicitly call other methods or classes within the provided context.
                called_by: This method is not called by any other functions or methods within the provided context.
            error: null
          - identifier: analyze_repository
            description:
              overall: "This method processes a list of file objects from a Git repository to build a comprehensive AST schema. It filters for Python files, reads their content, and uses an ASTVisitor to parse each file's Abstract Syntax Tree. The extracted AST nodes (imports, functions, classes) are then organized into a full_schema dictionary, handling potential parsing errors."
              parameters[2]{name,type,description}:
                files,list,"A list of file objects, each expected to have 'path' and 'content' attributes."
                repo,GitRepository,"An object representing the Git repository, though it's not directly used in the provided snippet beyond its type hint."
              returns[1]{name,type,description}:
                full_schema,dict,A dictionary representing the full AST schema of the analyzed repository.
              usage_context:
                calls: This method calls ASTVisitor for parsing Python files.
                called_by: This method is not called by any other functions or methods within the provided context.
            error: null
        usage_context:
          dependencies: The class depends on backend.AST_Schema.ASTVisitor for parsing individual ASTs.
          instantiated_by: The class is not explicitly instantiated by any known components within the provided context.
      error: null
    backend.File_Dependency.FileDependencyGraph:
      identifier: backend.File_Dependency.FileDependencyGraph
      description:
        overall: "The FileDependencyGraph class, inheriting from ast.NodeVisitor, is designed to analyze Python source code files and build a graph of their import dependencies. It traverses the Abstract Syntax Tree (AST) of a given file, specifically looking for import and from ... import ... statements. The class effectively distinguishes between absolute and relative imports, using a sophisticated internal mechanism (_resolve_module_name) to correctly resolve relative paths within a repository structure. Its primary function is to populate an import_dependencies dictionary, mapping each file to the set of modules it imports, thereby creating a foundational data structure for dependency analysis."
        init_method:
          description: This constructor initializes a FileDependencyGraph instance by setting the filename and repo_root attributes. These attributes are crucial for identifying the current file and navigating the repository structure to resolve file dependencies.
          parameters[2]{name,type,description}:
            filename,str,The path to the file for which dependencies are being analyzed.
            repo_root,Any,"The root directory of the repository, used for resolving file paths."
        methods[3]:
          - identifier: _resolve_module_name
            description:
              overall: "This method is responsible for resolving relative Python imports (e.g., from .. import name). It calculates the correct base directory based on the import level and the current file's location within the repository. It then iterates through the imported names, checking if they correspond to existing module files or symbols exported via __init__.py files. The method uses two nested helper functions: module_file_exists to check for .py files or package __init__.py files, and init_exports_symbol to parse __init__.py files for __all__ exports or direct definitions. If no matching modules or symbols are found, it raises an ImportError."
              parameters[1]{name,type,description}:
                node,ImportFrom,The AST ImportFrom node representing the relative import statement.
              returns[1]{name,type,description}:
                resolved,"list[str]",A sorted list of resolved module or symbol names that actually exist.
              usage_context:
                calls: "This method calls get_all_temp_files to retrieve all files in the repository. It also internally defines and calls module_file_exists and init_exports_symbol to verify the existence of modules or symbols. It utilizes Path from pathlib, iskeyword from keyword, and various AST components like parse, walk, Assign, Name, FunctionDef, ClassDef, and literal_eval."
                called_by: This method is called by visit_ImportFrom to handle relative import statements.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method processes Import and ImportFrom AST nodes to record dependencies. It ensures that the current filename is a key in the import_dependencies dictionary. For each imported alias, it adds either the provided base_name (if available, typically for from ... import ... statements where the module part is resolved) or the alias's direct name to the set of dependencies for the current file. Finally, it calls self.generic_visit(node) to continue traversing the AST."
              parameters[2]{name,type,description}:
                node,Import | ImportFrom,The AST node representing an import statement.
                base_name,str | None,"An optional base name for the import, used when resolving from ... import ... statements."
              returns[0]:
              usage_context:
                calls: This method calls self.generic_visit to continue AST traversal.
                called_by: This method is called by visit_ImportFrom after resolving the module name.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method specifically handles ImportFrom AST nodes. If the import is an absolute import (i.e., node.module is present), it extracts the last component of the module name and passes it to visit_Import along with the node. If it's a relative import (i.e., node.module is None), it attempts to resolve the module names using the _resolve_module_name helper method. For each successfully resolved base name, it calls visit_Import. Any ImportError during resolution is caught and printed. Finally, it calls self.generic_visit(node) to ensure proper AST traversal."
              parameters[1]{name,type,description}:
                node,ImportFrom,The AST ImportFrom node to be processed.
              returns[0]:
              usage_context:
                calls: This method calls self.visit_Import to record the dependencies and self._resolve_module_name to handle relative imports. It also calls self.generic_visit for AST traversal.
                called_by: This method is implicitly called by the NodeVisitor framework when traversing an AST and encountering an ImportFrom node.
            error: null
        usage_context:
          dependencies: "The class depends on get_all_temp_files for repository file listing, and internally uses init_exports_symbol and module_file_exists for resolving module and symbol existence. It also relies on ast module components for AST traversal and parsing, pathlib for path manipulation, and keyword for identifier checks."
          instantiated_by: "The class is not explicitly shown to be instantiated by other components in the provided context, suggesting its instantiation points are external to this specific snippet or not yet defined."
      error: null
    backend.HelperLLM.LLMHelper:
      identifier: backend.HelperLLM.LLMHelper
      description:
        overall: "The LLMHelper class provides a centralized interface for interacting with various Large Language Models (LLMs) to generate structured documentation for Python functions and classes. It abstracts away the complexities of different LLM providers (Gemini, OpenAI, Ollama, custom APIs), handles API key validation, loads system prompts, and configures LLM clients with structured output capabilities using Pydantic schemas. The class supports batch processing of requests and incorporates rate limiting mechanisms to ensure robust and efficient interaction with LLM APIs."
        init_method:
          description: "The constructor initializes the LLMHelper class by validating the provided API key and loading system prompts for function and class analysis from specified file paths. It then configures the appropriate Language Model (LLM) client (Google Gemini, OpenAI, custom API, or Ollama) based on the model_name and sets up structured output parsers for FunctionAnalysis and ClassAnalysis schemas. Finally, it configures batch processing settings for the chosen model."
          parameters[5]{name,type,description}:
            api_key,str,"The API key for the chosen LLM service (e.g., Gemini, OpenAI)."
            function_prompt_path,str,The file path to the system prompt for function analysis.
            class_prompt_path,str,The file path to the system prompt for class analysis.
            model_name,str,"The name of the LLM model to use (default: \"gemini-2.0-flash-lite\")."
            base_url,str,The base URL for custom LLM endpoints like Ollama or other OpenAI-compatible APIs.
        methods[3]:
          - identifier: _configure_batch_settings
            description:
              overall: "This private method sets the `batch_size` attribute of the `LLMHelper` instance based on the provided `model_name`. It uses a series of conditional checks to assign specific batch sizes for various Gemini, Llama, and GPT models, as well as custom API models. If the model name is not recognized, it logs a warning and defaults to a conservative batch size of 2."
              parameters[1]{name,type,description}:
                model_name,str,The name of the LLM model for which to configure batch settings.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other functions or methods.
                called_by: This method is called by the `__init__` method of the `LLMHelper` class.
            error: null
          - identifier: generate_for_functions
            description:
              overall: "This method generates and validates documentation for a list of functions by interacting with the configured LLM. It takes a list of `FunctionAnalysisInput` objects, converts them into JSON payloads, and constructs conversations with the `function_system_prompt`. The method then processes these conversations in batches, calling the `function_llm.batch` method, and includes a waiting period between batches to respect rate limits. It handles potential errors during LLM calls by logging them and extending the results list with `None` for failed items."
              parameters[1]{name,type,description}:
                function_inputs,"List[FunctionAnalysisInput]",A list of input objects containing function details for which documentation is to be generated.
              returns[1]{name,type,description}:
                all_validated_functions,"List[Optional[FunctionAnalysis]]","A list of `FunctionAnalysis` objects, or `None` for inputs where generation or validation failed."
              usage_context:
                calls: "This method calls `json.dumps` to serialize input models, `len` to get list length, `min` for batch indexing, `logging.info` and `logging.error` for logging, `self.function_llm.batch` to send requests to the LLM, and `time.sleep` for rate limiting."
                called_by: The input context does not specify where this method is called.
            error: null
          - identifier: generate_for_classes
            description:
              overall: "This method is responsible for generating and validating documentation for a batch of classes using the configured LLM. It accepts a list of `ClassAnalysisInput` objects, converts them into JSON strings, and prepares them as `HumanMessage` content alongside the `class_system_prompt`. The method then iterates through these inputs in defined batches, sending them to the `class_llm` for processing. It includes error handling for LLM calls and implements a waiting period between batches to manage API rate limits."
              parameters[1]{name,type,description}:
                class_inputs,"List[ClassAnalysisInput]",A list of input objects containing class details for which documentation is to be generated.
              returns[1]{name,type,description}:
                all_validated_classes,"List[Optional[ClassAnalysis]]","A list of `ClassAnalysis` objects, or `None` for inputs where generation or validation failed."
              usage_context:
                calls: "This method calls `json.dumps` to serialize input models, `len` to get list length, `min` for batch indexing, `logging.info` and `logging.error` for logging, `self.class_llm.batch` to send requests to the LLM, and `time.sleep` for rate limiting."
                called_by: The input context does not specify where this method is called.
            error: null
        usage_context:
          dependencies: "The class depends on `logging` for output, `json` for serialization, `time` for delays, `langchain_google_genai.ChatGoogleGenerativeAI`, `langchain_ollama.ChatOllama`, `langchain_openai.ChatOpenAI` for LLM integrations, `langchain.messages.HumanMessage`, `langchain.messages.SystemMessage` for message formatting, and `schemas.types.FunctionAnalysis`, `schemas.types.ClassAnalysis`, `schemas.types.FunctionAnalysisInput`, `schemas.types.ClassAnalysisInput` for structured input/output."
          instantiated_by: The input context does not specify where this class is instantiated.
      error: null
    backend.MainLLM.MainLLM:
      identifier: backend.MainLLM.MainLLM
      description:
        overall: "The MainLLM class provides a unified interface for interacting with various Large Language Models (LLMs), abstracting the underlying provider specifics. It initializes an LLM client based on the provided model name, supporting Gemini, OpenAI-compatible APIs (including custom ones), and Ollama. The class manages a system prompt loaded from a file, which is prepended to all user inputs. It offers methods for both single-response interactions and streaming responses, making it a flexible component for integrating diverse LLM capabilities into an application."
        init_method:
          description: "The constructor initializes the MainLLM class by configuring the LLM client and loading the system prompt. It validates the API key and attempts to read the system prompt from the specified file path, raising an error if the file is not found. Based on the `model_name`, it dynamically instantiates the appropriate LangChain LLM client (ChatGoogleGenerativeAI, ChatOpenAI, or ChatOllama) and sets it as an instance attribute."
          parameters[4]{name,type,description}:
            api_key,str,The API key required for authenticating with the chosen LLM service.
            prompt_file_path,str,The file path to a text file containing the system-level instructions or prompt for the LLM.
            model_name,str,"The name of the LLM model to be used, defaulting to 'gemini-2.5-pro'. This name dictates which LLM client is initialized."
            base_url,str,"An optional base URL for custom LLM API endpoints, particularly relevant for self-hosted or custom OpenAI-compatible models."
        methods[2]:
          - identifier: call_llm
            description:
              overall: "This method sends a user's input to the initialized LLM and retrieves a single, complete response. It constructs a list of messages, including the class's system prompt and the provided user input, then invokes the LLM client. The method includes logging for the call process and incorporates error handling to catch exceptions during the LLM interaction, returning the response content or None upon failure."
              parameters[1]{name,type,description}:
                user_input,str,The user's query or message to be sent to the LLM.
              returns[1]{name,type,description}:
                content,str | None,"The generated response content from the LLM as a string, or None if an error occurred during the call."
              usage_context:
                calls: "This method calls SystemMessage, HumanMessage, self.llm.invoke, and various logging functions."
                called_by: This method is not explicitly called by other methods within the provided context.
            error: null
          - identifier: stream_llm
            description:
              overall: "This method interacts with the LLM to obtain a streamed response, yielding chunks of content as they become available. It prepares the messages, including the system prompt and user input, and then utilizes the `self.llm.stream` method. The method iterates through the streamed chunks, yielding each chunk's content. It also includes error handling, yielding an error message if an exception occurs during the streaming process."
              parameters[1]{name,type,description}:
                user_input,str,The user's query or message for which a streamed response is requested from the LLM.
              returns[1]{name,type,description}:
                chunk.content,str,"An iterator that yields individual string chunks of the LLM's generated response. In case of an error, it yields a descriptive error message string."
              usage_context:
                calls: "This method calls SystemMessage, HumanMessage, self.llm.stream, and various logging functions."
                called_by: This method is not explicitly called by other methods within the provided context.
            error: null
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies within the provided context.
          instantiated_by: This class is not explicitly instantiated by other components within the provided context.
      error: null
    backend.basic_info.ProjektInfoExtractor:
      identifier: backend.basic_info.ProjektInfoExtractor
      description:
        overall: "The ProjektInfoExtractor class is designed to automatically gather and structure essential project information from common project configuration files and documentation. It systematically parses pyproject.toml, requirements.txt, and README files to extract details such as project title, description, features, tech stack, installation steps, and dependencies. The class prioritizes information sources and provides a consolidated, structured dictionary of project metadata, offering a holistic overview of a software project."
        init_method:
          description: "Initializes the ProjektInfoExtractor instance by setting a default \"Information not found\" string and establishing the `self.info` dictionary structure with placeholder values for project overview and installation details. This ensures a consistent data schema for extracted information."
          parameters[0]:
        methods[7]:
          - identifier: _clean_content
            description:
              overall: "This private helper method is responsible for sanitizing input string content by removing null bytes (\\x00). Null bytes can appear due to encoding errors, such as reading UTF-16 encoded files as UTF-8, and can interfere with text processing. The method ensures that the content is clean before further parsing."
              parameters[1]{name,type,description}:
                content,str,The string content to be cleaned.
              returns[1]{name,type,description}:
                "null",str,"The cleaned string content, with null bytes removed."
              usage_context:
                calls: This method does not call any other functions or methods.
                called_by: This method is not called by any other functions or methods.
            error: null
          - identifier: _finde_datei
            description:
              overall: "This private helper method searches a list of file objects for a file whose path matches any of the provided patterns. The search is case-insensitive, making it robust to variations in file naming conventions (e.g., \"README.md\" vs. \"readme.md\"). It returns the first matching file object found or None if no match is made."
              parameters[2]{name,type,description}:
                patterns,"List[str]",A list of string patterns to match against file paths.
                dateien,"List[Any]","A list of file-like objects, each expected to have a 'path' attribute."
              returns[1]{name,type,description}:
                "null","Optional[Any]","The first file object whose path matches a pattern, or None if no match is found."
              usage_context:
                calls: This method does not call any other functions or methods.
                called_by: This method is not called by any other functions or methods.
            error: null
          - identifier: _extrahiere_sektion_aus_markdown
            description:
              overall: "This private method extracts text content located directly under a Markdown level 2 heading (##) that matches one of the specified keywords. It constructs a regular expression dynamically to find headings like \"## Features\" or \"## Tech Stack\" and captures all content until the next level 2 heading or the end of the document. The search is case-insensitive and handles various spacing."
              parameters[2]{name,type,description}:
                inhalt,str,The Markdown content string to parse.
                keywords,"List[str]",A list of keywords to match against Markdown headings.
              returns[1]{name,type,description}:
                "null","Optional[str]","The extracted text content under the matching heading, or None if no matching section is found."
              usage_context:
                calls: This method does not call any other functions or methods.
                called_by: This method is not called by any other functions or methods.
            error: null
          - identifier: _parse_readme
            description:
              overall: "This private method processes the content of a README file to extract various project details, including the title, description, key features, tech stack, current status, installation instructions, and a quick start guide. It utilizes regular expressions for title and description extraction and delegates to _extrahiere_sektion_aus_markdown for section-based information. The method updates the self.info dictionary with the extracted data, prioritizing existing INFO_NICHT_GEFUNDEN values."
              parameters[1]{name,type,description}:
                inhalt,str,The string content of the README file.
              returns[0]:
              usage_context:
                calls: This method does not call any other functions or methods.
                called_by: This method is not called by any other functions or methods.
            error: null
          - identifier: _parse_toml
            description:
              overall: "This private method parses the content of a pyproject.toml file to extract project metadata such as the project name, description, and dependencies. It uses the tomllib module to load and interpret the TOML format. If tomllib is not available or a parsing error occurs, it prints a warning and gracefully exits without updating the project information."
              parameters[1]{name,type,description}:
                inhalt,str,The string content of the pyproject.toml file.
              returns[0]:
              usage_context:
                calls: This method does not call any other functions or methods.
                called_by: This method is not called by any other functions or methods.
            error: null
          - identifier: _parse_requirements
            description:
              overall: "This private method parses the content of a requirements.txt file to extract project dependencies. It splits the content into lines, filters out empty lines and comments, and stores the remaining lines as dependencies. This method only updates the self.info dictionary if dependencies have not already been found from a pyproject.toml file, ensuring that pyproject.toml takes precedence."
              parameters[1]{name,type,description}:
                inhalt,str,The string content of the requirements.txt file.
              returns[0]:
              usage_context:
                calls: This method does not call any other functions or methods.
                called_by: This method is not called by any other functions or methods.
            error: null
          - identifier: extrahiere_info
            description:
              overall: "This public method orchestrates the entire process of extracting project information from various source files and a repository URL. It first identifies relevant files (README, pyproject.toml, requirements.txt) using _finde_datei. It then parses these files in a specific order of precedence (TOML, then requirements, then README) to populate the self.info dictionary. Finally, it formats the dependencies and attempts to derive a project title from the repository URL if no title was found in the files."
              parameters[2]{name,type,description}:
                dateien,"List[Any]","A list of file-like objects, each containing 'path' and 'content' attributes."
                repo_url,str,"The URL of the repository, used as a fallback for the project title."
              returns[1]{name,type,description}:
                "null","Dict[str, Any]",A dictionary containing the extracted project information.
              usage_context:
                calls: This method does not call any other functions or methods.
                called_by: This method is not called by any other functions or methods.
            error: null
        usage_context:
          dependencies: This class does not explicitly list external functional dependencies in the provided context.
          instantiated_by: This class is not explicitly instantiated by any other components in the provided context.
      error: null
    backend.callgraph.CallGraph:
      identifier: backend.callgraph.CallGraph
      description:
        overall: "The CallGraph class is an ast.NodeVisitor subclass designed to construct a call graph for a given Python source file. It traverses the Abstract Syntax Tree (AST) of a file, identifying function definitions, class definitions, import statements, and function calls. By maintaining context about the current function and class, and resolving names through local definitions and import mappings, it builds a directed graph representing the calling relationships between functions and methods within the file."
        init_method:
          description: "The constructor initializes the CallGraph instance by setting up various internal state variables. These include the filename being analyzed, placeholders for the current function and class context, dictionaries for local definitions and import mappings, a NetworkX DiGraph to store the call graph, a set to track all identified functions, and a dictionary to store the edges (caller-callee relationships)."
          parameters[1]{name,type,description}:
            filename,str,The path to the source file being analyzed.
        methods[11]:
          - identifier: _recursive_call
            description:
              overall: "This private helper method recursively traverses an Abstract Syntax Tree (AST) node, specifically designed to extract the components of a function or method call. It handles `ast.Call`, `ast.Name`, and `ast.Attribute` nodes to build a list of name components, effectively converting a complex call expression into a list representing its dotted path (e.g., `['pkg', 'mod', 'Class', 'method']`). This is crucial for identifying the full name of a called entity."
              parameters[1]{name,type,description}:
                node,ast.AST,"The AST node representing a call, name, or attribute access."
              returns[1]{name,type,description}:
                name_components,"list[str]",A list of string components representing the dotted path of the call.
              usage_context:
                calls: This method recursively calls itself to traverse the AST node structure.
                called_by: This method is called by `visit_Call`.
            error: null
          - identifier: _resolve_all_callee_names
            description:
              overall: "This private method takes a list of potential callee name components (e.g., `[['module', 'function']]`) and resolves them to their fully qualified names. It prioritizes resolution by checking local definitions (`self.local_defs`), then import mappings (`self.import_mapping`), and finally constructs a full name based on the current filename and class context if no other resolution is found. This ensures that calls are accurately linked to their definitions."
              parameters[1]{name,type,description}:
                callee_nodes,"list[list[str]]",A list where each inner list represents dotted name components of a callee.
              returns[1]{name,type,description}:
                resolved_names,"list[str]",A list of fully qualified string names for the callees.
              usage_context:
                calls: This method does not explicitly call other methods.
                called_by: This method is called by `visit_Call`.
            error: null
          - identifier: _make_full_name
            description:
              overall: This private utility method constructs a fully qualified name for a function or method. It prepends the `filename` and optionally the `class_name` to the `basename` to create a unique identifier for the entity within the context of the file and class. This standardized naming convention is crucial for building a consistent call graph.
              parameters[2]{name,type,description}:
                basename,str,The simple name of the function or method.
                class_name,str | None,"The name of the class if the function is a method, otherwise None."
              returns[1]{name,type,description}:
                full_name,str,The fully qualified name of the function or method.
              usage_context:
                calls: This method does not explicitly call other methods.
                called_by: This method is called by `visit_FunctionDef`.
            error: null
          - identifier: _current_caller
            description:
              overall: "This private method determines the identifier of the current calling context. If `self.current_function` is set, it returns that value, indicating a call within a specific function. Otherwise, it returns a placeholder indicating either the filename (if available) or a generic \"<global-scope>\" to represent calls made outside any defined function."
              parameters[0]:
              returns[1]{name,type,description}:
                caller_identifier,str,The identifier of the current caller context.
              usage_context:
                calls: This method does not explicitly call other methods.
                called_by: This method is called by `visit_Call`.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method, part of the `ast.NodeVisitor` pattern, processes `ast.Import` nodes. It iterates through the imported modules and their aliases, populating the `self.import_mapping` dictionary. This mapping helps resolve imported names to their original module names during call graph construction, ensuring external dependencies are correctly tracked. After processing, it calls `generic_visit` to continue traversing the AST."
              parameters[1]{name,type,description}:
                node,ast.Import,"The AST node representing an import statement (e.g., `import module as alias`)."
              returns[0]:
              usage_context:
                calls: This method calls `self.generic_visit` to continue AST traversal.
                called_by: This method is called by the `ast.NodeVisitor` framework during AST traversal.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method processes `ast.ImportFrom` nodes, which represent `from ... import ...` statements. It extracts the module name and then iterates through the imported names and their aliases, updating `self.import_mapping` to associate the aliased or original name with the module it came from. This is essential for correctly resolving calls to functions or classes imported from other modules."
              parameters[1]{name,type,description}:
                node,ast.ImportFrom,The AST node representing a `from ... import ...` statement.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other methods.
                called_by: This method is called by the `ast.NodeVisitor` framework during AST traversal.
            error: null
          - identifier: visit_ClassDef
            description:
              overall: "This method handles `ast.ClassDef` nodes, which represent class definitions. It temporarily sets `self.current_class` to the name of the current class being visited, allowing nested methods to correctly identify their parent class context. After visiting the class's body and its members, it restores the previous `current_class` context to ensure correct scope management."
              parameters[1]{name,type,description}:
                node,ast.ClassDef,The AST node representing a class definition.
              returns[0]:
              usage_context:
                calls: This method calls `self.generic_visit` to continue AST traversal within the class body.
                called_by: This method is called by the `ast.NodeVisitor` framework during AST traversal.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method processes `ast.FunctionDef` nodes, representing function definitions. It constructs a full name for the function using `_make_full_name`, updates `self.local_defs` with this full name, and sets `self.current_function` to track the current context. It adds the function as a node to the `self.graph` and ensures it's added to `self.function_set`. Finally, it restores the previous function context after visiting the function's body, maintaining proper scope."
              parameters[1]{name,type,description}:
                node,ast.FunctionDef,The AST node representing a function definition.
              returns[0]:
              usage_context:
                calls: "This method calls `self._make_full_name`, `self.graph.add_node`, and `self.generic_visit`."
                called_by: This method is called by the `ast.NodeVisitor` framework during AST traversal and by `visit_AsyncFunctionDef`.
            error: null
          - identifier: visit_AsyncFunctionDef
            description:
              overall: "This method handles `ast.AsyncFunctionDef` nodes, which represent asynchronous function definitions. For the purpose of call graph construction, it simply delegates the processing to the `visit_FunctionDef` method. This ensures that asynchronous functions are treated similarly to regular functions in terms of name resolution and graph node creation."
              parameters[1]{name,type,description}:
                node,ast.AsyncFunctionDef,The AST node representing an asynchronous function definition.
              returns[0]:
              usage_context:
                calls: This method calls `self.visit_FunctionDef`.
                called_by: This method is called by the `ast.NodeVisitor` framework during AST traversal.
            error: null
          - identifier: visit_Call
            description:
              overall: "This method processes `ast.Call` nodes, representing function or method calls. It identifies the `caller` using `_current_caller`, extracts the `callee` components using `_recursive_call`, and resolves the `callee`'s full name using `_resolve_all_callee_names`. It then records the call as an edge in the `self.edges` dictionary, mapping the caller to its set of callees, thereby building the core of the call graph."
              parameters[1]{name,type,description}:
                node,ast.Call,The AST node representing a function or method call.
              returns[0]:
              usage_context:
                calls: "This method calls `self._current_caller`, `self._recursive_call`, `self._resolve_all_callee_names`, and `self.generic_visit`."
                called_by: This method is called by the `ast.NodeVisitor` framework during AST traversal.
            error: null
          - identifier: visit_If
            description:
              overall: "This method processes `ast.If` nodes. It specifically checks for the common `if __name__ == \"__main__\":` pattern, which denotes the main execution block of a script. If detected, it temporarily sets `self.current_function` to \"<main_block>\" to correctly attribute calls made within this block. For all other `if` statements, it simply continues the generic AST traversal without special context handling."
              parameters[1]{name,type,description}:
                node,ast.If,The AST node representing an if statement.
              returns[0]:
              usage_context:
                calls: This method calls `self.generic_visit` to continue AST traversal within the if block.
                called_by: This method is called by the `ast.NodeVisitor` framework during AST traversal.
            error: null
        usage_context:
          dependencies: This class depends on the `ast` module for parsing Python code and the `networkx` library for graph data structures.
          instantiated_by: This class is not explicitly instantiated by any known components in the provided context.
      error: null
    backend.getRepo.RepoFile:
      identifier: backend.getRepo.RepoFile
      description:
        overall: "The RepoFile class serves as a representation of a single file found within a Git repository. It is designed to provide abstracted access to file properties such as its path, content, and size. A key feature is its lazy-loading mechanism for the Git blob object, file content, and file size, ensuring that these resources are only loaded into memory when they are first accessed, optimizing performance and memory usage. It also includes utility methods for basic content analysis and structured data export."
        init_method:
          description: "The `__init__` method initializes a new RepoFile object. It stores the file's path within the repository and the Git Tree object from which the file originates. It also sets up internal attributes (`_blob`, `_content`, `_size`) to `None`, indicating that these properties will be lazy-loaded upon their first access."
          parameters[2]{name,type,description}:
            file_path,str,The path to the file within the Git repository.
            commit_tree,git.Tree,The Git Tree object representing the commit from which this file is retrieved.
        methods[6]:
          - identifier: blob
            description:
              overall: "This property provides lazy loading for the Git blob object associated with the file. When accessed, it first checks if the `_blob` attribute has already been loaded. If not, it attempts to retrieve the corresponding blob from the `_tree` using the file's path. In case the file cannot be found within the commit tree, a `FileNotFoundError` is raised to indicate the issue."
              parameters[0]:
              returns[1]{name,type,description}:
                blob,git.Blob,The Git blob object representing the file's content and metadata within the repository.
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions based on the provided context."
                called_by: This method is not explicitly called by other functions or methods based on the provided context.
            error: null
          - identifier: content
            description:
              overall: "This property provides lazy loading for the decoded content of the file. Upon its first access, it checks if the `_content` attribute is already populated. If not, it retrieves the Git blob object via the `blob` property, reads its data stream, and decodes it into a UTF-8 string, ignoring any decoding errors. The decoded content is then stored and returned for subsequent accesses."
              parameters[0]:
              returns[1]{name,type,description}:
                content,str,The decoded content of the file as a string.
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions based on the provided context."
                called_by: This method is not explicitly called by other functions or methods based on the provided context.
            error: null
          - identifier: size
            description:
              overall: "This property provides lazy loading for the size of the file in bytes. When accessed, it first verifies if the `_size` attribute has already been determined. If it's `None`, it retrieves the Git blob object using the `blob` property and extracts its `size` attribute. This value, representing the file's size in bytes, is then stored and returned."
              parameters[0]:
              returns[1]{name,type,description}:
                size,int,The size of the file in bytes.
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions based on the provided context."
                called_by: This method is not explicitly called by other functions or methods based on the provided context.
            error: null
          - identifier: analyze_word_count
            description:
              overall: "This method serves as an example analysis function, demonstrating how to process the file's content. It calculates the total number of words present in the file. To achieve this, it accesses the `content` property, which ensures the file's content is loaded and decoded, then splits the content string by whitespace, and finally returns the count of the resulting words."
              parameters[0]:
              returns[1]{name,type,description}:
                word_count,int,The total number of words found in the file content.
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions based on the provided context."
                called_by: This method is not explicitly called by other functions or methods based on the provided context.
            error: null
          - identifier: __repr__
            description:
              overall: "This special method defines the official string representation of the RepoFile object. It is primarily used for debugging and logging purposes, providing a concise and informative string that uniquely identifies the object. The representation includes the class name and the path of the file, making it easy to distinguish different RepoFile instances."
              parameters[0]:
              returns[1]{name,type,description}:
                representation,str,"A string representation of the RepoFile object, typically showing its class and file path."
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions based on the provided context."
                called_by: This method is not explicitly called by other functions or methods based on the provided context.
            error: null
          - identifier: to_dict
            description:
              overall: "This method converts the RepoFile object's key attributes into a dictionary format, facilitating data serialization or structured access. It includes the file's path, its base name (extracted using `os.path.basename`), its size (lazy-loaded via the `size` property), and a fixed type of 'file'. Optionally, if `include_content` is set to True, the file's content (lazy-loaded via the `content` property) is also added to the dictionary."
              parameters[1]{name,type,description}:
                include_content,bool,A flag indicating whether the file's content should be included in the output dictionary. Defaults to False.
              returns[1]{name,type,description}:
                file_data,dict,"A dictionary containing metadata about the file, and optionally its content."
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions based on the provided context."
                called_by: This method is not explicitly called by other functions or methods based on the provided context.
            error: null
        usage_context:
          dependencies: This class does not have any explicitly listed external functional dependencies.
          instantiated_by: This class is not explicitly listed as being instantiated by other components.
      error: null
    backend.getRepo.GitRepository:
      identifier: backend.getRepo.GitRepository
      description:
        overall: "The GitRepository class is designed to manage a Git repository by cloning it into a temporary directory and providing structured access to its files. It acts as a context manager, ensuring that resources are properly handled upon exit. The class facilitates listing all files within the repository and organizing them into a hierarchical tree structure, making it easier to navigate and process repository content."
        init_method:
          description: "The constructor initializes the GitRepository instance by taking a repository URL. It creates a temporary directory, attempts to clone the specified Git repository into it, and stores references to the repository object, its latest commit, and the commit tree. It also initializes an empty list to hold RepoFile objects. If cloning fails, it cleans up the temporary directory reference and raises a RuntimeError."
          parameters[1]{name,type,description}:
            repo_url,string,The URL of the Git repository to be cloned.
        methods[5]:
          - identifier: get_all_files
            description:
              overall: "This method retrieves all file paths from the cloned Git repository using Git's `ls-files` command. It then iterates through these paths to create a list of `RepoFile` objects, each representing a file in the repository. These `RepoFile` instances are stored internally and also returned by the method, providing a flat list of all repository files."
              parameters[0]:
              returns[1]{name,type,description}:
                files,"list[RepoFile]",A list of RepoFile instances representing all files in the repository.
              usage_context:
                calls: This method calls 'backend.getRepo.RepoFile'.
                called_by: This method is not explicitly called by any other functions or methods in the provided context.
            error: null
          - identifier: close
            description:
              overall: "This method is intended to clean up resources associated with the Git repository. It prints a message indicating the deletion of the temporary directory and then nullifies the `self.temp_dir` attribute. It does not, however, perform the actual filesystem deletion of the directory, only removing the Python object's reference to it."
              parameters[0]:
              returns[0]:
              usage_context:
                calls: This method does not call any other functions or methods.
                called_by: This method is not explicitly called by any other functions or methods in the provided context.
            error: null
          - identifier: __enter__
            description:
              overall: "This special method allows the GitRepository instance to be used as a context manager. When entering a `with` statement, it simply returns the instance itself, making it available for operations within the context block."
              parameters[0]:
              returns[1]{name,type,description}:
                self,GitRepository,The instance of the GitRepository class.
              usage_context:
                calls: This method does not call any other functions or methods.
                called_by: This method is not explicitly called by any other functions or methods in the provided context.
            error: null
          - identifier: __exit__
            description:
              overall: "This special method is part of the context manager protocol and is automatically invoked when exiting a `with` statement. Its primary responsibility is to ensure that the `close` method is called, facilitating resource cleanup regardless of whether an exception occurred within the context block."
              parameters[3]{name,type,description}:
                exc_type,type,"The type of exception raised, if any."
                exc_val,Exception,"The exception instance raised, if any."
                exc_tb,traceback,"A traceback object encapsulating the call stack at the point where the exception originally occurred, if any."
              returns[0]:
              usage_context:
                calls: This method does not call any other functions or methods.
                called_by: This method is not explicitly called by any other functions or methods in the provided context.
            error: null
          - identifier: get_file_tree
            description:
              overall: "This method constructs a hierarchical dictionary representation of the repository's file structure. If the list of files has not yet been populated, it first calls `get_all_files`. It then iterates through each `RepoFile` object, splitting its path to build a nested dictionary where directories are represented as nodes with 'children' lists and files are appended at their respective locations. The resulting tree can optionally include file content."
              parameters[1]{name,type,description}:
                include_content,bool,A boolean flag indicating whether the content of the files should be included in the tree structure. Defaults to False.
              returns[1]{name,type,description}:
                tree,dict,A dictionary representing the hierarchical file tree of the repository.
              usage_context:
                calls: This method does not call any other functions or methods.
                called_by: This method is not explicitly called by any other functions or methods in the provided context.
            error: null
        usage_context:
          dependencies: This class depends on 'backend.getRepo.RepoFile' for representing individual files.
          instantiated_by: This class is not explicitly instantiated by any other components in the provided context.
      error: null
    backend.relationship_analyzer.ProjectAnalyzer:
      identifier: backend.relationship_analyzer.ProjectAnalyzer
      description:
        overall: "The ProjectAnalyzer class is designed to perform a static analysis of a Python project to build a comprehensive call graph. It identifies all Python files, extracts definitions of functions, methods, and classes, and then resolves calls between these entities. The class provides methods to initiate the analysis, retrieve the raw call graph, and present relationships in a structured incoming/outgoing format."
        init_method:
          description: "The constructor initializes the ProjectAnalyzer instance by setting the project's root directory, and preparing data structures like definitions, call_graph, and file_asts. It also defines a set of directories to ignore during file traversal, ensuring that common non-source code directories are skipped."
          parameters[1]{name,type,description}:
            project_root,string,The root directory of the project to be analyzed.
        methods[6]:
          - identifier: analyze
            description:
              overall: "This method orchestrates the entire project analysis process. It first identifies all Python files within the project using an internal helper. It then iterates through these files to collect function, method, and class definitions. Subsequently, it processes each file again to resolve function and method calls, populating the internal call graph. Finally, it clears temporary AST storage and returns the generated call graph."
              parameters[0]:
              returns[1]{name,type,description}:
                call_graph,defaultdict(list),A dictionary representing the call graph where keys are callee identifiers and values are lists of caller information.
              usage_context:
                calls: "This method calls `_find_py_files` to locate Python files, `_collect_definitions` to gather definitions from each file, and `_resolve_calls` to establish relationships between calls."
                called_by: This method is not explicitly called by any other methods within the provided context.
            error: null
          - identifier: get_raw_relationships
            description:
              overall: "This method processes the internal `call_graph` to generate a more structured representation of incoming and outgoing relationships between code entities. It initializes two defaultdicts for outgoing and incoming relationships. It then iterates through the call graph, extracting caller and callee identifiers to populate these dictionaries. The results are then sorted and returned as a dictionary."
              parameters[0]:
              returns[1]{name,type,description}:
                relationships,dict,"A dictionary containing two keys, \"outgoing\" and \"incoming\", each mapping entity identifiers to a sorted list of related entity identifiers."
              usage_context:
                calls: This method does not explicitly call other methods or functions within the provided context.
                called_by: This method is not explicitly called by any other methods within the provided context.
            error: null
          - identifier: _find_py_files
            description:
              overall: "This private helper method is responsible for recursively traversing the project directory to locate all Python files, while respecting a predefined list of directories to ignore. It uses `os.walk` to navigate the file system, filtering out specified directories such as '.git' and 'venv'. It collects absolute paths to files ending with '.py' and returns them."
              parameters[0]:
              returns[1]{name,type,description}:
                py_files,"list[str]",A list of absolute file paths to Python files found in the project.
              usage_context:
                calls: This method does not explicitly call other methods or functions within the provided context.
                called_by: This method is called by the `analyze` method.
            error: null
          - identifier: _collect_definitions
            description:
              overall: "This private method parses a given Python file to identify and store definitions of functions, methods, and classes. It reads the file, parses its Abstract Syntax Tree (AST), and stores the AST for later use in `self.file_asts`. It then walks the AST to find `FunctionDef` and `ClassDef` nodes, determining their full module path and type (function, method, or class), and records this information in the `self.definitions` dictionary. Error handling is included for file processing and AST parsing."
              parameters[1]{name,type,description}:
                filepath,str,The absolute path to the Python file to be analyzed.
              returns[0]:
              usage_context:
                calls: This method calls `path_to_module` to convert a file path to a module path and `_get_parent` to determine if a function is a method.
                called_by: This method is called by the `analyze` method.
            error: null
          - identifier: _get_parent
            description:
              overall: "This private helper method traverses an Abstract Syntax Tree (AST) to find the immediate parent node of a given child node. It iterates through all nodes in the tree and checks their children to identify if any child matches the provided node. If a match is found, the parent node is returned. If no parent is found (e.g., for the root node or if the node is not in the tree), it returns `None`."
              parameters[2]{name,type,description}:
                tree,ast.AST,The root of the Abstract Syntax Tree to search within.
                node,ast.AST,The child node for which to find the parent.
              returns[1]{name,type,description}:
                parent,ast.AST | None,"The parent AST node if found, otherwise `None`."
              usage_context:
                calls: This method does not explicitly call other methods or functions within the provided context.
                called_by: This method is called by the `_collect_definitions` method.
            error: null
          - identifier: _resolve_calls
            description:
              overall: "This private method is responsible for identifying and resolving function and method calls within a given Python file's AST. It retrieves the pre-parsed AST for the specified filepath from `self.file_asts`. It then instantiates a `CallResolverVisitor` with the file context and the collected definitions, and uses it to visit the AST. The resolved calls from the visitor are then extended into the `self.call_graph` attribute, mapping callees to their respective callers. Error handling is included for the call resolution process."
              parameters[1]{name,type,description}:
                filepath,str,The absolute path to the Python file whose calls are to be resolved.
              returns[0]:
              usage_context:
                calls: This method instantiates and uses `CallResolverVisitor` to process the AST and resolve calls.
                called_by: This method is called by the `analyze` method.
            error: null
        usage_context:
          dependencies: This class depends on `backend.relationship_analyzer.CallResolverVisitor` for resolving calls and `backend.relationship_analyzer.path_to_module` for converting file paths to module paths.
          instantiated_by: This class is not explicitly instantiated by any other components within the provided context.
      error: null
    backend.relationship_analyzer.CallResolverVisitor:
      identifier: backend.relationship_analyzer.CallResolverVisitor
      description:
        overall: "The CallResolverVisitor is an ast.NodeVisitor subclass designed to traverse an Abstract Syntax Tree (AST) of a Python file. Its primary purpose is to identify and record all function and method calls within the analyzed code. It resolves the fully qualified names of these calls by tracking imports, module paths, class definitions, and instance assignments. The visitor collects detailed information about each call, including the caller's context, to build a comprehensive map of inter-function relationships."
        init_method:
          description: "This constructor initializes the visitor with the file path, project root, and a dictionary of known definitions. It sets up internal state variables like `module_path`, `scope` for tracking imports, `instance_types` for tracking object types, and `calls` to store detected call relationships."
          parameters[3]{name,type,description}:
            filepath,string,The path to the file being analyzed.
            project_root,string,The root directory of the project.
            definitions,dict,"A dictionary containing known definitions within the project, used for resolving call targets."
        methods[7]:
          - identifier: visit_ClassDef
            description:
              overall: "This method overrides the ast.NodeVisitor's visit_ClassDef to handle class definitions. It temporarily updates `self.current_class_name` to the name of the class being visited, allowing nested methods to correctly form their full identifiers. After visiting the class's children, it restores the previous `current_class_name` to maintain correct scope."
              parameters[2]{name,type,description}:
                self,CallResolverVisitor,The instance of the visitor.
                node,ast.ClassDef,The AST node representing the class definition.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call other functions or methods within its body, besides `self.generic_visit(node)` which is part of the AST traversal mechanism."
                called_by: This method is called by the `ast.NodeVisitor`'s `visit` dispatch mechanism when traversing an AST that contains a class definition.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method handles function definitions, both global and within classes. It constructs a `full_identifier` for the function based on whether it's a method of a class or a standalone function. It then updates `self.current_caller_name` to this full identifier before recursively visiting the function's body, ensuring that calls within this function are correctly attributed. The previous caller name is restored upon exiting the function's scope."
              parameters[2]{name,type,description}:
                self,CallResolverVisitor,The instance of the visitor.
                node,ast.FunctionDef,The AST node representing the function definition.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call other functions or methods within its body, besides `self.generic_visit(node)` which is part of the AST traversal mechanism."
                called_by: This method is called by the `ast.NodeVisitor`'s `visit` dispatch mechanism when traversing an AST that contains a function definition.
            error: null
          - identifier: visit_Call
            description:
              overall: "This method is invoked when an `ast.Call` node is encountered, representing a function or method call. It attempts to resolve the qualified name of the called entity using `_resolve_call_qname`. If the callee is found in the `definitions`, it records the call, including the caller's file, line number, full identifier, and type (module, local function, method, or function), storing this information in `self.calls` for later analysis."
              parameters[2]{name,type,description}:
                self,CallResolverVisitor,The instance of the visitor.
                node,ast.Call,The AST node representing the call expression.
              returns[0]:
              usage_context:
                calls: This method calls `self._resolve_call_qname` to determine the qualified name of the called entity.
                called_by: This method is called by the `ast.NodeVisitor`'s `visit` dispatch mechanism when traversing an AST that contains a function or method call.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method processes `import` statements. For each imported module or alias, it stores the mapping from the local name (or alias) to the original module name in `self.scope`. This scope is crucial for resolving fully qualified names of imported entities later during call analysis."
              parameters[2]{name,type,description}:
                self,CallResolverVisitor,The instance of the visitor.
                node,ast.Import,The AST node representing an import statement.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call other functions or methods within its body, besides `self.generic_visit(node)` which is part of the AST traversal mechanism."
                called_by: This method is called by the `ast.NodeVisitor`'s `visit` dispatch mechanism when traversing an AST that contains an import statement.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method handles `from ... import ...` statements. It determines the full module path for the imported names, correctly accounting for relative imports based on `node.level`. It then stores the mapping from the local name (or alias) to its fully qualified path in `self.scope`, similar to `visit_Import`, to aid in subsequent name resolution."
              parameters[2]{name,type,description}:
                self,CallResolverVisitor,The instance of the visitor.
                node,ast.ImportFrom,The AST node representing a `from ... import ...` statement.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call other functions or methods within its body, besides `self.generic_visit(node)` which is part of the AST traversal mechanism."
                called_by: This method is called by the `ast.NodeVisitor`'s `visit` dispatch mechanism when traversing an AST that contains a `from ... import ...` statement.
            error: null
          - identifier: visit_Assign
            description:
              overall: "This method processes assignment statements to identify instances of classes. If an assignment involves a call to a constructor (e.g., `x = MyClass()`), it attempts to resolve the qualified name of the class. If successful and the class is a known definition, it records the type of the assigned variable in `self.instance_types`, mapping the variable name to its qualified class name. This mapping is used to resolve method calls on these instances."
              parameters[2]{name,type,description}:
                self,CallResolverVisitor,The instance of the visitor.
                node,ast.Assign,The AST node representing an assignment statement.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call other functions or methods within its body, besides `self.generic_visit(node)` which is part of the AST traversal mechanism."
                called_by: This method is called by the `ast.NodeVisitor`'s `visit` dispatch mechanism when traversing an AST that contains an assignment statement.
            error: null
          - identifier: _resolve_call_qname
            description:
              overall: "This helper method attempts to determine the fully qualified name (QName) of a function or method being called. It handles two main cases: direct name calls (`ast.Name`) and attribute calls (`ast.Attribute`). For `ast.Name`, it checks `self.scope` (for imports) and local module definitions. For `ast.Attribute`, it checks `self.instance_types` (for object methods) and `self.scope` (for module-level attributes/functions) to accurately resolve the target."
              parameters[2]{name,type,description}:
                self,CallResolverVisitor,The instance of the visitor.
                func_node,ast.expr,"The AST node representing the function part of a call expression (e.g., `ast.Name` or `ast.Attribute`)."
              returns[1]{name,type,description}:
                name,string | None,"The fully qualified name of the callable if resolved, otherwise `None`."
              usage_context:
                calls: This method does not explicitly call other functions or methods within its body.
                called_by: This method is called by `CallResolverVisitor.visit_Call` to resolve the qualified name of a function or method being invoked.
            error: null
        usage_context:
          dependencies: The class depends on `backend.relationship_analyzer.path_to_module` for converting file paths to module paths.
          instantiated_by: This class is not explicitly shown to be instantiated by any other components in the provided context.
      error: null
    schemas.types.ParameterDescription:
      identifier: schemas.types.ParameterDescription
      description:
        overall: "The ParameterDescription class is a Pydantic BaseModel designed to represent the structured details of a single function parameter. It serves as a data container, ensuring that parameter information‚Äîspecifically its name, type, and a descriptive explanation‚Äîis consistently formatted and validated. This class is fundamental for building more complex data structures that require detailed function signature analysis."
        init_method:
          description: "This class, being a Pydantic BaseModel, automatically generates an `__init__` method. It initializes an instance of `ParameterDescription` by accepting values for `name`, `type`, and `description`, which are then validated and stored as instance attributes."
          parameters[3]{name,type,description}:
            name,str,The name of the parameter.
            type,str,The inferred type of the parameter.
            description,str,A brief description of the parameter's purpose.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list external functional dependencies in the provided context.
          instantiated_by: The specific instantiation points for this class are not provided in the current context.
      error: null
    schemas.types.ReturnDescription:
      identifier: schemas.types.ReturnDescription
      description:
        overall: "The ReturnDescription class is a Pydantic BaseModel designed to standardize the description of a function's return value. It encapsulates three essential pieces of information: the name of the returned item, its data type, and a textual description of its purpose or content. This model facilitates structured documentation and validation of function outputs within a larger system."
        init_method:
          description: "This class, being a Pydantic BaseModel, automatically generates an `__init__` method. This constructor initializes an instance of `ReturnDescription` by accepting `name`, `type`, and `description` as keyword arguments, which are then validated and assigned as instance attributes."
          parameters[3]{name,type,description}:
            name,str,"The name of the return value, if it has a specific identifier."
            type,str,The Python type hint or a descriptive string of the return value's data type.
            description,str,A detailed explanation of what the return value represents or its purpose.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list external functional dependencies in the provided context.
          instantiated_by: The specific points of instantiation for this class are not provided in the current context.
      error: null
    schemas.types.UsageContext:
      identifier: schemas.types.UsageContext
      description:
        overall: The UsageContext class is a Pydantic BaseModel designed to encapsulate information about how a function or method interacts with other parts of a system. It provides a structured way to describe what functions or methods an entity calls and by what other entities it is called. This class serves as a data structure for representing the contextual usage of code components.
        init_method:
          description: "As a Pydantic BaseModel, the `__init__` method is automatically generated. It accepts keyword arguments corresponding to its defined fields, `calls` and `called_by`, and initializes these instance attributes after performing type validation."
          parameters[2]{name,type,description}:
            calls,str,"A string summarizing the functions, methods, or classes that this entity calls within its implementation."
            called_by,str,"A string summarizing the functions, methods, or classes that invoke or instantiate this entity."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies.
          instantiated_by: The specific points of instantiation for this class are not provided in the current context.
      error: null
    schemas.types.FunctionDescription:
      identifier: schemas.types.FunctionDescription
      description:
        overall: "The FunctionDescription class is a Pydantic BaseModel designed to encapsulate a comprehensive analysis of a Python function. It serves as a structured data container for various aspects of a function, including its high-level purpose, detailed parameter descriptions, expected return values, and its operational context within a software system. This class ensures data integrity and type validation for function analysis reports."
        init_method:
          description: "As a Pydantic BaseModel, the `__init__` method for FunctionDescription is implicitly generated. It handles the instantiation of a FunctionDescription object by validating and assigning values to its fields: `overall`, `parameters`, `returns`, and `usage_context` based on the provided arguments."
          parameters[4]{name,type,description}:
            overall,str,A high-level summary of the function's purpose and implementation.
            parameters,"List[ParameterDescription]",A list of objects describing each parameter of the function.
            returns,"List[ReturnDescription]",A list of objects describing the return values of the function.
            usage_context,UsageContext,An object describing where the function is called and what it calls.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly declare external functional dependencies within its definition.
          instantiated_by: The instantiation points for this class are not explicitly provided in the context.
      error: null
    schemas.types.FunctionAnalysis:
      identifier: schemas.types.FunctionAnalysis
      description:
        overall: "The FunctionAnalysis class is a Pydantic BaseModel designed to structure and standardize the comprehensive analysis of a Python function. It serves as a data container, encapsulating the function's unique identifier, a detailed description object, and an optional field for reporting any errors encountered during its analysis. This model is crucial for ensuring consistent data representation across a system that processes and documents Python functions."
        init_method:
          description: "This class, being a Pydantic BaseModel, implicitly generates a constructor that initializes its fields: `identifier`, `description`, and an optional `error`. These fields are directly mapped from the arguments provided during instantiation, ensuring type validation and data integrity."
          parameters[3]{name,type,description}:
            identifier,str,The unique name or identifier of the function being analyzed.
            description,FunctionDescription,"A detailed analysis object containing the function's purpose, parameters, returns, and usage context."
            error,"Optional[str]",An optional string describing any error encountered during the function's analysis. Defaults to None.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies.
          instantiated_by: The specific points of instantiation for this class are not provided in the current context.
      error: null
    schemas.types.ConstructorDescription:
      identifier: schemas.types.ConstructorDescription
      description:
        overall: "The ConstructorDescription class is a Pydantic BaseModel designed to structure the documentation for a Python class's `__init__` method. It serves as a data model to hold a textual description of the constructor and a list of its parameters, each described by a ParameterDescription object. This class provides a standardized format for representing constructor details within a larger documentation or analysis system."
        init_method:
          description: The `__init__` method for ConstructorDescription is implicitly generated by Pydantic's BaseModel. It initializes an instance of ConstructorDescription by validating and assigning the provided `description` string and `parameters` list to the corresponding instance attributes. This ensures that all constructor descriptions conform to the defined schema.
          parameters[2]{name,type,description}:
            description,str,A string providing a summary or detailed explanation of the constructor's purpose and behavior.
            parameters,"List[ParameterDescription]","A list of ParameterDescription objects, each detailing a single parameter accepted by the constructor, including its name, type, and individual description."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external dependencies within the provided context.
          instantiated_by: The specific locations where this class is instantiated are not provided in the current context.
      error: null
    schemas.types.ClassContext:
      identifier: schemas.types.ClassContext
      description:
        overall: "The ClassContext class is a Pydantic BaseModel designed to structure and validate information about a class's external dependencies and its instantiation points. It acts as a data container, ensuring that context-related metadata is consistently represented for analysis or documentation purposes."
        init_method:
          description: "As a Pydantic BaseModel, the `__init__` method for ClassContext is implicitly generated by Pydantic. It automatically handles the validation and assignment of the `dependencies` and `instantiated_by` fields based on the arguments provided during object creation."
          parameters[2]{name,type,description}:
            dependencies,str,A string describing the external dependencies of the class.
            instantiated_by,str,A string indicating the location or context where this class is instantiated.
        methods[0]:
        usage_context:
          dependencies: No explicit external dependencies were provided in the input context for this class.
          instantiated_by: No explicit instantiation points were provided in the input context for this class.
      error: null
    schemas.types.ClassDescription:
      identifier: schemas.types.ClassDescription
      description:
        overall: "The `ClassDescription` class is a Pydantic BaseModel designed to encapsulate a comprehensive analysis of another Python class. It acts as a structured data container, holding a high-level summary of the class, detailed information about its constructor, a list of analyses for each of its methods, and its overall usage context within a larger system. This schema is crucial for organizing and standardizing the output of class analysis."
        init_method:
          description: "As a Pydantic BaseModel, `ClassDescription` implicitly generates its `__init__` method based on the defined fields. Instances are initialized by providing values for `overall`, `init_method`, `methods`, and `usage_context` as keyword arguments, which Pydantic then validates and assigns to the corresponding instance attributes."
          parameters[0]:
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external dependencies in the provided context.
          instantiated_by: The instantiation points for this class are not specified in the provided context.
      error: null
    schemas.types.ClassAnalysis:
      identifier: schemas.types.ClassAnalysis
      description:
        overall: "The ClassAnalysis class serves as the root Pydantic model for representing a comprehensive analysis of a Python class. It encapsulates the class's unique identifier, a detailed ClassDescription object containing its constructor and method analyses, and an optional error field to indicate any issues during analysis. This model is designed to structure the output of an AI Code Analyst, providing a standardized format for class metadata and behavioral insights."
        init_method:
          description: "This class does not explicitly define an __init__ method. As a Pydantic BaseModel, its constructor is implicitly generated by Pydantic based on the defined fields, allowing instantiation with keyword arguments corresponding to its attributes."
          parameters[0]:
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies.
          instantiated_by: The instantiation points for this class are not provided in the current context.
      error: null
    schemas.types.CallInfo:
      identifier: schemas.types.CallInfo
      description:
        overall: "The CallInfo class is a Pydantic BaseModel designed to represent a specific call event within a system, such as a function call or class instantiation. It serves as a structured data container, encapsulating details like the originating file, the name of the calling function, the mode of the call (e.g., 'method', 'function'), and the line number where the call occurs. This model is primarily used to track and store information about how and where other components are utilized, specifically in 'called_by' and 'instantiated_by' lists."
        init_method:
          description: "The CallInfo class, being a Pydantic BaseModel, uses an implicitly generated constructor. This constructor initializes the instance attributes `file`, `function`, `mode`, and `line` based on the provided arguments, ensuring data validation and type enforcement according to the defined schema."
          parameters[4]{name,type,description}:
            file,str,The path to the file where the call event occurred.
            function,str,The name of the function or method that performed the call.
            mode,str,"The type or mode of the call, such as 'method', 'function', or 'module'."
            line,int,The line number in the file where the call event is located.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly depend on other components within the analyzed context.
          instantiated_by: This class is not explicitly shown to be instantiated by other components within the analyzed context.
      error: null
    schemas.types.FunctionContextInput:
      identifier: schemas.types.FunctionContextInput
      description:
        overall: "This class serves as a Pydantic data model, `FunctionContextInput`, designed to encapsulate structured context for analyzing a function. It defines two primary attributes: `calls`, which is a list of strings representing functions or methods called by the target function, and `called_by`, a list of `CallInfo` objects indicating where the target function is invoked. This model provides a standardized format for exchanging function-specific contextual information within a larger system."
        init_method:
          description: "The class implicitly initializes its attributes `calls` and `called_by` through Pydantic's `BaseModel` constructor. Pydantic handles the validation and assignment of values provided during instantiation, ensuring that the data conforms to the specified types."
          parameters[0]:
        methods[0]:
        usage_context:
          dependencies: This class has no explicit external functional dependencies listed in the provided context.
          instantiated_by: The specific locations where this class is instantiated are not provided in the current context.
      error: null
    schemas.types.FunctionAnalysisInput:
      identifier: schemas.types.FunctionAnalysisInput
      description:
        overall: "The `FunctionAnalysisInput` class is a Pydantic BaseModel designed to serve as a structured input for initiating a function analysis. It defines the necessary data fields, such as the analysis mode, the function's identifier, its source code, relevant import statements, and additional contextual information. This class acts as a data contract, ensuring that all required information is provided in a consistent format before proceeding with the analysis."
        init_method:
          description: "The `FunctionAnalysisInput` class utilizes an implicitly generated `__init__` method from Pydantic's BaseModel. This constructor is responsible for validating and initializing the instance attributes based on the provided input data, ensuring they conform to the specified types and constraints."
          parameters[5]{name,type,description}:
            mode,"Literal[\"function_analysis\"]","A literal string indicating the analysis mode, fixed to 'function_analysis' to specify the type of analysis being requested."
            identifier,str,The unique name or identifier of the function that is to be analyzed.
            source_code,str,"The raw source code of the entire function definition, including its signature and body."
            imports,"List[str]","A list of strings, each representing an import statement relevant to the function's execution context."
            context,FunctionContextInput,"An object containing additional contextual information pertinent to the function's analysis, such as its dependencies or callers."
        methods[0]:
        usage_context:
          dependencies: "This class does not explicitly list external functional dependencies within its definition, relying on Pydantic's BaseModel for its core functionality."
          instantiated_by: "The instantiation points for this class are not provided in the current context, but it is typically instantiated by a system or service that requires a structured input for function analysis."
      error: null
    schemas.types.MethodContextInput:
      identifier: schemas.types.MethodContextInput
      description:
        overall: "The `MethodContextInput` class is a Pydantic BaseModel that defines a structured schema for representing the contextual information of a Python method. It serves as a data container, encapsulating key details such as the method's unique identifier, a list of other functions or methods it calls, a list of `CallInfo` objects indicating where it is invoked, its expected arguments, and its docstring. This model is crucial for standardizing the input format for method analysis within a larger system."
        init_method:
          description: "The `__init__` method for `MethodContextInput` is implicitly generated by Pydantic's `BaseModel`. It handles the instantiation of the class by validating and assigning values to its defined fields: `identifier`, `calls`, `called_by`, `args`, and `docstring`. This constructor ensures that instances conform to the specified data types and structure."
          parameters[5]{name,type,description}:
            identifier,str,The unique name or identifier of the method.
            calls,"List[str]","A list of identifiers for other methods, classes, or functions that this method calls."
            called_by,"List[CallInfo]",A list of `CallInfo` objects indicating where this method is called from.
            args,"List[str]",A list of argument names expected by the method.
            docstring,"Optional[str]","The docstring content of the method, if available, otherwise null."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in its provided context.
          instantiated_by: The specific locations where this class is instantiated are not provided in the current context.
      error: null
    schemas.types.ClassContextInput:
      identifier: schemas.types.ClassContextInput
      description:
        overall: "The ClassContextInput class is a Pydantic model designed to structure contextual information required for analyzing a Python class. It serves as a data container, holding lists of external dependencies, details about where the class is instantiated, and specific context for each of its methods. This model facilitates a comprehensive, structured approach to class analysis by aggregating relevant data points."
        init_method:
          description: "This class does not explicitly define an __init__ method. It inherits from pydantic.BaseModel, and its constructor is implicitly generated by Pydantic based on the defined fields."
          parameters[3]{name,type,description}:
            dependencies,"List[str]",A list of external dependencies relevant to the class being analyzed.
            instantiated_by,"List[CallInfo]",A list of CallInfo objects indicating where the class being analyzed is instantiated.
            method_context,"List[MethodContextInput]",A list of MethodContextInput objects providing specific context for each method within the class being analyzed.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly declare external dependencies within the provided context.
          instantiated_by: The instantiation points for this class are not specified in the provided context.
      error: null
    schemas.types.ClassAnalysisInput:
      identifier: schemas.types.ClassAnalysisInput
      description:
        overall: "The ClassAnalysisInput class is a Pydantic BaseModel designed to define the structured input required for generating a ClassAnalysis object. It acts as a schema to validate and organize the data needed for a class analysis process. This includes the mode of operation, the class identifier, its raw source code, relevant import statements, and additional contextual information."
        init_method:
          description: "The ClassAnalysisInput class does not explicitly define an `__init__` method. As a Pydantic BaseModel, its constructor is implicitly generated by Pydantic, allowing instances to be created by passing keyword arguments corresponding to its defined fields: `mode`, `identifier`, `source_code`, `imports`, and `context`."
          parameters[5]{name,type,description}:
            mode,"Literal[\"class_analysis\"]","Specifies the operational mode for the analysis, which is fixed to 'class_analysis'."
            identifier,str,The unique name or identifier of the Python class that is to be analyzed.
            source_code,str,The complete raw source code string of the class definition to be analyzed.
            imports,"List[str]","A list of import statements found in the original source file, potentially relevant to the class or its methods."
            context,ClassContextInput,"An object containing additional contextual information pertinent to the class analysis, such as dependencies and instantiation points."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies.
          instantiated_by: The instantiation points for this class are not explicitly provided in the context.
      error: null