basic_info:
  projekt_uebersicht:
    titel: Repo Onboarding Agent üöÄ
    beschreibung: ```
    aktueller_status: Information not found
    key_features: Information not found
    tech_stack: Information not found
  installation:
    dependencies: "- altair==4.2.2\n- annotated-types==0.7.0\n- anyio==4.11.0\n- attrs==25.4.0\n- bcrypt==5.0.0\n- blinker==1.9.0\n- cachetools==6.2.2\n- captcha==0.7.1\n- certifi==2025.11.12\n- cffi==2.0.0\n- charset-normalizer==3.4.4\n- click==8.3.1\n- colorama==0.4.6\n- contourpy==1.3.3\n- cryptography==46.0.3\n- cycler==0.12.1\n- distro==1.9.0\n- dnspython==2.8.0\n- dotenv==0.9.9\n- entrypoints==0.4\n- extra-streamlit-components==0.1.81\n- filetype==1.2.0\n- fonttools==4.61.0\n- gitdb==4.0.12\n- GitPython==3.1.45\n- google-ai-generativelanguage==0.9.0\n- google-api-core==2.28.1\n- google-auth==2.43.0\n- googleapis-common-protos==1.72.0\n- grpcio==1.76.0\n- grpcio-status==1.76.0\n- h11==0.16.0\n- httpcore==1.0.9\n- httpx==0.28.1\n- idna==3.11\n- Jinja2==3.1.6\n- jiter==0.12.0\n- jsonpatch==1.33\n- jsonpointer==3.0.0\n- jsonschema==4.25.1\n- jsonschema-specifications==2025.9.1\n- kiwisolver==1.4.9\n- langchain==1.0.8\n- langchain-core==1.1.0\n- langchain-google-genai==3.1.0\n- langchain-ollama==1.0.0\n- langchain-openai==1.1.0\n- langgraph==1.0.3\n- langgraph-checkpoint==3.0.1\n- langgraph-prebuilt==1.0.5\n- langgraph-sdk==0.2.9\n- langsmith==0.4.46\n- MarkupSafe==3.0.3\n- matplotlib==3.10.7\n- narwhals==2.12.0\n- networkx==3.6\n- numpy==2.3.5\n- ollama==0.6.1\n- openai==2.8.1\n- orjson==3.11.4\n- ormsgpack==1.12.0\n- packaging==25.0\n- pandas==2.3.3\n- pillow==12.0.0\n- proto-plus==1.26.1\n- protobuf==6.33.1\n- pyarrow==21.0.0\n- pyasn1==0.6.1\n- pyasn1_modules==0.4.2\n- pycparser==2.23\n- pydantic==2.12.4\n- pydantic_core==2.41.5\n- pydeck==0.9.1\n- PyJWT==2.10.1\n- pymongo==4.15.4\n- pyparsing==3.2.5\n- python-dateutil==2.9.0.post0\n- python-dotenv==1.2.1\n- pytz==2025.2\n- PyYAML==6.0.3\n- referencing==0.37.0\n- regex==2025.11.3\n- requests==2.32.5\n- requests-toolbelt==1.0.0\n- rpds-py==0.29.0\n- rsa==4.9.1\n- setuptools==75.9.1\n- six==1.17.0\n- smmap==5.0.2\n- sniffio==1.3.1\n- streamlit==1.51.0\n- streamlit-authenticator==0.4.2\n- streamlit-mermaid==0.3.0\n- tenacity==9.1.2\n- tiktoken==0.12.0\n- toml==0.10.2\n- toolz==1.1.0\n- toon_format @ git+https://github.com/toon-format/toon-python.git@9c4f0c0c24f2a0b0b376315f4b8707f8c9006de6\n- tornado==6.5.2\n- tqdm==4.67.1\n- typing-inspection==0.4.2\n- typing_extensions==4.15.0\n- tzdata==2025.2\n- urllib3==2.5.0\n- watchdog==6.0.0\n- xxhash==3.6.0\n- zstandard==0.25.0\n- nbformat"
    setup_anleitung: Information not found
    quick_start_guide: Information not found
file_tree:
  name: root
  type: directory
  children[16]:
    - path: .env.example
      name: .env.example
      size: 48
      type: file
    - path: .gitignore
      name: .gitignore
      size: 184
      type: file
    - name: SystemPrompts
      type: directory
      children[6]{path,name,size,type}:
        SystemPrompts/SystemPromptClassHelperLLM.txt,SystemPromptClassHelperLLM.txt,6645,file
        SystemPrompts/SystemPromptFunctionHelperLLM.txt,SystemPromptFunctionHelperLLM.txt,5546,file
        SystemPrompts/SystemPromptHelperLLM.txt,SystemPromptHelperLLM.txt,9350,file
        SystemPrompts/SystemPromptMainLLM.txt,SystemPromptMainLLM.txt,8380,file
        SystemPrompts/SystemPromptMainLLMToon.txt,SystemPromptMainLLMToon.txt,8456,file
        SystemPrompts/SystemPromptNotebookLLM.txt,SystemPromptNotebookLLM.txt,6921,file
    - path: analysis_output.json
      name: analysis_output.json
      size: 569396
      type: file
    - name: backend
      type: directory
      children[12]{path,name,size,type}:
        backend/AST_Schema.py,AST_Schema.py,6622,file
        backend/File_Dependency.py,File_Dependency.py,7384,file
        backend/HelperLLM.py,HelperLLM.py,17280,file
        backend/MainLLM.py,MainLLM.py,4281,file
        backend/__init__.py,__init__.py,0,file
        backend/basic_info.py,basic_info.py,7284,file
        backend/callgraph.py,callgraph.py,9020,file
        backend/converter.py,converter.py,3594,file
        backend/getRepo.py,getRepo.py,4739,file
        backend/main.py,main.py,25591,file
        backend/relationship_analyzer.py,relationship_analyzer.py,8279,file
        backend/scads_key_test.py,scads_key_test.py,663,file
    - name: database
      type: directory
      children[1]{path,name,size,type}:
        database/db.py,db.py,8222,file
    - name: frontend
      type: directory
      children[4]:
        - name: .streamlit
          type: directory
          children[1]{path,name,size,type}:
            frontend/.streamlit/config.toml,config.toml,165,file
        - path: frontend/__init__.py
          name: __init__.py
          size: 0
          type: file
        - path: frontend/frontend.py
          name: frontend.py
          size: 31176
          type: file
        - name: gifs
          type: directory
          children[1]{path,name,size,type}:
            frontend/gifs/4j.gif,4j.gif,3306723,file
    - name: notizen
      type: directory
      children[8]:
        - path: notizen/Report Agenda.txt
          name: Report Agenda.txt
          size: 3492
          type: file
        - path: notizen/Zwischenpraesentation Agenda.txt
          name: Zwischenpraesentation Agenda.txt
          size: 809
          type: file
        - path: notizen/doc_bestandteile.md
          name: doc_bestandteile.md
          size: 847
          type: file
        - name: grafiken
          type: directory
          children[4]:
            - name: "1"
              type: directory
              children[5]{path,name,size,type}:
                notizen/grafiken/1/File_Dependency_Graph_Repo.dot,File_Dependency_Graph_Repo.dot,1227,file
                notizen/grafiken/1/global_callgraph.png,global_callgraph.png,940354,file
                notizen/grafiken/1/global_graph.png,global_graph.png,4756519,file
                notizen/grafiken/1/global_graph_2.png,global_graph_2.png,242251,file
                notizen/grafiken/1/repo.dot,repo.dot,13870,file
            - name: "2"
              type: directory
              children[12]{path,name,size,type}:
                notizen/grafiken/2/FDG_repo.dot,FDG_repo.dot,9150,file
                notizen/grafiken/2/fdg_graph.png,fdg_graph.png,425374,file
                notizen/grafiken/2/fdg_graph_2.png,fdg_graph_2.png,4744718,file
                notizen/grafiken/2/filtered_callgraph_flask.png,filtered_callgraph_flask.png,614631,file
                notizen/grafiken/2/filtered_callgraph_repo-agent.png,filtered_callgraph_repo-agent.png,280428,file
                notizen/grafiken/2/filtered_callgraph_repo-agent_3.png,filtered_callgraph_repo-agent_3.png,1685838,file
                notizen/grafiken/2/filtered_repo_callgraph_flask.dot,filtered_repo_callgraph_flask.dot,19984,file
                notizen/grafiken/2/filtered_repo_callgraph_repo-agent-3.dot,filtered_repo_callgraph_repo-agent-3.dot,20120,file
                notizen/grafiken/2/filtered_repo_callgraph_repo-agent.dot,filtered_repo_callgraph_repo-agent.dot,3118,file
                notizen/grafiken/2/global_callgraph.png,global_callgraph.png,608807,file
                notizen/grafiken/2/graph_flask.md,graph_flask.md,17398,file
                notizen/grafiken/2/repo.dot,repo.dot,19984,file
            - name: Flask-Repo
              type: directory
              children[65]{path,name,size,type}:
                notizen/grafiken/Flask-Repo/__init__.dot,__init__.dot,89,file
                notizen/grafiken/Flask-Repo/__main__.dot,__main__.dot,87,file
                notizen/grafiken/Flask-Repo/app.dot,app.dot,78,file
                notizen/grafiken/Flask-Repo/auth.dot,auth.dot,1126,file
                notizen/grafiken/Flask-Repo/blog.dot,blog.dot,939,file
                notizen/grafiken/Flask-Repo/blueprints.dot,blueprints.dot,4601,file
                notizen/grafiken/Flask-Repo/cli.dot,cli.dot,8269,file
                notizen/grafiken/Flask-Repo/conf.dot,conf.dot,489,file
                notizen/grafiken/Flask-Repo/config.dot,config.dot,2130,file
                notizen/grafiken/Flask-Repo/conftest.dot,conftest.dot,1756,file
                notizen/grafiken/Flask-Repo/ctx.dot,ctx.dot,3009,file
                notizen/grafiken/Flask-Repo/db.dot,db.dot,782,file
                notizen/grafiken/Flask-Repo/debughelpers.dot,debughelpers.dot,1903,file
                notizen/grafiken/Flask-Repo/factory.dot,factory.dot,220,file
                notizen/grafiken/Flask-Repo/flask.dot,flask.dot,82,file
                notizen/grafiken/Flask-Repo/globals.dot,globals.dot,370,file
                notizen/grafiken/Flask-Repo/hello.dot,hello.dot,152,file
                notizen/grafiken/Flask-Repo/helpers.dot,helpers.dot,2510,file
                notizen/grafiken/Flask-Repo/importerrorapp.dot,importerrorapp.dot,155,file
                notizen/grafiken/Flask-Repo/logging.dot,logging.dot,564,file
                notizen/grafiken/Flask-Repo/make_celery.dot,make_celery.dot,99,file
                notizen/grafiken/Flask-Repo/multiapp.dot,multiapp.dot,88,file
                notizen/grafiken/Flask-Repo/provider.dot,provider.dot,1769,file
                notizen/grafiken/Flask-Repo/scaffold.dot,scaffold.dot,3700,file
                notizen/grafiken/Flask-Repo/sessions.dot,sessions.dot,4178,file
                notizen/grafiken/Flask-Repo/signals.dot,signals.dot,133,file
                notizen/grafiken/Flask-Repo/tag.dot,tag.dot,3750,file
                notizen/grafiken/Flask-Repo/tasks.dot,tasks.dot,312,file
                notizen/grafiken/Flask-Repo/templating.dot,templating.dot,2277,file
                notizen/grafiken/Flask-Repo/test_appctx.dot,test_appctx.dot,2470,file
                notizen/grafiken/Flask-Repo/test_async.dot,test_async.dot,1919,file
                notizen/grafiken/Flask-Repo/test_auth.dot,test_auth.dot,725,file
                notizen/grafiken/Flask-Repo/test_basic.dot,test_basic.dot,17383,file
                notizen/grafiken/Flask-Repo/test_blog.dot,test_blog.dot,1105,file
                notizen/grafiken/Flask-Repo/test_blueprints.dot,test_blueprints.dot,11515,file
                notizen/grafiken/Flask-Repo/test_cli.dot,test_cli.dot,6435,file
                notizen/grafiken/Flask-Repo/test_config.dot,test_config.dot,2854,file
                notizen/grafiken/Flask-Repo/test_config.png,test_config.png,288693,file
                notizen/grafiken/Flask-Repo/test_converters.dot,test_converters.dot,937,file
                notizen/grafiken/Flask-Repo/test_db.dot,test_db.dot,481,file
                notizen/grafiken/Flask-Repo/test_factory.dot,test_factory.dot,201,file
                notizen/grafiken/Flask-Repo/test_helpers.dot,test_helpers.dot,5857,file
                notizen/grafiken/Flask-Repo/test_instance_config.dot,test_instance_config.dot,1249,file
                notizen/grafiken/Flask-Repo/test_js_example.dot,test_js_example.dot,453,file
                notizen/grafiken/Flask-Repo/test_json.dot,test_json.dot,4021,file
                notizen/grafiken/Flask-Repo/test_json_tag.dot,test_json_tag.dot,1452,file
                notizen/grafiken/Flask-Repo/test_logging.dot,test_logging.dot,1348,file
                notizen/grafiken/Flask-Repo/test_regression.dot,test_regression.dot,763,file
                notizen/grafiken/Flask-Repo/test_reqctx.dot,test_reqctx.dot,3809,file
                notizen/grafiken/Flask-Repo/test_request.dot,test_request.dot,668,file
                notizen/grafiken/Flask-Repo/test_session_interface.dot,test_session_interface.dot,667,file
                notizen/grafiken/Flask-Repo/test_signals.dot,test_signals.dot,1671,file
                notizen/grafiken/Flask-Repo/test_subclassing.dot,test_subclassing.dot,612,file
                notizen/grafiken/Flask-Repo/test_templating.dot,test_templating.dot,5129,file
                notizen/grafiken/Flask-Repo/test_testing.dot,test_testing.dot,4081,file
                notizen/grafiken/Flask-Repo/test_user_error_handler.dot,test_user_error_handler.dot,5984,file
                notizen/grafiken/Flask-Repo/test_views.dot,test_views.dot,2641,file
                notizen/grafiken/Flask-Repo/testing.dot,testing.dot,2785,file
                notizen/grafiken/Flask-Repo/typing.dot,typing.dot,86,file
                notizen/grafiken/Flask-Repo/typing_app_decorators.dot,typing_app_decorators.dot,500,file
                notizen/grafiken/Flask-Repo/typing_error_handler.dot,typing_error_handler.dot,420,file
                notizen/grafiken/Flask-Repo/typing_route.dot,typing_route.dot,1717,file
                notizen/grafiken/Flask-Repo/views.dot,views.dot,1125,file
                notizen/grafiken/Flask-Repo/wrappers.dot,wrappers.dot,911,file
                notizen/grafiken/Flask-Repo/wsgi.dot,wsgi.dot,19,file
            - name: Repo-onboarding
              type: directory
              children[15]{path,name,size,type}:
                notizen/grafiken/Repo-onboarding/AST.dot,AST.dot,1361,file
                notizen/grafiken/Repo-onboarding/Frontend.dot,Frontend.dot,538,file
                notizen/grafiken/Repo-onboarding/HelperLLM.dot,HelperLLM.dot,1999,file
                notizen/grafiken/Repo-onboarding/HelperLLM.png,HelperLLM.png,234861,file
                notizen/grafiken/Repo-onboarding/MainLLM.dot,MainLLM.dot,2547,file
                notizen/grafiken/Repo-onboarding/agent.dot,agent.dot,2107,file
                notizen/grafiken/Repo-onboarding/basic_info.dot,basic_info.dot,1793,file
                notizen/grafiken/Repo-onboarding/callgraph.dot,callgraph.dot,1478,file
                notizen/grafiken/Repo-onboarding/getRepo.dot,getRepo.dot,1431,file
                notizen/grafiken/Repo-onboarding/graph_AST.png,graph_AST.png,132743,file
                notizen/grafiken/Repo-onboarding/graph_AST2.png,graph_AST2.png,12826,file
                notizen/grafiken/Repo-onboarding/graph_AST3.png,graph_AST3.png,136016,file
                notizen/grafiken/Repo-onboarding/main.dot,main.dot,1091,file
                notizen/grafiken/Repo-onboarding/tools.dot,tools.dot,1328,file
                notizen/grafiken/Repo-onboarding/types.dot,types.dot,133,file
        - path: notizen/notizen.md
          name: notizen.md
          size: 4937
          type: file
        - path: notizen/paul_notizen.md
          name: paul_notizen.md
          size: 2033
          type: file
        - path: notizen/praesentation_notizen.md
          name: praesentation_notizen.md
          size: 2738
          type: file
        - path: notizen/technische_notizen.md
          name: technische_notizen.md
          size: 3616
          type: file
    - path: output.json
      name: output.json
      size: 454008
      type: file
    - path: output.toon
      name: output.toon
      size: 341854
      type: file
    - path: readme.md
      name: readme.md
      size: 1905
      type: file
    - path: requirements.txt
      name: requirements.txt
      size: 4286
      type: file
    - name: result
      type: directory
      children[28]{path,name,size,type}:
        result/ast_schema_01_12_2025_11-49-24.json,ast_schema_01_12_2025_11-49-24.json,201215,file
        result/notebook_report_23_12_2025_12-56-24_NotebookLLM_gemini-2.5-flash.md,notebook_report_23_12_2025_12-56-24_NotebookLLM_gemini-2.5-flash.md,15776,file
        result/notebook_report_27_12_2025_15-06-09_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-06-09_NotebookLLM_gemini-2.5-flash.md,14804,file
        result/notebook_report_27_12_2025_15-09-29_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-09-29_NotebookLLM_gemini-2.5-flash.md,17598,file
        result/notebook_report_27_12_2025_15-26-34_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-26-34_NotebookLLM_gemini-2.5-flash.md,20458,file
        result/notebook_report_27_12_2025_15-33-06_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-33-06_NotebookLLM_gemini-2.5-flash.md,15268,file
        result/notebook_report_29_12_2025_15-03-21_NotebookLLM_gemini-2.5-flash.md,notebook_report_29_12_2025_15-03-21_NotebookLLM_gemini-2.5-flash.md,16111,file
        result/report_01_12_2025_12-26-46_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_12-26-46_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,79523,file
        result/report_01_12_2025_12-55-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_12-55-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,143883,file
        result/report_01_12_2025_13-37-30_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_13-37-30_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,141924,file
        result/report_01_12_2025_14-15-04_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_14-15-04_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,129176,file
        result/report_01_12_2025_14-42-38_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_14-42-38_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,38933,file
        result/report_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,138015,file
        result/report_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.md,report_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.md,127747,file
        result/report_03_12_2025_22-46-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_03_12_2025_22-46-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,80305,file
        result/report_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,163921,file
        result/report_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.md,report_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.md,128520,file
        result/report_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.md,report_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.md,7245,file
        result/report_14_11_2025_14-52-36.md,report_14_11_2025_14-52-36.md,16393,file
        result/report_14_11_2025_15-21-53.md,report_14_11_2025_15-21-53.md,108585,file
        result/report_14_11_2025_15-26-24.md,report_14_11_2025_15-26-24.md,11659,file
        result/report_21_11_2025_15-43-30.md,report_21_11_2025_15-43-30.md,57940,file
        result/report_21_11_2025_16-06-12.md,report_21_11_2025_16-06-12.md,76423,file
        result/report_22_11_2025_14-01-50_Helper_llama3_Main_geminipro.md,report_22_11_2025_14-01-50_Helper_llama3_Main_geminipro.md,39595,file
        result/report_22_11_2025_14-39-55_Helper_llama3_MainLLM_llama3.md,report_22_11_2025_14-39-55_Helper_llama3_MainLLM_llama3.md,2013,file
        result/result_2025-11-11_12-30-53.md,result_2025-11-11_12-30-53.md,1232,file
        result/result_2025-11-11_12-43-51.md,result_2025-11-11_12-43-51.md,33683,file
        result/result_2025-11-11_12-45-37.md,result_2025-11-11_12-45-37.md,1193,file
    - name: schemas
      type: directory
      children[1]{path,name,size,type}:
        schemas/types.py,types.py,7559,file
    - name: statistics
      type: directory
      children[5]{path,name,size,type}:
        statistics/savings_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.png,savings_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.png,23279,file
        statistics/savings_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.png,savings_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.png,23650,file
        statistics/savings_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.png,savings_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.png,23805,file
        statistics/savings_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.png,savings_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.png,22698,file
        statistics/savings_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.png,savings_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.png,22539,file
    - path: test.json
      name: test.json
      size: 241669
      type: file
ast_schema:
  files:
    "backend/AST_Schema.py":
      ast_nodes:
        imports[3]: ast,os,getRepo.GitRepository
        functions[1]:
          - mode: function_analysis
            identifier: backend.AST_Schema.path_to_module
            name: path_to_module
            args[2]: filepath,project_root
            docstring: Wandelt einen Dateipfad in einen Python-Modulpfad um.
            source_code: "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    \n    module_path = rel_path.replace(os.path.sep, '.')\n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n    return module_path"
            start_line: 5
            end_line: 18
            context:
              calls[0]:
              called_by[1]: backend.AST_Schema.ASTVisitor.__init__
        classes[2]:
          - mode: class_analysis
            identifier: backend.AST_Schema.ASTVisitor
            name: ASTVisitor
            docstring: null
            source_code: "class ASTVisitor(ast.NodeVisitor):\n    def __init__(self, source_code: str, file_path: str, project_root: str):\n        self.source_code = source_code\n        self.file_path = file_path\n        self.project_root = project_root\n        self.module_path = path_to_module(self.file_path, self.project_root)\n        self.schema = {\"imports\": [], \"functions\": [], \"classes\": []}\n        self._current_class = None\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(f\"{node.module}.{alias.name}\")\n        self.generic_visit(node)\n\n    def visit_ClassDef(self, node):\n        class_identifier = f\"{self.module_path}.{node.name}\"\n        class_info = {\n            \"mode\": \"class_analysis\",\n            \"identifier\": class_identifier,\n            \"name\": node.name,\n            \"docstring\": ast.get_docstring(node),\n            \"source_code\": ast.get_source_segment(self.source_code, node),\n            \"start_line\": node.lineno,\n            \"end_line\": node.end_lineno,\n            \"context\": {\n                \"dependencies\": [],\n                \"instantiated_by\": [],\n                \"method_context\": []\n            },   \n        }\n        self.schema[\"classes\"].append(class_info)\n        self._current_class = class_info \n        self.generic_visit(node)\n        self._current_class = None\n\n    def visit_FunctionDef(self, node):\n        if self._current_class:\n            method_identifier = f\"{self._current_class['identifier']}.{node.name}\"\n            method_context_info = {\n                \"identifier\": method_identifier,\n                \"name\": node.name,\n                \"calls\": [],\n                \"called_by\": [],\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n            }\n            self._current_class[\"context\"][\"method_context\"].append(method_context_info)\n        else:\n            func_identifier = f\"{self.module_path}.{node.name}\"\n            func_info = {\n                \"mode\": \"function_analysis\",\n                \"identifier\": func_identifier,\n                \"name\": node.name,\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"source_code\": ast.get_source_segment(self.source_code, node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n                \"context\": {\n                    \"calls\": [],\n                    \"called_by\": []\n                }\n            }\n            self.schema[\"functions\"].append(func_info)\n        self.generic_visit(node)\n    \n    def visit_AsyncFunctionDef(self, node):\n        self.visit_FunctionDef(node)"
            start_line: 20
            end_line: 94
            context:
              dependencies[1]: backend.AST_Schema.path_to_module
              instantiated_by[1]: backend.AST_Schema.ASTAnalyzer.analyze_repository
              method_context[6]:
                - identifier: backend.AST_Schema.ASTVisitor.__init__
                  name: __init__
                  calls[1]: backend.AST_Schema.path_to_module
                  called_by[0]:
                  args[4]: self,source_code,file_path,project_root
                  docstring: null
                  start_line: 21
                  end_line: 27
                - identifier: backend.AST_Schema.ASTVisitor.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 29
                  end_line: 32
                - identifier: backend.AST_Schema.ASTVisitor.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 34
                  end_line: 37
                - identifier: backend.AST_Schema.ASTVisitor.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 39
                  end_line: 58
                - identifier: backend.AST_Schema.ASTVisitor.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 60
                  end_line: 91
                - identifier: backend.AST_Schema.ASTVisitor.visit_AsyncFunctionDef
                  name: visit_AsyncFunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 93
                  end_line: 94
          - mode: class_analysis
            identifier: backend.AST_Schema.ASTAnalyzer
            name: ASTAnalyzer
            docstring: null
            source_code: "class ASTAnalyzer:\n\n    def __init__(self):\n        pass\n\n    def merge_relationship_data(self, full_schema: dict, raw_relationships: dict) -> dict:\n\n        outgoing_calls = raw_relationships.get(\"outgoing\", {})\n        incoming_calls = raw_relationships.get(\"incoming\", {})\n\n        for file_path, file_data in full_schema.get(\"files\", {}).items():\n            ast_nodes = file_data.get(\"ast_nodes\", {})\n            \n            for func in ast_nodes.get(\"functions\", []):\n                func_id = func.get(\"identifier\")\n                if func_id:\n                    func[\"context\"][\"calls\"] = outgoing_calls.get(func_id, [])\n                    func[\"context\"][\"called_by\"] = incoming_calls.get(func_id, [])\n\n            for cls in ast_nodes.get(\"classes\", []):\n                cls_id = cls.get(\"identifier\")\n                \n                if cls_id:\n                    cls[\"context\"][\"instantiated_by\"] = incoming_calls.get(cls_id, [])\n\n                class_dependencies = set()\n                \n                for method in cls[\"context\"].get(\"method_context\", []):\n                    method_id = method.get(\"identifier\")\n                    if method_id:\n                        m_calls = outgoing_calls.get(method_id, [])\n                        method[\"calls\"] = m_calls\n                        method[\"called_by\"] = incoming_calls.get(method_id, [])\n                        \n                        for callee in m_calls:\n                            if cls_id and not callee.startswith(f\"{cls_id}.\"):\n                                class_dependencies.add(callee)\n                \n                cls[\"context\"][\"dependencies\"] = sorted(list(class_dependencies))\n        \n        return full_schema\n\n    def analyze_repository(self, files: list, repo: GitRepository) -> dict:\n        full_schema = {\n            \"files\": {}\n        }\n\n        all_paths = [file_obj.path for file_obj in files]\n        if not all_paths:\n            return full_schema\n        \n        project_root = os.path.commonpath(all_paths)\n        if os.path.isfile(project_root):\n            project_root = os.path.dirname(project_root)\n\n        for file_obj in files:\n            if not file_obj.path.endswith('.py'):\n                continue\n            \n            file_content = file_obj.content\n            if not file_content.strip():\n                continue\n\n            try:\n                tree = ast.parse(file_content)\n                visitor = ASTVisitor(\n                    source_code=file_content, \n                    file_path=file_obj.path, \n                    project_root=project_root\n                )\n                visitor.visit(tree)\n                file_schema_nodes = visitor.schema\n\n                if file_schema_nodes[\"imports\"] or file_schema_nodes[\"functions\"] or file_schema_nodes[\"classes\"]:\n                    full_schema[\"files\"][file_obj.path] = {\n                        \"ast_nodes\": file_schema_nodes\n                    }\n\n            except (SyntaxError, ValueError) as e:\n                print(f\"Warnung: Konnte Datei '{file_obj.path}' nicht parsen. Fehler: {e}\")\n\n        return full_schema"
            start_line: 97
            end_line: 178
            context:
              dependencies[1]: backend.AST_Schema.ASTVisitor
              instantiated_by[1]: backend.main.main_workflow
              method_context[3]:
                - identifier: backend.AST_Schema.ASTAnalyzer.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 99
                  end_line: 100
                - identifier: backend.AST_Schema.ASTAnalyzer.merge_relationship_data
                  name: merge_relationship_data
                  calls[0]:
                  called_by[1]: backend.main.main_workflow
                  args[3]: self,full_schema,raw_relationships
                  docstring: null
                  start_line: 102
                  end_line: 137
                - identifier: backend.AST_Schema.ASTAnalyzer.analyze_repository
                  name: analyze_repository
                  calls[1]: backend.AST_Schema.ASTVisitor
                  called_by[1]: backend.main.main_workflow
                  args[3]: self,files,repo
                  docstring: null
                  start_line: 139
                  end_line: 178
    "backend/File_Dependency.py":
      ast_nodes:
        imports[17]: networkx,os,ast.Assign,ast.AST,ast.ClassDef,ast.FunctionDef,ast.Import,ast.ImportFrom,ast.Name,ast.NodeVisitor,ast.literal_eval,ast.parse,ast.walk,keyword.iskeyword,pathlib.Path,getRepo.GitRepository,callgraph.make_safe_dot
        functions[3]:
          - mode: function_analysis
            identifier: backend.File_Dependency.build_file_dependency_graph
            name: build_file_dependency_graph
            args[3]: filename,tree,repo_root
            docstring: null
            source_code: "def build_file_dependency_graph(filename: str, tree: AST, repo_root: str) -> nx.DiGraph:\n    graph = nx.DiGraph()\n\n    tree_visitor = FileDependencyGraph(filename, repo_root)\n    tree_visitor.visit(tree)\n\n    for caller, callees in tree_visitor.import_dependencies.items():\n        graph.add_node(caller)\n        graph.add_nodes_from(callees)\n        for callee in callees:\n            graph.add_edge(caller, callee)\n    \n    return graph"
            start_line: 153
            end_line: 165
            context:
              calls[1]: backend.File_Dependency.FileDependencyGraph
              called_by[1]: backend.File_Dependency.build_repository_graph
          - mode: function_analysis
            identifier: backend.File_Dependency.build_repository_graph
            name: build_repository_graph
            args[1]: repository
            docstring: null
            source_code: "def build_repository_graph(repository: GitRepository) -> nx.DiGraph:\n    all_files = repository.get_all_files()\n    repo_root = repository.temp_dir\n    global_graph = nx.DiGraph()\n\n    for file in all_files: \n        if not file.path.endswith(\".py\"):\n            continue\n        filename = str(os.path.basename(file.path)).removesuffix(\".py\")\n        tree = parse(file.content)\n        graph = build_file_dependency_graph(filename, tree, repo_root)\n        \n        for node in graph.nodes:\n            global_graph.add_node(node)\n\n        for caller, callee in graph.edges:\n            if callee:\n                global_graph.add_node(callee)\n                global_graph.add_edge(caller, callee)\n    \n    return global_graph"
            start_line: 167
            end_line: 187
            context:
              calls[1]: backend.File_Dependency.build_file_dependency_graph
              called_by[1]: backend.File_Dependency
          - mode: function_analysis
            identifier: backend.File_Dependency.get_all_temp_files
            name: get_all_temp_files
            args[1]: directory
            docstring: null
            source_code: "def get_all_temp_files(directory: str) -> list[Path]:\n    root_path = Path(directory).resolve()\n    all_files = [file.relative_to(root_path) for file in root_path.rglob(\"*.py\")]\n\n    return all_files"
            start_line: 189
            end_line: 193
            context:
              calls[0]:
              called_by[1]: backend.File_Dependency.FileDependencyGraph._resolve_module_name
        classes[1]:
          - mode: class_analysis
            identifier: backend.File_Dependency.FileDependencyGraph
            name: FileDependencyGraph
            docstring: null
            source_code: "class FileDependencyGraph(NodeVisitor):\n\n    import_dependencies: dict[str, set[str]] = {}\n    def __init__(self, filename: str, repo_root):\n        \"\"\"\n        Initialisiert den File Dependency Graphen\n\n        Args:\n\n        \"\"\"\n        self.filename = filename\n        self.repo_root = repo_root\n\n    def _resolve_module_name(self, node: ImportFrom) -> list[str]:\n        \"\"\"\n        L√∂st relative Imports der Form `from .. import name1, name2` auf.\n        Liefert die Liste der tats√§chlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\n        Wirft ImportError, wenn nichts aufgel√∂st werden konnte.\n        \"\"\"\n        level_depth = node.level\n        names = [alias.name for alias in node.names]  \n        all_files = get_all_temp_files(self.repo_root)  \n\n        candidates = [p for p in all_files if p.stem == Path(self.filename).stem or p.name == f\"{self.filename}.py\"]\n\n        if not candidates:\n            raise ImportError(f\"Kann aktuelle Datei '{self.filename}' im Repo nicht finden.\")\n\n\n        candidates.sort(key=lambda p: len(p.parts))\n        current_rel_path = candidates[0]\n        base_dir = current_rel_path.parent\n        if level_depth <= 0:\n            raise ImportError(\"Erwarteter relativer Import (level >= 1).\")\n        for _ in range(level_depth - 1):\n            if base_dir == base_dir.parent and len(base_dir.parts) == 0:\n                raise ImportError(f\"Relative Import-Ebene ({level_depth}) zu gro√ü f√ºr Datei '{current_rel_path}'.\")\n            base_dir = base_dir.parent\n\n        repo_root_path = Path(self.repo_root).resolve()\n        resolved: list[str] = []\n\n        def module_file_exists(rel_base: Path, name: str) -> bool:\n            file_path = repo_root_path / rel_base / f\"{name}.py\"\n            pkg_init = repo_root_path / rel_base / name / \"__init__.py\"\n            return file_path.exists() or pkg_init.exists()\n\n        def init_exports_symbol(rel_base: Path, symbol: str) -> bool:\n            \"\"\"\n            Pr√ºft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\n            oder als Name (funktion/klasse/assign) definiert ist.\n            \"\"\"\n            init_path = repo_root_path / rel_base / \"__init__.py\"\n            if not init_path.exists():\n                return False\n            try:\n                src = init_path.read_text(encoding=\"utf-8\")\n                mod = parse(src, filename=str(init_path))\n            except Exception:\n                return False\n\n            for node_ in walk(mod):\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == \"__all__\":\n                            try:\n                                value = literal_eval(node_.value)\n                                if isinstance(value, (list, tuple)) and symbol in value:\n                                    return True\n                            except Exception:\n                                pass\n                if isinstance(node_, (FunctionDef, ClassDef)) and node_.name == symbol:\n                    return True\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == symbol:\n                            return True\n            return False\n\n        for name in names:\n            if not name.isidentifier() or iskeyword(name):\n                continue\n\n            if module_file_exists(base_dir, name):\n                resolved.append(name)\n                continue\n            if init_exports_symbol(base_dir, name):\n                resolved.append(name)\n                continue\n\n        resolved = sorted(set(resolved))\n\n        if not resolved:\n            raise ImportError(\n                f\"Kein passendes Modul/Symbol f√ºr relative Import-Aufl√∂sung gefunden \"\n                f\"(level={level_depth}, names={names}, base_dir={base_dir})\"\n            )\n\n        return resolved\n    \n    def visit_Import(self, node: Import | ImportFrom, base_name: str | None = None):\n        for alias in node.names:\n        \n            if self.filename not in self.import_dependencies:\n                self.import_dependencies[self.filename] = set()\n\n            if base_name:\n                self.import_dependencies[self.filename].add(base_name)\n            else:\n                self.import_dependencies[self.filename].add(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node: ImportFrom):\n        \"\"\"\n        Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\n        also c, und dieser wird als callee f√ºr den caller, das File, gesetzt.\n        \"\"\"\n        module_name = node.module\n        if module_name:\n            module_base = module_name.split(\".\")[-1]\n            self.visit_Import(node, module_base)\n        else:\n            try:\n                resolved = self._resolve_module_name(node)\n                for base in resolved:\n                    self.visit_Import(node, base)\n            except ImportError as e:\n                print(f\"Aufl√∂sung eines relativen Imports fehlgeschlagen: {e}\")\n\n        self.generic_visit(node)"
            start_line: 22
            end_line: 151
            context:
              dependencies[3]: backend.File_Dependency.get_all_temp_files,backend.File_Dependency.init_exports_symbol,backend.File_Dependency.module_file_exists
              instantiated_by[1]: backend.File_Dependency.build_file_dependency_graph
              method_context[6]:
                - identifier: backend.File_Dependency.FileDependencyGraph.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[3]: self,filename,repo_root
                  docstring: "Initialisiert den File Dependency Graphen\n\nArgs:"
                  start_line: 25
                  end_line: 33
                - identifier: backend.File_Dependency.FileDependencyGraph._resolve_module_name
                  name: _resolve_module_name
                  calls[3]: backend.File_Dependency.get_all_temp_files,backend.File_Dependency.init_exports_symbol,backend.File_Dependency.module_file_exists
                  called_by[0]:
                  args[2]: self,node
                  docstring: "L√∂st relative Imports der Form `from .. import name1, name2` auf.\nLiefert die Liste der tats√§chlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\nWirft ImportError, wenn nichts aufgel√∂st werden konnte."
                  start_line: 35
                  end_line: 120
                - identifier: backend.File_Dependency.FileDependencyGraph.module_file_exists
                  name: module_file_exists
                  calls[0]:
                  called_by[0]:
                  args[2]: rel_base,name
                  docstring: null
                  start_line: 64
                  end_line: 67
                - identifier: backend.File_Dependency.FileDependencyGraph.init_exports_symbol
                  name: init_exports_symbol
                  calls[0]:
                  called_by[0]:
                  args[2]: rel_base,symbol
                  docstring: "Pr√ºft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\noder als Name (funktion/klasse/assign) definiert ist."
                  start_line: 69
                  end_line: 99
                - identifier: backend.File_Dependency.FileDependencyGraph.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[3]: self,node,base_name
                  docstring: null
                  start_line: 122
                  end_line: 132
                - identifier: backend.File_Dependency.FileDependencyGraph.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: "Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\nalso c, und dieser wird als callee f√ºr den caller, das File, gesetzt."
                  start_line: 134
                  end_line: 151
    "backend/HelperLLM.py":
      ast_nodes:
        imports[23]: os,json,logging,time,typing.List,typing.Dict,typing.Any,typing.Optional,typing.Union,dotenv.load_dotenv,langchain_google_genai.ChatGoogleGenerativeAI,langchain_ollama.ChatOllama,langchain_openai.ChatOpenAI,langchain.messages.HumanMessage,langchain.messages.SystemMessage,langchain.messages.AIMessage,pydantic.ValidationError,schemas.types.FunctionAnalysis,schemas.types.ClassAnalysis,schemas.types.FunctionAnalysisInput,schemas.types.FunctionContextInput,schemas.types.ClassAnalysisInput,schemas.types.ClassContextInput
        functions[1]:
          - mode: function_analysis
            identifier: backend.HelperLLM.main_orchestrator
            name: main_orchestrator
            args[0]:
            docstring: "Dummy Data and processing loop for testing the LLMHelper class.\nThis version is syntactically correct and logically matches the Pydantic models."
            source_code: "def main_orchestrator():\n    \"\"\"\n    Dummy Data and processing loop for testing the LLMHelper class.\n    This version is syntactically correct and logically matches the Pydantic models.\n    \"\"\"\n    \n    # --- Step 1: Define the pre-computed analysis for each method ---\n    \n    # Example Input 1: For the 'add_item' function\n    add_item_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"source_code\": \"\"\"def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\\\"\\\"\\\"\n        if not isinstance(quantity, int) or quantity <= 0:\n            raise ValueError(\"Quantity must be a positive integer.\")\n    \n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n        print(f\"Added {quantity} of {item_name}. New total: {self.inventory[item_name]}\")\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"process_shipment\", \"restock_api_endpoint\"]\n        }\n    })\n\n    # Example Input 2: For the 'check_stock' function\n    check_stock_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"source_code\": \"\"\"def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Retrieves the current stock quantity for a given item.\\\"\\\"\\\"\n        stock_level = self.inventory.get(item_name, 0)\n        return stock_level\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"fulfill_order\", \"ui_display_handler\"]\n        }\n    })\n\n    # Example Input 3: For the 'generate_report' function\n    generate_report_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"source_code\": \"\"\"def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a timestamped string summary of the current inventory state.\\\"\\\"\\\"\n        timestamp = datetime.now()\n        report_header = \"Inventory Report as of: {ts}\\\\n\".format(ts=timestamp)\n        report_lines = [report_header]\n        \n        for item, quantity in self.inventory.items():\n            line = \"- {name}: {qty}\".format(name=item, qty=quantity)\n            report_lines.append(line)\n            \n        return \"\\\\n\".join(report_lines)\"\"\",\n        \"imports\": [\"from datetime import datetime\"],\n        \"context\": {\n            \"calls\": [\"datetime.now\", \"str.format\"],\n            \"called_by\": [\"daily_cron_job\", \"admin_dashboard_export\"]\n        }\n    })\n\n    add_item_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"description\": {\n            \"overall\": \"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to add.\"},\n                {\"name\": \"quantity\", \"type\": \"int\", \"description\": \"The number of units to add. Must be a positive integer.\"}\n            ],\n            \"returns\": [],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"process_shipment and restock_api_endpoint\" }\n        },\n        \"error\": None\n    })\n\n    check_stock_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"description\": {\n            \"overall\": \"Retrieves the current stock quantity for a given item.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to check.\"}\n            ],\n            \"returns\": [\n                {\"name\": \"stock_level\", \"type\": \"int\", \"description\": \"The quantity of the item in stock. Returns 0 if the item is not found.\"}\n            ],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"fulfill_order and ui_display_handler\" }\n        },\n        \"error\": None\n    })\n\n    generate_report_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"description\": {\n            \"overall\": \"Generates a timestamped string summary of the current inventory state.\",\n            \"parameters\": [ {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"} ],\n            \"returns\": [\n                {\"name\": \"report\", \"type\": \"str\", \"description\": \"A formatted string detailing all items and their quantities.\"}\n            ],\n            \"usage_context\": { \"calls\": \"datetime.now and str.format\", \"called_by\": \"daily_cron_job and admin_dashboard_export\" }\n        },\n        \"error\": None\n    })\n\n    class_input = ClassAnalysisInput(\n        mode=\"class_analysis\",\n        identifier=\"InventoryManager\",\n        # NOTE: The indentation of this string is now fixed. It starts at column 0.\n        source_code=\"\"\"class InventoryManager:\n    \\\"\\\"\\\"Manages stock levels for products in a warehouse.\\\"\\\"\\\"\n    def __init__(self, warehouse_id: str):\n        self.warehouse_id = warehouse_id\n        self.inventory = {}  # item_name: quantity\n\n    def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds an item to the inventory.\\\"\\\"\\\"\n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n\n    def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Checks the stock of a specific item.\\\"\\\"\\\"\n        return self.inventory.get(item_name, 0)\n\n    def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a summary report of the inventory.\\\"\\\"\\\"\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        report_lines = [f\"Inventory Report for {self.warehouse_id} at {timestamp}\"]\n        if not self.inventory:\n            report_lines.append(\"Inventory is empty.\")\n        else:\n            for item, quantity in self.inventory.items():\n                report_lines.append(f\"- {item}: {quantity}\")\n        return \"\\\\n\".join(report_lines)\n\"\"\",\n        imports=[\"from datetime import datetime\"],\n        context=ClassContextInput(\n            dependencies=[\"datetime\"],\n            instantiated_by=[\"main_app_startup\", \"warehouse_worker_script\"],\n            methods_analysis=[\n                add_item_analysis,\n                check_stock_analysis,\n                generate_report_analysis\n            ]\n        )\n    )\n\n    \n    # The helper methods expect a LIST of inputs, so we wrap our single object in a list.\n    input = [add_item_input, check_stock_input, generate_report_input]\n    analysis = [add_item_analysis, check_stock_analysis, generate_report_analysis] \n\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    llm_helper = LLMHelper(api_key=GEMINI_API_KEY, function_prompt_path=function_prompt_file, class_prompt_path=class_prompt_file)\n\n    # This will be the final JSON containing all documentation\n    final_documentation = {}\n\n    # The analysis of the methods is already provided in the context.\n    logging.info(\"\\n--- Generating documentation for classes ---\")\n    \n    # The `generate_for_classes` method returns a list of results.\n    analysis_results = llm_helper.generate_for_functions(input)\n\n    # --- Step 4: Process the results ---\n    \n    # We loop through the list of results (even though there's only one in this case).\n    for doc in analysis_results:\n        if doc:\n            logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n            if \"classes\" not in final_documentation:\n                final_documentation[\"classes\"] = {}\n            # Use .model_dump() to convert the Pydantic object back to a dict for JSON serialization\n            final_documentation[\"classes\"][doc.identifier] = doc.model_dump() \n        else:\n            logging.warning(f\"Failed to generate doc for a class\")\n\n    # --- Step 5: Display the final aggregated result ---\n    logging.info(\"\\n--- Final Generated Documentation ---\")\n    print(json.dumps(final_documentation, indent=2))"
            start_line: 227
            end_line: 413
            context:
              calls[3]: backend.HelperLLM.LLMHelper,schemas.types.ClassAnalysisInput,schemas.types.ClassContextInput
              called_by[1]: backend.HelperLLM
        classes[1]:
          - mode: class_analysis
            identifier: backend.HelperLLM.LLMHelper
            name: LLMHelper
            docstring: "A class to interact with Google Gemini for generating code snippet documentation.\nIt centralizes API interaction, error handling, and validates I/O using Pydantic."
            source_code: "class LLMHelper:\n    \"\"\"\n    A class to interact with Google Gemini for generating code snippet documentation.\n    It centralizes API interaction, error handling, and validates I/O using Pydantic.\n    \"\"\"\n    def __init__(self, api_key: str, function_prompt_path: str, class_prompt_path: str, model_name: str = \"gemini-2.0-flash-lite\", base_url: str = None):\n        if not api_key:\n            raise ValueError(\"Gemini API Key must be set.\")\n        \n        try:\n            with open(function_prompt_path, 'r', encoding='utf-8') as f:\n                self.function_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Function system prompt file not found at: {function_prompt_path}\")\n            raise\n\n        # Handle the second file\n        try:\n            with open(class_prompt_path, 'r', encoding='utf-8') as f:\n                self.class_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Class system prompt file not found at: {class_prompt_path}\")\n            raise\n        \n        # Batch-Size config\n        self.model_name = model_name\n        self._configure_batch_settings(model_name)\n\n\n        if model_name.startswith(\"gemini-\"):\n            base_llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=0.3, \n            )\n        \n        elif model_name.startswith(\"gpt-\") and \"openGPT\" not in model_name:\n            base_llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=0.3,\n            )\n\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            if not SCADSLLM_URL:\n                raise ValueError(f\"SCADSLLM_URL environment variable is required for model {model_name}\")\n            \n            logging.info(f\"Connecting to Custom API at {SCADSLLM_URL} for model {model_name}\")\n            \n            base_llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                base_url=SCADSLLM_URL,\n                temperature=0.3,\n            )\n\n        else:\n            target_url = base_url if base_url else OLLAMA_BASE_URL\n            logging.info(f\"Using Ollama at {target_url} for model {model_name}\")\n            base_llm = ChatOllama(\n                model=model_name,\n                temperature=0.3,\n                base_url=target_url,\n            )\n\n        self.function_llm = base_llm.with_structured_output(FunctionAnalysis, method=\"json_schema\")\n        self.class_llm = base_llm.with_structured_output(ClassAnalysis, method=\"json_schema\")\n\n        self.raw_llm = base_llm\n\n        logging.info(f\"LLMHelper initialized with model '{model_name}'. Batch Size: {self.batch_size}\")\n\n    def _configure_batch_settings(self, model_name: str):\n\n        if model_name == \"gemini-2.0-flash-lite\":\n            self.batch_size = 30\n        \n        elif model_name == \"gemini-flash-latest\":\n            self.batch_size = 10\n\n        elif model_name == \"gemini-2.5-flash\":\n            self.batch_size = 10\n            \n        elif model_name == \"gemini-2.5-pro\":\n            self.batch_size = 2\n        \n        elif model_name == \"gemini-2.0-flash\":\n            self.batch_size = 15\n\n        elif model_name == \"gemini-2.5-flash-lite\":\n            self.batch_size = 15\n\n        elif model_name == \"llama3\":\n            self.batch_size = 50\n\n        elif model_name == \"gpt-5.1\":\n            self.batch_size = 500\n\n        elif model_name == \"gpt-5-mini\":\n            self.batch_size = 500\n\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            self.batch_size = 500\n            \n        else:\n            logging.warning(f\"Unknown model '{model_name}', using conservative defaults.\")\n            self.batch_size = 2\n\n    def generate_for_functions(self, function_inputs: List[FunctionAnalysisInput]) -> List[Optional[FunctionAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of functions.\"\"\"\n\n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 62\n\n        if not function_inputs:\n            return []\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(function_input.model_dump(), indent=2) \n            for function_input in function_inputs\n        ]\n       \n        conversations = [[SystemMessage(content=self.function_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_functions = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n        \n            try:\n                batch_results = self.function_llm.batch(batch_conversations, config={\"max_concurrency\": self.batch_size})\n                all_validated_functions.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                all_validated_functions.extend([None] * len(batch_conversations))\n            \n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_functions\n        \n            \n    \n\n    def generate_for_classes(self, class_inputs: List[ClassAnalysisInput]) -> List[Optional[ClassAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of classes.\"\"\"\n        if not class_inputs:\n            return []\n        \n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 62\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(class_input.model_dump(), indent=2)\n            for class_input in class_inputs\n        ]\n\n        conversations = [[SystemMessage(content=self.class_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_classes = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n\n            try:\n                batch_results = self.class_llm.batch(batch_conversations , config={\"max_concurrency\": self.batch_size})\n                all_validated_classes.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                # Falls ein Fehler auftritt, f√ºllen wir die Liste mit None auf, um die Reihenfolge zu wahren\n                all_validated_classes.extend([None] * len(batch_conversations))\n\n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_classes"
            start_line: 31
            end_line: 221
            context:
              dependencies[0]:
              instantiated_by[2]: backend.HelperLLM.main_orchestrator,backend.main.main_workflow
              method_context[4]:
                - identifier: backend.HelperLLM.LLMHelper.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[6]: self,api_key,function_prompt_path,class_prompt_path,model_name,base_url
                  docstring: null
                  start_line: 36
                  end_line: 101
                - identifier: backend.HelperLLM.LLMHelper._configure_batch_settings
                  name: _configure_batch_settings
                  calls[0]:
                  called_by[0]:
                  args[2]: self,model_name
                  docstring: null
                  start_line: 103
                  end_line: 137
                - identifier: backend.HelperLLM.LLMHelper.generate_for_functions
                  name: generate_for_functions
                  calls[0]:
                  called_by[1]: backend.main.main_workflow
                  args[2]: self,function_inputs
                  docstring: Generates and validates documentation for a batch of functions.
                  start_line: 139
                  end_line: 178
                - identifier: backend.HelperLLM.LLMHelper.generate_for_classes
                  name: generate_for_classes
                  calls[0]:
                  called_by[1]: backend.main.main_workflow
                  args[2]: self,class_inputs
                  docstring: Generates and validates documentation for a batch of classes.
                  start_line: 183
                  end_line: 221
    "backend/MainLLM.py":
      ast_nodes:
        imports[9]: os,logging,sys,dotenv.load_dotenv,langchain_google_genai.ChatGoogleGenerativeAI,langchain_ollama.ChatOllama,langchain_openai.ChatOpenAI,langchain.messages.HumanMessage,langchain.messages.SystemMessage
        functions[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.MainLLM.MainLLM
            name: MainLLM
            docstring: Hauptklasse f√ºr die Interaktion mit dem LLM.
            source_code: "class MainLLM:\n    \"\"\"\n    Hauptklasse f√ºr die Interaktion mit dem LLM.\n    \"\"\"\n    def __init__(self, api_key: str, prompt_file_path: str, model_name: str = \"gemini-2.5-pro\", base_url: str = None):\n        if not api_key:\n            raise ValueError(\"Gemini API Key must be set.\")\n        \n        try:\n            with open(prompt_file_path, 'r', encoding='utf-8') as f:\n                self.system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"System prompt file not found at: {prompt_file_path}\")\n            raise\n        \n        self.model_name = model_name\n\n        if model_name.startswith(\"gemini-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n\n        elif model_name.startswith(\"gpt-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            if not SCADSLLM_URL:\n                raise ValueError(f\"SCADSLLM_URL environment variable is required for model {model_name}\")\n            \n            logging.info(f\"Connecting to Custom API at {SCADSLLM_URL} for model {model_name}\")\n            \n            self.llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                base_url=SCADSLLM_URL,\n                temperature=1.0,\n            )\n\n        else:\n            target_url = base_url if base_url else OLLAMA_BASE_URL\n            self.llm = ChatOllama(\n                model=model_name,\n                temperature=1.0,\n                base_url=target_url,\n            )\n\n        logging.info(f\"Main LLM initialized with model '{model_name}'.\")\n    \n    def call_llm(self, user_input: str):\n\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with HelperLLM input...\")\n\n        try:\n            response = self.llm.invoke(messages)\n            logging.info(\"LLM call successful.\")\n            return response.content\n        except Exception as e:\n            logging.error(f\"Error during LLM call: {e}\")\n            return None\n\n    def stream_llm(self, user_input: str):\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with 'stream'...\")\n\n        try:\n            stream_iterator = self.llm.stream(messages)\n            \n            for chunk in stream_iterator:\n                yield chunk.content\n        except Exception as e:\n            error_message = f\"\\n--- Error during LLM stream call: {e} ---\"\n            logging.error(error_message)\n            yield error_message"
            start_line: 22
            end_line: 106
            context:
              dependencies[0]:
              instantiated_by[2]: backend.main.main_workflow,backend.main.notebook_workflow
              method_context[3]:
                - identifier: backend.MainLLM.MainLLM.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[5]: self,api_key,prompt_file_path,model_name,base_url
                  docstring: null
                  start_line: 26
                  end_line: 73
                - identifier: backend.MainLLM.MainLLM.call_llm
                  name: call_llm
                  calls[0]:
                  called_by[2]: backend.main.main_workflow,backend.main.notebook_workflow
                  args[2]: self,user_input
                  docstring: null
                  start_line: 75
                  end_line: 89
                - identifier: backend.MainLLM.MainLLM.stream_llm
                  name: stream_llm
                  calls[0]:
                  called_by[0]:
                  args[2]: self,user_input
                  docstring: null
                  start_line: 91
                  end_line: 106
    "backend/basic_info.py":
      ast_nodes:
        imports[7]: re,os,tomllib,typing.List,typing.Dict,typing.Any,typing.Optional
        functions[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.basic_info.ProjektInfoExtractor
            name: ProjektInfoExtractor
            docstring: "Extrahiert grundlegende Projektinformationen aus g√§ngigen Projektdateien\nwie README, pyproject.toml und requirements.txt."
            source_code: "class ProjektInfoExtractor:\n    \"\"\"\n    Extrahiert grundlegende Projektinformationen aus g√§ngigen Projektdateien\n    wie README, pyproject.toml und requirements.txt.\n    \"\"\"\n    def __init__(self):\n        self.INFO_NICHT_GEFUNDEN = \"Information not found\"\n        # Initialisiert die Struktur mit Platzhaltern\n        self.info = {\n            \"projekt_uebersicht\": {\n                \"titel\": self.INFO_NICHT_GEFUNDEN,\n                \"beschreibung\": self.INFO_NICHT_GEFUNDEN,\n                \"aktueller_status\": self.INFO_NICHT_GEFUNDEN,\n                \"key_features\": self.INFO_NICHT_GEFUNDEN,\n                \"tech_stack\": self.INFO_NICHT_GEFUNDEN,\n            },\n            \"installation\": {\n                \"dependencies\": self.INFO_NICHT_GEFUNDEN,\n                \"setup_anleitung\": self.INFO_NICHT_GEFUNDEN,\n                \"quick_start_guide\": self.INFO_NICHT_GEFUNDEN,\n            }\n        }\n\n    def _clean_content(self, content: str) -> str:\n        \"\"\"Entfernt Null-Bytes, die durch Encoding-Fehler (UTF-16 als UTF-8 gelesen) entstehen.\"\"\"\n        if not content:\n            return \"\"\n        return content.replace('\\x00', '')\n\n    def _finde_datei(self, patterns: List[str], dateien: List[Any]) -> Optional[Any]:\n        \"\"\"Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht.\"\"\"\n        for datei in dateien:\n            for pattern in patterns:\n                if datei.path.lower().endswith(pattern.lower()):\n                    return datei\n        return None\n\n    def _extrahiere_sektion_aus_markdown(self, inhalt: str, keywords: List[str]) -> Optional[str]:\n        \"\"\"\n        Extrahiert den Text unter einer Markdown-√úberschrift (##).\n        \"\"\"\n        # Erstellt ein Regex-Pattern, das auf jedes der Schl√ºsselw√∂rter reagiert\n        keyword_pattern = \"|\".join(re.escape(k) for k in keywords)\n        \n        # Sucht nach \"## Schl√ºsselwort\" und erfasst alles bis zur n√§chsten \"##\" oder dem Dateiende\n        pattern = re.compile(\n            rf\"##\\s*({keyword_pattern})\\s*\\n(.*?)(?=\\n##|\\Z)\",\n            re.IGNORECASE | re.DOTALL\n        )\n        match = pattern.search(inhalt)\n        if match:\n            return match.group(2).strip()\n        return None\n\n    def _parse_readme(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer README-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        if self.info[\"projekt_uebersicht\"][\"titel\"] == self.INFO_NICHT_GEFUNDEN:\n            title_match = re.search(r\"^\\s*#\\s*(.*)\", inhalt)\n            if title_match:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = title_match.group(1).strip()\n\n        # Beschreibung (Fallback)\n        if self.info[\"projekt_uebersicht\"][\"beschreibung\"] == self.INFO_NICHT_GEFUNDEN:\n            desc_match = re.search(r\"^\\s*#\\s*.*\\n+([^#\\n].*)\", inhalt, re.DOTALL)\n            if desc_match:\n                 self.info[\"projekt_uebersicht\"][\"beschreibung\"] = desc_match.group(1).strip().split('\\n\\n')[0]\n\n        # Key Features\n        features = self._extrahiere_sektion_aus_markdown(inhalt, [\"Features\", \"Key Features\", \"Merkmale\"])\n        if features:\n            self.info[\"projekt_uebersicht\"][\"key_features\"] = features\n            \n        # Tech Stack\n        tech_stack = self._extrahiere_sektion_aus_markdown(inhalt, [\"Tech Stack\", \"Technology\", \"Technologien\"])\n        if tech_stack:\n            self.info[\"projekt_uebersicht\"][\"tech_stack\"] = tech_stack\n            \n        # Status\n        status = self._extrahiere_sektion_aus_markdown(inhalt, [\"Status\", \"Current Status\"])\n        if status:\n            self.info[\"projekt_uebersicht\"][\"aktueller_status\"] = status\n\n        # Setup-Anleitung\n        setup = self._extrahiere_sektion_aus_markdown(inhalt, [\"Installation\", \"Setup\", \"Getting Started\"])\n        if setup:\n            self.info[\"installation\"][\"setup_anleitung\"] = setup\n            \n        # Quick Start\n        quick_start = self._extrahiere_sektion_aus_markdown(inhalt, [\"Quick Start\", \"Schnellstart\"])\n        if quick_start:\n            self.info[\"installation\"][\"quick_start_guide\"] = quick_start\n\n    def _parse_toml(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer pyproject.toml-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        if not tomllib:\n            print(\"Warnung: 'tomli' ist nicht installiert. pyproject.toml kann nicht analysiert werden.\")\n            return\n            \n        try:\n            data = tomllib.loads(inhalt)\n            project_data = data.get(\"project\", {})\n            \n            if \"name\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = project_data[\"name\"]\n            if \"description\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"beschreibung\"] = project_data[\"description\"]\n            if \"dependencies\" in project_data:\n                self.info[\"installation\"][\"dependencies\"] = project_data[\"dependencies\"]\n        except tomllib.TOMLDecodeError as e:\n            print(f\"Warnung: Fehler beim Parsen der pyproject.toml: {e}\")\n\n    def _parse_requirements(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer requirements.txt-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        # Nur f√ºllen, wenn noch keine Dependencies aus toml gefunden wurden\n        if self.info[\"installation\"][\"dependencies\"] == self.INFO_NICHT_GEFUNDEN:\n            lines = inhalt.splitlines()\n            dependencies = [\n                line.strip() for line in lines \n                if line.strip() and not line.strip().startswith('#')\n            ]\n            if dependencies:\n                self.info[\"installation\"][\"dependencies\"] = dependencies\n\n    def extrahiere_info(self, dateien: List[Any], repo_url: str) -> Dict[str, Any]:\n        \"\"\"\n        Orchestriert die Extraktion von Informationen.\n        \"\"\"\n        # Relevante Dateien finden\n        readme_datei = self._finde_datei([\"readme.md\", \"readme.rst\", \"readme.txt\", \"readme\"], dateien)\n        toml_datei = self._finde_datei([\"pyproject.toml\"], dateien)\n        req_datei = self._finde_datei([\"requirements.txt\"], dateien)\n\n        # Dateien parsen (mit Priorisierung)\n        if toml_datei:\n            self._parse_toml(toml_datei.content)\n\n        if req_datei:\n            self._parse_requirements(req_datei.content)\n            \n        if readme_datei:\n            self._parse_readme(readme_datei.content)\n            \n        # Finale Formatierung der Dependencies\n        deps = self.info[\"installation\"][\"dependencies\"]\n        if isinstance(deps, list):\n            if not deps:\n                self.info[\"installation\"][\"dependencies\"] = self.INFO_NICHT_GEFUNDEN\n            else:\n                self.info[\"installation\"][\"dependencies\"] = \"\\n\".join(f\"- {dep}\" for dep in deps)\n        \n        # Titel aus URL ableiten, falls nichts gefunden wurde (oder als Zusatzinfo)\n        if repo_url:\n            repo_name = os.path.basename(repo_url.removesuffix('.git'))\n            # Wenn noch kein Titel da ist oder er generisch war\n            if self.info[\"projekt_uebersicht\"][\"titel\"] == self.INFO_NICHT_GEFUNDEN:\n                 self.info[\"projekt_uebersicht\"][\"titel\"] = f\"{repo_name} documentation\"\n\n        return self.info"
            start_line: 9
            end_line: 172
            context:
              dependencies[0]:
              instantiated_by[2]: backend.main.main_workflow,backend.main.notebook_workflow
              method_context[8]:
                - identifier: backend.basic_info.ProjektInfoExtractor.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 14
                  end_line: 30
                - identifier: backend.basic_info.ProjektInfoExtractor._clean_content
                  name: _clean_content
                  calls[0]:
                  called_by[0]:
                  args[2]: self,content
                  docstring: "Entfernt Null-Bytes, die durch Encoding-Fehler (UTF-16 als UTF-8 gelesen) entstehen."
                  start_line: 32
                  end_line: 36
                - identifier: backend.basic_info.ProjektInfoExtractor._finde_datei
                  name: _finde_datei
                  calls[0]:
                  called_by[0]:
                  args[3]: self,patterns,dateien
                  docstring: "Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht."
                  start_line: 38
                  end_line: 44
                - identifier: backend.basic_info.ProjektInfoExtractor._extrahiere_sektion_aus_markdown
                  name: _extrahiere_sektion_aus_markdown
                  calls[0]:
                  called_by[0]:
                  args[3]: self,inhalt,keywords
                  docstring: Extrahiert den Text unter einer Markdown-√úberschrift (##).
                  start_line: 46
                  end_line: 61
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_readme
                  name: _parse_readme
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer README-Datei.
                  start_line: 63
                  end_line: 101
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_toml
                  name: _parse_toml
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer pyproject.toml-Datei.
                  start_line: 103
                  end_line: 122
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_requirements
                  name: _parse_requirements
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer requirements.txt-Datei.
                  start_line: 124
                  end_line: 136
                - identifier: backend.basic_info.ProjektInfoExtractor.extrahiere_info
                  name: extrahiere_info
                  calls[0]:
                  called_by[2]: backend.main.main_workflow,backend.main.notebook_workflow
                  args[3]: self,dateien,repo_url
                  docstring: Orchestriert die Extraktion von Informationen.
                  start_line: 138
                  end_line: 172
    "backend/callgraph.py":
      ast_nodes:
        imports[9]: ast,networkx,os,pathlib.Path,typing.Dict,getRepo.GitRepository,getRepo.GitRepository,basic_info.ProjektInfoExtractor,os
        functions[2]:
          - mode: function_analysis
            identifier: backend.callgraph.make_safe_dot
            name: make_safe_dot
            args[2]: graph,out_path
            docstring: null
            source_code: "def make_safe_dot(graph: nx.DiGraph, out_path: str):\n    mapping = {}\n    H = graph.copy()\n    for i, n in enumerate(list(graph.nodes())):\n        safe = f\"n{i}\"           \n        mapping[n] = safe\n    H = nx.relabel_nodes(H, mapping)\n    for orig, safe in mapping.items():\n        H.nodes[safe][\"label\"] = orig\n    nx.drawing.nx_pydot.write_dot(H, out_path)"
            start_line: 173
            end_line: 182
            context:
              calls[0]:
              called_by[1]: backend.callgraph
          - mode: function_analysis
            identifier: backend.callgraph.build_filtered_callgraph
            name: build_filtered_callgraph
            args[1]: repo
            docstring: Baut den globalen Call-Graphen und filtert ihn auf selbst geschriebene Funktionen.
            source_code: "def build_filtered_callgraph(repo: GitRepository) -> nx.DiGraph:\n    \"\"\"\n    Baut den globalen Call-Graphen und filtert ihn auf selbst geschriebene Funktionen.\n    \"\"\"\n    all_file_objects = repo.get_all_files()\n    own_functions = set()\n    file_trees = {}\n\n    for file in all_file_objects:\n        if not file.path.endswith(\".py\"):\n            continue\n        filename = Path(file.path).stem\n        tree = ast.parse(file.content)\n        file_trees[filename] = tree\n        visitor = CallGraph(filename)\n        visitor.visit(tree)\n        own_functions.update(visitor.function_set)\n\n    # Globalen Call-Graph bauen\n    global_graph = nx.DiGraph()\n    for filename, tree in file_trees.items():\n        visitor = CallGraph(filename)\n        visitor.visit(tree)\n        for caller, callees in visitor.edges.items():\n            if caller not in own_functions:\n                continue \n            for callee in callees:\n                if callee in own_functions:\n                    global_graph.add_edge(caller, callee)\n                    global_graph.add_node(caller)\n                    global_graph.add_node(callee)\n    return global_graph"
            start_line: 185
            end_line: 216
            context:
              calls[1]: backend.callgraph.CallGraph
              called_by[1]: backend.callgraph
        classes[1]:
          - mode: class_analysis
            identifier: backend.callgraph.CallGraph
            name: CallGraph
            docstring: null
            source_code: "class CallGraph(ast.NodeVisitor):\n    def __init__(self, filename: str):\n        self.filename = filename\n        self.current_function = None\n        self.current_class = None\n        \n        self.local_defs: dict[str, str] = {}\n        self.graph = nx.DiGraph()\n        self.import_mapping: dict[str, str] = {}\n        self.function_set: set[str] = set()\n        self.edges: Dict[str, set[str]] = {}\n\n    def _recursive_call(self, node):\n        \"\"\"\n        Liefert eine Liste der Namenskomponenten als dotted string z.B. ['pkg', 'mod', 'Class', 'method']\n        \"\"\"\n        if isinstance(node, ast.Call):\n            return self._recursive_call(node.func)\n        if isinstance(node, ast.Name):\n            return [node.id]\n        if isinstance(node, ast.Attribute):\n            parts = self._recursive_call(node.value)\n            parts.append(node.attr)\n            return parts\n        return []\n\n    def _resolve_all_callee_names(self, callee_nodes: list[list[str]]) -> list[str]:\n        \"\"\"\n        callee_nodes ist jetzt eine Liste von Listen (Name-Steps).\n        Wir pr√ºfen zuerst lokale Definitionen, dann import_mapping.\n        \"\"\"\n        resolved = []\n        for parts in callee_nodes:\n            if not parts:\n                continue\n            simple = parts[-1]\n            dotted = \".\".join(parts[-2:]) if len(parts) >= 2 else simple\n\n            if simple in self.local_defs:\n                resolved.append(self.local_defs[simple])\n                continue\n            if dotted in self.local_defs:\n                resolved.append(self.local_defs[dotted])\n                continue\n\n            first = parts[0]\n            if first in self.import_mapping:\n                mod = self.import_mapping[first]\n                rest = \"::\".join(parts[1:]) if len(parts) > 1 else simple\n                resolved.append(f\"{mod}::{rest}\")\n                continue\n\n            if self.current_class:\n                resolved.append(f\"{self.filename}::{parts[0]}::{simple}\" if len(parts)==1 else f\"{self.filename}::{'::'.join(parts)}\")\n            else:\n                resolved.append(f\"{self.filename}::{ '::'.join(parts)}\")\n        return resolved\n\n    def _make_full_name(self, basename: str, class_name: str | None = None) -> str:\n        if class_name:\n            return f\"{self.filename}::{class_name}::{basename}\"\n        return f\"{self.filename}::{basename}\"\n    \n    def _current_caller(self) -> str:\n        if self.current_function:\n            return self.current_function\n        return f\"<{self.filename}>\" if self.filename else \"<global-scope>\"\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            module_name = alias.name\n            module_asname = alias.asname if alias.asname else module_name\n            self.import_mapping[module_asname] = module_name\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        module_name = node.module.split(\".\")[-1] if node.module else \"\"\n        for alias in node.names:\n            self.import_mapping[alias.asname or alias.name] = f\"{module_name}\" if module_name else alias.name\n\n    def visit_ClassDef(self, node: ast.ClassDef):\n        prev_class = self.current_class\n        self.current_class = node.name\n        self.generic_visit(node)\n        self.current_class = prev_class\n        \n    def visit_FunctionDef(self, node):\n        prev_function = self.current_function\n        full_name = self._make_full_name(node.name, self.current_class)\n        self.local_defs[node.name] = full_name\n        if self.current_class:\n            self.local_defs[f\"{self.current_class}.{node.name}\"] = full_name\n\n        self.current_function = full_name\n        self.graph.add_node(self.current_function)\n        self.generic_visit(node)\n        self.function_set.add(self.current_function)\n        self.current_function = prev_function\n\n    def visit_AsyncFunctionDef(self, node):\n        self.visit_FunctionDef(node)\n\n    def visit_Call(self, node):\n        caller = self._current_caller()\n        parts = self._recursive_call(node)\n        resolved_callees = self._resolve_all_callee_names([parts])\n\n        if caller not in self.edges:\n            self.edges[caller] = set()\n\n        for callee in resolved_callees:\n            if callee:\n                self.edges[caller].add(callee)\n        self.generic_visit(node)\n\n    def visit_If(self, node):\n        if (isinstance(node.test, ast.Compare) and\n            isinstance(node.test.left, ast.Name) and \n            node.test.left.id == \"__name__\"):\n            caller_backup = self.current_function\n            self.current_function = \"<main_block>\"\n            self.generic_visit(node)\n            self.current_function = caller_backup\n        else:\n            self.generic_visit(node)"
            start_line: 10
            end_line: 134
            context:
              dependencies[0]:
              instantiated_by[1]: backend.callgraph.build_filtered_callgraph
              method_context[12]:
                - identifier: backend.callgraph.CallGraph.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,filename
                  docstring: null
                  start_line: 11
                  end_line: 20
                - identifier: backend.callgraph.CallGraph._recursive_call
                  name: _recursive_call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: "Liefert eine Liste der Namenskomponenten als dotted string z.B. ['pkg', 'mod', 'Class', 'method']"
                  start_line: 22
                  end_line: 34
                - identifier: backend.callgraph.CallGraph._resolve_all_callee_names
                  name: _resolve_all_callee_names
                  calls[0]:
                  called_by[0]:
                  args[2]: self,callee_nodes
                  docstring: "callee_nodes ist jetzt eine Liste von Listen (Name-Steps).\nWir pr√ºfen zuerst lokale Definitionen, dann import_mapping."
                  start_line: 36
                  end_line: 66
                - identifier: backend.callgraph.CallGraph._make_full_name
                  name: _make_full_name
                  calls[0]:
                  called_by[0]:
                  args[3]: self,basename,class_name
                  docstring: null
                  start_line: 68
                  end_line: 71
                - identifier: backend.callgraph.CallGraph._current_caller
                  name: _current_caller
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 73
                  end_line: 76
                - identifier: backend.callgraph.CallGraph.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 78
                  end_line: 83
                - identifier: backend.callgraph.CallGraph.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 85
                  end_line: 88
                - identifier: backend.callgraph.CallGraph.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 90
                  end_line: 94
                - identifier: backend.callgraph.CallGraph.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 96
                  end_line: 107
                - identifier: backend.callgraph.CallGraph.visit_AsyncFunctionDef
                  name: visit_AsyncFunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 109
                  end_line: 110
                - identifier: backend.callgraph.CallGraph.visit_Call
                  name: visit_Call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 112
                  end_line: 123
                - identifier: backend.callgraph.CallGraph.visit_If
                  name: visit_If
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 125
                  end_line: 134
    "backend/converter.py":
      ast_nodes:
        imports[4]: logging,nbformat,base64,nbformat.reader.NotJSONError
        functions[5]:
          - mode: function_analysis
            identifier: backend.converter.wrap_cdata
            name: wrap_cdata
            args[1]: content
            docstring: Wraps content in CDATA tags.
            source_code: "def wrap_cdata(content):\n    \"\"\"Wraps content in CDATA tags.\"\"\"\n    return f\"<![CDATA[\\n{content}\\n]]>\""
            start_line: 8
            end_line: 10
            context:
              calls[0]:
              called_by[1]: backend.converter.convert_notebook_to_xml
          - mode: function_analysis
            identifier: backend.converter.extract_output_content
            name: extract_output_content
            args[2]: outputs,image_list
            docstring: "Extracts text and handles images by decoding Base64 to bytes.\nReturns: A list of text strings or placeholders."
            source_code: "def extract_output_content(outputs, image_list):\n    \"\"\"\n    Extracts text and handles images by decoding Base64 to bytes.\n    Returns: A list of text strings or placeholders.\n    \"\"\"\n    extracted_xml_snippets = []\n    \n    for output in outputs:\n\n        if output.output_type in ('display_data', 'execute_result') and hasattr(output, 'data'):\n            data = output.data\n            \n            # Helper to process image\n            def process_image(mime_type):\n                if mime_type in data:\n                    try:\n                        b64_str = data[mime_type]\n                        b64_str = b64_str.replace('\\n', '')\n\n                        image_index = len(image_list)\n                        image_list.append({\n                            \"mime_type\": mime_type,\n                            \"data\": b64_str\n                        })\n\n                        return f'\\n<IMAGE_PLACEHOLDER index=\"{image_index}\" mime=\"{mime_type}\"/>\\n'\n                    except Exception as e:\n                        return f\"<ERROR>Could not decode image: {e}</ERROR>\"\n                return None\n\n            # Ignore jpeg if png is found\n            img_xml = process_image('image/png')\n            if not img_xml:\n                img_xml = process_image('image/jpeg')\n            \n            if img_xml:\n                extracted_xml_snippets.append(img_xml)\n            elif 'text/plain' in data:\n                extracted_xml_snippets.append(data['text/plain'])\n\n        elif output.output_type == 'stream':\n            extracted_xml_snippets.append(output.text)\n            \n        elif output.output_type == 'error':\n            extracted_xml_snippets.append(f\"{output.ename}: {output.evalue}\")\n\n    return extracted_xml_snippets"
            start_line: 12
            end_line: 58
            context:
              calls[1]: backend.converter.process_image
              called_by[1]: backend.converter.convert_notebook_to_xml
          - mode: function_analysis
            identifier: backend.converter.process_image
            name: process_image
            args[1]: mime_type
            docstring: null
            source_code: "def process_image(mime_type):\n                if mime_type in data:\n                    try:\n                        b64_str = data[mime_type]\n                        b64_str = b64_str.replace('\\n', '')\n\n                        image_index = len(image_list)\n                        image_list.append({\n                            \"mime_type\": mime_type,\n                            \"data\": b64_str\n                        })\n\n                        return f'\\n<IMAGE_PLACEHOLDER index=\"{image_index}\" mime=\"{mime_type}\"/>\\n'\n                    except Exception as e:\n                        return f\"<ERROR>Could not decode image: {e}</ERROR>\"\n                return None"
            start_line: 25
            end_line: 40
            context:
              calls[0]:
              called_by[1]: backend.converter.extract_output_content
          - mode: function_analysis
            identifier: backend.converter.convert_notebook_to_xml
            name: convert_notebook_to_xml
            args[1]: file_content
            docstring: null
            source_code: "def convert_notebook_to_xml(file_content):\n    try:\n        nb = nbformat.reads(file_content, as_version=4)\n    except NotJSONError:\n        return \"<ERROR>Could not parse file as JSON/Notebook</ERROR>\", []\n\n    xml_parts = []\n    extracted_images = [] \n\n    for cell in nb.cells:\n        if cell.cell_type == 'markdown':\n            cell_xml = f'<CELL type=\"markdown\">\\n{cell.source}\\n</CELL>'\n            xml_parts.append(cell_xml)\n\n        elif cell.cell_type == 'code':\n            source_code = wrap_cdata(cell.source)\n            xml_parts.append(f'<CELL type=\"code\">\\n{source_code}\\n</CELL>')\n\n            if hasattr(cell, 'outputs') and cell.outputs:\n                snippets = extract_output_content(cell.outputs, extracted_images)\n                \n                content = \"\\n\".join(snippets)\n                \n                if content.strip():\n                    wrapped_output = wrap_cdata(content)\n                    xml_parts.append(f'<CELL type=\"output\">\\n{wrapped_output}\\n</CELL>')\n\n    return \"\\n\\n\".join(xml_parts), extracted_images"
            start_line: 60
            end_line: 87
            context:
              calls[2]: backend.converter.extract_output_content,backend.converter.wrap_cdata
              called_by[1]: backend.converter.process_repo_notebooks
          - mode: function_analysis
            identifier: backend.converter.process_repo_notebooks
            name: process_repo_notebooks
            args[1]: repo_files
            docstring: null
            source_code: "def process_repo_notebooks(repo_files):\n    notebook_files = [f for f in repo_files if f.path.endswith('.ipynb')]\n    logging.info(f\"Found {len(notebook_files)} notebooks.\")\n        \n    results = {}\n\n    for nb_file in notebook_files:\n        logging.info(f\"Processing: {nb_file.path}\")\n\n        xml_output, images = convert_notebook_to_xml(nb_file.content)\n        results[nb_file.path] = {\n            \"xml\": xml_output,\n            \"images\": images\n        }\n            \n    return results"
            start_line: 90
            end_line: 105
            context:
              calls[1]: backend.converter.convert_notebook_to_xml
              called_by[1]: backend.main.notebook_workflow
        classes[0]:
    "backend/getRepo.py":
      ast_nodes:
        imports[5]: tempfile,git.Repo,git.GitCommandError,logging,os
        functions[0]:
        classes[2]:
          - mode: class_analysis
            identifier: backend.getRepo.RepoFile
            name: RepoFile
            docstring: "Repr√§sentiert eine einzelne Datei in einem Git-Repository.\n\nDer Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tats√§chlichem Zugriff."
            source_code: "class RepoFile:\n    \"\"\"\n    Repr√§sentiert eine einzelne Datei in einem Git-Repository.\n    \n    Der Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tats√§chlichem Zugriff.\n    \"\"\"\n    def __init__(self, file_path, commit_tree):\n        \"\"\"\n        Initialisiert das RepoFile-Objekt.\n\n        Args:\n            file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n            commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt.\n        \"\"\"\n        self.path = file_path\n        self._tree = commit_tree\n        \n\n        self._blob = None\n        self._content = None\n        self._size = None\n\n    @property\n    def blob(self):\n        \"\"\"Lazy-l√§dt das Git-Blob-Objekt.\"\"\"\n        if self._blob is None:\n            try:\n                self._blob = self._tree[self.path]\n            except KeyError:\n                raise FileNotFoundError(f\"Datei '{self.path}' konnte im Commit-Tree nicht gefunden werden.\")\n        return self._blob\n\n    @property\n    def content(self):\n        \"\"\"Lazy-l√§dt und gibt den dekodierten Inhalt der Datei zur√ºck.\"\"\"\n        if self._content is None:\n            self._content = self.blob.data_stream.read().decode('utf-8', errors='ignore')\n        return self._content\n\n    @property\n    def size(self):\n        \"\"\"Lazy-l√§dt und gibt die Gr√∂√üe der Datei in Bytes zur√ºck.\"\"\"\n        if self._size is None:\n            self._size = self.blob.size\n        return self._size\n        \n    def analyze_word_count(self):\n        \"\"\"\n        Eine Beispiel-Analyse-Methode. Z√§hlt die W√∂rter im Dateiinhalt.\n        \"\"\"\n        return len(self.content.split())\n\n    def __repr__(self):\n        \"\"\"Gibt eine n√ºtzliche String-Repr√§sentation des Objekts zur√ºck.\"\"\"\n        return f\"<RepoFile(path='{self.path}')>\"\n    \n    def to_dict(self, include_content=False):\n        data = {\n            \"path\": self.path,\n            \"name\": os.path.basename(self.path),\n            \"size\": self.size,\n            \"type\": \"file\"\n        }\n        if include_content:\n            data[\"content\"] = self.content\n        return data"
            start_line: 7
            end_line: 72
            context:
              dependencies[0]:
              instantiated_by[1]: backend.getRepo.GitRepository.get_all_files
              method_context[7]:
                - identifier: backend.getRepo.RepoFile.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[3]: self,file_path,commit_tree
                  docstring: "Initialisiert das RepoFile-Objekt.\n\nArgs:\n    file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n    commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt."
                  start_line: 13
                  end_line: 27
                - identifier: backend.getRepo.RepoFile.blob
                  name: blob
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt das Git-Blob-Objekt.
                  start_line: 30
                  end_line: 37
                - identifier: backend.getRepo.RepoFile.content
                  name: content
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt und gibt den dekodierten Inhalt der Datei zur√ºck.
                  start_line: 40
                  end_line: 44
                - identifier: backend.getRepo.RepoFile.size
                  name: size
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt und gibt die Gr√∂√üe der Datei in Bytes zur√ºck.
                  start_line: 47
                  end_line: 51
                - identifier: backend.getRepo.RepoFile.analyze_word_count
                  name: analyze_word_count
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Eine Beispiel-Analyse-Methode. Z√§hlt die W√∂rter im Dateiinhalt.
                  start_line: 53
                  end_line: 57
                - identifier: backend.getRepo.RepoFile.__repr__
                  name: __repr__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Gibt eine n√ºtzliche String-Repr√§sentation des Objekts zur√ºck.
                  start_line: 59
                  end_line: 61
                - identifier: backend.getRepo.RepoFile.to_dict
                  name: to_dict
                  calls[0]:
                  called_by[0]:
                  args[2]: self,include_content
                  docstring: null
                  start_line: 63
                  end_line: 72
          - mode: class_analysis
            identifier: backend.getRepo.GitRepository
            name: GitRepository
            docstring: "Verwaltet ein Git-Repository, einschlie√ülich Klonen in ein tempor√§res\nVerzeichnis und Bereitstellung von RepoFile-Objekten."
            source_code: "class GitRepository:\n    \"\"\"\n    Verwaltet ein Git-Repository, einschlie√ülich Klonen in ein tempor√§res\n    Verzeichnis und Bereitstellung von RepoFile-Objekten.\n    \"\"\"\n    def __init__(self, repo_url):\n        self.repo_url = repo_url\n        self.temp_dir = tempfile.mkdtemp()\n        self.repo = None\n\n        self.files = []\n        \n        try:\n            logging.info(f\"Clone Repository {self.repo_url}...\")\n            self.repo = Repo.clone_from(self.repo_url, self.temp_dir)\n            self.latest_commit = self.repo.head.commit\n            self.commit_tree = self.latest_commit.tree\n            logging.info(\"Cloning successful.\")\n        except GitCommandError as e:\n            self.close()\n            raise RuntimeError(f\"Error cloning repository: {e}\") from e\n\n    def get_all_files(self):\n        \"\"\"\n        Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zur√ºck.\n\n        Returns:\n            list[RepoFile]: Eine Liste von RepoFile-Instanzen.\n        \"\"\"\n        file_paths = self.repo.git.ls_files().split('\\n')\n        self.files = [RepoFile(path, self.commit_tree) for path in file_paths if path]\n        return self.files\n\n    def close(self):\n        \"\"\"L√∂scht das tempor√§re Verzeichnis und dessen Inhalt.\"\"\"\n        if self.temp_dir:\n            print(f\"\\nL√∂sche tempor√§res Verzeichnis: {self.temp_dir}\")\n            self.temp_dir = None\n            \n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def get_file_tree(self, include_content=False):\n\n        if not self.files:\n            self.get_all_files()\n\n        tree = {\"name\": \"root\", \"type\": \"directory\", \"children\": []}\n\n        for file_obj in self.files:\n            parts = file_obj.path.split('/')\n            current_level = tree[\"children\"]\n            \n            # Iteriere durch die Ordnerstruktur\n            for part in parts[:-1]:\n                # Suche, ob Ordner schon existiert\n                found = next((item for item in current_level if item[\"name\"] == part and item[\"type\"] == \"directory\"), None)\n                if not found:\n                    new_dir = {\"name\": part, \"type\": \"directory\", \"children\": []}\n                    current_level.append(new_dir)\n                    current_level = new_dir[\"children\"]\n                else:\n                    current_level = found[\"children\"]\n            \n            # Datei am Ende hinzuf√ºgen\n            current_level.append(file_obj.to_dict(include_content=include_content))\n\n        return tree"
            start_line: 78
            end_line: 148
            context:
              dependencies[1]: backend.getRepo.RepoFile
              instantiated_by[2]: backend.main.main_workflow,backend.main.notebook_workflow
              method_context[6]:
                - identifier: backend.getRepo.GitRepository.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,repo_url
                  docstring: null
                  start_line: 83
                  end_line: 98
                - identifier: backend.getRepo.GitRepository.get_all_files
                  name: get_all_files
                  calls[1]: backend.getRepo.RepoFile
                  called_by[0]:
                  args[1]: self
                  docstring: "Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zur√ºck.\n\nReturns:\n    list[RepoFile]: Eine Liste von RepoFile-Instanzen."
                  start_line: 100
                  end_line: 109
                - identifier: backend.getRepo.GitRepository.close
                  name: close
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: L√∂scht das tempor√§re Verzeichnis und dessen Inhalt.
                  start_line: 111
                  end_line: 115
                - identifier: backend.getRepo.GitRepository.__enter__
                  name: __enter__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 117
                  end_line: 118
                - identifier: backend.getRepo.GitRepository.__exit__
                  name: __exit__
                  calls[0]:
                  called_by[0]:
                  args[4]: self,exc_type,exc_val,exc_tb
                  docstring: null
                  start_line: 120
                  end_line: 121
                - identifier: backend.getRepo.GitRepository.get_file_tree
                  name: get_file_tree
                  calls[0]:
                  called_by[0]:
                  args[2]: self,include_content
                  docstring: null
                  start_line: 123
                  end_line: 148
    "backend/main.py":
      ast_nodes:
        imports[28]: json,math,logging,os,re,time,math,datetime.datetime,matplotlib.pyplot,datetime.datetime,pathlib.Path,dotenv.load_dotenv,getRepo.GitRepository,AST_Schema.ASTAnalyzer,MainLLM.MainLLM,basic_info.ProjektInfoExtractor,HelperLLM.LLMHelper,relationship_analyzer.ProjectAnalyzer,schemas.types.FunctionContextInput,schemas.types.FunctionAnalysisInput,schemas.types.ClassContextInput,schemas.types.ClassAnalysisInput,schemas.types.MethodContextInput,toon_format.encode,toon_format.count_tokens,toon_format.estimate_savings,toon_format.compare_formats,converter.process_repo_notebooks
        functions[7]:
          - mode: function_analysis
            identifier: backend.main.create_savings_chart
            name: create_savings_chart
            args[4]: json_tokens,toon_tokens,savings_percent,output_path
            docstring: Erstellt ein Balkendiagramm f√ºr den Token-Vergleich und speichert es.
            source_code: "def create_savings_chart(json_tokens, toon_tokens, savings_percent, output_path):\n    \"\"\"Erstellt ein Balkendiagramm f√ºr den Token-Vergleich und speichert es.\"\"\"\n    labels = ['JSON', 'TOON']\n    values = [json_tokens, toon_tokens]\n    colors = ['#ff9999', '#66b3ff']\n\n    plt.figure(figsize=(8, 6))\n    bars = plt.bar(labels, values, color=colors, width=0.5)\n\n    # Titel und Beschriftungen\n    plt.title(f'Token-Vergleich: {savings_percent:.2f}% Einsparung', fontsize=14)\n    plt.ylabel('Anzahl Token')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n\n    # Werte √ºber den Balken anzeigen\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width() / 2.0, height, \n                 f'{int(height):,}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n\n    # Speichern\n    plt.savefig(output_path)\n    plt.close()"
            start_line: 33
            end_line: 55
            context:
              calls[0]:
              called_by[1]: backend.main.main_workflow
          - mode: function_analysis
            identifier: backend.main.calculate_net_time
            name: calculate_net_time
            args[5]: start_time,end_time,total_items,batch_size,model_name
            docstring: Berechnet die Dauer abz√ºglich der Sleep-Zeiten f√ºr Rate-Limits.
            source_code: "def calculate_net_time(start_time, end_time, total_items, batch_size, model_name):\n    \"\"\"Berechnet die Dauer abz√ºglich der Sleep-Zeiten f√ºr Rate-Limits.\"\"\"\n    total_duration = end_time - start_time\n    \n    if not model_name.startswith(\"gemini-\"):\n        return total_duration\n\n    if total_items == 0:\n        return 0\n\n    num_batches = math.ceil(total_items / batch_size)\n    sleep_count = max(0, num_batches - 1)\n    total_sleep_time = sleep_count * 61\n    \n    net_time = total_duration - total_sleep_time\n    return max(0, net_time)"
            start_line: 57
            end_line: 72
            context:
              calls[0]:
              called_by[1]: backend.main.main_workflow
          - mode: function_analysis
            identifier: backend.main.main_workflow
            name: main_workflow
            args[4]: input,api_keys,model_names,status_callback
            docstring: null
            source_code: "def main_workflow(input, api_keys: dict, model_names: dict, status_callback=None):\n    \n    def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)\n\n    update_status(\"üîç Analysiere Input...\")\n    \n    user_input = input\n    \n    # API Key & Ollama Base URL aus Frontend holen\n    gemini_api_key = api_keys.get(\"gemini\")\n    openai_api_key = api_keys.get(\"gpt\")\n    scadsllm_api_key = api_keys.get(\"scadsllm\")\n    scadsllm_base_url = api_keys.get(\"scadsllm_base_url\")\n    ollama_base_url = api_keys.get(\"ollama\")\n    base_url = None\n\n    if model_names[\"helper\"].startswith(\"gpt-\"):\n        helper_api_key = openai_api_key\n    elif model_names[\"helper\"].startswith(\"gemini-\"):\n        helper_api_key = gemini_api_key\n    elif \"/\" in model_names[\"helper\"] or model_names[\"helper\"].startswith(\"alias-\") or any(x in model_names[\"helper\"] for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        helper_api_key = scadsllm_api_key\n        base_url = scadsllm_base_url\n    else:\n        helper_api_key = None\n        base_url = ollama_base_url\n    if model_names[\"main\"].startswith(\"gpt-\"):\n        main_api_key = openai_api_key\n    elif model_names[\"main\"].startswith(\"gemini-\"):\n        main_api_key = gemini_api_key\n    elif \"/\" in model_names[\"main\"] or model_names[\"main\"].startswith(\"alias-\") or any(x in model_names[\"main\"] for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        main_api_key = scadsllm_api_key\n        base_url = scadsllm_base_url\n    else:\n        main_api_key = None\n        base_url = ollama_base_url\n\n    # Standardeinstellungen f√ºr Modelle\n    helper_model = model_names.get(\"helper\", \"gpt-5-mini\")\n    main_model = model_names.get(\"main\", \"gpt-5.1\")\n\n    # Error Handling f√ºr fehlende API Keys\n    if not gemini_api_key and \"gemini\" in helper_model:\n        raise ValueError(\"Gemini API Key was not provided in api_keys dictionary.\")\n\n    # URL Extraktion\n    repo_url = None\n    url_pattern = r\"https?://(?:www\\.)?github\\.com/[^\\s]+\"\n    match = re.search(url_pattern, user_input)\n\n    if match:\n        repo_url = match.group(0)\n        logging.info(f\"Extracted repository URL: {repo_url}\")\n    else:\n        raise ValueError(\"Could not find a valid URL in the provided input.\")\n    \n    # Repo klonen und Dateien extrahieren\n    update_status(f\"‚¨áÔ∏è Klone Repository: {repo_url} ...\")\n    \n    repo_files = []\n    local_repo_path = \"\" \n\n    try: \n\n        with GitRepository(repo_url) as repo:\n            repo_files = repo.get_all_files()\n            local_repo_path = repo.temp_dir\n\n            logging.info(f\"Total files retrieved: {len(repo_files)}\")\n\n    except Exception as e:\n        logging.error(f\"Error cloning repository: {e}\")\n        raise \n\n\n    # Extrahiere Basic Infos\n    update_status(\"‚ÑπÔ∏è Extrahiere Basis-Informationen...\")\n    try:\n        info_extractor = ProjektInfoExtractor()\n        basic_project_info = info_extractor.extrahiere_info(dateien=repo_files, repo_url=repo_url)\n        logging.info(\"Basic project info extracted\")\n    except Exception as e:\n        logging.error(f\"Error extracting basic project info: {e}\")\n        basic_project_info = \"Could not extract basic info.\"\n\n    # Erstelle Repository Dateibaum\n    update_status(\"üå≤ Erstelle Repository Dateibaum...\")\n    try:\n        repo_file_tree = repo.get_file_tree()\n        logging.info(\"Repository file tree constructed\")\n    except Exception as e:\n        logging.error(f\"Error constructing repository file tree: {e}\")\n        repo_file_tree = \"Could not create file tree.\"\n\n    # Relationship Analyse durchf√ºhren\n    update_status(\"üîó Analysiere Beziehungen (Calls & Instanziierungen)...\")\n    try:\n        rel_analyzer = ProjectAnalyzer(project_root=local_repo_path)\n        rel_analyzer.analyze()\n        \n        raw_relationships = rel_analyzer.get_raw_relationships()\n        \n        logging.info(f\"Relationships analyzed.\")\n    except Exception as e:\n        logging.error(f\"Error in relationship analyzer: {e}\")\n        raw_relationships = {\"outgoing\": {}, \"incoming\": {}}\n\n    # Erstelle AST Schema\n    update_status(\"üå≥ Erstelle Abstract Syntax Tree (AST)...\")\n    try:        \n        ast_analyzer = ASTAnalyzer()   \n        ast_schema = ast_analyzer.analyze_repository(files=repo_files, repo=repo)\n        logging.info(\"AST schema created\")\n    except Exception as e:\n        logging.error(f\"Error retrieving repository files: {e}\")\n        raise\n\n    # Anreichern des AST Schemas mit Relationship Daten\n    update_status(\"‚ûï Reiche AST mit Beziehungsdaten an...\")            \n    try:   \n        ast_schema = ast_analyzer.merge_relationship_data(ast_schema, raw_relationships)\n        logging.info(\"AST schema created and enriched\")\n\n    except Exception as e:\n        logging.error(f\"Error processing repository: {e}\")\n        raise\n\n    # Vorbereitung der HelperLLM Eingaben\n    update_status(\"‚öôÔ∏è Bereite Daten f√ºr Helper LLM vor...\")\n    \n    helper_llm_function_input = []\n    helper_llm_class_input = []\n\n    try:\n        for filename, file_data in ast_schema['files'].items():\n            ast_nodes = file_data.get('ast_nodes', {})\n            imports = ast_nodes.get('imports', [])\n            functions = ast_nodes.get('functions', [])\n            classes = ast_nodes.get('classes', [])\n\n            for function in functions:\n                context = function.get('context', {})\n                \n                raw_called_by = context.get('called_by', [])\n                clean_called_by = [cb for cb in raw_called_by if isinstance(cb, dict)]\n\n                function_context = FunctionContextInput(\n                    calls = context.get('calls', []),\n                    called_by = clean_called_by \n                )\n                \n                function_input = FunctionAnalysisInput(\n                    mode = function.get('mode', 'function_analysis'),\n                    identifier = function.get('identifier'),\n                    source_code = function.get('source_code'),\n                    imports = imports,\n                    context = function_context\n                )\n                \n                helper_llm_function_input.append(function_input)\n\n            for _class in classes:\n                context = _class.get('context', {})\n                \n                method_context_inputs = []\n                for method in context.get('method_context', []):\n                    \n                    raw_method_called_by = method.get('called_by', [])\n                    clean_method_called_by = [cb for cb in raw_method_called_by if isinstance(cb, dict)]\n\n                    method_context_inputs.append(\n                        MethodContextInput(\n                            identifier=method.get('identifier'),\n                            calls=method.get('calls', []),\n                            called_by=clean_method_called_by, \n                            args=method.get('args', []),\n                            docstring=method.get('docstring')\n                        )\n                    )\n\n                raw_instantiated_by = context.get('instantiated_by', [])\n                clean_instantiated_by = [ib for ib in raw_instantiated_by if isinstance(ib, dict)]\n\n                class_context = ClassContextInput(\n                    dependencies = context.get('dependencies', []),\n                    instantiated_by = clean_instantiated_by, \n                    method_context = method_context_inputs\n                )\n\n                class_input = ClassAnalysisInput(\n                    mode = _class.get('mode', 'class_analysis'),\n                    identifier =_class.get('identifier'),\n                    source_code = _class.get('source_code'), \n                    imports = imports, \n                    context = class_context\n                )\n                \n                helper_llm_class_input.append(class_input)\n\n    except Exception as e:\n        logging.error(f\"Error preparing inputs for Helper LLM: {e}\")\n        raise\n    \n    # Initialisiere HelperLLM\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    \n    llm_helper = LLMHelper(\n        api_key=helper_api_key, \n        function_prompt_path=function_prompt_file, \n        class_prompt_path=class_prompt_file,\n        model_name=helper_model,\n        base_url=base_url,\n    )\n    \n    if ollama_base_url:\n        os.environ[\"OLLAMA_BASE_URL\"] = ollama_base_url\n\n    # Initialisiere Ergebniscontainer\n    analysis_results = {}\n    function_analysis_results = []\n    class_analysis_results = []\n\n    # Call HelperLLM f√ºr Funktionen\n    update_status(f\"ü§ñ Helper LLM: Analysiere {len(helper_llm_function_input)} Funktionen ({helper_model})...\")\n\n    try:\n        net_time_func = 0\n        if len(helper_llm_function_input) > 0:\n\n            logging.info(\"\\n--- Generating documentation for Functions ---\")\n            t_start_func = time.time()    \n            function_analysis_results = llm_helper.generate_for_functions(helper_llm_function_input)    \n            t_end_func = time.time()\n            net_time_func = calculate_net_time(t_start_func, t_end_func, len(helper_llm_function_input), llm_helper.batch_size, helper_model)\n\n        if len(function_analysis_results) != 0:\n            for doc in function_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"functions\" not in analysis_results:\n                        analysis_results[\"functions\"] = {}\n                    analysis_results[\"functions\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(f\"Failed to generate doc for a function\") \n    except Exception as e:\n        logging.error(f\"Error during Helper LLM function analysis: {e}\")\n        raise\n    \n\n    # Call HelperLLM f√ºr Klassen\n    try:\n        net_time_class = 0\n        if len(helper_llm_class_input) > 0:\n            # Rate Limit Sleep f√ºr Gemini Modelle\n            if llm_helper.model_name.startswith(\"gemini-\") & (len(helper_llm_function_input) > 0):\n                update_status(\"üí§ Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n                time.sleep(65)\n            \n            update_status(f\"ü§ñ Helper LLM: Analysiere {len(helper_llm_class_input)} Klassen ({helper_model})...\")\n            \n            logging.info(\"\\n--- Generating documentation for Classes ---\")\n            t_start_class = time.time()\n            class_analysis_results = llm_helper.generate_for_classes(helper_llm_class_input)\n            t_end_class = time.time()\n            net_time_class = calculate_net_time(t_start_class, t_end_class, len(helper_llm_class_input), llm_helper.batch_size, helper_model)\n\n        if len(class_analysis_results) != 0:\n            for doc in class_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"classes\" not in analysis_results:\n                        analysis_results[\"classes\"] = {}\n                    analysis_results[\"classes\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(f\"Failed to generate doc for a class\")\n    except Exception as e:\n        logging.error(f\"Error during Helper LLM class analysis: {e}\")\n        raise\n\n    total_helper_time = net_time_func + net_time_class\n\n    # MainLLM Input Vorbereitung\n    main_llm_input = {\n        \"basic_info\": basic_project_info,\n        \"file_tree\": repo_file_tree,\n        \"ast_schema\": ast_schema,\n        \"analysis_results\": analysis_results\n    }\n\n    # Speichern als JSON (Optional)\n    main_llm_input_json = json.dumps(main_llm_input, indent=2)\n    # with open(\"output.json\", \"w\", encoding=\"utf-8\") as f:\n    #     f.write(main_llm_input_json)\n    #     logging.info(\"JSON-Datei wurde gespeichert.\")\n\n    # Konvertiere Input in Toon Format\n    main_llm_input_toon = encode(main_llm_input)\n\n    #Speichern in TOON Format (Optional)\n    # with open(\"output.toon\", \"w\", encoding=\"utf-8\") as f:\n    #     f.write(main_llm_input_toon)\n    #     logging.info(\"output.toon erfolgreich gespeichert.\")\n    \n    # Token Evaluation\n    savings_data = None\n    try:\n        logging.info(\"--- Evaluierung der Token-Ersparnis ---\")\n        savings_data = estimate_savings(main_llm_input)\n        logging.info(f\"JSON Tokens: {savings_data['json_tokens']}\")\n        logging.info(f\"TOON Tokens: {savings_data['toon_tokens']}\")\n        logging.info(f\"Ersparnis:   {savings_data['savings_percent']:.2f}%\")\n        \n    except Exception as e:\n        logging.warning(f\"Token evaluation could not be performed: {e}\")    \n\n\n\n    prompt_file_mainllm = \"SystemPrompts/SystemPromptMainLLM.txt\"\n    prompt_file_mainllm_toon = \"SystemPrompts/SystemPromptMainLLMToon.txt\"\n    # MainLLM Ausf√ºhrung\n    main_llm = MainLLM(\n        api_key=main_api_key, \n        prompt_file_path=prompt_file_mainllm_toon,\n        model_name=main_model,\n        base_url=base_url,\n    )\n\n\n    # RPM Limit Sleep f√ºr Gemini Modelle\n    if llm_helper.model_name == main_llm.model_name and main_llm.model_name.startswith(\"gemini-\"):\n        time.sleep(65)\n        update_status(\"üí§ Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n\n    # Call MainLLM f√ºr finalen Report\n    update_status(f\"üß† Main LLM: Generiere finalen Report ({main_model})...\")\n    try:\n        total_main_time = 0\n        logging.info(\"\\n--- Generating Final Report ---\")\n        t_start_main = time.time()\n        final_report = main_llm.call_llm(main_llm_input_toon)\n        t_end_main = time.time()\n        total_main_time = t_end_main - t_start_main\n    except Exception as e:\n        logging.error(f\"Error during Main LLM final report generation: {e}\")\n        raise\n\n    \n    # --- Speichern der Ergebnisse ---\n    output_dir = \"result\"\n    stats_dir = \"Statistics\" # Neuer Ordner\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(stats_dir, exist_ok=True) # Stelle sicher, dass Statistics Ordner existiert\n\n    total_active_time = total_helper_time + total_main_time\n    \n    # Dateiname f√ºr Report\n    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M-%S\")\n    report_filename = f\"report_{timestamp}_Helper_{llm_helper.model_name}_MainLLM_{main_llm.model_name}.md\"\n    report_filepath = os.path.join(output_dir, report_filename)\n    \n    if final_report:\n        with open(report_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(final_report)\n        logging.info(f\"Final report saved to '{report_filepath}'.\")\n        \n        if savings_data:\n            try:\n                savings_filename = report_filename.replace(\"report_\", \"savings_\").replace(\".md\", \".png\")\n                savings_filepath = os.path.join(stats_dir, savings_filename)\n                \n                logging.info(f\"Erstelle Token-Chart: {savings_filepath}\")\n                create_savings_chart(\n                    json_tokens=savings_data['json_tokens'], \n                    toon_tokens=savings_data['toon_tokens'], \n                    savings_percent=savings_data['savings_percent'],\n                    output_path=savings_filepath\n                )\n            except Exception as chart_error:\n                logging.error(f\"Konnte Diagramm nicht erstellen: {chart_error}\")\n\n    else:\n        final_report = \"Error: Report generation failed or returned empty.\"\n\n    metrics = {\n        \"helper_time\": round(total_helper_time, 2),\n        \"main_time\": round(total_main_time, 2),\n        \"total_time\": round(total_active_time, 2),\n        \"helper_model\": helper_model,\n        \"main_model\": main_model,\n        \"json_tokens\": savings_data['json_tokens'] if savings_data else None,\n        \"toon_tokens\": savings_data['toon_tokens'] if savings_data else None,\n        \"savings_percent\": round(savings_data['savings_percent'], 2) if savings_data else None\n    \n    }\n\n    return {\n        \"report\": final_report,\n        \"metrics\": metrics\n    }"
            start_line: 75
            end_line: 477
            context:
              calls[22]: backend.AST_Schema.ASTAnalyzer,backend.AST_Schema.ASTAnalyzer.analyze_repository,backend.AST_Schema.ASTAnalyzer.merge_relationship_data,backend.HelperLLM.LLMHelper,backend.HelperLLM.LLMHelper.generate_for_classes,backend.HelperLLM.LLMHelper.generate_for_functions,backend.MainLLM.MainLLM,backend.MainLLM.MainLLM.call_llm,backend.basic_info.ProjektInfoExtractor,backend.basic_info.ProjektInfoExtractor.extrahiere_info,backend.getRepo.GitRepository,backend.main.calculate_net_time,backend.main.create_savings_chart,backend.main.update_status,backend.relationship_analyzer.ProjectAnalyzer,backend.relationship_analyzer.ProjectAnalyzer.analyze,backend.relationship_analyzer.ProjectAnalyzer.get_raw_relationships,schemas.types.ClassAnalysisInput,schemas.types.ClassContextInput,schemas.types.FunctionAnalysisInput,schemas.types.FunctionContextInput,schemas.types.MethodContextInput
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: backend.main.update_status
            name: update_status
            args[1]: msg
            docstring: null
            source_code: "def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)"
            start_line: 77
            end_line: 80
            context:
              calls[0]:
              called_by[2]: backend.main.main_workflow,backend.main.notebook_workflow
          - mode: function_analysis
            identifier: backend.main.notebook_workflow
            name: notebook_workflow
            args[4]: input,api_keys,model,status_callback
            docstring: null
            source_code: "def notebook_workflow(input, api_keys, model, status_callback=None):\n    t_start = time.time()\n    final_report = \"keine Notebooks gefunden\"\n    def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)\n\n    def gemini_payload(basic_info, nb_path, xml_content, images):\n        \n        intro_json = json.dumps({\n            \"basic_info\": basic_info,\n            \"current_notebook_path\": nb_path\n        }, indent=2)\n        \n        payload_content = []\n\n        payload_content.append({\n            \"type\": \"text\",\n            \"text\": f\"Context Information:\\n{intro_json}\\n\\nNotebook XML Structure:\\n\"\n        })\n\n        # Regex to find: <IMAGE_PLACEHOLDER index=\"0\" mime=\"image/png\"/>\n        pattern = r'(<IMAGE_PLACEHOLDER index=\"(\\d+)\" mime=\"([^\"]+)\"/>)'\n        last_pos = 0\n        \n        for match in re.finditer(pattern, xml_content):\n            text_segment = xml_content[last_pos:match.start()]\n            if text_segment.strip():\n                payload_content.append({\n                    \"type\": \"text\",\n                    \"text\": text_segment\n                })\n                \n            image_index = int(match.group(2))\n            mime_type = match.group(3)\n            \n            if image_index < len(images):\n                img_data = images[image_index]\n                b64_string = img_data['data']\n                \n                payload_content.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:{mime_type};base64,{b64_string}\"\n                    }\n                })\n                \n            last_pos = match.end()\n\n        # Add any remaining text after the last image\n        remaining_text = xml_content[last_pos:]\n        if remaining_text.strip():\n            payload_content.append({\n                \"type\": \"text\",\n                \"text\": remaining_text\n            })\n\n        return payload_content\n\n\n    base_url = None\n\n    if model.startswith(\"gpt-\"):\n        input_api_key = api_keys.get(\"gpt\")\n    elif model.startswith(\"gemini-\"):\n        input_api_key = api_keys.get(\"gemini\")\n    elif \"/\" in model or model.startswith(\"alias-\") or any(x in model for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        input_api_key = api_keys.get(\"scadsllm\")\n        base_url = api_keys.get(\"scadsllm_base_url\")\n    else:\n        input_api_key = None\n        base_url = api_keys.get(\"ollama\")\n\n    update_status(\"üîç Analysiere Input...\")\n\n    # URL Extraktion\n    repo_url = None\n    url_pattern = r\"https?://(?:www\\.)?github\\.com/[^\\s]+\"\n    match = re.search(url_pattern, input)\n\n    if match:\n        repo_url = match.group(0)\n        logging.info(f\"Extracted repository URL: {repo_url}\")\n    else:\n        raise ValueError(\"Could not find a valid URL in the provided input.\")\n    \n    update_status(f\"‚¨áÔ∏è Klone Repository: {repo_url} ...\")\n\n    try: \n        with GitRepository(repo_url) as repo:\n            repo_files = repo.get_all_files()\n    except Exception as e:\n        logging.error(f\"Error cloning repository: {e}\")\n        raise\n    \n    update_status(f\"üîó Bereite Input vor...\")\n\n    # convert to XML\n    processed_data = process_repo_notebooks(repo_files)\n\n    \n    # Extrahiere Basic Infos\n    update_status(\"‚ÑπÔ∏è Extrahiere Basis-Informationen...\")\n    try:\n        info_extractor = ProjektInfoExtractor()\n        basic_project_info = info_extractor.extrahiere_info(dateien=repo_files, repo_url=repo_url)\n        logging.info(\"Basic project info extracted\")\n    except Exception as e:\n        logging.error(f\"Error extracting basic project info: {e}\")\n        basic_project_info = \"Could not extract basic info.\"\n\n\n\n    prompt_file_notebook_llm = \"SystemPrompts/SystemPromptNotebookLLM.txt\"\n    notebook_llm = MainLLM(\n        api_key=input_api_key, \n        prompt_file_path=prompt_file_notebook_llm,\n        model_name=model,\n        base_url=base_url,\n    )\n\n    notebook_reports = []\n    total_notebooks = len(processed_data)\n    \n    logging.info(f\"Starting sequential processing of {total_notebooks} notebooks...\")\n\n    # Iterate over each notebook file individually\n    for index, (nb_path, nb_data) in enumerate(processed_data.items(), 1):\n        \n        update_status(f\"üß† Generiere Report ({index}/{total_notebooks}): {os.path.basename(nb_path)}\")\n        \n        nb_xml = nb_data['xml']\n        nb_images = nb_data['images']\n\n        llm_payload = gemini_payload(basic_project_info, nb_path, nb_xml, nb_images)\n\n        try:\n            single_report = notebook_llm.call_llm(llm_payload)\n            if single_report and isinstance(single_report, str):\n                notebook_reports.append(single_report)\n            else:\n                logging.warning(f\"LLM lieferte kein Ergebnis f√ºr {nb_path}\")\n                notebook_reports.append(f\"## Analyse f√ºr {os.path.basename(nb_path)}\\nFehler: Das Modell lieferte keine Antwort.\")\n            \n        except Exception as e:\n            logging.error(f\"Error generating report for {nb_path}: {e}\")\n            notebook_reports.append(f\"# Error processing {nb_path}\\n\\nError: {str(e)}\\n\\n---\\n\")\n\n    # Concatenate all reports\n    final_report = \"\\n\\n<br>\\n<br>\\n<br>\\n\\n\".join([str(r) for r in notebook_reports if r is not None])\n\n    \n    # --- Speichern der Ergebnisse ---\n    output_dir = \"result\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Dateiname f√ºr Report\n    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M-%S\")\n    report_filename = f\"notebook_report_{timestamp}_NotebookLLM_{notebook_llm.model_name}.md\"\n    report_filepath = os.path.join(output_dir, report_filename)\n    \n    if final_report:\n        with open(report_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(final_report)\n        logging.info(f\"Final report saved to '{report_filepath}'.\")\n\n    # Am Ende der Funktion nach dem Speichern:\n    total_time = time.time() - t_start\n    \n    # Metriken f√ºr das Frontend bauen\n    metrics = {\n        \"helper_time\": \"0\", # Notebook-Workflow hat meist keinen separaten Helper\n        \"main_time\": round(total_time, 2),\n        \"total_time\": round(total_time, 2),\n        \"helper_model\": \"None\",\n        \"main_model\": model,\n        \"json_tokens\": 0,\n        \"toon_tokens\": 0,\n        \"savings_percent\": 0\n    }\n\n    return {\n        \"report\": final_report,\n        \"metrics\": metrics\n    }"
            start_line: 483
            end_line: 668
            context:
              calls[8]: backend.MainLLM.MainLLM,backend.MainLLM.MainLLM.call_llm,backend.basic_info.ProjektInfoExtractor,backend.basic_info.ProjektInfoExtractor.extrahiere_info,backend.converter.process_repo_notebooks,backend.getRepo.GitRepository,backend.main.gemini_payload,backend.main.update_status
              called_by[2]: backend.main,frontend.frontend
          - mode: function_analysis
            identifier: backend.main.update_status
            name: update_status
            args[1]: msg
            docstring: null
            source_code: "def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)"
            start_line: 486
            end_line: 489
            context:
              calls[0]:
              called_by[2]: backend.main.main_workflow,backend.main.notebook_workflow
          - mode: function_analysis
            identifier: backend.main.gemini_payload
            name: gemini_payload
            args[4]: basic_info,nb_path,xml_content,images
            docstring: null
            source_code: "def gemini_payload(basic_info, nb_path, xml_content, images):\n        \n        intro_json = json.dumps({\n            \"basic_info\": basic_info,\n            \"current_notebook_path\": nb_path\n        }, indent=2)\n        \n        payload_content = []\n\n        payload_content.append({\n            \"type\": \"text\",\n            \"text\": f\"Context Information:\\n{intro_json}\\n\\nNotebook XML Structure:\\n\"\n        })\n\n        # Regex to find: <IMAGE_PLACEHOLDER index=\"0\" mime=\"image/png\"/>\n        pattern = r'(<IMAGE_PLACEHOLDER index=\"(\\d+)\" mime=\"([^\"]+)\"/>)'\n        last_pos = 0\n        \n        for match in re.finditer(pattern, xml_content):\n            text_segment = xml_content[last_pos:match.start()]\n            if text_segment.strip():\n                payload_content.append({\n                    \"type\": \"text\",\n                    \"text\": text_segment\n                })\n                \n            image_index = int(match.group(2))\n            mime_type = match.group(3)\n            \n            if image_index < len(images):\n                img_data = images[image_index]\n                b64_string = img_data['data']\n                \n                payload_content.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:{mime_type};base64,{b64_string}\"\n                    }\n                })\n                \n            last_pos = match.end()\n\n        # Add any remaining text after the last image\n        remaining_text = xml_content[last_pos:]\n        if remaining_text.strip():\n            payload_content.append({\n                \"type\": \"text\",\n                \"text\": remaining_text\n            })\n\n        return payload_content"
            start_line: 491
            end_line: 541
            context:
              calls[0]:
              called_by[1]: backend.main.notebook_workflow
        classes[0]:
    "backend/relationship_analyzer.py":
      ast_nodes:
        imports[4]: ast,os,logging,collections.defaultdict
        functions[1]:
          - mode: function_analysis
            identifier: backend.relationship_analyzer.path_to_module
            name: path_to_module
            args[2]: filepath,project_root
            docstring: Wandelt einen Dateipfad in einen Python-Modulpfad um.
            source_code: "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    module_path = rel_path.replace(os.path.sep, '.')\n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n    return module_path"
            start_line: 6
            end_line: 18
            context:
              calls[0]:
              called_by[2]: backend.relationship_analyzer.CallResolverVisitor.__init__,backend.relationship_analyzer.ProjectAnalyzer._collect_definitions
        classes[2]:
          - mode: class_analysis
            identifier: backend.relationship_analyzer.ProjectAnalyzer
            name: ProjectAnalyzer
            docstring: null
            source_code: "class ProjectAnalyzer:\n    \n    def __init__(self, project_root):\n        self.project_root = os.path.abspath(project_root)\n        self.definitions = {}\n        self.call_graph = defaultdict(list)\n        self.file_asts = {} \n        self.ignore_dirs = {'.git', '.venv', 'venv', '__pycache__', 'node_modules', 'dist', 'build', 'docs'}\n\n    def analyze(self):\n        py_files = self._find_py_files()\n        \n        for filepath in py_files:\n            self._collect_definitions(filepath)\n            \n        for filepath in py_files:\n            self._resolve_calls(filepath)\n            \n        self.file_asts.clear()\n        \n        return self.call_graph\n\n    def get_raw_relationships(self):\n\n        outgoing = defaultdict(set)\n        incoming = defaultdict(set)\n\n        for callee_id, callers_info_list in self.call_graph.items():\n            for info in callers_info_list:\n                caller_id = info['caller']\n                \n                if caller_id and callee_id:\n                    outgoing[caller_id].add(callee_id)\n                    incoming[callee_id].add(caller_id)\n        \n        return {\n            \"outgoing\": {k: sorted(list(v)) for k, v in outgoing.items()},\n            \"incoming\": {k: sorted(list(v)) for k, v in incoming.items()}\n        }\n\n    def _find_py_files(self):\n        py_files = []\n        for root, dirs, files in os.walk(self.project_root):\n            dirs[:] = [d for d in dirs if d not in self.ignore_dirs]\n            for file in files:\n                if file.endswith(\".py\"):\n                    py_files.append(os.path.join(root, file))\n        return py_files\n\n    def _collect_definitions(self, filepath):\n        try:\n            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n                source = f.read()\n                tree = ast.parse(source, filename=filepath)\n                \n            self.file_asts[filepath] = tree\n            module_path = path_to_module(filepath, self.project_root)\n            \n            for node in ast.walk(tree):\n                if isinstance(node, ast.FunctionDef):\n                    parent = self._get_parent(tree, node)\n                    if isinstance(parent, ast.ClassDef):\n                        path_name = f\"{module_path}.{parent.name}.{node.name}\"\n                        def_type = 'method'\n                    else:\n                        path_name = f\"{module_path}.{node.name}\"\n                        def_type = 'function'\n                    self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': def_type}\n                elif isinstance(node, ast.ClassDef):\n                    path_name = f\"{module_path}.{node.name}\"\n                    self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': 'class'}\n        except Exception as e:\n            logging.error(f\"Error collecting definitions in {filepath}: {e}\")\n            self.file_asts[filepath] = None\n\n    def _get_parent(self, tree, node):\n        for parent in ast.walk(tree):\n            for child in ast.iter_child_nodes(parent):\n                if child is node:\n                    return parent\n        return None\n\n    def _resolve_calls(self, filepath):\n        tree = self.file_asts.get(filepath)\n        if not tree:\n            return\n\n        try:\n            resolver = CallResolverVisitor(filepath, self.project_root, self.definitions)\n            resolver.visit(tree)\n            \n            for callee_pathname, caller_info_list in resolver.calls.items():\n                self.call_graph[callee_pathname].extend(caller_info_list)\n        except Exception as e:\n            logging.error(f\"Error resolving calls in {filepath}: {e}\")"
            start_line: 20
            end_line: 114
            context:
              dependencies[2]: backend.relationship_analyzer.CallResolverVisitor,backend.relationship_analyzer.path_to_module
              instantiated_by[1]: backend.main.main_workflow
              method_context[7]:
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,project_root
                  docstring: null
                  start_line: 22
                  end_line: 27
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.analyze
                  name: analyze
                  calls[0]:
                  called_by[1]: backend.main.main_workflow
                  args[1]: self
                  docstring: null
                  start_line: 29
                  end_line: 40
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.get_raw_relationships
                  name: get_raw_relationships
                  calls[0]:
                  called_by[1]: backend.main.main_workflow
                  args[1]: self
                  docstring: null
                  start_line: 42
                  end_line: 58
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._find_py_files
                  name: _find_py_files
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 60
                  end_line: 67
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._collect_definitions
                  name: _collect_definitions
                  calls[1]: backend.relationship_analyzer.path_to_module
                  called_by[0]:
                  args[2]: self,filepath
                  docstring: null
                  start_line: 69
                  end_line: 93
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._get_parent
                  name: _get_parent
                  calls[0]:
                  called_by[0]:
                  args[3]: self,tree,node
                  docstring: null
                  start_line: 95
                  end_line: 100
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._resolve_calls
                  name: _resolve_calls
                  calls[1]: backend.relationship_analyzer.CallResolverVisitor
                  called_by[0]:
                  args[2]: self,filepath
                  docstring: null
                  start_line: 102
                  end_line: 114
          - mode: class_analysis
            identifier: backend.relationship_analyzer.CallResolverVisitor
            name: CallResolverVisitor
            docstring: null
            source_code: "class CallResolverVisitor(ast.NodeVisitor):\n    def __init__(self, filepath, project_root, definitions):\n        self.filepath = filepath\n        self.module_path = path_to_module(filepath, project_root)\n        self.definitions = definitions\n        self.scope = {}\n        self.instance_types = {}\n        self.current_caller_name = self.module_path\n        self.current_class_name = None\n        self.calls = defaultdict(list)\n\n    def visit_ClassDef(self, node):\n        old_class_name = self.current_class_name\n        self.current_class_name = node.name\n        self.generic_visit(node)\n        self.current_class_name = old_class_name\n\n    def visit_FunctionDef(self, node):\n        old_caller = self.current_caller_name\n        \n        if self.current_class_name:\n            full_identifier = f\"{self.module_path}.{self.current_class_name}.{node.name}\"\n        else:\n            full_identifier = f\"{self.module_path}.{node.name}\"\n            \n        self.current_caller_name = full_identifier\n        self.generic_visit(node)\n        self.current_caller_name = old_caller\n\n    def visit_Call(self, node):\n        callee_pathname = self._resolve_call_qname(node.func)\n        if callee_pathname and callee_pathname in self.definitions:\n            \n            if self.current_caller_name == self.module_path:\n                caller_type = 'module'\n            elif '.<locals>.' in self.current_caller_name:\n                caller_type = 'local_function'\n            elif self.current_class_name:\n                caller_type = 'method'\n            else:\n                caller_type = 'function'\n            \n            caller_info = {\n                'file': os.path.basename(self.filepath),\n                'line': node.lineno,\n                'caller': self.current_caller_name,\n                'caller_type': caller_type \n            }\n            self.calls[callee_pathname].append(caller_info)\n        self.generic_visit(node)\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.scope[alias.asname or alias.name] = alias.name\n        self.generic_visit(node)\n        \n    def visit_ImportFrom(self, node):\n        module = node.module or ''\n        for alias in node.names:\n            name = alias.asname or alias.name\n            if node.level > 0:\n                base = self.module_path.split('.')\n                prefix = '.'.join(base[:-node.level])\n                full_module_path = f\"{prefix}.{module}\" if module else prefix\n            else:\n                full_module_path = module\n            self.scope[name] = f\"{full_module_path}.{alias.name}\"\n        self.generic_visit(node)\n\n    def visit_Assign(self, node):\n        if isinstance(node.value, ast.Call) and isinstance(node.value.func, ast.Name):\n            class_name = node.value.func.id\n            if class_name in self.scope:\n                qualified_class_name = self.scope[class_name]\n                if qualified_class_name in self.definitions:\n                    for target in node.targets:\n                        if isinstance(target, ast.Name):\n                            self.instance_types[target.id] = qualified_class_name\n        self.generic_visit(node)\n\n    def _resolve_call_qname(self, func_node):\n        if isinstance(func_node, ast.Name):\n            name = func_node.id\n            if name in self.scope:\n                return self.scope[name]\n            local_pathname = f\"{self.module_path}.{name}\"\n            if local_pathname in self.definitions:\n                return local_pathname\n        elif isinstance(func_node, ast.Attribute) and isinstance(func_node.value, ast.Name):\n            var_name = func_node.value.id\n            method_name = func_node.attr\n            if var_name in self.instance_types:\n                class_pathname = self.instance_types[var_name]\n                return f\"{class_pathname}.{method_name}\"\n            if var_name in self.scope:\n                module_pathname = self.scope[var_name]\n                return f\"{module_pathname}.{method_name}\"\n        return None"
            start_line: 117
            end_line: 214
            context:
              dependencies[1]: backend.relationship_analyzer.path_to_module
              instantiated_by[1]: backend.relationship_analyzer.ProjectAnalyzer._resolve_calls
              method_context[8]:
                - identifier: backend.relationship_analyzer.CallResolverVisitor.__init__
                  name: __init__
                  calls[1]: backend.relationship_analyzer.path_to_module
                  called_by[0]:
                  args[4]: self,filepath,project_root,definitions
                  docstring: null
                  start_line: 118
                  end_line: 126
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 128
                  end_line: 132
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 134
                  end_line: 144
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Call
                  name: visit_Call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 146
                  end_line: 166
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 168
                  end_line: 171
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 173
                  end_line: 184
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Assign
                  name: visit_Assign
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 186
                  end_line: 195
                - identifier: backend.relationship_analyzer.CallResolverVisitor._resolve_call_qname
                  name: _resolve_call_qname
                  calls[0]:
                  called_by[0]:
                  args[2]: self,func_node
                  docstring: null
                  start_line: 197
                  end_line: 214
    "backend/scads_key_test.py":
      ast_nodes:
        imports[3]: os,dotenv.load_dotenv,openai.OpenAI
        functions[0]:
        classes[0]:
    "database/db.py":
      ast_nodes:
        imports[7]: datetime.datetime,pymongo.MongoClient,dotenv.load_dotenv,streamlit_authenticator,cryptography.fernet.Fernet,uuid,os
        functions[29]:
          - mode: function_analysis
            identifier: database.db.encrypt_text
            name: encrypt_text
            args[1]: text
            docstring: null
            source_code: "def encrypt_text(text: str) -> str:\n    if not text or not cipher_suite: return text\n    return cipher_suite.encrypt(text.strip().encode()).decode()"
            start_line: 28
            end_line: 30
            context:
              calls[0]:
              called_by[3]: database.db.update_gemini_key,database.db.update_gpt_key,database.db.update_opensrc_key
          - mode: function_analysis
            identifier: database.db.decrypt_text
            name: decrypt_text
            args[1]: text
            docstring: null
            source_code: "def decrypt_text(text: str) -> str:\n    if not text or not cipher_suite: return text\n    try:\n        return cipher_suite.decrypt(text.strip().encode()).decode()\n    except Exception:\n        return text"
            start_line: 32
            end_line: 37
            context:
              calls[0]:
              called_by[1]: database.db.get_decrypted_api_keys
          - mode: function_analysis
            identifier: database.db.insert_user
            name: insert_user
            args[3]: username,name,password
            docstring: null
            source_code: "def insert_user(username: str, name: str, password: str):\n    user = {\n        \"_id\": username,\n        \"name\": name, \n        \"hashed_password\": stauth.Hasher.hash(password),\n        \"gemini_api_key\": \"\",\n        \"ollama_base_url\": \"\",\n        \"gpt_api_key\": \"\"\n    }\n    result = dbusers.insert_one(user)\n    return result.inserted_id"
            start_line: 43
            end_line: 53
            context:
              calls[0]:
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: database.db.fetch_all_users
            name: fetch_all_users
            args[0]:
            docstring: null
            source_code: "def fetch_all_users():\n    return list(dbusers.find())"
            start_line: 55
            end_line: 56
            context:
              calls[0]:
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: database.db.fetch_user
            name: fetch_user
            args[1]: username
            docstring: null
            source_code: "def fetch_user(username: str):\n    return dbusers.find_one({\"_id\": username})"
            start_line: 58
            end_line: 59
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_user_name
            name: update_user_name
            args[2]: username,new_name
            docstring: null
            source_code: "def update_user_name(username: str, new_name: str):\n    # Achtung: _id kann in Mongo nicht einfach so ge√§ndert werden. \n    # Hier wird nur das Name-Feld geupdated.\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"name\": new_name}})\n    return result.modified_count"
            start_line: 61
            end_line: 65
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_gemini_key
            name: update_gemini_key
            args[2]: username,gemini_api_key
            docstring: null
            source_code: "def update_gemini_key(username: str, gemini_api_key: str):\n    encrypted_key = encrypt_text(gemini_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"gemini_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 67
            end_line: 70
            context:
              calls[1]: database.db.encrypt_text
              called_by[2]: frontend.frontend,frontend.frontend.save_gemini_cb
          - mode: function_analysis
            identifier: database.db.update_gpt_key
            name: update_gpt_key
            args[2]: username,gpt_api_key
            docstring: null
            source_code: "def update_gpt_key(username: str, gpt_api_key: str):\n    encrypted_key = encrypt_text(gpt_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"gpt_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 72
            end_line: 75
            context:
              calls[1]: database.db.encrypt_text
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: database.db.update_ollama_url
            name: update_ollama_url
            args[2]: username,ollama_base_url
            docstring: null
            source_code: "def update_ollama_url(username: str, ollama_base_url: str):\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"ollama_base_url\": ollama_base_url.strip()}})\n    return result.modified_count"
            start_line: 77
            end_line: 79
            context:
              calls[0]:
              called_by[2]: frontend.frontend,frontend.frontend.save_ollama_cb
          - mode: function_analysis
            identifier: database.db.update_opensrc_key
            name: update_opensrc_key
            args[2]: username,opensrc_api_key
            docstring: null
            source_code: "def update_opensrc_key(username: str, opensrc_api_key: str):\n    encrypted_key = encrypt_text(opensrc_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"opensrc_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 81
            end_line: 84
            context:
              calls[1]: database.db.encrypt_text
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: database.db.update_opensrc_url
            name: update_opensrc_url
            args[2]: username,opensrc_base_url
            docstring: null
            source_code: "def update_opensrc_url(username: str, opensrc_base_url: str):\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"opensrc_base_url\": opensrc_base_url.strip()}})\n    return result.modified_count"
            start_line: 86
            end_line: 88
            context:
              calls[0]:
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: database.db.fetch_gemini_key
            name: fetch_gemini_key
            args[1]: username
            docstring: null
            source_code: "def fetch_gemini_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"gemini_api_key\": 1, \"_id\": 0})\n    return user.get(\"gemini_api_key\") if user else None"
            start_line: 90
            end_line: 92
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_ollama_url
            name: fetch_ollama_url
            args[1]: username
            docstring: null
            source_code: "def fetch_ollama_url(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"ollama_base_url\": 1, \"_id\": 0})\n    return user.get(\"ollama_base_url\") if user else None"
            start_line: 94
            end_line: 96
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_gpt_key
            name: fetch_gpt_key
            args[1]: username
            docstring: null
            source_code: "def fetch_gpt_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"gpt_api_key\": 1, \"_id\": 0})\n    return user.get(\"gpt_api_key\") if user else None"
            start_line: 98
            end_line: 100
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_opensrc_key
            name: fetch_opensrc_key
            args[1]: username
            docstring: null
            source_code: "def fetch_opensrc_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"opensrc_api_key\": 1, \"_id\": 0})\n    return user.get(\"opensrc_api_key\") if user else None"
            start_line: 102
            end_line: 104
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_opensrc_url
            name: fetch_opensrc_url
            args[1]: username
            docstring: null
            source_code: "def fetch_opensrc_url(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"opensrc_base_url\": 1, \"_id\": 0})\n    return user.get(\"opensrc_base_url\") if user else None"
            start_line: 106
            end_line: 108
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.delete_user
            name: delete_user
            args[1]: username
            docstring: null
            source_code: "def delete_user(username: str):\n    return dbusers.delete_one({\"_id\": username}).deleted_count"
            start_line: 110
            end_line: 111
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.get_decrypted_api_keys
            name: get_decrypted_api_keys
            args[1]: username
            docstring: null
            source_code: "def get_decrypted_api_keys(username: str):\n    user = dbusers.find_one({\"_id\": username})\n    if not user: return None, None\n    gemini_plain = decrypt_text(user.get(\"gemini_api_key\", \"\"))\n    ollama_plain = user.get(\"ollama_base_url\", \"\")\n    gpt_plain = decrypt_text(user.get(\"gpt_api_key\", \"\"))\n    opensrc_plain = decrypt_text(user.get(\"opensrc_api_key\", \"\"))\n    opensrc_url = user.get(\"opensrc_base_url\", \"\")\n    return gemini_plain, ollama_plain, gpt_plain, opensrc_plain, opensrc_url"
            start_line: 113
            end_line: 121
            context:
              calls[1]: database.db.decrypt_text
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: database.db.insert_chat
            name: insert_chat
            args[2]: username,chat_name
            docstring: Erstellt einen neuen Chat-Eintrag.
            source_code: "def insert_chat(username: str, chat_name: str):\n    \"\"\"Erstellt einen neuen Chat-Eintrag.\"\"\"\n    chat = {\n        \"_id\": str(uuid.uuid4()),\n        \"username\": username,\n        \"chat_name\": chat_name,\n        \"created_at\": datetime.now()\n    }\n    result = dbchats.insert_one(chat)\n    return result.inserted_id"
            start_line: 127
            end_line: 136
            context:
              calls[0]:
              called_by[3]: frontend.frontend,frontend.frontend.handle_delete_chat,frontend.frontend.load_data_from_db
          - mode: function_analysis
            identifier: database.db.fetch_chats_by_user
            name: fetch_chats_by_user
            args[1]: username
            docstring: Holt alle definierten Chats eines Users.
            source_code: "def fetch_chats_by_user(username: str):\n    \"\"\"Holt alle definierten Chats eines Users.\"\"\"\n    # Sortieren nach Erstellung macht Sinn\n    chats = list(dbchats.find({\"username\": username}).sort(\"created_at\", 1))\n    return chats"
            start_line: 138
            end_line: 142
            context:
              calls[0]:
              called_by[1]: frontend.frontend.load_data_from_db
          - mode: function_analysis
            identifier: database.db.check_chat_exists
            name: check_chat_exists
            args[2]: username,chat_name
            docstring: null
            source_code: "def check_chat_exists(username: str, chat_name: str):\n    return dbchats.find_one({\"username\": username, \"chat_name\": chat_name}) is not None"
            start_line: 144
            end_line: 145
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.rename_chat_fully
            name: rename_chat_fully
            args[3]: username,old_name,new_name
            docstring: Benennt einen Chat und alle zugeh√∂rigen Exchanges um.
            source_code: "def rename_chat_fully(username: str, old_name: str, new_name: str):\n    \"\"\"\n    Benennt einen Chat und alle zugeh√∂rigen Exchanges um.\n    \"\"\"\n    # 1. Chat-Eintrag umbenennen\n    result = dbchats.update_one(\n        {\"username\": username, \"chat_name\": old_name}, \n        {\"$set\": {\"chat_name\": new_name}}\n    )\n    \n    # 2. Alle Messages (Exchanges) umh√§ngen\n    dbexchanges.update_many(\n        {\"username\": username, \"chat_name\": old_name},\n        {\"$set\": {\"chat_name\": new_name}}\n    )\n    return result.modified_count"
            start_line: 148
            end_line: 163
            context:
              calls[0]:
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: database.db.insert_exchange
            name: insert_exchange
            args[13]: question,answer,feedback,username,chat_name,helper_used,main_used,total_time,helper_time,main_time,json_tokens,toon_tokens,savings_percent
            docstring: null
            source_code: "def insert_exchange(question: str, answer: str, feedback: str, username: str, chat_name: str, \n                    helper_used: str=\"\", main_used: str=\"\", total_time: str=\"\", helper_time: str=\"\", main_time: str=\"\",\n                    json_tokens=0, toon_tokens=0, savings_percent=0.0):\n    new_id = str(uuid.uuid4())\n    exchange = {\n        \"_id\": new_id,\n        \"question\": question,\n        \"answer\": answer,\n        \"feedback\": feedback,\n        \"feedback_message\": \"\",\n        \"chat_name\": chat_name,\n        \"username\": username,\n        \"helper_used\": helper_used,\n        \"main_used\": main_used,\n        \"total_time\": total_time,\n        \"helper_time\": helper_time,\n        \"main_time\": main_time,\n        \"json_tokens\": json_tokens,\n        \"toon_tokens\": toon_tokens,\n        \"savings_percent\": savings_percent,\n        \"created_at\": datetime.now()\n    }\n    try:\n        dbexchanges.insert_one(exchange)\n        return new_id\n    except Exception as e:\n        print(f\"DB Error: {e}\")\n        return None"
            start_line: 169
            end_line: 196
            context:
              calls[0]:
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: database.db.fetch_exchanges_by_user
            name: fetch_exchanges_by_user
            args[1]: username
            docstring: null
            source_code: "def fetch_exchanges_by_user(username: str):\n    # Sortieren nach Zeitstempel wichtig f√ºr die Anzeige\n    exchanges = list(dbexchanges.find({\"username\": username}).sort(\"created_at\", 1))\n    return exchanges"
            start_line: 198
            end_line: 201
            context:
              calls[0]:
              called_by[1]: frontend.frontend.load_data_from_db
          - mode: function_analysis
            identifier: database.db.fetch_exchanges_by_chat
            name: fetch_exchanges_by_chat
            args[2]: username,chat_name
            docstring: null
            source_code: "def fetch_exchanges_by_chat(username: str, chat_name: str):\n    exchanges = list(dbexchanges.find({\"username\": username, \"chat_name\": chat_name}).sort(\"created_at\", 1))\n    return exchanges"
            start_line: 203
            end_line: 205
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_exchange_feedback
            name: update_exchange_feedback
            args[2]: exchange_id,feedback
            docstring: null
            source_code: "def update_exchange_feedback(exchange_id, feedback: int):\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback\": feedback}})\n    return result.modified_count"
            start_line: 207
            end_line: 209
            context:
              calls[0]:
              called_by[1]: frontend.frontend.handle_feedback_change
          - mode: function_analysis
            identifier: database.db.update_exchange_feedback_message
            name: update_exchange_feedback_message
            args[2]: exchange_id,feedback_message
            docstring: null
            source_code: "def update_exchange_feedback_message(exchange_id, feedback_message: str):\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback_message\": feedback_message}})\n    return result.modified_count"
            start_line: 211
            end_line: 213
            context:
              calls[0]:
              called_by[1]: frontend.frontend.render_exchange
          - mode: function_analysis
            identifier: database.db.delete_exchange_by_id
            name: delete_exchange_by_id
            args[1]: exchange_id
            docstring: null
            source_code: "def delete_exchange_by_id(exchange_id: str):\n    result = dbexchanges.delete_one({\"_id\": exchange_id})\n    return result.deleted_count"
            start_line: 215
            end_line: 217
            context:
              calls[0]:
              called_by[1]: frontend.frontend.handle_delete_exchange
          - mode: function_analysis
            identifier: database.db.delete_full_chat
            name: delete_full_chat
            args[2]: username,chat_name
            docstring: "L√∂scht den Chat UND alle zugeh√∂rigen Exchanges.\nDas sorgt f√ºr Konsistenz zwischen Frontend und Backend."
            source_code: "def delete_full_chat(username: str, chat_name: str):\n    \"\"\"\n    L√∂scht den Chat UND alle zugeh√∂rigen Exchanges.\n    Das sorgt f√ºr Konsistenz zwischen Frontend und Backend.\n    \"\"\"\n    # 1. Alle Nachrichten in diesem Chat l√∂schen\n    del_exchanges = dbexchanges.delete_many({\"username\": username, \"chat_name\": chat_name})\n    \n    # 2. Den Chat selbst aus der Chat-Liste l√∂schen\n    del_chat = dbchats.delete_one({\"username\": username, \"chat_name\": chat_name})\n    \n    return del_chat.deleted_count"
            start_line: 221
            end_line: 232
            context:
              calls[0]:
              called_by[1]: frontend.frontend.handle_delete_chat
        classes[0]:
    "frontend/frontend.py":
      ast_nodes:
        imports[16]: numpy,datetime.datetime,time,pymongo.MongoClient,dotenv.load_dotenv,os,sys,urllib.parse.urlparse,logging,traceback,re,streamlit_mermaid.st_mermaid,backend.main,database.db,streamlit,streamlit_authenticator
        functions[12]:
          - mode: function_analysis
            identifier: frontend.frontend.clean_names
            name: clean_names
            args[1]: model_list
            docstring: null
            source_code: "def clean_names(model_list):\n    return [m.split(\"/\")[-1] for m in model_list]"
            start_line: 54
            end_line: 55
            context:
              calls[0]:
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: frontend.frontend.get_filtered_models
            name: get_filtered_models
            args[2]: source_list,category_name
            docstring: Filtert eine Liste basierend auf der gew√§hlten Kategorie.
            source_code: "def get_filtered_models(source_list, category_name):\n    \"\"\"Filtert eine Liste basierend auf der gew√§hlten Kategorie.\"\"\"\n    keywords = CATEGORY_KEYWORDS.get(category_name, [\"\"])\n    \n    if \"STANDARD\" in keywords:\n        # Nur Modelle zur√ºckgeben, die auch in der Standard-Liste sind\n        return [m for m in source_list if m in STANDARD_MODELS]\n    \n    filtered = []\n    for model in source_list:\n        # Pr√ºfen ob ein Keyword im Namen steckt\n        if any(k in model.lower() for k in keywords):\n            filtered.append(model)\n            \n    return filtered if filtered else source_list"
            start_line: 86
            end_line: 100
            context:
              calls[0]:
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: frontend.frontend.save_gemini_cb
            name: save_gemini_cb
            args[0]:
            docstring: null
            source_code: "def save_gemini_cb():\n    new_key = st.session_state.get(\"in_gemini_key\", \"\")\n    if new_key:\n        db.update_gemini_key(st.session_state[\"username\"], new_key)\n        st.session_state[\"in_gemini_key\"] = \"\"\n        st.toast(\"Gemini Key erfolgreich gespeichert! ‚úÖ\")"
            start_line: 143
            end_line: 148
            context:
              calls[1]: database.db.update_gemini_key
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.save_ollama_cb
            name: save_ollama_cb
            args[0]:
            docstring: null
            source_code: "def save_ollama_cb():\n    new_url = st.session_state.get(\"in_ollama_url\", \"\")\n    if new_url:\n        db.update_ollama_url(st.session_state[\"username\"], new_url)\n        st.toast(\"Ollama URL gespeichert! ‚úÖ\")"
            start_line: 150
            end_line: 154
            context:
              calls[1]: database.db.update_ollama_url
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.load_data_from_db
            name: load_data_from_db
            args[1]: username
            docstring: L√§dt Chats und Exchanges konsistent aus der DB.
            source_code: "def load_data_from_db(username: str):\n    \"\"\"L√§dt Chats und Exchanges konsistent aus der DB.\"\"\"\n    \n    # Nur laden, wenn neuer User oder noch nicht geladen\n    if \"loaded_user\" not in st.session_state or st.session_state.loaded_user != username:\n        st.session_state.chats = {}\n        \n        # 1. Erst die definierten Chats laden (damit auch leere Chats da sind)\n        db_chats = db.fetch_chats_by_user(username)\n        for c in db_chats:\n            c_name = c.get(\"chat_name\")\n            if c_name:\n                st.session_state.chats[c_name] = {\"exchanges\": []}\n\n        # 2. Dann die Exchanges laden und einsortieren\n        db_exchanges = db.fetch_exchanges_by_user(username)\n        for ex in db_exchanges:\n            c_name = ex.get(\"chat_name\", \"Unbenannt\")\n            \n            # Falls Exchanges existieren f√ºr Chats, die nicht in dbchats sind (Legacy Support)\n            if c_name not in st.session_state.chats:\n                st.session_state.chats[c_name] = {\"exchanges\": []}\n            \n            if \"feedback\" not in ex or ex[\"feedback\"] is None:\n                ex[\"feedback\"] = np.nan\n            \n            st.session_state.chats[c_name][\"exchanges\"].append(ex)\n\n        # 3. Default Chat erstellen, falls gar nichts existiert\n        if not st.session_state.chats:\n            initial_name = \"Chat 1\"\n            # Konsistent in DB anlegen\n            db.insert_chat(username, initial_name)\n            st.session_state.chats[initial_name] = {\"exchanges\": []}\n            st.session_state.active_chat = initial_name\n        else:\n            # Active Chat setzen, falls n√∂tig\n            if \"active_chat\" not in st.session_state or st.session_state.active_chat not in st.session_state.chats:\n                # Nimm den ersten verf√ºgbaren Chat\n                st.session_state.active_chat = list(st.session_state.chats.keys())[0]\n        \n        st.session_state.loaded_user = username"
            start_line: 160
            end_line: 201
            context:
              calls[3]: database.db.fetch_chats_by_user,database.db.fetch_exchanges_by_user,database.db.insert_chat
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: frontend.frontend.handle_feedback_change
            name: handle_feedback_change
            args[2]: ex,val
            docstring: null
            source_code: "def handle_feedback_change(ex, val):\n    ex[\"feedback\"] = val\n    db.update_exchange_feedback(ex[\"_id\"], val)\n    st.rerun()"
            start_line: 207
            end_line: 210
            context:
              calls[1]: database.db.update_exchange_feedback
              called_by[1]: frontend.frontend.render_exchange
          - mode: function_analysis
            identifier: frontend.frontend.handle_delete_exchange
            name: handle_delete_exchange
            args[2]: chat_name,ex
            docstring: null
            source_code: "def handle_delete_exchange(chat_name, ex):\n    db.delete_exchange_by_id(ex[\"_id\"])\n    if chat_name in st.session_state.chats:\n        if ex in st.session_state.chats[chat_name][\"exchanges\"]:\n            st.session_state.chats[chat_name][\"exchanges\"].remove(ex)\n    st.rerun()"
            start_line: 212
            end_line: 217
            context:
              calls[1]: database.db.delete_exchange_by_id
              called_by[1]: frontend.frontend.render_exchange
          - mode: function_analysis
            identifier: frontend.frontend.handle_delete_chat
            name: handle_delete_chat
            args[2]: username,chat_name
            docstring: null
            source_code: "def handle_delete_chat(username, chat_name):\n    # KONSISTENZ: Ruft jetzt delete_full_chat in DB auf\n    db.delete_full_chat(username, chat_name)\n    \n    # State bereinigen\n    if chat_name in st.session_state.chats:\n        del st.session_state.chats[chat_name]\n    \n    # Active Chat neu setzen\n    if len(st.session_state.chats) > 0:\n        st.session_state.active_chat = list(st.session_state.chats.keys())[0]\n    else:\n        # Wenn alles weg ist, neuen leeren Chat anlegen\n        new_name = \"Chat 1\"\n        db.insert_chat(username, new_name)\n        st.session_state.chats[new_name] = {\"exchanges\": []}\n        st.session_state.active_chat = new_name\n        \n    st.rerun()"
            start_line: 219
            end_line: 237
            context:
              calls[2]: database.db.delete_full_chat,database.db.insert_chat
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: frontend.frontend.extract_repo_name
            name: extract_repo_name
            args[1]: text
            docstring: null
            source_code: "def extract_repo_name(text):\n    url_match = re.search(r'(https?://[^\\s]+)', text)\n    if url_match:\n        url = url_match.group(0)\n        parsed = urlparse(url)\n        path = parsed.path.strip(\"/\")\n        if path:\n            repo_name = path.split(\"/\")[-1]\n            if repo_name.endswith(\".git\"):\n                repo_name = repo_name[:-4]\n            return repo_name\n    return None"
            start_line: 243
            end_line: 254
            context:
              calls[0]:
              called_by[1]: frontend.frontend
          - mode: function_analysis
            identifier: frontend.frontend.stream_text_generator
            name: stream_text_generator
            args[1]: text
            docstring: null
            source_code: "def stream_text_generator(text):\n    for word in text.split(\" \"):\n        yield word + \" \"\n        time.sleep(0.01)"
            start_line: 256
            end_line: 259
            context:
              calls[0]:
              called_by[1]: frontend.frontend.render_text_with_mermaid
          - mode: function_analysis
            identifier: frontend.frontend.render_text_with_mermaid
            name: render_text_with_mermaid
            args[2]: markdown_text,should_stream
            docstring: null
            source_code: "def render_text_with_mermaid(markdown_text, should_stream=False):\n    if not markdown_text:\n        return\n\n    parts = re.split(r\"```mermaid\\s+(.*?)\\s+```\", markdown_text, flags=re.DOTALL)\n\n    for i, part in enumerate(parts):\n        if i % 2 == 0:\n            if part.strip():\n                if should_stream:\n                    st.write_stream(stream_text_generator(part))\n                else:\n                    st.markdown(part)\n        else:\n            try:\n                st_mermaid(part, key=f\"mermaid_{hash(part)}_{i}\")\n            except Exception:\n                st.code(part, language=\"mermaid\")"
            start_line: 261
            end_line: 278
            context:
              calls[1]: frontend.frontend.stream_text_generator
              called_by[2]: frontend.frontend,frontend.frontend.render_exchange
          - mode: function_analysis
            identifier: frontend.frontend.render_exchange
            name: render_exchange
            args[2]: ex,current_chat_name
            docstring: null
            source_code: "def render_exchange(ex, current_chat_name):\n    st.chat_message(\"user\").write(ex[\"question\"])\n\n    with st.chat_message(\"assistant\"):\n        answer_text = ex.get(\"answer\", \"\")\n        is_error = answer_text.startswith(\"Fehler\") or answer_text.startswith(\"Error\")\n\n        # --- 1. TOOLBAR CONTAINER ---\n        # Nutzt die neuen Parameter horizontal=True und alignment\n        # border=True rahmt die Toolbar ein\n        # horizontal_alignment=\"right\" schiebt alles nach rechts\n        with st.container(horizontal=True, horizontal_alignment=\"right\"):\n            \n            if not is_error:\n                # 1. Status Text (wird jetzt links von den Buttons angezeigt, aber rechtsb√ºndig im Container)\n                st.write(\"hier die Antwort:\")\n                if ex.get(\"feedback\") == 1:\n                    st.caption(\"‚úÖ Hilfreich\")\n                elif ex.get(\"feedback\") == 0:\n                    st.caption(\"‚ùå Nicht hilfreich\")\n                \n                # 2. Die Buttons (einfach untereinander im Code, erscheinen nebeneinander im UI)\n                \n                # Like\n                type_primary_up = ex.get(\"feedback\") == 1\n                if st.button(\"üëç\", key=f\"up_{ex['_id']}\", type=\"primary\" if type_primary_up else \"secondary\", help=\"Hilfreich\"):\n                    handle_feedback_change(ex, 1)\n\n                # Dislike\n                type_primary_down = ex.get(\"feedback\") == 0\n                if st.button(\"üëé\", key=f\"down_{ex['_id']}\", type=\"primary\" if type_primary_down else \"secondary\", help=\"Nicht hilfreich\"):\n                    handle_feedback_change(ex, 0)\n\n                # Comment Popover\n                with st.popover(\"üí¨\", help=\"Notiz hinzuf√ºgen\"):\n                    msg = st.text_area(\"Notiz:\", value=ex.get(\"feedback_message\", \"\"), key=f\"txt_{ex['_id']}\")\n                    if st.button(\"Speichern\", key=f\"save_{ex['_id']}\"):\n                        ex[\"feedback_message\"] = msg\n                        db.update_exchange_feedback_message(ex[\"_id\"], msg)\n                        st.success(\"Gespeichert!\")\n                        time.sleep(0.5)\n                        st.rerun()\n\n                # Download\n                st.download_button(\n                    label=\"üì•\",\n                    data=ex[\"answer\"],\n                    file_name=f\"response_{ex['_id']}.md\",\n                    mime=\"text/markdown\",\n                    key=f\"dl_{ex['_id']}\",\n                    help=\"Als Markdown herunterladen\"\n                )\n\n                # Delete\n                if st.button(\"üóëÔ∏è\", key=f\"del_{ex['_id']}\", help=\"Nachricht l√∂schen\", type=\"secondary\"):\n                    handle_delete_exchange(current_chat_name, ex)\n\n            else:\n                # Fehlerfall\n                st.error(\"‚ö†Ô∏è Fehler\")\n                if st.button(\"üóëÔ∏è L√∂schen\", key=f\"del_err_{ex['_id']}\"):\n                    handle_delete_exchange(current_chat_name, ex)\n\n        # --- 2. CONTENT (Antwort) ---\n        with st.container(height=500, border=True):\n             render_text_with_mermaid(ex[\"answer\"], should_stream=False)"
            start_line: 284
            end_line: 349
            context:
              calls[4]: database.db.update_exchange_feedback_message,frontend.frontend.handle_delete_exchange,frontend.frontend.handle_feedback_change,frontend.frontend.render_text_with_mermaid
              called_by[1]: frontend.frontend
        classes[0]:
    "schemas/types.py":
      ast_nodes:
        imports[5]: typing.List,typing.Optional,typing.Literal,pydantic.BaseModel,pydantic.ValidationError
        functions[0]:
        classes[15]:
          - mode: class_analysis
            identifier: schemas.types.ParameterDescription
            name: ParameterDescription
            docstring: Describes a single parameter of a function.
            source_code: "class ParameterDescription(BaseModel):\n    \"\"\"Describes a single parameter of a function.\"\"\"\n    name: str\n    type: str\n    description: str"
            start_line: 6
            end_line: 10
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ReturnDescription
            name: ReturnDescription
            docstring: Describes the return value of a function.
            source_code: "class ReturnDescription(BaseModel):\n    \"\"\"Describes the return value of a function.\"\"\"\n    name: str\n    type: str\n    description: str"
            start_line: 12
            end_line: 16
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.UsageContext
            name: UsageContext
            docstring: Describes the calling context of a function.
            source_code: "class UsageContext(BaseModel):\n    \"\"\"Describes the calling context of a function.\"\"\"\n    calls: str\n    called_by: str"
            start_line: 18
            end_line: 21
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionDescription
            name: FunctionDescription
            docstring: Contains the detailed analysis of a function's purpose and signature.
            source_code: "class FunctionDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a function's purpose and signature.\"\"\"\n    overall: str\n    parameters: List[ParameterDescription]\n    returns: List[ReturnDescription]\n    usage_context: UsageContext"
            start_line: 23
            end_line: 28
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionAnalysis
            name: FunctionAnalysis
            docstring: The main model representing the entire JSON schema for a function.
            source_code: "class FunctionAnalysis(BaseModel):\n    \"\"\"The main model representing the entire JSON schema for a function.\"\"\"\n    identifier: str\n    description: FunctionDescription\n    error: Optional[str] = None"
            start_line: 30
            end_line: 34
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ConstructorDescription
            name: ConstructorDescription
            docstring: Describes the __init__ method of a class.
            source_code: "class ConstructorDescription(BaseModel):\n    \"\"\"Describes the __init__ method of a class.\"\"\"\n    description: str\n    parameters: List[ParameterDescription]"
            start_line: 39
            end_line: 42
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassContext
            name: ClassContext
            docstring: Describes the class's external dependencies and primary points of instantiation.
            source_code: "class ClassContext(BaseModel):\n    \"\"\"Describes the class's external dependencies and primary points of instantiation.\"\"\"\n    dependencies: str\n    instantiated_by: str"
            start_line: 44
            end_line: 47
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassDescription
            name: ClassDescription
            docstring: "Contains the detailed analysis of a class's purpose, constructor, and methods."
            source_code: "class ClassDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a class's purpose, constructor, and methods.\"\"\"\n    overall: str\n    init_method: ConstructorDescription\n    methods: List[FunctionAnalysis]\n    usage_context: ClassContext"
            start_line: 49
            end_line: 54
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassAnalysis
            name: ClassAnalysis
            docstring: The main model for the entire JSON schema for a class.
            source_code: "class ClassAnalysis(BaseModel):\n    \"\"\"The main model for the entire JSON schema for a class.\"\"\"\n    identifier: str\n    description: ClassDescription\n    error: Optional[str] = None"
            start_line: 56
            end_line: 60
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.CallInfo
            name: CallInfo
            docstring: "Represents a specific call event from the relationship analyzer.\nUsed in 'called_by' and 'instantiated_by' lists."
            source_code: "class CallInfo(BaseModel):\n    \"\"\"\n    Represents a specific call event from the relationship analyzer.\n    Used in 'called_by' and 'instantiated_by' lists.\n    \"\"\"\n    file: str\n    function: str  # Name des Aufrufers\n    mode: str      # z.B. 'method', 'function', 'module'\n    line: int"
            start_line: 65
            end_line: 73
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionContextInput
            name: FunctionContextInput
            docstring: Structured context for analyzing a function.
            source_code: "class FunctionContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a function.\"\"\"\n    calls: List[str]\n    called_by: List[CallInfo]"
            start_line: 78
            end_line: 81
            context:
              dependencies[0]:
              instantiated_by[1]: backend.main.main_workflow
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionAnalysisInput
            name: FunctionAnalysisInput
            docstring: The required input to generate a FunctionAnalysis object.
            source_code: "class FunctionAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a FunctionAnalysis object.\"\"\"\n    mode: Literal[\"function_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: FunctionContextInput"
            start_line: 83
            end_line: 89
            context:
              dependencies[0]:
              instantiated_by[1]: backend.main.main_workflow
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.MethodContextInput
            name: MethodContextInput
            docstring: Structured context for a classes methods
            source_code: "class MethodContextInput(BaseModel):\n    \"\"\"Structured context for a classes methods\"\"\"\n    identifier: str\n    calls: List[str]\n    called_by: List[CallInfo]\n    args: List[str]\n    docstring: Optional[str]"
            start_line: 94
            end_line: 100
            context:
              dependencies[0]:
              instantiated_by[1]: backend.main.main_workflow
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassContextInput
            name: ClassContextInput
            docstring: Structured context for analyzing a class.
            source_code: "class ClassContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a class.\"\"\"\n    dependencies: List[str]\n    instantiated_by: List[CallInfo]\n    method_context: List[MethodContextInput]"
            start_line: 102
            end_line: 106
            context:
              dependencies[0]:
              instantiated_by[2]: backend.HelperLLM.main_orchestrator,backend.main.main_workflow
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassAnalysisInput
            name: ClassAnalysisInput
            docstring: The required input to generate a ClassAnalysis object.
            source_code: "class ClassAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a ClassAnalysis object.\"\"\"\n    mode: Literal[\"class_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: ClassContextInput"
            start_line: 108
            end_line: 114
            context:
              dependencies[0]:
              instantiated_by[2]: backend.HelperLLM.main_orchestrator,backend.main.main_workflow
              method_context[0]:
analysis_results:
  functions:
    backend.AST_Schema.path_to_module:
      identifier: backend.AST_Schema.path_to_module
      description:
        overall: "This function converts a given file path into its corresponding Python module path. It first determines the relative path of the file with respect to a specified project root. If the file is a Python file, it removes the '.py' extension. Subsequently, it replaces path separators with dots to form the module path. Special handling is included for '__init__.py' files, where the '.__init__' suffix is removed to represent the package itself."
        parameters[2]{name,type,description}:
          filepath,str,The absolute or relative path to the Python file.
          project_root,str,"The root directory of the project, used to calculate the relative path."
        returns[1]{name,type,description}:
          module_path,str,The converted Python module path string.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.File_Dependency.build_file_dependency_graph:
      identifier: backend.File_Dependency.build_file_dependency_graph
      description:
        overall: "This function constructs a directed graph representing file-level import dependencies within a given Abstract Syntax Tree (AST). It initializes a NetworkX directed graph and uses a custom `FileDependencyGraph` visitor to traverse the AST and identify import relationships. The visitor populates an internal dictionary of import dependencies. The function then iterates through these identified dependencies, adding nodes for both importing and imported files, and creating directed edges from the importer to the imported files. The resulting graph illustrates which files depend on others based on their import statements."
        parameters[3]{name,type,description}:
          filename,str,The path to the file being analyzed for dependencies.
          tree,AST,The Abstract Syntax Tree (AST) of the file to be analyzed.
          repo_root,str,"The root directory of the repository, used for resolving relative import paths."
        returns[1]{name,type,description}:
          graph,networkx.DiGraph,A directed graph where nodes represent files and edges represent import dependencies (from importer to imported).
        usage_context:
          calls: This function calls backend.File_Dependency.FileDependencyGraph.
          called_by: This function is called by no other functions.
      error: null
    backend.File_Dependency.build_repository_graph:
      identifier: backend.File_Dependency.build_repository_graph
      description:
        overall: "This function constructs a directed graph representing the dependencies between Python files within a given Git repository. It first retrieves all files from the repository and filters for Python files. For each Python file, it parses its content to build a file-specific dependency graph using an external helper function. Finally, it aggregates all these individual file graphs into a single global directed graph, which is then returned."
        parameters[1]{name,type,description}:
          repository,GitRepository,The Git repository object from which to build the dependency graph.
        returns[1]{name,type,description}:
          global_graph,nx.DiGraph,A NetworkX directed graph representing the dependencies between Python files across the entire repository.
        usage_context:
          calls: This function calls backend.File_Dependency.build_file_dependency_graph.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.File_Dependency.get_all_temp_files:
      identifier: backend.File_Dependency.get_all_temp_files
      description:
        overall: "This function, `get_all_temp_files`, is designed to locate all Python files within a specified directory and its subdirectories. It takes a directory path as input and returns a list of `Path` objects. The function first converts the input directory string into an absolute and canonical `Path` object. It then recursively searches for all files ending with \".py\" within this root path. Finally, it returns these found file paths as a list, with each path made relative to the initial root directory."
        parameters[1]{name,type,description}:
          directory,str,The path to the root directory to search for Python files.
        returns[1]{name,type,description}:
          all_files,"list[Path]","A list of `pathlib.Path` objects, where each path represents a Python file found within the specified directory, relative to the root directory."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    backend.HelperLLM.main_orchestrator:
      identifier: backend.HelperLLM.main_orchestrator
      description:
        overall: "This function serves as a dummy data and processing loop, primarily for testing the LLMHelper class. It defines pre-computed analysis inputs and outputs for several example functions, such as 'add_item', 'check_stock', and 'generate_report', using Pydantic models. It then instantiates an LLMHelper and simulates generating documentation for these functions, logging the process and displaying the final aggregated results. The function demonstrates how to use the `FunctionAnalysisInput` and `FunctionAnalysis` models."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: "This function calls backend.HelperLLM.LLMHelper, schemas.types.ClassAnalysisInput, and schemas.types.ClassContextInput."
          called_by: This function is called by no other functions.
      error: null
    backend.callgraph.make_safe_dot:
      identifier: backend.callgraph.make_safe_dot
      description:
        overall: "This function takes a NetworkX directed graph and a file path, then generates a DOT file representation of the graph. It first creates a copy of the input graph. It then relabels all nodes in the copied graph with simple, safe identifiers (e.g., \"n0\", \"n1\") to ensure compatibility with DOT format. The original node names are preserved by adding them as a 'label' attribute to the newly relabeled nodes before writing the graph to the specified output path."
        parameters[2]{name,type,description}:
          graph,nx.DiGraph,The NetworkX directed graph to be converted into a DOT file.
          out_path,str,The file path where the generated DOT graph will be saved.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.callgraph.build_filtered_callgraph:
      identifier: backend.callgraph.build_filtered_callgraph
      description:
        overall: "This function constructs a filtered call graph for a given Git repository. It begins by iterating through all Python files within the repository, parsing their Abstract Syntax Trees (ASTs) to identify and collect a set of 'own functions' defined within the project. Subsequently, it initializes a `networkx.DiGraph` and re-processes the parsed ASTs. During this second pass, it detects caller-callee relationships and adds an edge to the graph only if both the calling and called functions are part of the previously identified 'own functions' set. The function ultimately returns this directed graph, which represents the internal call structure exclusively among the project's own codebase."
        parameters[1]{name,type,description}:
          repo,GitRepository,The Git repository object from which to extract Python files and build the call graph.
        returns[1]{name,type,description}:
          global_graph,nx.DiGraph,"A directed graph representing the filtered call relationships between functions defined within the repository, excluding external calls."
        usage_context:
          calls: This function calls backend.callgraph.CallGraph.
          called_by: This function is called by no other functions.
      error: null
    backend.converter.wrap_cdata:
      identifier: backend.converter.wrap_cdata
      description:
        overall: "The `wrap_cdata` function is designed to encapsulate a given string `content` within XML CDATA tags. It constructs a new string by prepending \"<![CDATA[\\n\" and appending \"\\n]]>\" to the input content. This ensures that the enclosed content is treated as character data, preventing XML parsers from interpreting it as markup. The function directly returns this newly formatted string."
        parameters[1]{name,type,description}:
          content,str,The string content to be wrapped within CDATA tags.
        returns[1]{name,type,description}:
          wrapped_content,str,A new string containing the original content enclosed within CDATA tags.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.extract_output_content:
      identifier: backend.converter.extract_output_content
      description:
        overall: "This function processes a list of output objects, typically from a notebook execution, to extract their content. It handles various output types, including display data (like images and text), stream outputs, and error messages. For images, it prioritizes PNG over JPEG, encodes them as Base64 strings, stores them in a provided list, and inserts an XML-like placeholder into the output. Text content is extracted directly, and errors are formatted into a string. The function returns a list of these extracted text snippets or image placeholders."
        parameters[2]{name,type,description}:
          outputs,list,"A list of output objects, each potentially containing different types of data such as display data, stream text, or error information. Each object is expected to have attributes like 'output_type', 'data', 'text', 'ename', and 'evalue'."
          image_list,list,A list that is modified in-place to store dictionaries of image data (mime_type and Base64 string). This list accumulates all images processed by the function.
        returns[1]{name,type,description}:
          extracted_xml_snippets,"list[str]","A list of strings, where each string is either extracted text content, a formatted error message, or an XML-like placeholder for an image that was processed and added to the 'image_list'."
        usage_context:
          calls: This function calls backend.converter.process_image.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.process_image:
      identifier: backend.converter.process_image
      description:
        overall: "This function, `process_image`, is designed to handle image data based on a given MIME type. It expects to find the image's base64 encoded string within an external `data` dictionary, using the `mime_type` as a key. Upon successful retrieval, it cleans the base64 string and appends a dictionary containing the `mime_type` and the cleaned data to an external `image_list`. The function then returns a unique placeholder string that includes the image's assigned index and its MIME type. If the `mime_type` is not found in `data`, it returns `None`; if any error occurs during processing, it returns an error message string."
        parameters[1]{name,type,description}:
          mime_type,str,"The MIME type of the image to be processed, which serves as a key to retrieve the corresponding base64 encoded image data from an external `data` dictionary."
        returns[3]{name,type,description}:
          image_placeholder_tag,str,"A formatted string representing an image placeholder, containing the image's index in `image_list` and its MIME type, if processing is successful."
          error_message,str,"An error message string, prefixed with \"<ERROR>\", if an exception occurs during the image data processing."
          no_image_data,NoneType,None if the provided `mime_type` is not found as a key in the external `data` dictionary.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.convert_notebook_to_xml:
      identifier: backend.converter.convert_notebook_to_xml
      description:
        overall: "This function converts the content of a Jupyter notebook, provided as a string, into an XML representation. It attempts to parse the input as a notebook and handles `NotJSONError` by returning an error message. It iterates through each cell, converting markdown cells to XML markdown tags and code cells to XML code tags. If code cells have outputs, it processes them to extract content and images, then appends them as XML output tags. Finally, it returns the concatenated XML string and a list of any extracted images."
        parameters[1]{name,type,description}:
          file_content,str,"The raw content of a Jupyter notebook file, expected to be a JSON string."
        returns[2]{name,type,description}:
          xml_representation,str,"A string containing the XML representation of the notebook, or an error message if parsing fails."
          extracted_images,list,A list of images extracted from the notebook's output cells.
        usage_context:
          calls: This function calls backend.converter.extract_output_content and backend.converter.wrap_cdata.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.process_repo_notebooks:
      identifier: backend.converter.process_repo_notebooks
      description:
        overall: "This function processes a collection of repository files, identifying and converting Jupyter notebooks. It filters the input `repo_files` to select only those with a '.ipynb' extension. For each identified notebook, it extracts its content and invokes `convert_notebook_to_xml` to generate XML output and associated image data. The function then aggregates these conversion results into a dictionary, mapping each notebook's file path to its corresponding XML and image data, before returning the complete set of processed information."
        parameters[1]{name,type,description}:
          repo_files,list,"An iterable collection of file objects, where each object is expected to have a 'path' attribute (string) and a 'content' attribute (string or bytes)."
        returns[1]{name,type,description}:
          results,dict,A dictionary where keys are the paths of the processed notebook files (string) and values are dictionaries containing the 'xml' output (string) and 'images' (list or dictionary of image data) generated from each notebook.
        usage_context:
          calls: This function calls backend.converter.convert_notebook_to_xml.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.create_savings_chart:
      identifier: backend.main.create_savings_chart
      description:
        overall: "This function generates a bar chart to visually compare the number of tokens between JSON and TOON formats. It takes the token counts for both formats, a savings percentage, and an output file path as input. The chart displays two bars, one for JSON tokens and one for TOON tokens, with their respective values shown above each bar. The chart is titled with the token comparison and the provided savings percentage, then saved to the specified output path before closing the plot."
        parameters[4]{name,type,description}:
          json_tokens,int,The number of tokens associated with the JSON format.
          toon_tokens,int,The number of tokens associated with the TOON format.
          savings_percent,float,The calculated savings percentage to be displayed in the chart's title.
          output_path,str,The file path where the generated bar chart image will be saved.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.calculate_net_time:
      identifier: backend.main.calculate_net_time
      description:
        overall: "This function calculates the net processing time by subtracting estimated sleep durations, which are incurred due to rate-limiting, from the total elapsed time. It takes start and end times, total items, batch size, and the model name as input. If the model is not a 'gemini-' model, it returns the total duration directly. Otherwise, it computes the number of batches and corresponding sleep time, then subtracts this from the total duration to yield the net time."
        parameters[5]{name,type,description}:
          start_time,float,The starting timestamp or time value of the operation.
          end_time,float,The ending timestamp or time value of the operation.
          total_items,int,The total number of items processed during the operation.
          batch_size,int,The number of items processed in each batch.
          model_name,str,"The name of the model used, which determines if rate-limiting adjustments are applied."
        returns[1]{name,type,description}:
          net_time,float,"The calculated net duration of the operation, adjusted for estimated rate-limiting sleep times, or the total duration if no adjustment is needed."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.main_workflow:
      identifier: backend.main.main_workflow
      description:
        overall: "This function orchestrates a comprehensive workflow for analyzing a software repository. It begins by parsing input to extract API keys, model configurations, and a GitHub repository URL. The repository is then cloned, and its contents are processed to extract basic project information, construct a file tree, perform relationship analysis, and generate an Abstract Syntax Tree (AST) schema. The AST schema is subsequently enriched with the extracted relationship data. Finally, the function prepares and dispatches analysis tasks to a Helper LLM for individual functions and classes, and then to a Main LLM to synthesize a final report, while also calculating token usage metrics."
        parameters[4]{name,type,description}:
          input,str,"The initial user input, expected to contain a GitHub repository URL."
          api_keys,dict,"A dictionary containing various API keys (e.g., 'gemini', 'gpt', 'scadsllm') and base URLs ('scadsllm_base_url', 'ollama') required for LLM interactions."
          model_names,dict,"A dictionary specifying the names of the 'helper' and 'main' LLM models to be used (e.g., 'gpt-5-mini', 'gpt-5.1')."
          status_callback,callable | None,An optional callback function used to provide status updates during the workflow execution.
        returns[1]{name,type,description}:
          result,dict,A dictionary containing the 'report' (the final generated markdown report) and 'metrics' (a dictionary of performance and token usage statistics).
        usage_context:
          calls: "This function calls backend.AST_Schema.ASTAnalyzer, backend.AST_Schema.ASTAnalyzer.analyze_repository, backend.AST_Schema.ASTAnalyzer.merge_relationship_data, backend.HelperLLM.LLMHelper, backend.HelperLLM.LLMHelper.generate_for_classes, backend.HelperLLM.LLMHelper.generate_for_functions, backend.MainLLM.MainLLM, backend.MainLLM.MainLLM.call_llm, backend.basic_info.ProjektInfoExtractor, backend.basic_info.ProjektInfoExtractor.extrahiere_info, backend.getRepo.GitRepository, backend.main.calculate_net_time, backend.main.create_savings_chart, backend.main.update_status, backend.relationship_analyzer.ProjectAnalyzer, backend.relationship_analyzer.ProjectAnalyzer.analyze, backend.relationship_analyzer.ProjectAnalyzer.get_raw_relationships, schemas.types.ClassAnalysisInput, schemas.types.ClassContextInput, schemas.types.FunctionAnalysisInput, schemas.types.FunctionContextInput, and schemas.types.MethodContextInput."
          called_by: This function is called by no other functions.
      error: null
    backend.main.update_status:
      identifier: backend.main.update_status
      description:
        overall: "This function, `update_status`, processes and logs a given status message. It first checks for the existence of a `status_callback` and, if present, invokes it with the provided message. Subsequently, it logs the message using the `logging.info` facility. The primary purpose is to provide a centralized mechanism for status updates that can optionally trigger an external callback and always ensures logging."
        parameters[1]{name,type,description}:
          msg,str,The status message string to be processed and logged.
        returns[1]{name,type,description}:
          None,None,This function does not return any value; it performs side effects by potentially calling a callback and always logging the message.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.notebook_workflow:
      identifier: backend.main.notebook_workflow
      description:
        overall: "This function orchestrates a workflow to analyze Jupyter notebooks within a specified GitHub repository using a Large Language Model (LLM). It begins by extracting a repository URL from the input, cloning the repository, and then processing its notebooks into an XML-like structure with embedded images. The function identifies the appropriate API key and base URL based on the chosen LLM model. It extracts basic project information and then iteratively generates individual reports for each notebook by constructing a specific payload and calling the LLM. Finally, it consolidates these reports, saves the comprehensive analysis to a markdown file, and returns the final report along with execution metrics."
        parameters[4]{name,type,description}:
          input,str,"The input string, which is expected to contain a GitHub repository URL from which notebooks will be processed."
          api_keys,dict,"A dictionary containing various API keys required for different LLM services (e.g., 'gpt', 'gemini', 'scadsllm', 'ollama')."
          model,str,"The identifier for the specific Large Language Model to be used for generating reports (e.g., 'gpt-4', 'gemini-pro')."
          status_callback,callable | None,"An optional callback function that receives status messages during the workflow execution, allowing for real-time updates."
        returns[2]{name,type,description}:
          report,str,A concatenated string containing the final markdown report generated by the LLM for all processed notebooks.
          metrics,dict,"A dictionary providing performance and usage metrics of the workflow, including execution times and LLM model details."
        usage_context:
          calls: "This function calls backend.MainLLM.MainLLM, backend.MainLLM.MainLLM.call_llm, backend.basic_info.ProjektInfoExtractor, backend.basic_info.ProjektInfoExtractor.extrahiere_info, backend.converter.process_repo_notebooks, backend.getRepo.GitRepository, backend.main.gemini_payload, and backend.main.update_status."
          called_by: This function is not called by any other functions.
      error: null
    backend.main.gemini_payload:
      identifier: backend.main.gemini_payload
      description:
        overall: "This function constructs a multimodal payload suitable for the Gemini API. It takes basic project information, a notebook path, XML content of a notebook, and a list of image data. The function first serializes basic information and the notebook path into an introductory JSON string. It then processes the XML content, extracting text segments and replacing image placeholders with base64 encoded image URLs, assembling these into a list of content parts. The final output is a structured list containing text and image objects, ready for a multimodal AI model."
        parameters[4]{name,type,description}:
          basic_info,object,A dictionary or object containing basic project information to be included in the payload context.
          nb_path,str,"The file path of the current notebook, included in the payload context."
          xml_content,str,"The XML string content of the notebook, which may contain image placeholders to be replaced."
          images,list,"A list of image data objects, where each object contains base64 encoded image data and its MIME type, corresponding to the image placeholders in the XML content."
        returns[1]{name,type,description}:
          payload_content,list,"A list of dictionaries, each representing a content part (text or image_url) formatted for a Gemini API multimodal payload."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.relationship_analyzer.path_to_module:
      identifier: backend.relationship_analyzer.path_to_module
      description:
        overall: "This function converts a given file system path into a Python module path string. It first attempts to calculate the path relative to a specified project root. If that fails, it uses the base name of the file. It then removes the '.py' extension if present, replaces path separators with dots, and finally adjusts the path for '__init__.py' files to represent their parent package."
        parameters[2]{name,type,description}:
          filepath,str,The absolute or relative path to a Python file.
          project_root,str,"The root directory of the project, used to calculate the relative path."
        returns[1]{name,type,description}:
          module_path,str,The converted Python module path string.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.encrypt_text:
      identifier: database.db.encrypt_text
      description:
        overall: "This function encrypts a given string using a `cipher_suite` object. It first checks if the input `text` or `cipher_suite` is empty; if either is, it returns the original text without encryption. Otherwise, it strips whitespace from the text, encodes it to bytes, encrypts it using `cipher_suite.encrypt`, and then decodes the resulting bytes back into a string before returning it. This process ensures that sensitive text data can be securely stored or transmitted."
        parameters[1]{name,type,description}:
          text,str,The string value to be encrypted.
        returns[1]{name,type,description}:
          encrypted_text,str,"The encrypted string, or the original string if encryption was skipped due to missing text or cipher_suite."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.decrypt_text:
      identifier: database.db.decrypt_text
      description:
        overall: "This function attempts to decrypt a given text string using a `cipher_suite` object. It first checks if the input `text` or `cipher_suite` is empty, returning the original text if either condition is true. If both are present, it proceeds to strip whitespace from the text, encode it to bytes, decrypt it using `cipher_suite.decrypt`, and then decode the result back into a string. The function includes error handling, returning the original text if any exception occurs during the decryption process."
        parameters[1]{name,type,description}:
          text,str,The string value to be decrypted.
        returns[1]{name,type,description}:
          decrypted_or_original_text,str,"The decrypted string if successful, or the original string if decryption is not performed (due to empty input/cipher_suite) or if an error occurs during decryption."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.insert_user:
      identifier: database.db.insert_user
      description:
        overall: "This function is responsible for creating a new user entry in the database. It takes a username, a user's name, and a plaintext password as input. The password is first hashed using a security utility before being stored. The function constructs a user document with the provided details, including empty fields for various API keys, and then inserts this document into the 'dbusers' collection. Finally, it returns the unique identifier of the newly inserted user document."
        parameters[3]{name,type,description}:
          username,str,"The unique identifier for the user, which will be used as the document's _id."
          name,str,The full name of the user.
          password,str,"The plaintext password provided by the user, which will be hashed before storage."
        returns[1]{name,type,description}:
          inserted_id,Any,The unique identifier (typically a string or ObjectId) of the newly inserted user document.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_all_users:
      identifier: database.db.fetch_all_users
      description:
        overall: "The `fetch_all_users` function retrieves all user records from a database collection. It invokes the `find()` method on the `dbusers` object, which is expected to be a database collection, to obtain a cursor of all documents. The function then converts this cursor into a standard Python list, effectively returning all user data as a collection of documents. This provides a direct method to access the complete set of user information stored."
        parameters[0]:
        returns[1]{name,type,description}:
          users,list,"A list containing all user documents (e.g., dictionaries) retrieved from the 'dbusers' collection."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_user:
      identifier: database.db.fetch_user
      description:
        overall: The `fetch_user` function is designed to retrieve a single user document from a database collection. It takes a `username` as an argument and uses it to query the `dbusers` collection. The function specifically searches for a document where the `_id` field matches the provided username. It then returns the first matching document found.
        parameters[1]{name,type,description}:
          username,str,The unique identifier (username) used to locate a specific user document in the database.
        returns[1]{name,type,description}:
          user_document,dict | None,"A dictionary representing the user document if a match is found, otherwise None if no user with the given username exists."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.update_user_name:
      identifier: database.db.update_user_name
      description:
        overall: This function updates the 'name' field for a specific user in the 'dbusers' collection. It identifies the target user document using the provided 'username' as the '_id' field. The function then sets the 'name' field of that document to the 'new_name' value. It returns an integer indicating the number of documents that were modified by this update operation.
        parameters[2]{name,type,description}:
          username,str,"The unique identifier of the user, used to locate the document by its '_id' field."
          new_name,str,The new name to be assigned to the user's 'name' field.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_gemini_key:
      identifier: database.db.update_gemini_key
      description:
        overall: This function updates a user's Gemini API key in a database. It accepts a username and the new API key as input. The provided Gemini API key is first stripped of any leading or trailing whitespace and then encrypted using the `encrypt_text` utility. The function then attempts to locate the user document by their `_id` (username) in the `dbusers` collection and sets the `gemini_api_key` field to the newly encrypted value. It returns the count of documents that were modified by this update operation.
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose Gemini API key needs to be updated.
          gemini_api_key,str,"The new Gemini API key to be stored, which will be encrypted before being saved to the database."
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents that were modified by the update operation. A value of 1 typically indicates a successful update, while 0 suggests the user was not found or no change was necessary."
        usage_context:
          calls: This function calls database.db.encrypt_text.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_gpt_key:
      identifier: database.db.update_gpt_key
      description:
        overall: "This function updates a user's GPT API key in the database. It takes a username and a new GPT API key as input. The provided API key is first stripped of whitespace and then encrypted using the `encrypt_text` function. The function then performs an update operation on the `dbusers` collection, locating the user by their `_id` (username) and setting the `gpt_api_key` field to the newly encrypted value. It returns the count of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose GPT API key is to be updated.
          gpt_api_key,str,"The new GPT API key provided by the user, which will be encrypted before storage."
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents modified by the update operation, typically 0 or 1."
        usage_context:
          calls: This function calls database.db.encrypt_text.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_ollama_url:
      identifier: database.db.update_ollama_url
      description:
        overall: "This function updates the Ollama base URL associated with a specific user in a database. It takes a username and a new Ollama base URL as input. The provided URL is first stripped of any leading or trailing whitespace before being stored. The function then performs an update operation on the database, targeting the user identified by the given username. It returns the count of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          username,str,The unique identifier (username) of the user whose Ollama base URL needs to be updated.
          ollama_base_url,str,The new base URL for the Ollama service. Any leading or trailing whitespace will be removed before storage.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation. A value of 1 typically indicates a successful update for the specified user.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_opensrc_key:
      identifier: database.db.update_opensrc_key
      description:
        overall: "This function is responsible for updating a user's Open Source API key within a database. It takes a username and the new API key as input. The provided API key is first stripped of any leading or trailing whitespace and then encrypted using a helper function. Finally, the function updates the 'opensrc_api_key' field for the specified user in the 'dbusers' collection with the encrypted value."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose Open Source API key needs to be updated.
          opensrc_api_key,str,"The new Open Source API key to be stored for the user, which will be encrypted before storage."
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents that were modified by the update operation. Typically 1 if the user was found and updated, or 0 if no matching user was found."
        usage_context:
          calls: This function calls database.db.encrypt_text.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_opensrc_url:
      identifier: database.db.update_opensrc_url
      description:
        overall: "This function updates the 'opensrc_base_url' for a specific user in a database. It takes a username and a new opensrc base URL as input. The function uses the provided username to locate the user record and then sets the new base URL, ensuring any leading or trailing whitespace is removed from the URL. It returns the count of documents that were modified by this update operation."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose opensrc base URL needs to be updated.
          opensrc_base_url,str,"The new base URL for opensrc, which will be stripped of leading/trailing whitespace before being stored."
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_gemini_key:
      identifier: database.db.fetch_gemini_key
      description:
        overall: "This function is designed to retrieve a user's Gemini API key from a database. It takes a username as input and queries a 'dbusers' collection to find a matching user document. The function specifically projects the 'gemini_api_key' field. If a user is found, it safely extracts and returns the 'gemini_api_key' using the .get() method. If no user document is found for the given username, or if the 'gemini_api_key' field is absent, the function returns None."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose Gemini API key is to be fetched from the database.
        returns[1]{name,type,description}:
          gemini_api_key,str | None,"The Gemini API key string associated with the specified username, or None if the user is not found or the key does not exist in the user's document."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_ollama_url:
      identifier: database.db.fetch_ollama_url
      description:
        overall: "This function retrieves the Ollama base URL associated with a given username from a database. It queries the `dbusers` collection, using the provided username as the document's `_id`. The function specifically projects and fetches the `ollama_base_url` field. If a user document is found, it returns the value of `ollama_base_url`; otherwise, it returns `None`."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose Ollama base URL is to be fetched.
        returns[1]{name,type,description}:
          ollama_base_url,str | None,"The Ollama base URL associated with the user, or `None` if the user is not found in the database."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_gpt_key:
      identifier: database.db.fetch_gpt_key
      description:
        overall: "This function is designed to retrieve a user's GPT API key from a database. It takes a username as input and performs a database query on a 'dbusers' collection, likely a MongoDB collection, to locate the corresponding user document. The query specifically fetches the 'gpt_api_key' field. If a user is found, the function returns the value of their 'gpt_api_key'; otherwise, it returns None."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose GPT API key is to be fetched.
        returns[1]{name,type,description}:
          gpt_api_key,str | None,"The GPT API key associated with the user, or None if the user is not found or does not have a key."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.fetch_opensrc_key:
      identifier: database.db.fetch_opensrc_key
      description:
        overall: "This function, `fetch_opensrc_key`, is designed to retrieve an Open Source API key for a specific user from a database. It queries the `dbusers` collection using the provided username as the document's `_id`. The query is configured to return only the `opensrc_api_key` field, excluding the `_id` itself. If a matching user document is found, the function extracts and returns the `opensrc_api_key` value. If no user is found or the key is absent, the function returns `None`."
        parameters[1]{name,type,description}:
          username,str,The unique identifier (username) of the user whose Open Source API key is to be fetched.
        returns[1]{name,type,description}:
          opensrc_api_key,str | None,"The Open Source API key associated with the specified user, or None if the user is not found or the key does not exist."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_opensrc_url:
      identifier: database.db.fetch_opensrc_url
      description:
        overall: "This function retrieves the 'opensrc_base_url' for a specified user from a database. It queries the 'dbusers' collection, searching for a document where the '_id' field matches the provided username. If a matching user document is found, the function extracts the value of the 'opensrc_base_url' field. The function returns this URL if found, otherwise it returns None."
        parameters[1]{name,type,description}:
          username,str,The unique identifier (username) of the user whose opensrc base URL is to be fetched.
        returns[1]{name,type,description}:
          opensrc_base_url,str | None,"The opensrc base URL associated with the user, or None if the user is not found or the 'opensrc_base_url' field is not present."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.delete_user:
      identifier: database.db.delete_user
      description:
        overall: "This function removes a user record from the database. It takes a username as input, which serves as the unique identifier (`_id`) for the user document. The function executes a delete operation on the `dbusers` collection and returns the count of documents that were successfully deleted."
        parameters[1]{name,type,description}:
          username,str,"The unique identifier of the user to be deleted, corresponding to the `_id` field in the database."
        returns[1]{name,type,description}:
          deleted_count,int,The number of user documents that were successfully deleted from the database. This will typically be 0 or 1.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.get_decrypted_api_keys:
      identifier: database.db.get_decrypted_api_keys
      description:
        overall: "This function retrieves and decrypts various API keys and base URLs associated with a given username from a database. It queries the `dbusers` collection for a user document matching the provided `username`. If no user is found, it returns `None` for all expected values. Otherwise, it extracts specific API keys and URLs, decrypting the `gemini_api_key`, `gpt_api_key`, and `opensrc_api_key` using the `decrypt_text` function. The `ollama_base_url` and `opensrc_base_url` are retrieved without decryption."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose API keys and URLs are to be retrieved.
        returns[5]{name,type,description}:
          gemini_api_key,str | None,"The decrypted Gemini API key, or an empty string if not found, or None if the user does not exist."
          ollama_base_url,str | None,"The Ollama base URL, or an empty string if not found, or None if the user does not exist."
          gpt_api_key,str | None,"The decrypted GPT API key, or an empty string if not found, or None if the user does not exist."
          opensrc_api_key,str | None,"The decrypted Open Source API key, or an empty string if not found, or None if the user does not exist."
          opensrc_base_url,str | None,"The Open Source base URL, or an empty string if not found, or None if the user does not exist."
        usage_context:
          calls: This function calls database.db.decrypt_text.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.insert_chat:
      identifier: database.db.insert_chat
      description:
        overall: "This function creates a new chat entry in the database. It constructs a chat document with a unique identifier generated by `uuid.uuid4()`, the provided username, the chat name, and the current timestamp using `datetime.now()`. The constructed chat document is then inserted into the `dbchats` collection. Finally, it returns the `_id` of the newly inserted chat document."
        parameters[2]{name,type,description}:
          username,str,The username associated with the new chat entry.
          chat_name,str,The name of the chat to be created.
        returns[1]{name,type,description}:
          inserted_id,str,The unique identifier of the newly created chat entry in the database.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.fetch_chats_by_user:
      identifier: database.db.fetch_chats_by_user
      description:
        overall: "This function retrieves all chat records associated with a specific user from a database. It queries a collection, presumably `dbchats`, for documents where the 'username' field matches the provided input. The results are then sorted in ascending order based on their 'created_at' timestamp before being returned as a list."
        parameters[1]{name,type,description}:
          username,str,The username for which to fetch chat records.
        returns[1]{name,type,description}:
          chats,list,"A list of chat records (documents) associated with the specified username, sorted by their creation date."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.check_chat_exists:
      identifier: database.db.check_chat_exists
      description:
        overall: This function checks for the existence of a specific chat within the 'dbchats' collection. It takes a username and a chat name as input and queries the database to see if a document matching both criteria exists. The function returns a boolean value indicating whether such a chat was found.
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to be checked.
          chat_name,str,The name of the chat to check for existence.
        returns[1]{name,type,description}:
          chat_exists,bool,"True if a chat matching the provided username and chat name exists in the database, False otherwise."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.rename_chat_fully:
      identifier: database.db.rename_chat_fully
      description:
        overall: "This function is designed to fully rename a chat entry and all its associated message exchanges within a database. It first updates the chat's name in the `dbchats` collection, changing the `chat_name` from `old_name` to `new_name` for a given `username`. Subsequently, it updates all related message exchanges in the `dbexchanges` collection, ensuring their `chat_name` also reflects the `new_name`. The function returns the count of chat entries modified during the initial update operation."
        parameters[3]{name,type,description}:
          username,str,The username associated with the chat to be renamed.
          old_name,str,The current name of the chat that needs to be changed.
          new_name,str,The new desired name for the chat.
        returns[1]{name,type,description}:
          modified_count,int,The number of chat documents modified in the 'dbchats' collection.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.insert_exchange:
      identifier: database.db.insert_exchange
      description:
        overall: "This function `insert_exchange` is responsible for storing a new chat exchange record into a database collection. It first generates a unique identifier for the exchange using `uuid.uuid4()`. It then compiles an `exchange` dictionary containing the provided question, answer, feedback, user details, chat name, various usage metrics (helper/main used, time, tokens), and a `created_at` timestamp. Finally, it attempts to insert this dictionary into the database. If successful, it returns the newly generated ID; otherwise, it catches any exceptions, prints an error, and returns `None`."
        parameters[13]{name,type,description}:
          question,str,The user's question in the exchange.
          answer,str,The generated answer for the question.
          feedback,str,Feedback provided for the exchange.
          username,str,The username associated with this exchange.
          chat_name,str,The name of the chat session this exchange belongs to.
          helper_used,str,"Indicates which helper was used, if any. Defaults to an empty string."
          main_used,str,"Indicates which main system was used, if any. Defaults to an empty string."
          total_time,str,The total time taken for the exchange. Defaults to an empty string.
          helper_time,str,The time taken by the helper. Defaults to an empty string.
          main_time,str,The time taken by the main system. Defaults to an empty string.
          json_tokens,int,The number of JSON tokens used. Defaults to 0.
          toon_tokens,int,The number of Toon tokens used. Defaults to 0.
          savings_percent,float,The percentage of savings achieved. Defaults to 0.0.
        returns[1]{name,type,description}:
          result,str | None,"The unique identifier (string) of the newly inserted exchange if successful, or None if a database error occurs during insertion."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by other functions in the provided context.
      error: null
    database.db.fetch_exchanges_by_user:
      identifier: database.db.fetch_exchanges_by_user
      description:
        overall: "The `fetch_exchanges_by_user` function is responsible for retrieving exchange records from a database. It queries the `dbexchanges` collection, filtering documents by a specified username. The results are then sorted by their `created_at` timestamp in ascending order, which is noted as important for display. Finally, the function returns the sorted list of exchange records."
        parameters[1]{name,type,description}:
          username,str,The username used to filter the exchange records from the database.
        returns[1]{name,type,description}:
          exchanges,list,"A list of exchange records associated with the given username, sorted by their creation timestamp."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_exchanges_by_chat:
      identifier: database.db.fetch_exchanges_by_chat
      description:
        overall: "This function is designed to retrieve chat exchanges from a database. It queries a collection, likely named 'dbexchanges', using a specified username and chat name as filtering criteria. The retrieved exchanges are then sorted chronologically by their 'created_at' timestamp in ascending order. The function processes the database cursor into a list before returning the results."
        parameters[2]{name,type,description}:
          username,str,The username used to filter the chat exchanges.
          chat_name,str,The name of the chat to filter the exchanges by.
        returns[1]{name,type,description}:
          exchanges,list,"A list of chat exchange documents that match the provided username and chat name, sorted by their creation time."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_exchange_feedback:
      identifier: database.db.update_exchange_feedback
      description:
        overall: "This function is responsible for updating the feedback value associated with a specific exchange record in a database. It accepts an exchange identifier and an integer feedback value. The function performs an update operation on the 'dbexchanges' collection, targeting the document matching the provided 'exchange_id' and setting its 'feedback' field. It then returns the count of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          exchange_id,Any,The unique identifier of the exchange record to be updated. Its specific type is inferred to be compatible with a database document's '_id' field.
          feedback,int,The integer value representing the feedback to be set for the specified exchange.
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents that were modified by the update operation, typically 0 or 1."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_exchange_feedback_message:
      identifier: database.db.update_exchange_feedback_message
      description:
        overall: "This function is responsible for updating the feedback message associated with a specific exchange record in a database. It takes an exchange identifier and a new feedback message string as input. The function uses an `update_one` operation to locate the exchange by its `_id` and then sets the `feedback_message` field to the provided string. Finally, it returns the count of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          exchange_id,Any,The unique identifier of the exchange record to be updated.
          feedback_message,str,The new feedback message string to be stored for the specified exchange.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.delete_exchange_by_id:
      identifier: database.db.delete_exchange_by_id
      description:
        overall: "This function is designed to remove a specific exchange record from a database collection. It accepts a unique identifier, `exchange_id`, to locate the target document. The function interacts with the `dbexchanges` collection, likely a MongoDB collection object, to execute a delete operation based on the provided ID. Upon completion, it returns an integer indicating how many documents were successfully removed. This count will typically be 1 if a matching record was found and deleted, or 0 if no such record existed."
        parameters[1]{name,type,description}:
          exchange_id,str,The unique identifier of the exchange record to be deleted from the database.
        returns[1]{name,type,description}:
          deleted_count,int,The number of documents that were successfully deleted (0 or 1).
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.delete_full_chat:
      identifier: database.db.delete_full_chat
      description:
        overall: "This function is designed to completely remove a specified chat and all its associated message exchanges from the database. It first deletes all message exchanges linked to the given username and chat name. Following this, it proceeds to delete the chat entry itself. This two-step deletion process ensures data consistency by preventing orphaned message records. The function returns the count of chat documents successfully deleted."
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to be deleted.
          chat_name,str,The name of the chat to be deleted.
        returns[1]{name,type,description}:
          deleted_count,int,"The number of chat documents deleted from the database. This will typically be 1 if the chat was found and deleted, or 0 if it was not found."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.clean_names:
      identifier: frontend.frontend.clean_names
      description:
        overall: "This function processes a list of strings, where each string is expected to represent a model name or path. It iterates through the provided list and for each item, it splits the string by the '/' character. The function then extracts the last component of the split string, effectively removing any preceding path information. The result is a new list containing these cleaned, simplified model names."
        parameters[1]{name,type,description}:
          model_list,"List[str]","A list of strings, where each string is expected to be a model identifier or path that may contain '/' separators."
        returns[1]{name,type,description}:
          cleaned_model_names,"List[str]","A new list containing strings, where each string is the last component of the original model name after splitting by '/'."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.get_filtered_models:
      identifier: frontend.frontend.get_filtered_models
      description:
        overall: "This function, `get_filtered_models`, filters a provided list of models based on a specified category name. It retrieves a set of keywords associated with the `category_name` from a global `CATEGORY_KEYWORDS` mapping. If the category's keywords include \"STANDARD\", the function returns only those models from the `source_list` that are also present in a `STANDARD_MODELS` list. Otherwise, it iterates through the `source_list`, checking if any of the category's keywords are contained within each model's lowercase name. The function returns the resulting filtered list, or the original `source_list` if no models matched the filtering criteria."
        parameters[2]{name,type,description}:
          source_list,list,The initial list of models to be filtered.
          category_name,str,The name of the category used to determine filtering keywords.
        returns[1]{name,type,description}:
          filtered_models,list,"A list of models filtered according to the specified category, or the original `source_list` if no models match the filter."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.save_gemini_cb:
      identifier: frontend.frontend.save_gemini_cb
      description:
        overall: "This function serves as a callback to handle the saving of a Gemini API key. It retrieves a potential new Gemini key from the Streamlit session state. If a valid key is found, it proceeds to update this key in the database, associating it with the currently logged-in username. After a successful update, the input field for the Gemini key in the session state is cleared, and a success toast notification is displayed to the user."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: This function calls database.db.update_gemini_key.
          called_by: This function is called by no other functions.
      error: null
    frontend.frontend.save_ollama_cb:
      identifier: frontend.frontend.save_ollama_cb
      description:
        overall: "This function, `save_ollama_cb`, is designed to save a user-provided Ollama URL. It retrieves the potential new URL from the Streamlit session state under the key 'in_ollama_url'. If a valid, non-empty URL is found, the function proceeds to update this URL in the database. The update operation uses the current user's username, also retrieved from the session state, to associate the URL. Upon successful database update, a confirmation toast message is displayed to the user."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: This function calls database.db.update_ollama_url.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.load_data_from_db:
      identifier: frontend.frontend.load_data_from_db
      description:
        overall: "This function `load_data_from_db` is designed to load chat and exchange data for a specific user from the database into the Streamlit session state. It first verifies if the data for the provided username is already loaded to avoid redundant operations. The process involves fetching predefined chats, then loading individual exchanges and associating them with their respective chats, including support for legacy data where exchanges might exist for undefined chats. Finally, it ensures that at least one chat exists, creating a default 'Chat 1' if none are found, and sets an active chat for the user's session state."
        parameters[1]{name,type,description}:
          username,str,The username for whom to load chat and exchange data.
        returns[0]:
        usage_context:
          calls: "This function calls database.db.fetch_chats_by_user, database.db.fetch_exchanges_by_user, and database.db.insert_chat."
          called_by: This function is called by no other functions.
      error: null
    frontend.frontend.handle_feedback_change:
      identifier: frontend.frontend.handle_feedback_change
      description:
        overall: "This function handles changes to feedback for an exchange object. It updates the 'feedback' key of the provided exchange dictionary 'ex' with the new 'val'. Subsequently, it persists this feedback change to the database by calling 'db.update_exchange_feedback' using the exchange's '_id' and the new feedback value. Finally, it triggers a rerun of the Streamlit application to reflect the changes in the UI."
        parameters[2]{name,type,description}:
          ex,dict,"The exchange object, expected to be a dictionary containing at least 'feedback' and '_id' keys."
          val,Any,The new feedback value to be assigned to the exchange object.
        returns[0]:
        usage_context:
          calls: This function calls database.db.update_exchange_feedback.
          called_by: This function is called by no other functions.
      error: null
    frontend.frontend.handle_delete_exchange:
      identifier: frontend.frontend.handle_delete_exchange
      description:
        overall: "This function handles the deletion of a specific exchange. It first removes the exchange from the database using its ID. Subsequently, it updates the Streamlit session state by locating the associated chat and removing the exchange from its list of exchanges. Finally, it triggers a full Streamlit rerun to reflect these changes in the UI."
        parameters[2]{name,type,description}:
          chat_name,str,"The name of the chat to which the exchange belongs, used to locate it within the Streamlit session state."
          ex,dict,"The exchange object to be deleted, which is expected to contain an '_id' key for database deletion and to be present in the chat's exchanges list."
        returns[0]:
        usage_context:
          calls: This function calls database.db.delete_exchange_by_id.
          called_by: This function is called by no other functions.
      error: null
    frontend.frontend.handle_delete_chat:
      identifier: frontend.frontend.handle_delete_chat
      description:
        overall: "This function handles the deletion of a specified chat for a given user. It first calls the database to delete the full chat record. Subsequently, it cleans up the chat from the Streamlit session state. If other chats exist, it sets the first available chat as the active one; otherwise, it creates a new default chat named 'Chat 1' and sets it as active. Finally, it triggers a Streamlit rerun to update the UI."
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to be deleted.
          chat_name,str,The name of the chat to be deleted.
        returns[0]:
        usage_context:
          calls: This function calls database.db.delete_full_chat and database.db.insert_chat.
          called_by: This function is called by no other functions.
      error: null
    frontend.frontend.extract_repo_name:
      identifier: frontend.frontend.extract_repo_name
      description:
        overall: "This function is designed to extract a repository name from a given text string. It first attempts to find a URL within the text using a regular expression. If a URL is successfully matched, it then parses this URL to isolate its path component. The last segment of the path is then considered the potential repository name. A specific check is included to remove a '.git' suffix if present, before returning the cleaned repository name. If no URL is found or a repository name cannot be extracted, the function returns None."
        parameters[1]{name,type,description}:
          text,str,"The input string from which to extract a repository name, potentially containing a URL."
        returns[1]{name,type,description}:
          repo_name,str | None,"The extracted repository name as a string, or None if no repository name could be found or extracted."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    frontend.frontend.stream_text_generator:
      identifier: frontend.frontend.stream_text_generator
      description:
        overall: "This function acts as a generator that streams an input string word by word. It takes a complete text string, splits it into individual words, and then yields each word sequentially, appending a space after it. A small delay is introduced between yielding each word to simulate a streaming effect, often used for displaying text progressively in user interfaces."
        parameters[1]{name,type,description}:
          text,str,The input string that needs to be streamed word by word.
        returns[1]{name,type,description}:
          word,str,"A single word from the input text, followed by a space, yielded sequentially."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    frontend.frontend.render_text_with_mermaid:
      identifier: frontend.frontend.render_text_with_mermaid
      description:
        overall: "This function processes a given markdown text, identifying and rendering embedded Mermaid diagrams. It splits the input text into parts based on '```mermaid' delimiters. Non-Mermaid text is rendered as standard markdown, optionally streamed if 'should_stream' is true. Mermaid diagram code blocks are attempted to be rendered using 'st_mermaid', with a fallback to displaying the code block if rendering fails. The function handles cases where the input markdown text is empty by returning early."
        parameters[2]{name,type,description}:
          markdown_text,str,"The input text, which may contain embedded Mermaid diagram syntax."
          should_stream,bool,A boolean flag indicating whether non-Mermaid text parts should be streamed using 'st.write_stream' or rendered directly with 'st.markdown'. Defaults to False.
        returns[0]:
        usage_context:
          calls: This function calls frontend.frontend.stream_text_generator.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.render_exchange:
      identifier: frontend.frontend.render_exchange
      description:
        overall: "The `render_exchange` function is responsible for displaying a single chat exchange, consisting of a user's question and an assistant's answer, within a Streamlit application. It first renders the user's question. Subsequently, it displays the assistant's response along with an interactive toolbar. This toolbar provides functionalities such as feedback (like/dislike), adding comments, downloading the response, and deleting the exchange. In cases where the answer indicates an error, a simplified error message and a delete option are presented. Finally, the function uses `render_text_with_mermaid` to display the answer content."
        parameters[2]{name,type,description}:
          ex,dict,"A dictionary representing a single chat exchange, containing keys like 'question', 'answer', '_id', 'feedback', and 'feedback_message'."
          current_chat_name,str,"The name of the current chat, used as context when performing operations like deleting an exchange."
        returns[0]:
        usage_context:
          calls: "This function calls database.db.update_exchange_feedback_message, frontend.frontend.handle_delete_exchange, frontend.frontend.handle_feedback_change, and frontend.frontend.render_text_with_mermaid."
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
  classes:
    backend.AST_Schema.ASTVisitor:
      identifier: backend.AST_Schema.ASTVisitor
      description:
        overall: "The ASTVisitor class extends `ast.NodeVisitor` to traverse an Abstract Syntax Tree (AST) of Python source code. Its primary purpose is to extract and structure metadata about imports, classes, and functions found within a given source file. It builds a schema containing lists of imports, functions, and classes, providing a programmatic representation of the code's structure."
        init_method:
          description: "The constructor initializes the ASTVisitor with the source code, file path, and project root. It calculates the module path, sets up an empty schema dictionary to store extracted information, and initializes `_current_class` to `None` for tracking the current class context during AST traversal."
          parameters[3]{name,type,description}:
            source_code,str,The raw source code of the file being analyzed.
            file_path,str,The absolute path to the Python file being visited.
            project_root,str,"The root directory of the project, used to determine the module path."
        methods[5]:
          - identifier: visit_Import
            description:
              overall: "This method processes `ast.Import` nodes, which represent `import module` statements. It iterates through each alias in the import statement, extracting the module name and appending it to the `self.schema[\"imports\"]` list. After recording the import, it calls `self.generic_visit(node)` to ensure that the AST traversal continues for any child nodes."
              parameters[1]{name,type,description}:
                node,ast.Import,The AST node representing an import statement.
              returns[0]:
              usage_context:
                calls: This method calls `self.generic_visit` to continue AST traversal.
                called_by: This method is called by the `ast.NodeVisitor`'s dispatch mechanism when an `ast.Import` node is encountered.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method handles `ast.ImportFrom` nodes, which correspond to `from module import name` statements. It constructs fully qualified import names by combining the module name (if present) with each alias name, then appends these to `self.schema[\"imports\"]`. This ensures that specific imports from modules are correctly captured. Finally, it invokes `self.generic_visit(node)` to continue the AST traversal."
              parameters[1]{name,type,description}:
                node,ast.ImportFrom,The AST node representing a 'from ... import ...' statement.
              returns[0]:
              usage_context:
                calls: This method calls `self.generic_visit` to continue AST traversal.
                called_by: This method is called by the `ast.NodeVisitor`'s dispatch mechanism when an `ast.ImportFrom` node is encountered.
            error: null
          - identifier: visit_ClassDef
            description:
              overall: "This method is responsible for processing `ast.ClassDef` nodes, which represent class definitions. It constructs a dictionary containing detailed information about the class, including its identifier, name, docstring, source code segment, and line numbers. This class information is then added to `self.schema[\"classes\"]`, and the `_current_class` attribute is temporarily set to this class's info to provide context for any nested methods. After visiting child nodes via `self.generic_visit(node)`, `_current_class` is reset to `None`."
              parameters[1]{name,type,description}:
                node,ast.ClassDef,The AST node representing a class definition.
              returns[0]:
              usage_context:
                calls: "This method calls `ast.get_docstring`, `ast.get_source_segment`, and `self.generic_visit`."
                called_by: This method is called by the `ast.NodeVisitor`'s dispatch mechanism when an `ast.ClassDef` node is encountered.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method processes `ast.FunctionDef` nodes, distinguishing between methods defined within a class and top-level functions. If `_current_class` is set, it means the function is a method, and its details (identifier, name, arguments, docstring, line numbers) are appended to the `method_context` of the current class. Otherwise, it's treated as a standalone function, and its details are added to `self.schema[\"functions\"]`. It ensures proper AST traversal by calling `self.generic_visit(node)`."
              parameters[1]{name,type,description}:
                node,ast.FunctionDef,The AST node representing a function definition.
              returns[0]:
              usage_context:
                calls: "This method calls `ast.get_docstring`, `ast.get_source_segment`, and `self.generic_visit`."
                called_by: "This method is called by the `ast.NodeVisitor`'s dispatch mechanism when an `ast.FunctionDef` node is encountered, and also by `visit_AsyncFunctionDef`."
            error: null
          - identifier: visit_AsyncFunctionDef
            description:
              overall: "This method handles `ast.AsyncFunctionDef` nodes, which represent asynchronous function definitions. Instead of implementing its own parsing logic, it delegates the processing directly to the `visit_FunctionDef` method. This approach ensures that both synchronous and asynchronous function definitions are handled uniformly, extracting the same structural and metadata information."
              parameters[1]{name,type,description}:
                node,ast.AsyncFunctionDef,The AST node representing an asynchronous function definition.
              returns[0]:
              usage_context:
                calls: This method calls `self.visit_FunctionDef`.
                called_by: This method is called by the `ast.NodeVisitor`'s dispatch mechanism when an `ast.AsyncFunctionDef` node is encountered.
            error: null
        usage_context:
          dependencies: This class depends on `backend.AST_Schema.path_to_module` for resolving module paths.
          instantiated_by: This class is not explicitly instantiated by any known components within the provided context.
      error: null
    backend.AST_Schema.ASTAnalyzer:
      identifier: backend.AST_Schema.ASTAnalyzer
      description:
        overall: "The ASTAnalyzer class is designed to process source code, particularly Python files within a Git repository, to build a structured Abstract Syntax Tree (AST) schema. It can analyze individual files to extract functions, classes, and their internal structures using an ASTVisitor. Additionally, it provides functionality to merge external relationship data, such as call graphs, into the generated AST schema, enriching the structural information with dynamic interaction details."
        init_method:
          description: "This constructor initializes the ASTAnalyzer class. It does not take any specific parameters beyond 'self' and performs no explicit setup or attribute assignments, effectively creating a stateless instance."
          parameters[0]:
        methods[2]:
          - identifier: merge_relationship_data
            description:
              overall: "This method integrates raw relationship data, specifically incoming and outgoing calls, into a structured full schema. It iterates through files, functions, and classes within the schema, updating their respective context fields with call and called-by information. For classes, it also calculates and stores external dependencies based on method calls."
              parameters[3]{name,type,description}:
                self,ASTAnalyzer,The instance of the class.
                full_schema,dict,"The comprehensive schema containing file, function, and class AST nodes."
                raw_relationships,dict,A dictionary containing raw 'outgoing' and 'incoming' call relationships.
              returns[1]{name,type,description}:
                full_schema,dict,The updated 'full_schema' dictionary with integrated relationship data.
              usage_context:
                calls: This method primarily uses dictionary 'get' methods and iterates over data structures. It does not call other distinct functions or classes.
                called_by: This method is not explicitly called by any other functions or methods in the provided context.
            error: null
          - identifier: analyze_repository
            description:
              overall: "This method processes a list of file objects from a Git repository to construct a full AST schema. It filters for Python files, reads their content, and uses an ASTVisitor to parse the Abstract Syntax Tree, extracting structural information. The method handles potential parsing errors and populates a 'full_schema' dictionary with the AST nodes for each successfully processed file."
              parameters[3]{name,type,description}:
                self,ASTAnalyzer,The instance of the class.
                files,list,"A list of file objects, each expected to have 'path' and 'content' attributes."
                repo,GitRepository,"An object representing the Git repository, though it's not directly used in the provided snippet beyond its type hint."
              returns[1]{name,type,description}:
                full_schema,dict,"A dictionary representing the AST schema of the analyzed repository, structured by file paths."
              usage_context:
                calls: "This method calls 'os.path.commonpath', 'os.path.isfile', 'os.path.dirname', 'ast.parse', and instantiates 'ASTVisitor'."
                called_by: This method is not explicitly called by any other functions or methods in the provided context.
            error: null
        usage_context:
          dependencies: "This class depends on 'backend.AST_Schema.ASTVisitor' for parsing ASTs, 'ast' for Python's built-in AST module, and 'os' for path manipulation."
          instantiated_by: This class is not explicitly instantiated by any known components in the provided context.
      error: null
    backend.File_Dependency.FileDependencyGraph:
      identifier: backend.File_Dependency.FileDependencyGraph
      description:
        overall: "The FileDependencyGraph class extends ast.NodeVisitor to traverse the Abstract Syntax Tree (AST) of a Python file and identify its import dependencies. It is designed to build a dictionary (import_dependencies) mapping the analyzed file to a set of modules it imports. The class handles both absolute and relative import statements, with a dedicated private method _resolve_module_name for robustly resolving relative imports within a given repository context. Its primary purpose is to establish a foundational understanding of file-level import relationships for a larger dependency analysis system."
        init_method:
          description: The constructor initializes the FileDependencyGraph instance by storing the filename of the file being analyzed and the repo_root directory. These attributes are crucial for resolving relative imports and locating files within the repository.
          parameters[2]{name,type,description}:
            filename,str,The name of the file currently being analyzed for dependencies.
            repo_root,str,"The root directory of the repository, used for resolving file paths."
        methods[3]:
          - identifier: _resolve_module_name
            description:
              overall: "This private method is responsible for resolving relative import statements (e.g., from .. import name). It calculates the correct base directory based on the import level and the current file's location within the repository. It then iterates through the imported names, checking if they correspond to existing module files or symbols exported by __init__.py files. If no matching modules or symbols are found, it raises an ImportError. Nested functions `module_file_exists` and `init_exports_symbol` are defined within this method to assist in verifying file and symbol existence."
              parameters[1]{name,type,description}:
                node,ImportFrom,The AST ImportFrom node representing the relative import statement.
              returns[1]{name,type,description}:
                resolved,"list[str]",A list of resolved module or symbol names as strings.
              usage_context:
                calls: "This method calls get_all_temp_files to get all files in the repository, Path for path manipulation, iskeyword to check for Python keywords, and internally defines and calls module_file_exists and init_exports_symbol to verify module existence and symbol exports. It also uses literal_eval, parse, and walk from the ast module within init_exports_symbol."
                called_by: This method is called by visit_ImportFrom when processing relative import statements.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method is part of the NodeVisitor pattern and is called for Import and ImportFrom AST nodes. It records the imported module names as dependencies for the current self.filename in the import_dependencies dictionary. If a base_name is provided (typically from visit_ImportFrom), it uses that; otherwise, it uses the alias name from the import node. After processing, it calls self.generic_visit(node) to continue the AST traversal."
              parameters[2]{name,type,description}:
                node,Import | ImportFrom,The AST node representing an import statement.
                base_name,str | None,"An optional base name for the module, typically used for from ... import ... statements where the module part is resolved separately."
              returns[0]:
              usage_context:
                calls: This method calls self.generic_visit to continue the AST traversal.
                called_by: "This method is called by the AST NodeVisitor when encountering Import nodes, and explicitly by visit_ImportFrom to record dependencies."
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method is invoked by the AST NodeVisitor when an ImportFrom node is encountered. It extracts the module name from the import statement. If it's an absolute import, it takes the last part of the module name and passes it to visit_Import. If it's a relative import (indicated by node.module being None), it attempts to resolve the module name using the _resolve_module_name helper method. Any resolved base names are then passed to visit_Import to record the dependency. Errors during relative import resolution are caught and printed. Finally, it ensures the AST traversal continues with self.generic_visit."
              parameters[1]{name,type,description}:
                node,ImportFrom,The AST ImportFrom node to be processed.
              returns[0]:
              usage_context:
                calls: This method calls self.visit_Import to record the dependency and self._resolve_module_name to handle relative imports. It also calls self.generic_visit to continue the AST traversal.
                called_by: This method is called by the AST NodeVisitor framework when traversing the AST and encountering an ImportFrom node.
            error: null
        usage_context:
          dependencies: "This class depends on get_all_temp_files for repository file discovery, and init_exports_symbol and module_file_exists for resolving module and symbol existence during relative import processing."
          instantiated_by: This class is not explicitly shown to be instantiated by other components within the provided context.
      error: null
    backend.HelperLLM.LLMHelper:
      identifier: backend.HelperLLM.LLMHelper
      description:
        overall: "The LLMHelper class provides a centralized interface for interacting with various Large Language Models (LLMs) to generate structured documentation for Python functions and classes. It abstracts away the complexities of LLM API integration, including model selection (supporting Gemini, OpenAI, custom APIs, and Ollama), system prompt management, batch processing, and rate limit handling. The class ensures that LLM outputs conform to predefined Pydantic schemas (FunctionAnalysis and ClassAnalysis), making it a robust tool for automated code documentation generation."
        init_method:
          description: "The constructor initializes the LLMHelper instance by setting up the API key, loading system prompts for function and class analysis from specified file paths, and configuring the underlying Language Model (LLM) based on the `model_name`. It supports various LLM providers like Google Gemini, OpenAI, custom APIs, and Ollama, and also configures batch processing settings."
          parameters[5]{name,type,description}:
            api_key,str,The API key for authenticating with the chosen LLM service.
            function_prompt_path,str,The file path to the system prompt used for function analysis.
            class_prompt_path,str,The file path to the system prompt used for class analysis.
            model_name,str,"The name of the LLM model to use (default: \"gemini-2.0-flash-lite\")."
            base_url,str,"An optional base URL for custom LLM endpoints, particularly for Ollama or custom OpenAI-compatible APIs."
        methods[3]:
          - identifier: _configure_batch_settings
            description:
              overall: "This private method sets the `batch_size` attribute of the LLMHelper instance based on the provided `model_name`. It uses a series of conditional statements to assign specific batch sizes for various known LLM models like different Gemini versions, Llama3, and GPT models. For unknown models or custom API models, it defaults to a conservative batch size of 2 or a larger size of 500, respectively, logging a warning if the model is unrecognized."
              parameters[1]{name,type,description}:
                model_name,str,The name of the LLM model for which to configure batch settings.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions based on the provided context."
                called_by: This method is not explicitly called by other functions or methods based on the provided context.
            error: null
          - identifier: generate_for_functions
            description:
              overall: "This method takes a list of `FunctionAnalysisInput` objects, converts them into JSON payloads, and then uses the configured `function_llm` to generate structured documentation for functions in batches. It iterates through the inputs, sending `BATCH_SIZE` conversations to the LLM concurrently, handles potential API errors by extending the results with `None` for failed items, and incorporates a waiting period between batches to respect rate limits. The method returns a list of `FunctionAnalysis` objects or `None` for failed generations."
              parameters[1]{name,type,description}:
                function_inputs,"List[FunctionAnalysisInput]","A list of input objects, each containing the necessary data for analyzing a single function."
              returns[1]{name,type,description}:
                None,"List[Optional[FunctionAnalysis]]","A list of `FunctionAnalysis` objects, where each object represents the structured documentation for a function, or `None` if the analysis for a specific function failed."
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions based on the provided context."
                called_by: This method is not explicitly called by other functions or methods based on the provided context.
            error: null
          - identifier: generate_for_classes
            description:
              overall: "This method is designed to generate structured documentation for a batch of classes. It takes a list of `ClassAnalysisInput` objects, serializes them into JSON, and then constructs conversations for the `class_llm`. The method processes these conversations in batches, sending them to the LLM, handling potential exceptions by marking failed items as `None`, and pausing between batches to manage API rate limits. It ultimately returns a list of `ClassAnalysis` objects or `None` for any failed analysis."
              parameters[1]{name,type,description}:
                class_inputs,"List[ClassAnalysisInput]","A list of input objects, each containing the necessary data for analyzing a single class."
              returns[1]{name,type,description}:
                None,"List[Optional[ClassAnalysis]]","A list of `ClassAnalysis` objects, where each object represents the structured documentation for a class, or `None` if the analysis for a specific class failed."
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions based on the provided context."
                called_by: This method is not explicitly called by other functions or methods based on the provided context.
            error: null
        usage_context:
          dependencies: The class does not have explicitly listed external dependencies in the provided context.
          instantiated_by: The class is not explicitly instantiated by other components in the provided context.
      error: null
    backend.MainLLM.MainLLM:
      identifier: backend.MainLLM.MainLLM
      description:
        overall: "The MainLLM class provides a unified interface for interacting with various Large Language Models (LLMs). It dynamically configures the appropriate LLM client (e.g., Gemini, OpenAI-compatible, Ollama) based on the specified model name and manages a system prompt loaded from a file. This class offers methods for both single-response and streaming interactions with the underlying LLM, incorporating robust error handling for API calls."
        init_method:
          description: "The constructor initializes the MainLLM instance by setting up the system prompt from a specified file and configuring the underlying Large Language Model (LLM) client based on the provided model name. It supports various LLM providers and performs validation for the API key and prompt file path, raising errors if essential configurations are missing."
          parameters[4]{name,type,description}:
            api_key,str,The API key required for authenticating with the chosen LLM service.
            prompt_file_path,str,The file path to the system prompt that will be used for LLM interactions.
            model_name,str,"The name of the LLM model to use, defaulting to 'gemini-2.5-pro'. This parameter determines which LLM client (Gemini, OpenAI, Ollama) is initialized."
            base_url,str | None,"An optional base URL for custom LLM endpoints, primarily used for Ollama or other OpenAI-compatible services if the SCADSLLM_URL environment variable is not set."
        methods[2]:
          - identifier: call_llm
            description:
              overall: "This method sends a user's input to the configured Large Language Model (LLM) and retrieves a single, complete response. It constructs a message list that includes the system prompt and the user's input, then invokes the LLM to get the generated content. The method includes error handling to log any issues during the LLM call and returns None if an exception occurs."
              parameters[1]{name,type,description}:
                user_input,str,"The user's query or message to be sent to the LLM for a single, non-streaming response."
              returns[1]{name,type,description}:
                content,str | None,"The generated content from the LLM if the call is successful, otherwise None in case of an error."
              usage_context:
                calls: "This method calls SystemMessage, HumanMessage, logging.info, self.llm.invoke, and logging.error."
                called_by: This method is not explicitly called by other methods in the provided context.
            error: null
          - identifier: stream_llm
            description:
              overall: "This method interacts with the LLM to obtain a streaming response, yielding chunks of content as they become available. It prepares the messages, including the system prompt and user input, and then iterates through the stream iterator provided by the LLM client, yielding each content chunk. Error handling is implemented to log any issues during the streaming call and yield an error message if an exception occurs."
              parameters[1]{name,type,description}:
                user_input,str,The user's query or message for which a streaming response is requested from the LLM.
              returns[2]{name,type,description}:
                chunk.content,str,Yields individual content chunks from the LLM's streaming response.
                error_message,str,Yields an error message string if an exception occurs during the streaming call.
              usage_context:
                calls: "This method calls SystemMessage, HumanMessage, logging.info, self.llm.stream, and logging.error."
                called_by: This method is not explicitly called by other methods in the provided context.
            error: null
        usage_context:
          dependencies: "The class depends on logging for output, os (implicitly for environment variables like SCADSLLM_URL, OLLAMA_BASE_URL), langchain_google_genai.ChatGoogleGenerativeAI, langchain_ollama.ChatOllama, langchain_openai.ChatOpenAI, langchain.messages.HumanMessage, and langchain.messages.SystemMessage for its core LLM functionalities."
          instantiated_by: This class is not explicitly instantiated by other components in the provided context.
      error: null
    backend.basic_info.ProjektInfoExtractor:
      identifier: backend.basic_info.ProjektInfoExtractor
      description:
        overall: "The ProjektInfoExtractor class is designed to systematically extract fundamental project details from various common project files, such as README.md, pyproject.toml, and requirements.txt. It orchestrates the parsing of these files, prioritizing certain sources (e.g., TOML over requirements.txt for dependencies), and consolidates the gathered information into a structured dictionary. The class also includes mechanisms for cleaning file content and deriving fallback information, like a project title from a repository URL, ensuring comprehensive data collection."
        init_method:
          description: "The constructor initializes the ProjektInfoExtractor instance by setting a default 'Information not found' string and establishing the core data structure, `self.info`. This dictionary is pre-populated with nested keys for project overview and installation details, all initially set to the 'Information not found' placeholder, preparing it for subsequent data extraction."
          parameters[0]:
        methods[7]:
          - identifier: _clean_content
            description:
              overall: "This private helper method is responsible for sanitizing string content by removing null bytes. Null bytes can often appear due to encoding errors, particularly when a file encoded in UTF-16 is mistakenly read as UTF-8. By removing these characters, the method ensures that subsequent parsing operations on the content are not disrupted by malformed data."
              parameters[1]{name,type,description}:
                content,str,The string content that needs to be cleaned of null bytes.
              returns[1]{name,type,description}:
                "",str,"The cleaned string with all null bytes removed, or an empty string if the input was empty."
              usage_context:
                calls: This method does not call any other functions or methods.
                called_by: "This method is called by _parse_readme, _parse_toml, and _parse_requirements."
            error: null
          - identifier: _finde_datei
            description:
              overall: "This private method searches through a list of file objects to find one whose path matches any of the provided patterns. The search is case-insensitive, making it robust against variations in file naming conventions. It iterates through each file and pattern, returning the first file that satisfies a match."
              parameters[2]{name,type,description}:
                patterns,"List[str]","A list of string patterns to match against file paths (e.g., 'readme.md')."
                dateien,"List[Any]","A list of file objects, where each object is expected to have a 'path' attribute."
              returns[1]{name,type,description}:
                "","Optional[Any]","The first file object found that matches any of the patterns, or None if no match is found."
              usage_context:
                calls: This method does not call any other functions or methods.
                called_by: This method is called by extrahiere_info.
            error: null
          - identifier: _extrahiere_sektion_aus_markdown
            description:
              overall: This private method extracts text content from a Markdown string that appears directly under an H2 heading (##) matching one of the specified keywords. It constructs a regular expression dynamically to find the heading and then captures all content following it until the next H2 heading or the end of the input string. This is useful for parsing structured sections within README files.
              parameters[2]{name,type,description}:
                inhalt,str,The Markdown content string to be parsed.
                keywords,"List[str]","A list of keywords to match against the H2 headings (e.g., 'Features', 'Installation')."
              returns[1]{name,type,description}:
                "","Optional[str]","The extracted text content under the matched H2 heading, or None if no matching section is found."
              usage_context:
                calls: "This method calls re.escape, re.compile, and pattern.search for regular expression operations."
                called_by: This method is called by _parse_readme.
            error: null
          - identifier: _parse_readme
            description:
              overall: "This private method parses the content of a README file to populate various fields within the `self.info` dictionary related to project overview and installation. It first cleans the content using `_clean_content` and then uses regular expressions to extract the project title and a fallback description. For structured sections like 'Key Features', 'Tech Stack', 'Status', 'Installation', and 'Quick Start', it leverages `_extrahiere_sektion_aus_markdown` to find and extract relevant information."
              parameters[1]{name,type,description}:
                inhalt,str,The raw string content of the README file.
              returns[0]:
              usage_context:
                calls: "This method calls _clean_content, re.search, and _extrahiere_sektion_aus_markdown."
                called_by: This method is called by extrahiere_info.
            error: null
          - identifier: _parse_toml
            description:
              overall: "This private method is responsible for parsing the content of a `pyproject.toml` file. It cleans the input content and then attempts to load it using the `tomllib` library. If `tomllib` is not available or a `TOMLDecodeError` occurs, it prints a warning. Upon successful parsing, it extracts the project name, description, and dependencies from the 'project' section of the TOML data and updates the `self.info` dictionary accordingly."
              parameters[1]{name,type,description}:
                inhalt,str,The raw string content of the pyproject.toml file.
              returns[0]:
              usage_context:
                calls: "This method calls _clean_content, tomllib.loads, and print."
                called_by: This method is called by extrahiere_info.
            error: null
          - identifier: _parse_requirements
            description:
              overall: "This private method parses the content of a `requirements.txt` file to extract project dependencies. It first cleans the input content and then processes each line, filtering out comments and empty lines. The extracted dependencies are then stored in the `self.info` dictionary, but only if the 'dependencies' field has not already been populated by a higher-priority source like `pyproject.toml`."
              parameters[1]{name,type,description}:
                inhalt,str,The raw string content of the requirements.txt file.
              returns[0]:
              usage_context:
                calls: This method calls _clean_content.
                called_by: This method is called by extrahiere_info.
            error: null
          - identifier: extrahiere_info
            description:
              overall: "This public method orchestrates the entire information extraction process. It identifies relevant project files (README, pyproject.toml, requirements.txt) from a given list of file objects using `_finde_datei`. It then parses these files in a specific order of priority: `pyproject.toml`, then `requirements.txt`, and finally the README file. After parsing, it formats the collected dependencies and, if no project title was found, attempts to derive one from the provided repository URL. The method returns a comprehensive dictionary of all extracted project information."
              parameters[2]{name,type,description}:
                dateien,"List[Any]","A list of file objects, each expected to have 'path' and 'content' attributes."
                repo_url,str,"The URL of the repository, used as a fallback to derive a project title."
              returns[1]{name,type,description}:
                "","Dict[str, Any]","A dictionary containing all extracted project information, including overview and installation details."
              usage_context:
                calls: "This method calls _finde_datei, _parse_toml, _parse_requirements, _parse_readme, os.path.basename, and repo_url.removesuffix."
                called_by: "This method is not explicitly called by other methods within the provided context, suggesting it is a primary entry point for external usage."
            error: null
        usage_context:
          dependencies: This class does not explicitly list external dependencies in the provided context.
          instantiated_by: This class is not explicitly instantiated by any known components in the provided context.
      error: null
    backend.callgraph.CallGraph:
      identifier: backend.callgraph.CallGraph
      description:
        overall: "The CallGraph class is an ast.NodeVisitor designed to construct a call graph for a given Python source file. It traverses the Abstract Syntax Tree (AST) of a file, identifying function and class definitions, import statements, and function calls. It maintains context about the current function and class to accurately resolve call targets, including local definitions and imported modules. The class builds a directed graph (networkx.DiGraph) where nodes represent functions/methods and edges represent calls between them, providing a structured representation of the code's execution flow."
        init_method:
          description: "Initializes the CallGraph instance, setting up the filename, tracking current function and class context during AST traversal, and initializing data structures like local_defs, graph (a NetworkX DiGraph), import_mapping, function_set, and edges to store call graph information."
          parameters[1]{name,type,description}:
            filename,str,"The name of the file being analyzed, used for full name resolution."
        methods[11]:
          - identifier: _recursive_call
            description:
              overall: "This private helper method recursively extracts the components of a function or method call from an AST node. It handles ast.Call, ast.Name, and ast.Attribute nodes to build a list of name parts, effectively converting an AST representation of a call (e.g., obj.method(), module.function()) into a list of its dotted components (e.g., ['obj', 'method'])."
              parameters[1]{name,type,description}:
                node,ast.AST,"The AST node representing a call, name, or attribute."
              returns[1]{name,type,description}:
                name,"list[str]",A list of string components representing the fully qualified name of the called entity.
              usage_context:
                calls: This method does not explicitly call other functions or methods within its own source code.
                called_by: This method is called by visit_Call.
            error: null
          - identifier: _resolve_all_callee_names
            description:
              overall: "This private method takes a list of potential callee name components (e.g., [['my_func'], ['obj', 'method']]) and attempts to resolve them to their full, unique identifiers within the context of the current file. It prioritizes local definitions, then import mappings, and finally constructs a full name based on the current filename and class context. This ensures that calls are correctly linked to their definitions."
              parameters[1]{name,type,description}:
                callee_nodes,"list[list[str]]",A list where each inner list contains string components of a potential callee's name.
              returns[1]{name,type,description}:
                name,"list[str]",A list of fully resolved string identifiers for the callees.
              usage_context:
                calls: This method does not explicitly call other functions or methods within its own source code.
                called_by: This method is called by visit_Call.
            error: null
          - identifier: _make_full_name
            description:
              overall: "This private helper method constructs a fully qualified name for a function or method. It prepends the filename and optionally the class_name to the given basename, using \"::\" as a separator. This standardized naming convention helps in uniquely identifying functions and methods across the project."
              parameters[2]{name,type,description}:
                basename,str,"The base name of the function or method (e.g., \"my_function\")."
                class_name,str | None,"The name of the class if the entity is a method, otherwise None."
              returns[1]{name,type,description}:
                name,str,"The fully qualified name (e.g., \"filename::ClassName::methodName\" or \"filename::functionName\")."
              usage_context:
                calls: This method does not explicitly call other functions or methods within its own source code.
                called_by: This method is called by visit_FunctionDef.
            error: null
          - identifier: _current_caller
            description:
              overall: "This private method determines the identifier of the current calling context. If self.current_function is set, it returns that. Otherwise, it returns a placeholder indicating the global scope, using the filename if available, or \"<global-scope>\" as a fallback. This is crucial for correctly attributing calls in the graph."
              parameters[0]:
              returns[1]{name,type,description}:
                name,str,The identifier of the current function or a global scope placeholder.
              usage_context:
                calls: This method does not explicitly call other functions or methods within its own source code.
                called_by: This method is called by visit_Call.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method, part of the ast.NodeVisitor pattern, is called when an ast.Import node is encountered. It processes top-level import statements (e.g., import module as alias) to populate the self.import_mapping dictionary, which maps aliases or module names to their original module names. After processing, it calls generic_visit to continue traversing the AST."
              parameters[1]{name,type,description}:
                node,ast.Import,The AST node representing an import statement.
              returns[0]:
              usage_context:
                calls: This method calls self.generic_visit.
                called_by: This method is called by the ast.NodeVisitor framework when an ast.Import node is visited.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method, part of the ast.NodeVisitor pattern, is called when an ast.ImportFrom node is encountered. It processes from ... import ... statements (e.g., from package.module import name as alias) to update self.import_mapping. It maps imported names (or their aliases) to their originating module. This helps in resolving fully qualified names later."
              parameters[1]{name,type,description}:
                node,ast.ImportFrom,The AST node representing a from ... import ... statement.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other functions or methods within its own source code.
                called_by: This method is called by the ast.NodeVisitor framework when an ast.ImportFrom node is visited.
            error: null
          - identifier: visit_ClassDef
            description:
              overall: "This method, part of the ast.NodeVisitor pattern, is invoked when an ast.ClassDef node is found. It manages the self.current_class context, setting it to the name of the current class before recursively visiting its children, and then restoring the previous class context upon exit. This ensures that methods defined within a class are correctly associated with that class."
              parameters[1]{name,type,description}:
                node,ast.ClassDef,The AST node representing a class definition.
              returns[0]:
              usage_context:
                calls: This method calls self.generic_visit.
                called_by: This method is called by the ast.NodeVisitor framework when an ast.ClassDef node is visited.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method, part of the ast.NodeVisitor pattern, is called for ast.FunctionDef nodes. It constructs the full, unique name for the function using _make_full_name, updates self.local_defs to map the function's simple name (and potentially class-qualified name) to its full name, and sets self.current_function to track the current scope. It adds the function as a node to the self.graph and then recursively visits its body, finally adding the function to self.function_set and restoring the previous function context."
              parameters[1]{name,type,description}:
                node,ast.FunctionDef,The AST node representing a function definition.
              returns[0]:
              usage_context:
                calls: "This method calls self._make_full_name, self.generic_visit, self.graph.add_node, and self.function_set.add."
                called_by: "This method is called by the ast.NodeVisitor framework when an ast.FunctionDef node is visited, and by visit_AsyncFunctionDef."
            error: null
          - identifier: visit_AsyncFunctionDef
            description:
              overall: "This method, part of the ast.NodeVisitor pattern, handles ast.AsyncFunctionDef nodes by simply delegating to visit_FunctionDef. This allows the call graph generation logic to treat synchronous and asynchronous function definitions identically for the purpose of identifying and tracking functions."
              parameters[1]{name,type,description}:
                node,ast.AsyncFunctionDef,The AST node representing an asynchronous function definition.
              returns[0]:
              usage_context:
                calls: This method calls self.visit_FunctionDef.
                called_by: This method is called by the ast.NodeVisitor framework when an ast.AsyncFunctionDef node is visited.
            error: null
          - identifier: visit_Call
            description:
              overall: "This method, part of the ast.NodeVisitor pattern, is triggered when an ast.Call node is encountered. It identifies the caller using _current_caller, extracts the components of the callee using _recursive_call, and then resolves the full name of the callee using _resolve_all_callee_names. Finally, it records the call relationship by adding the callee to the self.edges dictionary under the caller's entry, representing an edge in the call graph. It then continues AST traversal."
              parameters[1]{name,type,description}:
                node,ast.Call,The AST node representing a function or method call.
              returns[0]:
              usage_context:
                calls: "This method calls self._current_caller, self._recursive_call, self._resolve_all_callee_names, and self.generic_visit. It also interacts with self.edges."
                called_by: This method is called by the ast.NodeVisitor framework when an ast.Call node is visited.
            error: null
          - identifier: visit_If
            description:
              overall: "This method, part of the ast.NodeVisitor pattern, handles ast.If nodes. It specifically checks for the if __name__ == \"__main__\": block. If such a block is detected, it temporarily sets self.current_function to \"<main_block>\" before visiting the children of the if statement, effectively treating the main execution block as a distinct caller in the call graph. For other if statements, it simply delegates to generic_visit."
              parameters[1]{name,type,description}:
                node,ast.If,The AST node representing an if statement.
              returns[0]:
              usage_context:
                calls: This method calls self.generic_visit.
                called_by: This method is called by the ast.NodeVisitor framework when an ast.If node is visited.
            error: null
        usage_context:
          dependencies: The class relies on the ast module for parsing Python code and the networkx library for graph manipulation.
          instantiated_by: This class is not explicitly instantiated by any known entities based on the provided context.
      error: null
    backend.getRepo.RepoFile:
      identifier: backend.getRepo.RepoFile
      description:
        overall: "The RepoFile class represents a single file within a Git repository, providing a structured way to access its metadata and content. It implements lazy loading for the Git blob, file content, and size, ensuring that these potentially heavy operations are only performed when explicitly requested. The class offers properties to retrieve the Git blob, the decoded file content, and its size, along with utility methods for analysis and dictionary conversion."
        init_method:
          description: "The `__init__` method initializes a RepoFile object by storing the file path and the Git `commit_tree`. It sets up internal attributes (`_blob`, `_content`, `_size`) to `None` for lazy loading of the Git blob, file content, and size, respectively, deferring their actual retrieval until they are accessed."
          parameters[2]{name,type,description}:
            file_path,str,The path to the file within the repository.
            commit_tree,git.Tree,The Tree object of the commit from which the file originates.
        methods[6]:
          - identifier: blob
            description:
              overall: "This property provides lazy loading for the Git blob object associated with the file. It checks if the `_blob` attribute is already set; if not, it attempts to retrieve the blob from the `_tree` using the file's path. If the file is not found in the tree, it raises a `FileNotFoundError`."
              parameters[0]:
              returns[1]{name,type,description}:
                blob,git.Blob,The Git blob object representing the file.
              usage_context:
                calls: This method does not explicitly call other methods or functions.
                called_by: This method is not explicitly called by other components.
            error: null
          - identifier: content
            description:
              overall: "This property provides lazy loading for the decoded content of the file. It checks if the `_content` attribute is already set; if not, it accesses the `blob` property to get the Git blob, reads its data stream, and decodes it using UTF-8 with error ignoring."
              parameters[0]:
              returns[1]{name,type,description}:
                content,str,The decoded string content of the file.
              usage_context:
                calls: This method does not explicitly call other methods or functions.
                called_by: This method is not explicitly called by other components.
            error: null
          - identifier: size
            description:
              overall: "This property provides lazy loading for the size of the file in bytes. It checks if the `_size` attribute is already set; if not, it accesses the `blob` property to get the Git blob and retrieves its size attribute."
              parameters[0]:
              returns[1]{name,type,description}:
                size,int,The size of the file in bytes.
              usage_context:
                calls: This method does not explicitly call other methods or functions.
                called_by: This method is not explicitly called by other components.
            error: null
          - identifier: analyze_word_count
            description:
              overall: "This method serves as an example analysis function. It calculates and returns the total number of words in the file's content by accessing the `content` property, splitting the string by whitespace, and counting the resulting elements."
              parameters[0]:
              returns[1]{name,type,description}:
                word_count,int,The total number of words found in the file content.
              usage_context:
                calls: This method does not explicitly call other methods or functions.
                called_by: This method is not explicitly called by other components.
            error: null
          - identifier: __repr__
            description:
              overall: "This special method provides a developer-friendly string representation of the `RepoFile` object. It returns a string that includes the class name and the path of the file it represents, making debugging and logging easier."
              parameters[0]:
              returns[1]{name,type,description}:
                representation,str,"A string representation of the RepoFile object, including its path."
              usage_context:
                calls: This method does not explicitly call other methods or functions.
                called_by: This method is not explicitly called by other components.
            error: null
          - identifier: to_dict
            description:
              overall: "This method converts the `RepoFile` object into a dictionary representation. It includes the file's path, name (basename), size, and type. Optionally, it can also include the file's content if `include_content` is set to `True`."
              parameters[1]{name,type,description}:
                include_content,bool,A flag indicating whether to include the file's content in the dictionary.
              returns[1]{name,type,description}:
                data,dict,"A dictionary containing metadata about the file, optionally including its content."
              usage_context:
                calls: This method does not explicitly call other methods or functions.
                called_by: This method is not explicitly called by other components.
            error: null
        usage_context:
          dependencies: This class does not have any explicit external dependencies listed in the provided context.
          instantiated_by: This class is not explicitly shown to be instantiated by other components in the provided context.
      error: null
    backend.getRepo.GitRepository:
      identifier: backend.getRepo.GitRepository
      description:
        overall: "The GitRepository class is designed to manage a Git repository by cloning it into a temporary directory, providing structured access to its files, and ensuring proper cleanup. It acts as a context manager, allowing for automatic resource management. The class facilitates retrieving all files as `RepoFile` objects and organizing them into a hierarchical tree structure."
        init_method:
          description: "The `__init__` method initializes a GitRepository instance by cloning the specified `repo_url` into a newly created temporary directory. It sets up essential instance attributes such as the repository object, the latest commit, and its commit tree. The constructor includes error handling for `GitCommandError` during the cloning process, ensuring the temporary directory is cleaned up if cloning fails."
          parameters[1]{name,type,description}:
            repo_url,str,The URL of the Git repository to be cloned.
        methods[5]:
          - identifier: get_all_files
            description:
              overall: "This method is responsible for retrieving all file paths from the cloned Git repository. It uses the underlying Git command `ls-files` to get a list of all tracked files. For each file path, it instantiates a `RepoFile` object, associating it with the repository's commit tree. The method stores this list of `RepoFile` objects internally and returns it."
              parameters[1]{name,type,description}:
                self,GitRepository,The instance of the GitRepository.
              returns[1]{name,type,description}:
                files,"list[RepoFile]",A list of RepoFile instances representing all files in the repository.
              usage_context:
                calls: This method calls `backend.getRepo.RepoFile` to create `RepoFile` instances.
                called_by: This method is not explicitly called by other methods in the provided context.
            error: null
          - identifier: close
            description:
              overall: "The `close` method handles the cleanup of the temporary directory created during the repository cloning process. It checks if `self.temp_dir` is set to ensure a directory exists before attempting to delete it. After deletion, `self.temp_dir` is set to `None` to prevent further attempts to delete a non-existent directory."
              parameters[1]{name,type,description}:
                self,GitRepository,The instance of the GitRepository.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other functions or methods.
                called_by: This method is not explicitly called by other methods in the provided context.
            error: null
          - identifier: __enter__
            description:
              overall: "This special method enables the GitRepository class to be used as a context manager. When an instance of GitRepository is used in a `with` statement, the `__enter__` method is automatically invoked. It simply returns the instance itself, allowing it to be bound to a variable in the `with` statement."
              parameters[1]{name,type,description}:
                self,GitRepository,The instance of the GitRepository.
              returns[1]{name,type,description}:
                self,GitRepository,The instance of the GitRepository itself.
              usage_context:
                calls: This method does not explicitly call other functions or methods.
                called_by: This method is not explicitly called by other methods in the provided context.
            error: null
          - identifier: __exit__
            description:
              overall: "This special method is part of the context manager protocol and is automatically called when exiting a `with` statement block, regardless of whether an exception occurred. Its primary purpose is to ensure that the `close` method is invoked, thereby cleaning up the temporary repository directory. It accepts exception details but does not explicitly handle them, allowing them to propagate."
              parameters[4]{name,type,description}:
                self,GitRepository,The instance of the GitRepository.
                exc_type,type | None,"The type of the exception that caused the context to be exited, or None if no exception occurred."
                exc_val,Exception | None,"The exception instance that caused the context to be exited, or None."
                exc_tb,TracebackType | None,"A traceback object encapsulating the call stack at the point where the exception originally occurred, or None."
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other functions or methods.
                called_by: This method is not explicitly called by other methods in the provided context.
            error: null
          - identifier: get_file_tree
            description:
              overall: "This method generates a hierarchical dictionary representation of the repository's file structure, mimicking a file system tree. It first ensures that all files have been retrieved by calling `get_all_files` if `self.files` is empty. It then iterates through the `RepoFile` objects, parsing their paths to build a nested dictionary structure where directories are represented by dictionaries with a 'children' list, and files are appended at their respective levels."
              parameters[2]{name,type,description}:
                self,GitRepository,The instance of the GitRepository.
                include_content,bool,A flag indicating whether to include the file content in the dictionary representation. Defaults to False.
              returns[1]{name,type,description}:
                tree,dict,A dictionary representing the hierarchical file tree of the repository.
              usage_context:
                calls: This method does not explicitly call other functions or methods.
                called_by: This method is not explicitly called by other methods in the provided context.
            error: null
        usage_context:
          dependencies: This class depends on `backend.getRepo.RepoFile` for representing individual files.
          instantiated_by: This class is not explicitly instantiated by other components in the provided context.
      error: null
    backend.relationship_analyzer.ProjectAnalyzer:
      identifier: backend.relationship_analyzer.ProjectAnalyzer
      description:
        overall: "The ProjectAnalyzer class is designed to perform a static analysis of a Python project to build a comprehensive call graph. It identifies all Python files, collects definitions of functions, methods, and classes, and then resolves calls between these defined entities. The class provides methods to initiate the analysis and retrieve the raw relationships in a structured format, aiding in understanding the project's internal dependencies and structure."
        init_method:
          description: "The constructor initializes the ProjectAnalyzer instance by setting up the project's root directory and various internal data structures. It prepares dictionaries for storing definitions, the call graph, and file ASTs, along with a set of directories to ignore during file traversal."
          parameters[1]{name,type,description}:
            project_root,string,The root directory of the Python project to be analyzed.
        methods[6]:
          - identifier: analyze
            description:
              overall: "This method orchestrates the entire project analysis process. It first identifies all Python files within the project, then iterates through them to collect function, method, and class definitions. Subsequently, it resolves calls within each file to build a comprehensive call graph, clearing intermediate ASTs to free memory. The method returns the generated call graph upon completion."
              parameters[0]:
              returns[1]{name,type,description}:
                call_graph,defaultdict(list),"A dictionary representing the call graph, where keys are callee identifiers and values are lists of caller information."
              usage_context:
                calls: "This method calls _find_py_files, _collect_definitions, and _resolve_calls."
                called_by: This method is not explicitly called by other methods within the provided method_context.
            error: null
          - identifier: get_raw_relationships
            description:
              overall: "This method processes the internal call_graph to generate structured outgoing and incoming relationship dictionaries. It iterates through the call graph, extracting caller and callee identifiers, and populates two defaultdict(set) objects to store the relationships. The sets are then converted to sorted lists for the final output, providing a clear view of dependencies."
              parameters[0]:
              returns[1]{name,type,description}:
                relationships,dict,"A dictionary containing two keys, 'outgoing' and 'incoming', each mapping identifiers to sorted lists of related identifiers."
              usage_context:
                calls: "This method does not explicitly call other methods, classes, or functions within its source code."
                called_by: This method is not explicitly called by other methods within the provided method_context.
            error: null
          - identifier: _find_py_files
            description:
              overall: This private helper method traverses the project directory starting from self.project_root to locate all Python files. It uses os.walk and filters out specified ignore_dirs to avoid analyzing irrelevant directories. The method compiles and returns a list of absolute paths to all '.py' files found within the project scope.
              parameters[0]:
              returns[1]{name,type,description}:
                py_files,list,"A list of absolute file paths to all Python files found in the project, excluding ignored directories."
              usage_context:
                calls: This method calls os.walk and os.path.join.
                called_by: This method is called by analyze.
            error: null
          - identifier: _collect_definitions
            description:
              overall: "This private method is responsible for parsing a given Python file and collecting all function, method, and class definitions within it. It reads the file, parses it into an Abstract Syntax Tree (AST), and then walks the AST to identify FunctionDef and ClassDef nodes. For each definition, it constructs a unique path name and stores its file, line number, and type in self.definitions, also caching the AST in self.file_asts."
              parameters[1]{name,type,description}:
                filepath,string,The absolute path to the Python file being analyzed for definitions.
              returns[0]:
              usage_context:
                calls: "This method calls path_to_module, _get_parent, ast.parse, ast.walk, ast.FunctionDef, ast.ClassDef, and logging.error."
                called_by: This method is called by analyze.
            error: null
          - identifier: _get_parent
            description:
              overall: This private helper method finds the direct parent AST node of a given child node within a specific AST tree. It iterates through all nodes in the tree and checks their children to find a match for the provided node. This is primarily used to determine if a FunctionDef is nested within a ClassDef to correctly identify it as a method.
              parameters[2]{name,type,description}:
                tree,ast.AST,The root of the Abstract Syntax Tree to search within.
                node,ast.AST,The child node for which to find the parent.
              returns[1]{name,type,description}:
                parent_node,ast.AST or None,"The parent AST node of the given node if found, otherwise None."
              usage_context:
                calls: This method calls ast.walk and ast.iter_child_nodes.
                called_by: This method is called by _collect_definitions.
            error: null
          - identifier: _resolve_calls
            description:
              overall: "This private method takes a file's AST and uses a CallResolverVisitor to identify all function and method calls within that file. It retrieves the AST from self.file_asts, initializes the resolver with the file's context and collected definitions, and then visits the AST. The identified calls from the resolver are subsequently merged into the self.call_graph structure, with error logging for robustness."
              parameters[1]{name,type,description}:
                filepath,string,The absolute path to the Python file whose calls are to be resolved.
              returns[0]:
              usage_context:
                calls: This method calls CallResolverVisitor and logging.error.
                called_by: This method is called by analyze.
            error: null
        usage_context:
          dependencies: "This class depends on backend.relationship_analyzer.CallResolverVisitor and backend.relationship_analyzer.path_to_module. It also utilizes standard library modules such as ast, os, logging, and collections.defaultdict."
          instantiated_by: This class is not explicitly instantiated by any entities listed in the provided context.
      error: null
    backend.relationship_analyzer.CallResolverVisitor:
      identifier: backend.relationship_analyzer.CallResolverVisitor
      description:
        overall: "The CallResolverVisitor is an AST NodeVisitor designed to traverse Python source code's Abstract Syntax Tree to identify and resolve function and method calls. It systematically builds a call graph by tracking the fully qualified names of callers and callees, managing import scopes, and inferring types of instantiated objects. This class is crucial for understanding the dynamic relationships between different parts of a codebase by collecting detailed information about each detected call."
        init_method:
          description: "The constructor initializes the CallResolverVisitor with the file path of the code being analyzed, the project's root directory, and a dictionary of known definitions. It sets up internal state variables such as the module path, a scope dictionary for imports, a dictionary to track instance types, and a defaultdict to store the collected call relationships. This setup is essential for accurately resolving call targets during AST traversal."
          parameters[3]{name,type,description}:
            filepath,str,The absolute path to the Python file being analyzed.
            project_root,str,"The root directory of the entire project, used to determine module paths."
            definitions,dict,"A dictionary containing known fully qualified definitions within the project, used for validating resolved call targets."
        methods[7]:
          - identifier: visit_ClassDef
            description:
              overall: "This method is invoked when the AST visitor encounters a class definition (`ast.ClassDef`). Its primary role is to manage the `current_class_name` state variable, which tracks the class currently being processed. It saves the old class name, sets the new one, recursively visits all child nodes within the class, and then restores the `current_class_name` to its previous value. This ensures that methods defined within the class are correctly associated with their parent class for accurate call resolution."
              parameters[1]{name,type,description}:
                node,ast.ClassDef,The AST node representing a class definition.
              returns[0]:
              usage_context:
                calls: This method calls the generic visitor method to continue traversing the AST.
                called_by: This method is called by the AST visitor framework when a ClassDef node is encountered.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method handles `ast.FunctionDef` nodes, which represent function or method definitions. It constructs a fully qualified identifier for the current function, taking into account whether it's a method within a class or a top-level function. The method then updates `self.current_caller_name` to this identifier before recursively visiting the function's body, ensuring that any calls made within this function are correctly attributed. Finally, it restores the `current_caller_name` to its state before entering the function."
              parameters[1]{name,type,description}:
                node,ast.FunctionDef,The AST node representing a function definition.
              returns[0]:
              usage_context:
                calls: This method calls the generic visitor method to continue traversing the AST.
                called_by: This method is called by the AST visitor framework when a FunctionDef node is encountered.
            error: null
          - identifier: visit_Call
            description:
              overall: "This method is triggered upon encountering an `ast.Call` node, signifying a function or method invocation. It attempts to resolve the fully qualified name of the called entity using the `_resolve_call_qname` helper method. If a valid callee pathname is resolved and exists in the `self.definitions`, it records comprehensive information about the caller, including its file, line number, full identifier, and type (module, local function, or method). This collected caller information is then appended to the `self.calls` dictionary, keyed by the callee's pathname, effectively building the call graph."
              parameters[1]{name,type,description}:
                node,ast.Call,The AST node representing a function or method call.
              returns[0]:
              usage_context:
                calls: This method calls `_resolve_call_qname` to determine the callee's qualified name and the generic visitor method to continue traversing the AST.
                called_by: This method is called by the AST visitor framework when a Call node is encountered.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method processes `ast.Import` nodes, which correspond to `import module_name` statements. It iterates through all aliases defined in the import statement, mapping the local name (either the alias or the original module name) to its fully qualified name. This mapping is stored in `self.scope`, which is later used to resolve calls to imported modules or their members. After processing the import, it continues the generic AST traversal."
              parameters[1]{name,type,description}:
                node,ast.Import,The AST node representing an import statement.
              returns[0]:
              usage_context:
                calls: This method calls the generic visitor method to continue traversing the AST.
                called_by: This method is called by the AST visitor framework when an Import node is encountered.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method handles `ast.ImportFrom` nodes, which represent `from module import name` statements. It first determines the full module path, correctly handling relative imports based on `node.level`. For each imported name (and its alias, if present), it constructs its fully qualified path by combining the module path and the imported name. This resolved path is then stored in `self.scope`, mapping the local name to its global identifier, which is crucial for subsequent call resolution. The method then continues the generic AST traversal."
              parameters[1]{name,type,description}:
                node,ast.ImportFrom,The AST node representing a 'from ... import ...' statement.
              returns[0]:
              usage_context:
                calls: This method calls the generic visitor method to continue traversing the AST.
                called_by: This method is called by the AST visitor framework when an ImportFrom node is encountered.
            error: null
          - identifier: visit_Assign
            description:
              overall: "This method processes `ast.Assign` nodes, which represent assignment statements. It specifically looks for assignments where the right-hand side is a function call (e.g., `x = MyClass()`). If the called function is identified as a class constructor that exists in both `self.scope` and `self.definitions`, it records the fully qualified class name as the inferred type for the assigned variable. This type information is stored in `self.instance_types`, enabling more accurate resolution of method calls on these instances later in the analysis."
              parameters[1]{name,type,description}:
                node,ast.Assign,The AST node representing an assignment statement.
              returns[0]:
              usage_context:
                calls: This method calls the generic visitor method to continue traversing the AST.
                called_by: This method is called by the AST visitor framework when an Assign node is encountered.
            error: null
          - identifier: _resolve_call_qname
            description:
              overall: "This private helper method is responsible for determining the fully qualified name (QName) of a function or method being called. It handles two primary scenarios: direct function calls (`ast.Name`) and method calls on an object (`ast.Attribute`). For direct calls, it first checks `self.scope` for imported names, then local module paths. For attribute calls, it uses `self.instance_types` to resolve the class of an instance variable or `self.scope` for module-level attributes, subsequently appending the method name. If resolution is successful, it returns the QName; otherwise, it returns `None`."
              parameters[1]{name,type,description}:
                func_node,ast.expr,"The AST node representing the function or method being called (e.g., ast.Name or ast.Attribute)."
              returns[1]{name,type,description}:
                qualified_name,str | None,"The fully qualified name of the callable if resolved, otherwise None."
              usage_context:
                calls: This method does not explicitly call other methods within its own source code.
                called_by: This method is called by `visit_Call` to resolve the qualified name of a function or method invocation.
            error: null
        usage_context:
          dependencies: This class depends on `backend.relationship_analyzer.path_to_module` for converting file paths to module paths.
          instantiated_by: The class is not explicitly instantiated by any known components in the provided context.
      error: null
    schemas.types.ParameterDescription:
      identifier: schemas.types.ParameterDescription
      description:
        overall: "The ParameterDescription class is a Pydantic BaseModel designed to structure and validate information about a single parameter of a function. It serves as a data model, ensuring that parameter details such as its name, data type, and a descriptive explanation are consistently represented as strings. This class facilitates the standardized description of function parameters within a larger system."
        init_method:
          description: "This class, being a Pydantic BaseModel, automatically generates an __init__ method. It initializes an instance of ParameterDescription by accepting 'name', 'type', and 'description' as keyword arguments, validating them against their respective string types as defined by the model schema."
          parameters[3]{name,type,description}:
            name,str,The name of the parameter.
            type,str,The type hint or inferred type of the parameter.
            description,str,A textual description of the parameter's purpose.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in the provided context.
          instantiated_by: The specific locations where this class is instantiated are not provided in the current context.
      error: null
    schemas.types.ReturnDescription:
      identifier: schemas.types.ReturnDescription
      description:
        overall: "The `ReturnDescription` class is a Pydantic BaseModel designed to provide a structured representation for the return value of a function. It encapsulates essential information such as the return value's name, its data type, and a descriptive explanation. This class acts as a schema for consistent data handling when describing function outputs."
        init_method:
          description: "As a Pydantic BaseModel, the `__init__` method for `ReturnDescription` is automatically generated. It initializes an instance of the class by accepting `name`, `type`, and `description` as keyword arguments, performing type validation, and assigning these values as instance attributes."
          parameters[3]{name,type,description}:
            name,str,The name or identifier of the return value.
            type,str,The Python type hint or a string representation of the return value's data type.
            description,str,A detailed textual explanation of what the return value represents or its purpose.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies within the provided context.
          instantiated_by: The specific locations where this class is instantiated are not provided in the current context.
      error: null
    schemas.types.UsageContext:
      identifier: schemas.types.UsageContext
      description:
        overall: "This class serves as a Pydantic BaseModel, designed to structure and validate information regarding the usage context of a function. It defines two string attributes, `calls` and `called_by`, which are intended to describe what a function invokes and where it is invoked from, respectively. Essentially, it acts as a data container for contextual call information."
        init_method:
          description: "The `__init__` method, implicitly generated by Pydantic, initializes an instance of `UsageContext`. It expects `calls` and `called_by` as keyword arguments, which are then assigned as instance attributes, ensuring type validation according to their string annotations."
          parameters[2]{name,type,description}:
            calls,str,"A string describing other methods, classes, or functions that this method calls inside its source code."
            called_by,str,A string describing the name of another function or method that calls this method.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external dependencies within the provided context.
          instantiated_by: The instantiation points for this class are not specified in the provided context.
      error: null
    schemas.types.FunctionDescription:
      identifier: schemas.types.FunctionDescription
      description:
        overall: "The `FunctionDescription` class is a Pydantic BaseModel designed to encapsulate a comprehensive analysis of a Python function. It serves as a structured data schema to store details such as the function's high-level purpose, its input parameters, its return values, and its operational context within a larger system. This class ensures that function analysis data is consistently formatted and validated."
        init_method:
          description: "As a Pydantic BaseModel, the `__init__` method is implicitly generated to validate and assign values to the class's fields. It initializes an instance of `FunctionDescription` by taking the function's overall description, a list of its parameters, a list of its return values, and its usage context as arguments."
          parameters[4]{name,type,description}:
            overall,str,"A concise, high-level summary describing the function's purpose and its implementation details."
            parameters,"List[ParameterDescription]","A list of `ParameterDescription` objects, each detailing an input parameter of the function, including its name, type, and description."
            returns,"List[ReturnDescription]","A list of `ReturnDescription` objects, each describing a value returned by the function, including its name, type, and description."
            usage_context,UsageContext,"An object providing context on how the function interacts with other parts of the system, specifically what it calls and where it is called from."
        methods[0]:
        usage_context:
          dependencies: "This class primarily depends on Pydantic's BaseModel for data validation and structuring. It also relies on `List` from `typing` and other custom types like `ParameterDescription`, `ReturnDescription`, and `UsageContext` for its field definitions."
          instantiated_by: There are no explicit instantiation points provided in the context for this class.
      error: null
    schemas.types.FunctionAnalysis:
      identifier: schemas.types.FunctionAnalysis
      description:
        overall: "The FunctionAnalysis class is a Pydantic BaseModel designed to encapsulate the comprehensive analysis of a single Python function. It serves as a structured data container, holding the function's unique identifier, a detailed description of its purpose and signature, and an optional field for any errors encountered during its analysis. This model is fundamental for generating machine-readable reports on individual functions."
        init_method:
          description: "This class does not define an explicit `__init__` method. Pydantic's `BaseModel` handles the initialization of its fields (`identifier`, `description`, `error`) based on the provided type hints, allowing instances to be created by passing keyword arguments corresponding to these fields."
          parameters[0]:
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in the provided context.
          instantiated_by: The specific locations where this class is instantiated are not provided in the current context.
      error: null
    schemas.types.ConstructorDescription:
      identifier: schemas.types.ConstructorDescription
      description:
        overall: "The ConstructorDescription class is a Pydantic BaseModel designed to structure information about the `__init__` method of another class. It encapsulates a high-level textual description of the constructor and a list of its individual parameters, each detailed by a `ParameterDescription` object. This model is used to standardize the representation of constructor metadata within a larger system."
        init_method:
          description: "This class, being a Pydantic BaseModel, implicitly generates its `__init__` method. It initializes instances by accepting values for its `description` and `parameters` fields, performing data validation based on their type hints."
          parameters[2]{name,type,description}:
            description,str,A textual summary of the constructor's purpose.
            parameters,"List[ParameterDescription]","A list of objects, each describing a parameter of the constructor."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly depend on other components within the provided context.
          instantiated_by: This class is not explicitly shown to be instantiated by any other components in the provided context.
      error: null
    schemas.types.ClassContext:
      identifier: schemas.types.ClassContext
      description:
        overall: The ClassContext class is a Pydantic BaseModel designed to describe the contextual information of another class. It encapsulates details about the external dependencies that a class relies upon and specifies where that class is primarily instantiated within a larger system. This model serves as a structured way to convey usage and integration context.
        init_method:
          description: "This class does not define an explicit `__init__` method. Pydantic's `BaseModel` handles the initialization of its fields based on the provided type hints, automatically creating an initializer that accepts `dependencies` and `instantiated_by` as string arguments."
          parameters[0]:
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in the provided context.
          instantiated_by: The provided context does not specify where this class is instantiated.
      error: null
    schemas.types.ClassDescription:
      identifier: schemas.types.ClassDescription
      description:
        overall: "The `ClassDescription` class is a Pydantic model designed to encapsulate a comprehensive analysis of another Python class. It structures information about a class's overall purpose, its constructor, a list of its methods with detailed analysis, and its usage context within a larger system. This model serves as a data container for structured class metadata."
        init_method:
          description: "As a Pydantic BaseModel, `ClassDescription`'s constructor is implicitly generated. It initializes an instance by validating and assigning values to its defined fields: `overall`, `init_method`, `methods`, and `usage_context`."
          parameters[4]{name,type,description}:
            overall,str,A high-level summary of the class's purpose and functionality.
            init_method,ConstructorDescription,An object containing a detailed description of the class's constructor method.
            methods,"List[FunctionAnalysis]","A list of `FunctionAnalysis` objects, each providing a detailed analysis of a method within the class."
            usage_context,ClassContext,An object describing the class's external dependencies and where it is instantiated.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in its context.
          instantiated_by: This class is not explicitly listed as being instantiated by any other components in the provided context.
      error: null
    schemas.types.ClassAnalysis:
      identifier: schemas.types.ClassAnalysis
      description:
        overall: "The ClassAnalysis class is a Pydantic BaseModel designed to encapsulate a comprehensive analysis of a Python class. It serves as the top-level schema for structured output, containing the class's unique identifier, a detailed description object that further breaks down its constructor and methods, and an optional field for error reporting. This model provides a standardized format for representing the analytical results of a class."
        init_method:
          description: "The `ClassAnalysis` class, being a Pydantic BaseModel, has its `__init__` method implicitly generated by Pydantic. This constructor is responsible for initializing instances with the `identifier`, `description`, and an optional `error` field, ensuring type validation and data integrity upon object creation."
          parameters[3]{name,type,description}:
            identifier,str,The unique name or identifier of the class being analyzed.
            description,ClassDescription,"An object containing the detailed analysis of the class, including its overall purpose, constructor, and methods."
            error,"Optional[str]",An optional string field to store any error messages if the analysis process encounters issues.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in the provided context.
          instantiated_by: The specific instantiation points for this class are not provided in the given context.
      error: null
    schemas.types.CallInfo:
      identifier: schemas.types.CallInfo
      description:
        overall: "The CallInfo class is a Pydantic BaseModel designed to represent a specific call event identified by a relationship analyzer. It encapsulates details about where a call originated, including the file path, the name of the calling function, its type (mode), and the exact line number. This model is primarily used to structure data for 'called_by' and 'instantiated_by' lists within a larger system."
        init_method:
          description: "As a Pydantic BaseModel, CallInfo's constructor is automatically generated. It initializes an instance by validating and assigning values to its fields: file, function, mode, and line. This ensures that all call event data conforms to the specified types upon instantiation."
          parameters[4]{name,type,description}:
            file,str,The path to the file where the call event occurred.
            function,str,The name of the function or method that made the call.
            mode,str,"The type of the calling entity, e.g., 'method', 'function', 'module'."
            line,int,The line number in the file where the call event is located.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly depend on other components within the provided context.
          instantiated_by: This class is not explicitly instantiated by any known components within the provided context.
      error: null
    schemas.types.FunctionContextInput:
      identifier: schemas.types.FunctionContextInput
      description:
        overall: "The FunctionContextInput class serves as a Pydantic BaseModel for structuring the contextual information necessary to analyze a specific function. It encapsulates details about other functions or methods that the target function invokes, as well as information about where the target function itself is called within the codebase. This structured approach facilitates a comprehensive understanding of a function's interactions and dependencies."
        init_method:
          description: "This class, being a Pydantic BaseModel, has its constructor implicitly generated by Pydantic. It is responsible for initializing the `calls` and `called_by` attributes, ensuring that the provided input data conforms to the specified types for structured function context."
          parameters[2]{name,type,description}:
            calls,"List[str]","A list of identifiers (strings) representing other functions, methods, or classes that the function under analysis calls."
            called_by,"List[CallInfo]","A list of CallInfo objects, each providing details about a specific location or entity that calls the function under analysis."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list external functional dependencies in the provided context.
          instantiated_by: The specific points of instantiation for this class are not provided in the current context.
      error: null
    schemas.types.FunctionAnalysisInput:
      identifier: schemas.types.FunctionAnalysisInput
      description:
        overall: "The FunctionAnalysisInput class is a Pydantic BaseModel designed to define the structured input required for generating a FunctionAnalysis object. It serves as a data transfer object, ensuring that all necessary information for analyzing a Python function, such as its source code, identifier, and contextual data, is provided in a consistent and validated format. This class is crucial for standardizing the input for an AI code analysis system."
        init_method:
          description: "This class does not explicitly define an `__init__` method. It inherits from `pydantic.BaseModel`, which automatically generates a constructor based on the declared fields. The constructor will accept keyword arguments corresponding to the `mode`, `identifier`, `source_code`, `imports`, and `context` fields to initialize an instance."
          parameters[5]{name,type,description}:
            mode,"Literal[\"function_analysis\"]","Specifies the analysis mode, fixed to 'function_analysis' to indicate a function analysis request."
            identifier,str,The unique name or identifier of the function to be analyzed.
            source_code,str,The raw source code of the entire function definition.
            imports,"List[str]","A list of import statements relevant to the function's context, potentially including those from the source file."
            context,FunctionContextInput,"Additional contextual information required for the function analysis, such as call graphs."
        methods[0]:
        usage_context:
          dependencies: "This class does not explicitly list any external functional dependencies within its definition, relying primarily on Pydantic for its model capabilities."
          instantiated_by: "The instantiation points for this class are not provided in the current context, but it is typically instantiated by components that prepare data for function analysis tasks."
      error: null
    schemas.types.MethodContextInput:
      identifier: schemas.types.MethodContextInput
      description:
        overall: "The `MethodContextInput` class is a Pydantic BaseModel designed to encapsulate structured contextual information for a method. It serves as a data schema to store details such as the method's unique identifier, a list of other functions or methods it calls, a list of where it is called from, its arguments, and its docstring. This model provides a standardized format for representing method-level context, which is crucial for analysis and processing within a larger system."
        init_method:
          description: "The `__init__` method for `MethodContextInput` is implicitly generated by Pydantic's BaseModel. It initializes an instance of the class by accepting keyword arguments that correspond to its defined fields: `identifier`, `calls`, `called_by`, `args`, and `docstring`. Pydantic handles the validation and assignment of these values, ensuring they conform to their specified types."
          parameters[5]{name,type,description}:
            identifier,str,A unique string identifier for the method.
            calls,"List[str]","A list of strings representing other methods, classes, or functions called by this method."
            called_by,"List[CallInfo]",A list of `CallInfo` objects indicating where this method is called from.
            args,"List[str]",A list of strings representing the arguments of the method.
            docstring,"Optional[str]",An optional string containing the method's docstring.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies.
          instantiated_by: The specific instantiation points for this class are not provided in the current context.
      error: null
    schemas.types.ClassContextInput:
      identifier: schemas.types.ClassContextInput
      description:
        overall: "The `ClassContextInput` class is a Pydantic model designed to encapsulate structured context information relevant for analyzing a Python class. It serves as a data container, defining the expected structure for inputs related to class dependencies, instantiation points, and method-specific context. This model ensures that class analysis data adheres to a predefined schema."
        init_method:
          description: "The `__init__` method for `ClassContextInput` is implicitly generated by Pydantic's `BaseModel`. It initializes an instance of the class by validating and assigning values to its fields: `dependencies`, `instantiated_by`, and `method_context` based on the provided input during object creation."
          parameters[3]{name,type,description}:
            dependencies,"List[str]","A list of strings, each representing an external dependency of the class being analyzed."
            instantiated_by,"List[CallInfo]","A list of `CallInfo` objects, indicating the locations or contexts where the class being analyzed is instantiated."
            method_context,"List[MethodContextInput]","A list of `MethodContextInput` objects, providing specific context for each method within the class being analyzed."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies within its definition.
          instantiated_by: This class is not explicitly shown to be instantiated by any other components in the provided context.
      error: null
    schemas.types.ClassAnalysisInput:
      identifier: schemas.types.ClassAnalysisInput
      description:
        overall: "The ClassAnalysisInput class is a Pydantic BaseModel designed to define the structured input required for generating a ClassAnalysis object. It serves as a schema for validating and organizing the data necessary to perform a detailed analysis of a Python class. This includes the analysis mode, the class identifier, its source code, associated import statements, and contextual information."
        init_method:
          description: "This class does not explicitly define an `__init__` method. It inherits from Pydantic's `BaseModel`, and its initialization is handled implicitly by Pydantic based on the declared fields, ensuring type validation and data structuring upon instantiation."
          parameters[0]:
        methods[0]:
        usage_context:
          dependencies: "This class relies on `pydantic.BaseModel` for its core functionality and `typing.Literal` and `typing.List` for type hinting, but does not explicitly list other external functional dependencies."
          instantiated_by: "The instantiation points for this class are not specified in the provided context, but it is typically instantiated when validating input for a class analysis operation, likely by a data processing or API endpoint."
      error: null