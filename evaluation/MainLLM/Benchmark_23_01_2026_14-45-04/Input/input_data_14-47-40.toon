basic_info:
  projekt_uebersicht:
    titel: Repo Onboarding Agent üöÄ
    beschreibung: ```
    aktueller_status: Information not found
    key_features: Information not found
    tech_stack: Information not found
  installation:
    dependencies: "- altair==4.2.2\n- annotated-types==0.7.0\n- anyio==4.11.0\n- attrs==25.4.0\n- bcrypt==5.0.0\n- blinker==1.9.0\n- cachetools==6.2.2\n- captcha==0.7.1\n- certifi==2025.11.12\n- cffi==2.0.0\n- charset-normalizer==3.4.4\n- click==8.3.1\n- colorama==0.4.6\n- contourpy==1.3.3\n- cryptography==46.0.3\n- cycler==0.12.1\n- distro==1.9.0\n- dnspython==2.8.0\n- dotenv==0.9.9\n- entrypoints==0.4\n- extra-streamlit-components==0.1.81\n- filetype==1.2.0\n- fonttools==4.61.0\n- gitdb==4.0.12\n- GitPython==3.1.45\n- google-ai-generativelanguage==0.9.0\n- google-api-core==2.28.1\n- google-auth==2.43.0\n- googleapis-common-protos==1.72.0\n- grpcio==1.76.0\n- grpcio-status==1.76.0\n- h11==0.16.0\n- httpcore==1.0.9\n- httpx==0.28.1\n- idna==3.11\n- Jinja2==3.1.6\n- jiter==0.12.0\n- jsonpatch==1.33\n- jsonpointer==3.0.0\n- jsonschema==4.25.1\n- jsonschema-specifications==2025.9.1\n- kiwisolver==1.4.9\n- langchain==1.0.8\n- langchain-core==1.1.0\n- langchain-google-genai==3.1.0\n- langchain-ollama==1.0.0\n- langchain-openai==1.1.0\n- langgraph==1.0.3\n- langgraph-checkpoint==3.0.1\n- langgraph-prebuilt==1.0.5\n- langgraph-sdk==0.2.9\n- langsmith==0.4.46\n- MarkupSafe==3.0.3\n- matplotlib==3.10.7\n- narwhals==2.12.0\n- networkx==3.6\n- numpy==2.3.5\n- ollama==0.6.1\n- openai==2.8.1\n- orjson==3.11.4\n- ormsgpack==1.12.0\n- packaging==25.0\n- pandas==2.3.3\n- pillow==12.0.0\n- proto-plus==1.26.1\n- protobuf==6.33.1\n- pyarrow==21.0.0\n- pyasn1==0.6.1\n- pyasn1_modules==0.4.2\n- pycparser==2.23\n- pydantic==2.12.4\n- pydantic_core==2.41.5\n- pydeck==0.9.1\n- PyJWT==2.10.1\n- pymongo==4.15.4\n- pyparsing==3.2.5\n- python-dateutil==2.9.0.post0\n- python-dotenv==1.2.1\n- pytz==2025.2\n- PyYAML==6.0.3\n- referencing==0.37.0\n- regex==2025.11.3\n- requests==2.32.5\n- requests-toolbelt==1.0.0\n- rpds-py==0.29.0\n- rsa==4.9.1\n- setuptools==75.9.1\n- six==1.17.0\n- smmap==5.0.2\n- sniffio==1.3.1\n- streamlit==1.51.0\n- streamlit-authenticator==0.4.2\n- streamlit-mermaid==0.3.0\n- tenacity==9.1.2\n- tiktoken==0.12.0\n- toml==0.10.2\n- toolz==1.1.0\n- toon_format @ git+https://github.com/toon-format/toon-python.git@9c4f0c0c24f2a0b0b376315f4b8707f8c9006de6\n- tornado==6.5.2\n- tqdm==4.67.1\n- typing-inspection==0.4.2\n- typing_extensions==4.15.0\n- tzdata==2025.2\n- urllib3==2.5.0\n- watchdog==6.0.0\n- xxhash==3.6.0\n- zstandard==0.25.0\n- nbformat"
    setup_anleitung: Information not found
    quick_start_guide: Information not found
file_tree:
  name: root
  type: directory
  children[16]:
    - path: .env.example
      name: .env.example
      size: 48
      type: file
    - path: .gitignore
      name: .gitignore
      size: 184
      type: file
    - name: SystemPrompts
      type: directory
      children[6]{path,name,size,type}:
        SystemPrompts/SystemPromptClassHelperLLM.txt,SystemPromptClassHelperLLM.txt,6645,file
        SystemPrompts/SystemPromptFunctionHelperLLM.txt,SystemPromptFunctionHelperLLM.txt,5546,file
        SystemPrompts/SystemPromptHelperLLM.txt,SystemPromptHelperLLM.txt,9350,file
        SystemPrompts/SystemPromptMainLLM.txt,SystemPromptMainLLM.txt,8380,file
        SystemPrompts/SystemPromptMainLLMToon.txt,SystemPromptMainLLMToon.txt,8456,file
        SystemPrompts/SystemPromptNotebookLLM.txt,SystemPromptNotebookLLM.txt,6921,file
    - path: analysis_output.json
      name: analysis_output.json
      size: 569396
      type: file
    - name: backend
      type: directory
      children[12]{path,name,size,type}:
        backend/AST_Schema.py,AST_Schema.py,6622,file
        backend/File_Dependency.py,File_Dependency.py,7384,file
        backend/HelperLLM.py,HelperLLM.py,17280,file
        backend/MainLLM.py,MainLLM.py,4281,file
        backend/__init__.py,__init__.py,0,file
        backend/basic_info.py,basic_info.py,7284,file
        backend/callgraph.py,callgraph.py,9020,file
        backend/converter.py,converter.py,3594,file
        backend/getRepo.py,getRepo.py,4739,file
        backend/main.py,main.py,25591,file
        backend/relationship_analyzer.py,relationship_analyzer.py,8279,file
        backend/scads_key_test.py,scads_key_test.py,663,file
    - name: database
      type: directory
      children[1]{path,name,size,type}:
        database/db.py,db.py,8222,file
    - name: frontend
      type: directory
      children[4]:
        - name: .streamlit
          type: directory
          children[1]{path,name,size,type}:
            frontend/.streamlit/config.toml,config.toml,165,file
        - path: frontend/__init__.py
          name: __init__.py
          size: 0
          type: file
        - path: frontend/frontend.py
          name: frontend.py
          size: 31176
          type: file
        - name: gifs
          type: directory
          children[1]{path,name,size,type}:
            frontend/gifs/4j.gif,4j.gif,3306723,file
    - name: notizen
      type: directory
      children[8]:
        - path: notizen/Report Agenda.txt
          name: Report Agenda.txt
          size: 3492
          type: file
        - path: notizen/Zwischenpraesentation Agenda.txt
          name: Zwischenpraesentation Agenda.txt
          size: 809
          type: file
        - path: notizen/doc_bestandteile.md
          name: doc_bestandteile.md
          size: 847
          type: file
        - name: grafiken
          type: directory
          children[4]:
            - name: "1"
              type: directory
              children[5]{path,name,size,type}:
                notizen/grafiken/1/File_Dependency_Graph_Repo.dot,File_Dependency_Graph_Repo.dot,1227,file
                notizen/grafiken/1/global_callgraph.png,global_callgraph.png,940354,file
                notizen/grafiken/1/global_graph.png,global_graph.png,4756519,file
                notizen/grafiken/1/global_graph_2.png,global_graph_2.png,242251,file
                notizen/grafiken/1/repo.dot,repo.dot,13870,file
            - name: "2"
              type: directory
              children[12]{path,name,size,type}:
                notizen/grafiken/2/FDG_repo.dot,FDG_repo.dot,9150,file
                notizen/grafiken/2/fdg_graph.png,fdg_graph.png,425374,file
                notizen/grafiken/2/fdg_graph_2.png,fdg_graph_2.png,4744718,file
                notizen/grafiken/2/filtered_callgraph_flask.png,filtered_callgraph_flask.png,614631,file
                notizen/grafiken/2/filtered_callgraph_repo-agent.png,filtered_callgraph_repo-agent.png,280428,file
                notizen/grafiken/2/filtered_callgraph_repo-agent_3.png,filtered_callgraph_repo-agent_3.png,1685838,file
                notizen/grafiken/2/filtered_repo_callgraph_flask.dot,filtered_repo_callgraph_flask.dot,19984,file
                notizen/grafiken/2/filtered_repo_callgraph_repo-agent-3.dot,filtered_repo_callgraph_repo-agent-3.dot,20120,file
                notizen/grafiken/2/filtered_repo_callgraph_repo-agent.dot,filtered_repo_callgraph_repo-agent.dot,3118,file
                notizen/grafiken/2/global_callgraph.png,global_callgraph.png,608807,file
                notizen/grafiken/2/graph_flask.md,graph_flask.md,17398,file
                notizen/grafiken/2/repo.dot,repo.dot,19984,file
            - name: Flask-Repo
              type: directory
              children[65]{path,name,size,type}:
                notizen/grafiken/Flask-Repo/__init__.dot,__init__.dot,89,file
                notizen/grafiken/Flask-Repo/__main__.dot,__main__.dot,87,file
                notizen/grafiken/Flask-Repo/app.dot,app.dot,78,file
                notizen/grafiken/Flask-Repo/auth.dot,auth.dot,1126,file
                notizen/grafiken/Flask-Repo/blog.dot,blog.dot,939,file
                notizen/grafiken/Flask-Repo/blueprints.dot,blueprints.dot,4601,file
                notizen/grafiken/Flask-Repo/cli.dot,cli.dot,8269,file
                notizen/grafiken/Flask-Repo/conf.dot,conf.dot,489,file
                notizen/grafiken/Flask-Repo/config.dot,config.dot,2130,file
                notizen/grafiken/Flask-Repo/conftest.dot,conftest.dot,1756,file
                notizen/grafiken/Flask-Repo/ctx.dot,ctx.dot,3009,file
                notizen/grafiken/Flask-Repo/db.dot,db.dot,782,file
                notizen/grafiken/Flask-Repo/debughelpers.dot,debughelpers.dot,1903,file
                notizen/grafiken/Flask-Repo/factory.dot,factory.dot,220,file
                notizen/grafiken/Flask-Repo/flask.dot,flask.dot,82,file
                notizen/grafiken/Flask-Repo/globals.dot,globals.dot,370,file
                notizen/grafiken/Flask-Repo/hello.dot,hello.dot,152,file
                notizen/grafiken/Flask-Repo/helpers.dot,helpers.dot,2510,file
                notizen/grafiken/Flask-Repo/importerrorapp.dot,importerrorapp.dot,155,file
                notizen/grafiken/Flask-Repo/logging.dot,logging.dot,564,file
                notizen/grafiken/Flask-Repo/make_celery.dot,make_celery.dot,99,file
                notizen/grafiken/Flask-Repo/multiapp.dot,multiapp.dot,88,file
                notizen/grafiken/Flask-Repo/provider.dot,provider.dot,1769,file
                notizen/grafiken/Flask-Repo/scaffold.dot,scaffold.dot,3700,file
                notizen/grafiken/Flask-Repo/sessions.dot,sessions.dot,4178,file
                notizen/grafiken/Flask-Repo/signals.dot,signals.dot,133,file
                notizen/grafiken/Flask-Repo/tag.dot,tag.dot,3750,file
                notizen/grafiken/Flask-Repo/tasks.dot,tasks.dot,312,file
                notizen/grafiken/Flask-Repo/templating.dot,templating.dot,2277,file
                notizen/grafiken/Flask-Repo/test_appctx.dot,test_appctx.dot,2470,file
                notizen/grafiken/Flask-Repo/test_async.dot,test_async.dot,1919,file
                notizen/grafiken/Flask-Repo/test_auth.dot,test_auth.dot,725,file
                notizen/grafiken/Flask-Repo/test_basic.dot,test_basic.dot,17383,file
                notizen/grafiken/Flask-Repo/test_blog.dot,test_blog.dot,1105,file
                notizen/grafiken/Flask-Repo/test_blueprints.dot,test_blueprints.dot,11515,file
                notizen/grafiken/Flask-Repo/test_cli.dot,test_cli.dot,6435,file
                notizen/grafiken/Flask-Repo/test_config.dot,test_config.dot,2854,file
                notizen/grafiken/Flask-Repo/test_config.png,test_config.png,288693,file
                notizen/grafiken/Flask-Repo/test_converters.dot,test_converters.dot,937,file
                notizen/grafiken/Flask-Repo/test_db.dot,test_db.dot,481,file
                notizen/grafiken/Flask-Repo/test_factory.dot,test_factory.dot,201,file
                notizen/grafiken/Flask-Repo/test_helpers.dot,test_helpers.dot,5857,file
                notizen/grafiken/Flask-Repo/test_instance_config.dot,test_instance_config.dot,1249,file
                notizen/grafiken/Flask-Repo/test_js_example.dot,test_js_example.dot,453,file
                notizen/grafiken/Flask-Repo/test_json.dot,test_json.dot,4021,file
                notizen/grafiken/Flask-Repo/test_json_tag.dot,test_json_tag.dot,1452,file
                notizen/grafiken/Flask-Repo/test_logging.dot,test_logging.dot,1348,file
                notizen/grafiken/Flask-Repo/test_regression.dot,test_regression.dot,763,file
                notizen/grafiken/Flask-Repo/test_reqctx.dot,test_reqctx.dot,3809,file
                notizen/grafiken/Flask-Repo/test_request.dot,test_request.dot,668,file
                notizen/grafiken/Flask-Repo/test_session_interface.dot,test_session_interface.dot,667,file
                notizen/grafiken/Flask-Repo/test_signals.dot,test_signals.dot,1671,file
                notizen/grafiken/Flask-Repo/test_subclassing.dot,test_subclassing.dot,612,file
                notizen/grafiken/Flask-Repo/test_templating.dot,test_templating.dot,5129,file
                notizen/grafiken/Flask-Repo/test_testing.dot,test_testing.dot,4081,file
                notizen/grafiken/Flask-Repo/test_user_error_handler.dot,test_user_error_handler.dot,5984,file
                notizen/grafiken/Flask-Repo/test_views.dot,test_views.dot,2641,file
                notizen/grafiken/Flask-Repo/testing.dot,testing.dot,2785,file
                notizen/grafiken/Flask-Repo/typing.dot,typing.dot,86,file
                notizen/grafiken/Flask-Repo/typing_app_decorators.dot,typing_app_decorators.dot,500,file
                notizen/grafiken/Flask-Repo/typing_error_handler.dot,typing_error_handler.dot,420,file
                notizen/grafiken/Flask-Repo/typing_route.dot,typing_route.dot,1717,file
                notizen/grafiken/Flask-Repo/views.dot,views.dot,1125,file
                notizen/grafiken/Flask-Repo/wrappers.dot,wrappers.dot,911,file
                notizen/grafiken/Flask-Repo/wsgi.dot,wsgi.dot,19,file
            - name: Repo-onboarding
              type: directory
              children[15]{path,name,size,type}:
                notizen/grafiken/Repo-onboarding/AST.dot,AST.dot,1361,file
                notizen/grafiken/Repo-onboarding/Frontend.dot,Frontend.dot,538,file
                notizen/grafiken/Repo-onboarding/HelperLLM.dot,HelperLLM.dot,1999,file
                notizen/grafiken/Repo-onboarding/HelperLLM.png,HelperLLM.png,234861,file
                notizen/grafiken/Repo-onboarding/MainLLM.dot,MainLLM.dot,2547,file
                notizen/grafiken/Repo-onboarding/agent.dot,agent.dot,2107,file
                notizen/grafiken/Repo-onboarding/basic_info.dot,basic_info.dot,1793,file
                notizen/grafiken/Repo-onboarding/callgraph.dot,callgraph.dot,1478,file
                notizen/grafiken/Repo-onboarding/getRepo.dot,getRepo.dot,1431,file
                notizen/grafiken/Repo-onboarding/graph_AST.png,graph_AST.png,132743,file
                notizen/grafiken/Repo-onboarding/graph_AST2.png,graph_AST2.png,12826,file
                notizen/grafiken/Repo-onboarding/graph_AST3.png,graph_AST3.png,136016,file
                notizen/grafiken/Repo-onboarding/main.dot,main.dot,1091,file
                notizen/grafiken/Repo-onboarding/tools.dot,tools.dot,1328,file
                notizen/grafiken/Repo-onboarding/types.dot,types.dot,133,file
        - path: notizen/notizen.md
          name: notizen.md
          size: 4937
          type: file
        - path: notizen/paul_notizen.md
          name: paul_notizen.md
          size: 2033
          type: file
        - path: notizen/praesentation_notizen.md
          name: praesentation_notizen.md
          size: 2738
          type: file
        - path: notizen/technische_notizen.md
          name: technische_notizen.md
          size: 3616
          type: file
    - path: output.json
      name: output.json
      size: 454008
      type: file
    - path: output.toon
      name: output.toon
      size: 341854
      type: file
    - path: readme.md
      name: readme.md
      size: 1905
      type: file
    - path: requirements.txt
      name: requirements.txt
      size: 4286
      type: file
    - name: result
      type: directory
      children[28]{path,name,size,type}:
        result/ast_schema_01_12_2025_11-49-24.json,ast_schema_01_12_2025_11-49-24.json,201215,file
        result/notebook_report_23_12_2025_12-56-24_NotebookLLM_gemini-2.5-flash.md,notebook_report_23_12_2025_12-56-24_NotebookLLM_gemini-2.5-flash.md,15776,file
        result/notebook_report_27_12_2025_15-06-09_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-06-09_NotebookLLM_gemini-2.5-flash.md,14804,file
        result/notebook_report_27_12_2025_15-09-29_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-09-29_NotebookLLM_gemini-2.5-flash.md,17598,file
        result/notebook_report_27_12_2025_15-26-34_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-26-34_NotebookLLM_gemini-2.5-flash.md,20458,file
        result/notebook_report_27_12_2025_15-33-06_NotebookLLM_gemini-2.5-flash.md,notebook_report_27_12_2025_15-33-06_NotebookLLM_gemini-2.5-flash.md,15268,file
        result/notebook_report_29_12_2025_15-03-21_NotebookLLM_gemini-2.5-flash.md,notebook_report_29_12_2025_15-03-21_NotebookLLM_gemini-2.5-flash.md,16111,file
        result/report_01_12_2025_12-26-46_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_12-26-46_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,79523,file
        result/report_01_12_2025_12-55-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_12-55-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,143883,file
        result/report_01_12_2025_13-37-30_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_01_12_2025_13-37-30_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,141924,file
        result/report_01_12_2025_14-15-04_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_14-15-04_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,129176,file
        result/report_01_12_2025_14-42-38_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_14-42-38_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,38933,file
        result/report_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,report_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.md,138015,file
        result/report_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.md,report_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.md,127747,file
        result/report_03_12_2025_22-46-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_03_12_2025_22-46-01_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,80305,file
        result/report_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,report_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.md,163921,file
        result/report_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.md,report_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.md,128520,file
        result/report_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.md,report_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.md,7245,file
        result/report_14_11_2025_14-52-36.md,report_14_11_2025_14-52-36.md,16393,file
        result/report_14_11_2025_15-21-53.md,report_14_11_2025_15-21-53.md,108585,file
        result/report_14_11_2025_15-26-24.md,report_14_11_2025_15-26-24.md,11659,file
        result/report_21_11_2025_15-43-30.md,report_21_11_2025_15-43-30.md,57940,file
        result/report_21_11_2025_16-06-12.md,report_21_11_2025_16-06-12.md,76423,file
        result/report_22_11_2025_14-01-50_Helper_llama3_Main_geminipro.md,report_22_11_2025_14-01-50_Helper_llama3_Main_geminipro.md,39595,file
        result/report_22_11_2025_14-39-55_Helper_llama3_MainLLM_llama3.md,report_22_11_2025_14-39-55_Helper_llama3_MainLLM_llama3.md,2013,file
        result/result_2025-11-11_12-30-53.md,result_2025-11-11_12-30-53.md,1232,file
        result/result_2025-11-11_12-43-51.md,result_2025-11-11_12-43-51.md,33683,file
        result/result_2025-11-11_12-45-37.md,result_2025-11-11_12-45-37.md,1193,file
    - name: schemas
      type: directory
      children[1]{path,name,size,type}:
        schemas/types.py,types.py,7559,file
    - name: statistics
      type: directory
      children[5]{path,name,size,type}:
        statistics/savings_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.png,savings_01_12_2025_15-27-23_Helper_gemini-2.5-flash-lite_MainLLM_gemini-2.5-pro.png,23279,file
        statistics/savings_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.png,savings_02_12_2025_15-41-27_Helper_gemini-2.5-flash_MainLLM_gemini-2.5-pro.png,23650,file
        statistics/savings_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.png,savings_03_12_2025_23-13-20_Helper_gemini-flash-latest_MainLLM_gemini-2.5-pro.png,23805,file
        statistics/savings_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.png,savings_05_12_2025_11-07-10_Helper_alias-ha_MainLLM_gemini-2.5-pro.png,22698,file
        statistics/savings_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.png,savings_09_12_2025_14-07-49_Helper_alias-code_MainLLM_alias-ha.png,22539,file
    - path: test.json
      name: test.json
      size: 241669
      type: file
ast_schema:
  files:
    "backend/AST_Schema.py":
      ast_nodes:
        imports[3]: ast,os,getRepo.GitRepository
        functions[1]:
          - mode: function_analysis
            identifier: backend.AST_Schema.path_to_module
            name: path_to_module
            args[2]: filepath,project_root
            docstring: Wandelt einen Dateipfad in einen Python-Modulpfad um.
            source_code: "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    \n    module_path = rel_path.replace(os.path.sep, '.')\n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n    return module_path"
            start_line: 5
            end_line: 18
            context:
              calls[0]:
              called_by[0]:
        classes[2]:
          - mode: class_analysis
            identifier: backend.AST_Schema.ASTVisitor
            name: ASTVisitor
            docstring: null
            source_code: "class ASTVisitor(ast.NodeVisitor):\n    def __init__(self, source_code: str, file_path: str, project_root: str):\n        self.source_code = source_code\n        self.file_path = file_path\n        self.project_root = project_root\n        self.module_path = path_to_module(self.file_path, self.project_root)\n        self.schema = {\"imports\": [], \"functions\": [], \"classes\": []}\n        self._current_class = None\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(f\"{node.module}.{alias.name}\")\n        self.generic_visit(node)\n\n    def visit_ClassDef(self, node):\n        class_identifier = f\"{self.module_path}.{node.name}\"\n        class_info = {\n            \"mode\": \"class_analysis\",\n            \"identifier\": class_identifier,\n            \"name\": node.name,\n            \"docstring\": ast.get_docstring(node),\n            \"source_code\": ast.get_source_segment(self.source_code, node),\n            \"start_line\": node.lineno,\n            \"end_line\": node.end_lineno,\n            \"context\": {\n                \"dependencies\": [],\n                \"instantiated_by\": [],\n                \"method_context\": []\n            },   \n        }\n        self.schema[\"classes\"].append(class_info)\n        self._current_class = class_info \n        self.generic_visit(node)\n        self._current_class = None\n\n    def visit_FunctionDef(self, node):\n        if self._current_class:\n            method_identifier = f\"{self._current_class['identifier']}.{node.name}\"\n            method_context_info = {\n                \"identifier\": method_identifier,\n                \"name\": node.name,\n                \"calls\": [],\n                \"called_by\": [],\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n            }\n            self._current_class[\"context\"][\"method_context\"].append(method_context_info)\n        else:\n            func_identifier = f\"{self.module_path}.{node.name}\"\n            func_info = {\n                \"mode\": \"function_analysis\",\n                \"identifier\": func_identifier,\n                \"name\": node.name,\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"source_code\": ast.get_source_segment(self.source_code, node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n                \"context\": {\n                    \"calls\": [],\n                    \"called_by\": []\n                }\n            }\n            self.schema[\"functions\"].append(func_info)\n        self.generic_visit(node)\n    \n    def visit_AsyncFunctionDef(self, node):\n        self.visit_FunctionDef(node)"
            start_line: 20
            end_line: 94
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[6]:
                - identifier: backend.AST_Schema.ASTVisitor.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[4]: self,source_code,file_path,project_root
                  docstring: null
                  start_line: 21
                  end_line: 27
                - identifier: backend.AST_Schema.ASTVisitor.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 29
                  end_line: 32
                - identifier: backend.AST_Schema.ASTVisitor.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 34
                  end_line: 37
                - identifier: backend.AST_Schema.ASTVisitor.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 39
                  end_line: 58
                - identifier: backend.AST_Schema.ASTVisitor.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 60
                  end_line: 91
                - identifier: backend.AST_Schema.ASTVisitor.visit_AsyncFunctionDef
                  name: visit_AsyncFunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 93
                  end_line: 94
          - mode: class_analysis
            identifier: backend.AST_Schema.ASTAnalyzer
            name: ASTAnalyzer
            docstring: null
            source_code: "class ASTAnalyzer:\n\n    def __init__(self):\n        pass\n\n    def merge_relationship_data(self, full_schema: dict, raw_relationships: dict) -> dict:\n\n        outgoing_calls = raw_relationships.get(\"outgoing\", {})\n        incoming_calls = raw_relationships.get(\"incoming\", {})\n\n        for file_path, file_data in full_schema.get(\"files\", {}).items():\n            ast_nodes = file_data.get(\"ast_nodes\", {})\n            \n            for func in ast_nodes.get(\"functions\", []):\n                func_id = func.get(\"identifier\")\n                if func_id:\n                    func[\"context\"][\"calls\"] = outgoing_calls.get(func_id, [])\n                    func[\"context\"][\"called_by\"] = incoming_calls.get(func_id, [])\n\n            for cls in ast_nodes.get(\"classes\", []):\n                cls_id = cls.get(\"identifier\")\n                \n                if cls_id:\n                    cls[\"context\"][\"instantiated_by\"] = incoming_calls.get(cls_id, [])\n\n                class_dependencies = set()\n                \n                for method in cls[\"context\"].get(\"method_context\", []):\n                    method_id = method.get(\"identifier\")\n                    if method_id:\n                        m_calls = outgoing_calls.get(method_id, [])\n                        method[\"calls\"] = m_calls\n                        method[\"called_by\"] = incoming_calls.get(method_id, [])\n                        \n                        for callee in m_calls:\n                            if cls_id and not callee.startswith(f\"{cls_id}.\"):\n                                class_dependencies.add(callee)\n                \n                cls[\"context\"][\"dependencies\"] = sorted(list(class_dependencies))\n        \n        return full_schema\n\n    def analyze_repository(self, files: list, repo: GitRepository) -> dict:\n        full_schema = {\n            \"files\": {}\n        }\n\n        all_paths = [file_obj.path for file_obj in files]\n        if not all_paths:\n            return full_schema\n        \n        project_root = os.path.commonpath(all_paths)\n        if os.path.isfile(project_root):\n            project_root = os.path.dirname(project_root)\n\n        for file_obj in files:\n            if not file_obj.path.endswith('.py'):\n                continue\n            \n            file_content = file_obj.content\n            if not file_content.strip():\n                continue\n\n            try:\n                tree = ast.parse(file_content)\n                visitor = ASTVisitor(\n                    source_code=file_content, \n                    file_path=file_obj.path, \n                    project_root=project_root\n                )\n                visitor.visit(tree)\n                file_schema_nodes = visitor.schema\n\n                if file_schema_nodes[\"imports\"] or file_schema_nodes[\"functions\"] or file_schema_nodes[\"classes\"]:\n                    full_schema[\"files\"][file_obj.path] = {\n                        \"ast_nodes\": file_schema_nodes\n                    }\n\n            except (SyntaxError, ValueError) as e:\n                print(f\"Warnung: Konnte Datei '{file_obj.path}' nicht parsen. Fehler: {e}\")\n\n        return full_schema"
            start_line: 97
            end_line: 178
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[3]:
                - identifier: backend.AST_Schema.ASTAnalyzer.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 99
                  end_line: 100
                - identifier: backend.AST_Schema.ASTAnalyzer.merge_relationship_data
                  name: merge_relationship_data
                  calls[0]:
                  called_by[0]:
                  args[3]: self,full_schema,raw_relationships
                  docstring: null
                  start_line: 102
                  end_line: 137
                - identifier: backend.AST_Schema.ASTAnalyzer.analyze_repository
                  name: analyze_repository
                  calls[0]:
                  called_by[0]:
                  args[3]: self,files,repo
                  docstring: null
                  start_line: 139
                  end_line: 178
    "backend/File_Dependency.py":
      ast_nodes:
        imports[17]: networkx,os,ast.Assign,ast.AST,ast.ClassDef,ast.FunctionDef,ast.Import,ast.ImportFrom,ast.Name,ast.NodeVisitor,ast.literal_eval,ast.parse,ast.walk,keyword.iskeyword,pathlib.Path,getRepo.GitRepository,callgraph.make_safe_dot
        functions[3]:
          - mode: function_analysis
            identifier: backend.File_Dependency.build_file_dependency_graph
            name: build_file_dependency_graph
            args[3]: filename,tree,repo_root
            docstring: null
            source_code: "def build_file_dependency_graph(filename: str, tree: AST, repo_root: str) -> nx.DiGraph:\n    graph = nx.DiGraph()\n\n    tree_visitor = FileDependencyGraph(filename, repo_root)\n    tree_visitor.visit(tree)\n\n    for caller, callees in tree_visitor.import_dependencies.items():\n        graph.add_node(caller)\n        graph.add_nodes_from(callees)\n        for callee in callees:\n            graph.add_edge(caller, callee)\n    \n    return graph"
            start_line: 153
            end_line: 165
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.File_Dependency.build_repository_graph
            name: build_repository_graph
            args[1]: repository
            docstring: null
            source_code: "def build_repository_graph(repository: GitRepository) -> nx.DiGraph:\n    all_files = repository.get_all_files()\n    repo_root = repository.temp_dir\n    global_graph = nx.DiGraph()\n\n    for file in all_files: \n        if not file.path.endswith(\".py\"):\n            continue\n        filename = str(os.path.basename(file.path)).removesuffix(\".py\")\n        tree = parse(file.content)\n        graph = build_file_dependency_graph(filename, tree, repo_root)\n        \n        for node in graph.nodes:\n            global_graph.add_node(node)\n\n        for caller, callee in graph.edges:\n            if callee:\n                global_graph.add_node(callee)\n                global_graph.add_edge(caller, callee)\n    \n    return global_graph"
            start_line: 167
            end_line: 187
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.File_Dependency.get_all_temp_files
            name: get_all_temp_files
            args[1]: directory
            docstring: null
            source_code: "def get_all_temp_files(directory: str) -> list[Path]:\n    root_path = Path(directory).resolve()\n    all_files = [file.relative_to(root_path) for file in root_path.rglob(\"*.py\")]\n\n    return all_files"
            start_line: 189
            end_line: 193
            context:
              calls[0]:
              called_by[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.File_Dependency.FileDependencyGraph
            name: FileDependencyGraph
            docstring: null
            source_code: "class FileDependencyGraph(NodeVisitor):\n\n    import_dependencies: dict[str, set[str]] = {}\n    def __init__(self, filename: str, repo_root):\n        \"\"\"\n        Initialisiert den File Dependency Graphen\n\n        Args:\n\n        \"\"\"\n        self.filename = filename\n        self.repo_root = repo_root\n\n    def _resolve_module_name(self, node: ImportFrom) -> list[str]:\n        \"\"\"\n        L√∂st relative Imports der Form `from .. import name1, name2` auf.\n        Liefert die Liste der tats√§chlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\n        Wirft ImportError, wenn nichts aufgel√∂st werden konnte.\n        \"\"\"\n        level_depth = node.level\n        names = [alias.name for alias in node.names]  \n        all_files = get_all_temp_files(self.repo_root)  \n\n        candidates = [p for p in all_files if p.stem == Path(self.filename).stem or p.name == f\"{self.filename}.py\"]\n\n        if not candidates:\n            raise ImportError(f\"Kann aktuelle Datei '{self.filename}' im Repo nicht finden.\")\n\n\n        candidates.sort(key=lambda p: len(p.parts))\n        current_rel_path = candidates[0]\n        base_dir = current_rel_path.parent\n        if level_depth <= 0:\n            raise ImportError(\"Erwarteter relativer Import (level >= 1).\")\n        for _ in range(level_depth - 1):\n            if base_dir == base_dir.parent and len(base_dir.parts) == 0:\n                raise ImportError(f\"Relative Import-Ebene ({level_depth}) zu gro√ü f√ºr Datei '{current_rel_path}'.\")\n            base_dir = base_dir.parent\n\n        repo_root_path = Path(self.repo_root).resolve()\n        resolved: list[str] = []\n\n        def module_file_exists(rel_base: Path, name: str) -> bool:\n            file_path = repo_root_path / rel_base / f\"{name}.py\"\n            pkg_init = repo_root_path / rel_base / name / \"__init__.py\"\n            return file_path.exists() or pkg_init.exists()\n\n        def init_exports_symbol(rel_base: Path, symbol: str) -> bool:\n            \"\"\"\n            Pr√ºft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\n            oder als Name (funktion/klasse/assign) definiert ist.\n            \"\"\"\n            init_path = repo_root_path / rel_base / \"__init__.py\"\n            if not init_path.exists():\n                return False\n            try:\n                src = init_path.read_text(encoding=\"utf-8\")\n                mod = parse(src, filename=str(init_path))\n            except Exception:\n                return False\n\n            for node_ in walk(mod):\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == \"__all__\":\n                            try:\n                                value = literal_eval(node_.value)\n                                if isinstance(value, (list, tuple)) and symbol in value:\n                                    return True\n                            except Exception:\n                                pass\n                if isinstance(node_, (FunctionDef, ClassDef)) and node_.name == symbol:\n                    return True\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == symbol:\n                            return True\n            return False\n\n        for name in names:\n            if not name.isidentifier() or iskeyword(name):\n                continue\n\n            if module_file_exists(base_dir, name):\n                resolved.append(name)\n                continue\n            if init_exports_symbol(base_dir, name):\n                resolved.append(name)\n                continue\n\n        resolved = sorted(set(resolved))\n\n        if not resolved:\n            raise ImportError(\n                f\"Kein passendes Modul/Symbol f√ºr relative Import-Aufl√∂sung gefunden \"\n                f\"(level={level_depth}, names={names}, base_dir={base_dir})\"\n            )\n\n        return resolved\n    \n    def visit_Import(self, node: Import | ImportFrom, base_name: str | None = None):\n        for alias in node.names:\n        \n            if self.filename not in self.import_dependencies:\n                self.import_dependencies[self.filename] = set()\n\n            if base_name:\n                self.import_dependencies[self.filename].add(base_name)\n            else:\n                self.import_dependencies[self.filename].add(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node: ImportFrom):\n        \"\"\"\n        Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\n        also c, und dieser wird als callee f√ºr den caller, das File, gesetzt.\n        \"\"\"\n        module_name = node.module\n        if module_name:\n            module_base = module_name.split(\".\")[-1]\n            self.visit_Import(node, module_base)\n        else:\n            try:\n                resolved = self._resolve_module_name(node)\n                for base in resolved:\n                    self.visit_Import(node, base)\n            except ImportError as e:\n                print(f\"Aufl√∂sung eines relativen Imports fehlgeschlagen: {e}\")\n\n        self.generic_visit(node)"
            start_line: 22
            end_line: 151
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[6]:
                - identifier: backend.File_Dependency.FileDependencyGraph.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[3]: self,filename,repo_root
                  docstring: "Initialisiert den File Dependency Graphen\n\nArgs:"
                  start_line: 25
                  end_line: 33
                - identifier: backend.File_Dependency.FileDependencyGraph._resolve_module_name
                  name: _resolve_module_name
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: "L√∂st relative Imports der Form `from .. import name1, name2` auf.\nLiefert die Liste der tats√§chlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\nWirft ImportError, wenn nichts aufgel√∂st werden konnte."
                  start_line: 35
                  end_line: 120
                - identifier: backend.File_Dependency.FileDependencyGraph.module_file_exists
                  name: module_file_exists
                  calls[0]:
                  called_by[0]:
                  args[2]: rel_base,name
                  docstring: null
                  start_line: 64
                  end_line: 67
                - identifier: backend.File_Dependency.FileDependencyGraph.init_exports_symbol
                  name: init_exports_symbol
                  calls[0]:
                  called_by[0]:
                  args[2]: rel_base,symbol
                  docstring: "Pr√ºft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\noder als Name (funktion/klasse/assign) definiert ist."
                  start_line: 69
                  end_line: 99
                - identifier: backend.File_Dependency.FileDependencyGraph.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[3]: self,node,base_name
                  docstring: null
                  start_line: 122
                  end_line: 132
                - identifier: backend.File_Dependency.FileDependencyGraph.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: "Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\nalso c, und dieser wird als callee f√ºr den caller, das File, gesetzt."
                  start_line: 134
                  end_line: 151
    "backend/HelperLLM.py":
      ast_nodes:
        imports[23]: os,json,logging,time,typing.List,typing.Dict,typing.Any,typing.Optional,typing.Union,dotenv.load_dotenv,langchain_google_genai.ChatGoogleGenerativeAI,langchain_ollama.ChatOllama,langchain_openai.ChatOpenAI,langchain.messages.HumanMessage,langchain.messages.SystemMessage,langchain.messages.AIMessage,pydantic.ValidationError,schemas.types.FunctionAnalysis,schemas.types.ClassAnalysis,schemas.types.FunctionAnalysisInput,schemas.types.FunctionContextInput,schemas.types.ClassAnalysisInput,schemas.types.ClassContextInput
        functions[1]:
          - mode: function_analysis
            identifier: backend.HelperLLM.main_orchestrator
            name: main_orchestrator
            args[0]:
            docstring: "Dummy Data and processing loop for testing the LLMHelper class.\nThis version is syntactically correct and logically matches the Pydantic models."
            source_code: "def main_orchestrator():\n    \"\"\"\n    Dummy Data and processing loop for testing the LLMHelper class.\n    This version is syntactically correct and logically matches the Pydantic models.\n    \"\"\"\n    \n    # --- Step 1: Define the pre-computed analysis for each method ---\n    \n    # Example Input 1: For the 'add_item' function\n    add_item_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"source_code\": \"\"\"def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\\\"\\\"\\\"\n        if not isinstance(quantity, int) or quantity <= 0:\n            raise ValueError(\"Quantity must be a positive integer.\")\n    \n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n        print(f\"Added {quantity} of {item_name}. New total: {self.inventory[item_name]}\")\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"process_shipment\", \"restock_api_endpoint\"]\n        }\n    })\n\n    # Example Input 2: For the 'check_stock' function\n    check_stock_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"source_code\": \"\"\"def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Retrieves the current stock quantity for a given item.\\\"\\\"\\\"\n        stock_level = self.inventory.get(item_name, 0)\n        return stock_level\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"fulfill_order\", \"ui_display_handler\"]\n        }\n    })\n\n    # Example Input 3: For the 'generate_report' function\n    generate_report_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"source_code\": \"\"\"def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a timestamped string summary of the current inventory state.\\\"\\\"\\\"\n        timestamp = datetime.now()\n        report_header = \"Inventory Report as of: {ts}\\\\n\".format(ts=timestamp)\n        report_lines = [report_header]\n        \n        for item, quantity in self.inventory.items():\n            line = \"- {name}: {qty}\".format(name=item, qty=quantity)\n            report_lines.append(line)\n            \n        return \"\\\\n\".join(report_lines)\"\"\",\n        \"imports\": [\"from datetime import datetime\"],\n        \"context\": {\n            \"calls\": [\"datetime.now\", \"str.format\"],\n            \"called_by\": [\"daily_cron_job\", \"admin_dashboard_export\"]\n        }\n    })\n\n    add_item_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"description\": {\n            \"overall\": \"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to add.\"},\n                {\"name\": \"quantity\", \"type\": \"int\", \"description\": \"The number of units to add. Must be a positive integer.\"}\n            ],\n            \"returns\": [],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"process_shipment and restock_api_endpoint\" }\n        },\n        \"error\": None\n    })\n\n    check_stock_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"description\": {\n            \"overall\": \"Retrieves the current stock quantity for a given item.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to check.\"}\n            ],\n            \"returns\": [\n                {\"name\": \"stock_level\", \"type\": \"int\", \"description\": \"The quantity of the item in stock. Returns 0 if the item is not found.\"}\n            ],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"fulfill_order and ui_display_handler\" }\n        },\n        \"error\": None\n    })\n\n    generate_report_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"description\": {\n            \"overall\": \"Generates a timestamped string summary of the current inventory state.\",\n            \"parameters\": [ {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"} ],\n            \"returns\": [\n                {\"name\": \"report\", \"type\": \"str\", \"description\": \"A formatted string detailing all items and their quantities.\"}\n            ],\n            \"usage_context\": { \"calls\": \"datetime.now and str.format\", \"called_by\": \"daily_cron_job and admin_dashboard_export\" }\n        },\n        \"error\": None\n    })\n\n    class_input = ClassAnalysisInput(\n        mode=\"class_analysis\",\n        identifier=\"InventoryManager\",\n        # NOTE: The indentation of this string is now fixed. It starts at column 0.\n        source_code=\"\"\"class InventoryManager:\n    \\\"\\\"\\\"Manages stock levels for products in a warehouse.\\\"\\\"\\\"\n    def __init__(self, warehouse_id: str):\n        self.warehouse_id = warehouse_id\n        self.inventory = {}  # item_name: quantity\n\n    def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds an item to the inventory.\\\"\\\"\\\"\n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n\n    def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Checks the stock of a specific item.\\\"\\\"\\\"\n        return self.inventory.get(item_name, 0)\n\n    def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a summary report of the inventory.\\\"\\\"\\\"\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        report_lines = [f\"Inventory Report for {self.warehouse_id} at {timestamp}\"]\n        if not self.inventory:\n            report_lines.append(\"Inventory is empty.\")\n        else:\n            for item, quantity in self.inventory.items():\n                report_lines.append(f\"- {item}: {quantity}\")\n        return \"\\\\n\".join(report_lines)\n\"\"\",\n        imports=[\"from datetime import datetime\"],\n        context=ClassContextInput(\n            dependencies=[\"datetime\"],\n            instantiated_by=[\"main_app_startup\", \"warehouse_worker_script\"],\n            methods_analysis=[\n                add_item_analysis,\n                check_stock_analysis,\n                generate_report_analysis\n            ]\n        )\n    )\n\n    \n    # The helper methods expect a LIST of inputs, so we wrap our single object in a list.\n    input = [add_item_input, check_stock_input, generate_report_input]\n    analysis = [add_item_analysis, check_stock_analysis, generate_report_analysis] \n\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    llm_helper = LLMHelper(api_key=GEMINI_API_KEY, function_prompt_path=function_prompt_file, class_prompt_path=class_prompt_file)\n\n    # This will be the final JSON containing all documentation\n    final_documentation = {}\n\n    # The analysis of the methods is already provided in the context.\n    logging.info(\"\\n--- Generating documentation for classes ---\")\n    \n    # The `generate_for_classes` method returns a list of results.\n    analysis_results = llm_helper.generate_for_functions(input)\n\n    # --- Step 4: Process the results ---\n    \n    # We loop through the list of results (even though there's only one in this case).\n    for doc in analysis_results:\n        if doc:\n            logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n            if \"classes\" not in final_documentation:\n                final_documentation[\"classes\"] = {}\n            # Use .model_dump() to convert the Pydantic object back to a dict for JSON serialization\n            final_documentation[\"classes\"][doc.identifier] = doc.model_dump() \n        else:\n            logging.warning(f\"Failed to generate doc for a class\")\n\n    # --- Step 5: Display the final aggregated result ---\n    logging.info(\"\\n--- Final Generated Documentation ---\")\n    print(json.dumps(final_documentation, indent=2))"
            start_line: 227
            end_line: 413
            context:
              calls[0]:
              called_by[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.HelperLLM.LLMHelper
            name: LLMHelper
            docstring: "A class to interact with Google Gemini for generating code snippet documentation.\nIt centralizes API interaction, error handling, and validates I/O using Pydantic."
            source_code: "class LLMHelper:\n    \"\"\"\n    A class to interact with Google Gemini for generating code snippet documentation.\n    It centralizes API interaction, error handling, and validates I/O using Pydantic.\n    \"\"\"\n    def __init__(self, api_key: str, function_prompt_path: str, class_prompt_path: str, model_name: str = \"gemini-2.0-flash-lite\", base_url: str = None):\n        if not api_key:\n            raise ValueError(\"Gemini API Key must be set.\")\n        \n        try:\n            with open(function_prompt_path, 'r', encoding='utf-8') as f:\n                self.function_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Function system prompt file not found at: {function_prompt_path}\")\n            raise\n\n        # Handle the second file\n        try:\n            with open(class_prompt_path, 'r', encoding='utf-8') as f:\n                self.class_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Class system prompt file not found at: {class_prompt_path}\")\n            raise\n        \n        # Batch-Size config\n        self.model_name = model_name\n        self._configure_batch_settings(model_name)\n\n\n        if model_name.startswith(\"gemini-\"):\n            base_llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=0.3, \n            )\n        \n        elif model_name.startswith(\"gpt-\") and \"openGPT\" not in model_name:\n            base_llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=0.3,\n            )\n\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            if not SCADSLLM_URL:\n                raise ValueError(f\"SCADSLLM_URL environment variable is required for model {model_name}\")\n            \n            logging.info(f\"Connecting to Custom API at {SCADSLLM_URL} for model {model_name}\")\n            \n            base_llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                base_url=SCADSLLM_URL,\n                temperature=0.3,\n            )\n\n        else:\n            target_url = base_url if base_url else OLLAMA_BASE_URL\n            logging.info(f\"Using Ollama at {target_url} for model {model_name}\")\n            base_llm = ChatOllama(\n                model=model_name,\n                temperature=0.3,\n                base_url=target_url,\n            )\n\n        self.function_llm = base_llm.with_structured_output(FunctionAnalysis, method=\"json_schema\")\n        self.class_llm = base_llm.with_structured_output(ClassAnalysis, method=\"json_schema\")\n\n        self.raw_llm = base_llm\n\n        logging.info(f\"LLMHelper initialized with model '{model_name}'. Batch Size: {self.batch_size}\")\n\n    def _configure_batch_settings(self, model_name: str):\n\n        if model_name == \"gemini-2.0-flash-lite\":\n            self.batch_size = 30\n        \n        elif model_name == \"gemini-flash-latest\":\n            self.batch_size = 10\n\n        elif model_name == \"gemini-2.5-flash\":\n            self.batch_size = 10\n            \n        elif model_name == \"gemini-2.5-pro\":\n            self.batch_size = 2\n        \n        elif model_name == \"gemini-2.0-flash\":\n            self.batch_size = 15\n\n        elif model_name == \"gemini-2.5-flash-lite\":\n            self.batch_size = 15\n\n        elif model_name == \"llama3\":\n            self.batch_size = 50\n\n        elif model_name == \"gpt-5.1\":\n            self.batch_size = 500\n\n        elif model_name == \"gpt-5-mini\":\n            self.batch_size = 500\n\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            self.batch_size = 500\n            \n        else:\n            logging.warning(f\"Unknown model '{model_name}', using conservative defaults.\")\n            self.batch_size = 2\n\n    def generate_for_functions(self, function_inputs: List[FunctionAnalysisInput]) -> List[Optional[FunctionAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of functions.\"\"\"\n\n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 62\n\n        if not function_inputs:\n            return []\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(function_input.model_dump(), indent=2) \n            for function_input in function_inputs\n        ]\n       \n        conversations = [[SystemMessage(content=self.function_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_functions = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n        \n            try:\n                batch_results = self.function_llm.batch(batch_conversations, config={\"max_concurrency\": self.batch_size})\n                all_validated_functions.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                all_validated_functions.extend([None] * len(batch_conversations))\n            \n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_functions\n        \n            \n    \n\n    def generate_for_classes(self, class_inputs: List[ClassAnalysisInput]) -> List[Optional[ClassAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of classes.\"\"\"\n        if not class_inputs:\n            return []\n        \n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 62\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(class_input.model_dump(), indent=2)\n            for class_input in class_inputs\n        ]\n\n        conversations = [[SystemMessage(content=self.class_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_classes = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n\n            try:\n                batch_results = self.class_llm.batch(batch_conversations , config={\"max_concurrency\": self.batch_size})\n                all_validated_classes.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                # Falls ein Fehler auftritt, f√ºllen wir die Liste mit None auf, um die Reihenfolge zu wahren\n                all_validated_classes.extend([None] * len(batch_conversations))\n\n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_classes"
            start_line: 31
            end_line: 221
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[4]:
                - identifier: backend.HelperLLM.LLMHelper.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[6]: self,api_key,function_prompt_path,class_prompt_path,model_name,base_url
                  docstring: null
                  start_line: 36
                  end_line: 101
                - identifier: backend.HelperLLM.LLMHelper._configure_batch_settings
                  name: _configure_batch_settings
                  calls[0]:
                  called_by[0]:
                  args[2]: self,model_name
                  docstring: null
                  start_line: 103
                  end_line: 137
                - identifier: backend.HelperLLM.LLMHelper.generate_for_functions
                  name: generate_for_functions
                  calls[0]:
                  called_by[0]:
                  args[2]: self,function_inputs
                  docstring: Generates and validates documentation for a batch of functions.
                  start_line: 139
                  end_line: 178
                - identifier: backend.HelperLLM.LLMHelper.generate_for_classes
                  name: generate_for_classes
                  calls[0]:
                  called_by[0]:
                  args[2]: self,class_inputs
                  docstring: Generates and validates documentation for a batch of classes.
                  start_line: 183
                  end_line: 221
    "backend/MainLLM.py":
      ast_nodes:
        imports[9]: os,logging,sys,dotenv.load_dotenv,langchain_google_genai.ChatGoogleGenerativeAI,langchain_ollama.ChatOllama,langchain_openai.ChatOpenAI,langchain.messages.HumanMessage,langchain.messages.SystemMessage
        functions[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.MainLLM.MainLLM
            name: MainLLM
            docstring: Hauptklasse f√ºr die Interaktion mit dem LLM.
            source_code: "class MainLLM:\n    \"\"\"\n    Hauptklasse f√ºr die Interaktion mit dem LLM.\n    \"\"\"\n    def __init__(self, api_key: str, prompt_file_path: str, model_name: str = \"gemini-2.5-pro\", base_url: str = None):\n        if not api_key:\n            raise ValueError(\"Gemini API Key must be set.\")\n        \n        try:\n            with open(prompt_file_path, 'r', encoding='utf-8') as f:\n                self.system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"System prompt file not found at: {prompt_file_path}\")\n            raise\n        \n        self.model_name = model_name\n\n        if model_name.startswith(\"gemini-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n\n        elif model_name.startswith(\"gpt-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n        elif \"/\" in model_name or model_name.startswith(\"alias-\") or any(x in model_name for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n            if not SCADSLLM_URL:\n                raise ValueError(f\"SCADSLLM_URL environment variable is required for model {model_name}\")\n            \n            logging.info(f\"Connecting to Custom API at {SCADSLLM_URL} for model {model_name}\")\n            \n            self.llm = ChatOpenAI(\n                model=model_name,\n                api_key=api_key,\n                base_url=SCADSLLM_URL,\n                temperature=1.0,\n            )\n\n        else:\n            target_url = base_url if base_url else OLLAMA_BASE_URL\n            self.llm = ChatOllama(\n                model=model_name,\n                temperature=1.0,\n                base_url=target_url,\n            )\n\n        logging.info(f\"Main LLM initialized with model '{model_name}'.\")\n    \n    def call_llm(self, user_input: str):\n\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with HelperLLM input...\")\n\n        try:\n            response = self.llm.invoke(messages)\n            logging.info(\"LLM call successful.\")\n            return response.content\n        except Exception as e:\n            logging.error(f\"Error during LLM call: {e}\")\n            return None\n\n    def stream_llm(self, user_input: str):\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with 'stream'...\")\n\n        try:\n            stream_iterator = self.llm.stream(messages)\n            \n            for chunk in stream_iterator:\n                yield chunk.content\n        except Exception as e:\n            error_message = f\"\\n--- Error during LLM stream call: {e} ---\"\n            logging.error(error_message)\n            yield error_message"
            start_line: 22
            end_line: 106
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[3]:
                - identifier: backend.MainLLM.MainLLM.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[5]: self,api_key,prompt_file_path,model_name,base_url
                  docstring: null
                  start_line: 26
                  end_line: 73
                - identifier: backend.MainLLM.MainLLM.call_llm
                  name: call_llm
                  calls[0]:
                  called_by[0]:
                  args[2]: self,user_input
                  docstring: null
                  start_line: 75
                  end_line: 89
                - identifier: backend.MainLLM.MainLLM.stream_llm
                  name: stream_llm
                  calls[0]:
                  called_by[0]:
                  args[2]: self,user_input
                  docstring: null
                  start_line: 91
                  end_line: 106
    "backend/basic_info.py":
      ast_nodes:
        imports[7]: re,os,tomllib,typing.List,typing.Dict,typing.Any,typing.Optional
        functions[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.basic_info.ProjektInfoExtractor
            name: ProjektInfoExtractor
            docstring: "Extrahiert grundlegende Projektinformationen aus g√§ngigen Projektdateien\nwie README, pyproject.toml und requirements.txt."
            source_code: "class ProjektInfoExtractor:\n    \"\"\"\n    Extrahiert grundlegende Projektinformationen aus g√§ngigen Projektdateien\n    wie README, pyproject.toml und requirements.txt.\n    \"\"\"\n    def __init__(self):\n        self.INFO_NICHT_GEFUNDEN = \"Information not found\"\n        # Initialisiert die Struktur mit Platzhaltern\n        self.info = {\n            \"projekt_uebersicht\": {\n                \"titel\": self.INFO_NICHT_GEFUNDEN,\n                \"beschreibung\": self.INFO_NICHT_GEFUNDEN,\n                \"aktueller_status\": self.INFO_NICHT_GEFUNDEN,\n                \"key_features\": self.INFO_NICHT_GEFUNDEN,\n                \"tech_stack\": self.INFO_NICHT_GEFUNDEN,\n            },\n            \"installation\": {\n                \"dependencies\": self.INFO_NICHT_GEFUNDEN,\n                \"setup_anleitung\": self.INFO_NICHT_GEFUNDEN,\n                \"quick_start_guide\": self.INFO_NICHT_GEFUNDEN,\n            }\n        }\n\n    def _clean_content(self, content: str) -> str:\n        \"\"\"Entfernt Null-Bytes, die durch Encoding-Fehler (UTF-16 als UTF-8 gelesen) entstehen.\"\"\"\n        if not content:\n            return \"\"\n        return content.replace('\\x00', '')\n\n    def _finde_datei(self, patterns: List[str], dateien: List[Any]) -> Optional[Any]:\n        \"\"\"Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht.\"\"\"\n        for datei in dateien:\n            for pattern in patterns:\n                if datei.path.lower().endswith(pattern.lower()):\n                    return datei\n        return None\n\n    def _extrahiere_sektion_aus_markdown(self, inhalt: str, keywords: List[str]) -> Optional[str]:\n        \"\"\"\n        Extrahiert den Text unter einer Markdown-√úberschrift (##).\n        \"\"\"\n        # Erstellt ein Regex-Pattern, das auf jedes der Schl√ºsselw√∂rter reagiert\n        keyword_pattern = \"|\".join(re.escape(k) for k in keywords)\n        \n        # Sucht nach \"## Schl√ºsselwort\" und erfasst alles bis zur n√§chsten \"##\" oder dem Dateiende\n        pattern = re.compile(\n            rf\"##\\s*({keyword_pattern})\\s*\\n(.*?)(?=\\n##|\\Z)\",\n            re.IGNORECASE | re.DOTALL\n        )\n        match = pattern.search(inhalt)\n        if match:\n            return match.group(2).strip()\n        return None\n\n    def _parse_readme(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer README-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        if self.info[\"projekt_uebersicht\"][\"titel\"] == self.INFO_NICHT_GEFUNDEN:\n            title_match = re.search(r\"^\\s*#\\s*(.*)\", inhalt)\n            if title_match:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = title_match.group(1).strip()\n\n        # Beschreibung (Fallback)\n        if self.info[\"projekt_uebersicht\"][\"beschreibung\"] == self.INFO_NICHT_GEFUNDEN:\n            desc_match = re.search(r\"^\\s*#\\s*.*\\n+([^#\\n].*)\", inhalt, re.DOTALL)\n            if desc_match:\n                 self.info[\"projekt_uebersicht\"][\"beschreibung\"] = desc_match.group(1).strip().split('\\n\\n')[0]\n\n        # Key Features\n        features = self._extrahiere_sektion_aus_markdown(inhalt, [\"Features\", \"Key Features\", \"Merkmale\"])\n        if features:\n            self.info[\"projekt_uebersicht\"][\"key_features\"] = features\n            \n        # Tech Stack\n        tech_stack = self._extrahiere_sektion_aus_markdown(inhalt, [\"Tech Stack\", \"Technology\", \"Technologien\"])\n        if tech_stack:\n            self.info[\"projekt_uebersicht\"][\"tech_stack\"] = tech_stack\n            \n        # Status\n        status = self._extrahiere_sektion_aus_markdown(inhalt, [\"Status\", \"Current Status\"])\n        if status:\n            self.info[\"projekt_uebersicht\"][\"aktueller_status\"] = status\n\n        # Setup-Anleitung\n        setup = self._extrahiere_sektion_aus_markdown(inhalt, [\"Installation\", \"Setup\", \"Getting Started\"])\n        if setup:\n            self.info[\"installation\"][\"setup_anleitung\"] = setup\n            \n        # Quick Start\n        quick_start = self._extrahiere_sektion_aus_markdown(inhalt, [\"Quick Start\", \"Schnellstart\"])\n        if quick_start:\n            self.info[\"installation\"][\"quick_start_guide\"] = quick_start\n\n    def _parse_toml(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer pyproject.toml-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        if not tomllib:\n            print(\"Warnung: 'tomli' ist nicht installiert. pyproject.toml kann nicht analysiert werden.\")\n            return\n            \n        try:\n            data = tomllib.loads(inhalt)\n            project_data = data.get(\"project\", {})\n            \n            if \"name\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = project_data[\"name\"]\n            if \"description\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"beschreibung\"] = project_data[\"description\"]\n            if \"dependencies\" in project_data:\n                self.info[\"installation\"][\"dependencies\"] = project_data[\"dependencies\"]\n        except tomllib.TOMLDecodeError as e:\n            print(f\"Warnung: Fehler beim Parsen der pyproject.toml: {e}\")\n\n    def _parse_requirements(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer requirements.txt-Datei.\"\"\"\n        inhalt = self._clean_content(inhalt)\n\n        # Nur f√ºllen, wenn noch keine Dependencies aus toml gefunden wurden\n        if self.info[\"installation\"][\"dependencies\"] == self.INFO_NICHT_GEFUNDEN:\n            lines = inhalt.splitlines()\n            dependencies = [\n                line.strip() for line in lines \n                if line.strip() and not line.strip().startswith('#')\n            ]\n            if dependencies:\n                self.info[\"installation\"][\"dependencies\"] = dependencies\n\n    def extrahiere_info(self, dateien: List[Any], repo_url: str) -> Dict[str, Any]:\n        \"\"\"\n        Orchestriert die Extraktion von Informationen.\n        \"\"\"\n        # Relevante Dateien finden\n        readme_datei = self._finde_datei([\"readme.md\", \"readme.rst\", \"readme.txt\", \"readme\"], dateien)\n        toml_datei = self._finde_datei([\"pyproject.toml\"], dateien)\n        req_datei = self._finde_datei([\"requirements.txt\"], dateien)\n\n        # Dateien parsen (mit Priorisierung)\n        if toml_datei:\n            self._parse_toml(toml_datei.content)\n\n        if req_datei:\n            self._parse_requirements(req_datei.content)\n            \n        if readme_datei:\n            self._parse_readme(readme_datei.content)\n            \n        # Finale Formatierung der Dependencies\n        deps = self.info[\"installation\"][\"dependencies\"]\n        if isinstance(deps, list):\n            if not deps:\n                self.info[\"installation\"][\"dependencies\"] = self.INFO_NICHT_GEFUNDEN\n            else:\n                self.info[\"installation\"][\"dependencies\"] = \"\\n\".join(f\"- {dep}\" for dep in deps)\n        \n        # Titel aus URL ableiten, falls nichts gefunden wurde (oder als Zusatzinfo)\n        if repo_url:\n            repo_name = os.path.basename(repo_url.removesuffix('.git'))\n            # Wenn noch kein Titel da ist oder er generisch war\n            if self.info[\"projekt_uebersicht\"][\"titel\"] == self.INFO_NICHT_GEFUNDEN:\n                 self.info[\"projekt_uebersicht\"][\"titel\"] = f\"{repo_name} documentation\"\n\n        return self.info"
            start_line: 9
            end_line: 172
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[8]:
                - identifier: backend.basic_info.ProjektInfoExtractor.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 14
                  end_line: 30
                - identifier: backend.basic_info.ProjektInfoExtractor._clean_content
                  name: _clean_content
                  calls[0]:
                  called_by[0]:
                  args[2]: self,content
                  docstring: "Entfernt Null-Bytes, die durch Encoding-Fehler (UTF-16 als UTF-8 gelesen) entstehen."
                  start_line: 32
                  end_line: 36
                - identifier: backend.basic_info.ProjektInfoExtractor._finde_datei
                  name: _finde_datei
                  calls[0]:
                  called_by[0]:
                  args[3]: self,patterns,dateien
                  docstring: "Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht."
                  start_line: 38
                  end_line: 44
                - identifier: backend.basic_info.ProjektInfoExtractor._extrahiere_sektion_aus_markdown
                  name: _extrahiere_sektion_aus_markdown
                  calls[0]:
                  called_by[0]:
                  args[3]: self,inhalt,keywords
                  docstring: Extrahiert den Text unter einer Markdown-√úberschrift (##).
                  start_line: 46
                  end_line: 61
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_readme
                  name: _parse_readme
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer README-Datei.
                  start_line: 63
                  end_line: 101
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_toml
                  name: _parse_toml
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer pyproject.toml-Datei.
                  start_line: 103
                  end_line: 122
                - identifier: backend.basic_info.ProjektInfoExtractor._parse_requirements
                  name: _parse_requirements
                  calls[0]:
                  called_by[0]:
                  args[2]: self,inhalt
                  docstring: Parst den Inhalt einer requirements.txt-Datei.
                  start_line: 124
                  end_line: 136
                - identifier: backend.basic_info.ProjektInfoExtractor.extrahiere_info
                  name: extrahiere_info
                  calls[0]:
                  called_by[0]:
                  args[3]: self,dateien,repo_url
                  docstring: Orchestriert die Extraktion von Informationen.
                  start_line: 138
                  end_line: 172
    "backend/callgraph.py":
      ast_nodes:
        imports[9]: ast,networkx,os,pathlib.Path,typing.Dict,getRepo.GitRepository,getRepo.GitRepository,basic_info.ProjektInfoExtractor,os
        functions[2]:
          - mode: function_analysis
            identifier: backend.callgraph.make_safe_dot
            name: make_safe_dot
            args[2]: graph,out_path
            docstring: null
            source_code: "def make_safe_dot(graph: nx.DiGraph, out_path: str):\n    mapping = {}\n    H = graph.copy()\n    for i, n in enumerate(list(graph.nodes())):\n        safe = f\"n{i}\"           \n        mapping[n] = safe\n    H = nx.relabel_nodes(H, mapping)\n    for orig, safe in mapping.items():\n        H.nodes[safe][\"label\"] = orig\n    nx.drawing.nx_pydot.write_dot(H, out_path)"
            start_line: 173
            end_line: 182
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.callgraph.build_filtered_callgraph
            name: build_filtered_callgraph
            args[1]: repo
            docstring: Baut den globalen Call-Graphen und filtert ihn auf selbst geschriebene Funktionen.
            source_code: "def build_filtered_callgraph(repo: GitRepository) -> nx.DiGraph:\n    \"\"\"\n    Baut den globalen Call-Graphen und filtert ihn auf selbst geschriebene Funktionen.\n    \"\"\"\n    all_file_objects = repo.get_all_files()\n    own_functions = set()\n    file_trees = {}\n\n    for file in all_file_objects:\n        if not file.path.endswith(\".py\"):\n            continue\n        filename = Path(file.path).stem\n        tree = ast.parse(file.content)\n        file_trees[filename] = tree\n        visitor = CallGraph(filename)\n        visitor.visit(tree)\n        own_functions.update(visitor.function_set)\n\n    # Globalen Call-Graph bauen\n    global_graph = nx.DiGraph()\n    for filename, tree in file_trees.items():\n        visitor = CallGraph(filename)\n        visitor.visit(tree)\n        for caller, callees in visitor.edges.items():\n            if caller not in own_functions:\n                continue \n            for callee in callees:\n                if callee in own_functions:\n                    global_graph.add_edge(caller, callee)\n                    global_graph.add_node(caller)\n                    global_graph.add_node(callee)\n    return global_graph"
            start_line: 185
            end_line: 216
            context:
              calls[0]:
              called_by[0]:
        classes[1]:
          - mode: class_analysis
            identifier: backend.callgraph.CallGraph
            name: CallGraph
            docstring: null
            source_code: "class CallGraph(ast.NodeVisitor):\n    def __init__(self, filename: str):\n        self.filename = filename\n        self.current_function = None\n        self.current_class = None\n        \n        self.local_defs: dict[str, str] = {}\n        self.graph = nx.DiGraph()\n        self.import_mapping: dict[str, str] = {}\n        self.function_set: set[str] = set()\n        self.edges: Dict[str, set[str]] = {}\n\n    def _recursive_call(self, node):\n        \"\"\"\n        Liefert eine Liste der Namenskomponenten als dotted string z.B. ['pkg', 'mod', 'Class', 'method']\n        \"\"\"\n        if isinstance(node, ast.Call):\n            return self._recursive_call(node.func)\n        if isinstance(node, ast.Name):\n            return [node.id]\n        if isinstance(node, ast.Attribute):\n            parts = self._recursive_call(node.value)\n            parts.append(node.attr)\n            return parts\n        return []\n\n    def _resolve_all_callee_names(self, callee_nodes: list[list[str]]) -> list[str]:\n        \"\"\"\n        callee_nodes ist jetzt eine Liste von Listen (Name-Steps).\n        Wir pr√ºfen zuerst lokale Definitionen, dann import_mapping.\n        \"\"\"\n        resolved = []\n        for parts in callee_nodes:\n            if not parts:\n                continue\n            simple = parts[-1]\n            dotted = \".\".join(parts[-2:]) if len(parts) >= 2 else simple\n\n            if simple in self.local_defs:\n                resolved.append(self.local_defs[simple])\n                continue\n            if dotted in self.local_defs:\n                resolved.append(self.local_defs[dotted])\n                continue\n\n            first = parts[0]\n            if first in self.import_mapping:\n                mod = self.import_mapping[first]\n                rest = \"::\".join(parts[1:]) if len(parts) > 1 else simple\n                resolved.append(f\"{mod}::{rest}\")\n                continue\n\n            if self.current_class:\n                resolved.append(f\"{self.filename}::{parts[0]}::{simple}\" if len(parts)==1 else f\"{self.filename}::{'::'.join(parts)}\")\n            else:\n                resolved.append(f\"{self.filename}::{ '::'.join(parts)}\")\n        return resolved\n\n    def _make_full_name(self, basename: str, class_name: str | None = None) -> str:\n        if class_name:\n            return f\"{self.filename}::{class_name}::{basename}\"\n        return f\"{self.filename}::{basename}\"\n    \n    def _current_caller(self) -> str:\n        if self.current_function:\n            return self.current_function\n        return f\"<{self.filename}>\" if self.filename else \"<global-scope>\"\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            module_name = alias.name\n            module_asname = alias.asname if alias.asname else module_name\n            self.import_mapping[module_asname] = module_name\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        module_name = node.module.split(\".\")[-1] if node.module else \"\"\n        for alias in node.names:\n            self.import_mapping[alias.asname or alias.name] = f\"{module_name}\" if module_name else alias.name\n\n    def visit_ClassDef(self, node: ast.ClassDef):\n        prev_class = self.current_class\n        self.current_class = node.name\n        self.generic_visit(node)\n        self.current_class = prev_class\n        \n    def visit_FunctionDef(self, node):\n        prev_function = self.current_function\n        full_name = self._make_full_name(node.name, self.current_class)\n        self.local_defs[node.name] = full_name\n        if self.current_class:\n            self.local_defs[f\"{self.current_class}.{node.name}\"] = full_name\n\n        self.current_function = full_name\n        self.graph.add_node(self.current_function)\n        self.generic_visit(node)\n        self.function_set.add(self.current_function)\n        self.current_function = prev_function\n\n    def visit_AsyncFunctionDef(self, node):\n        self.visit_FunctionDef(node)\n\n    def visit_Call(self, node):\n        caller = self._current_caller()\n        parts = self._recursive_call(node)\n        resolved_callees = self._resolve_all_callee_names([parts])\n\n        if caller not in self.edges:\n            self.edges[caller] = set()\n\n        for callee in resolved_callees:\n            if callee:\n                self.edges[caller].add(callee)\n        self.generic_visit(node)\n\n    def visit_If(self, node):\n        if (isinstance(node.test, ast.Compare) and\n            isinstance(node.test.left, ast.Name) and \n            node.test.left.id == \"__name__\"):\n            caller_backup = self.current_function\n            self.current_function = \"<main_block>\"\n            self.generic_visit(node)\n            self.current_function = caller_backup\n        else:\n            self.generic_visit(node)"
            start_line: 10
            end_line: 134
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[12]:
                - identifier: backend.callgraph.CallGraph.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,filename
                  docstring: null
                  start_line: 11
                  end_line: 20
                - identifier: backend.callgraph.CallGraph._recursive_call
                  name: _recursive_call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: "Liefert eine Liste der Namenskomponenten als dotted string z.B. ['pkg', 'mod', 'Class', 'method']"
                  start_line: 22
                  end_line: 34
                - identifier: backend.callgraph.CallGraph._resolve_all_callee_names
                  name: _resolve_all_callee_names
                  calls[0]:
                  called_by[0]:
                  args[2]: self,callee_nodes
                  docstring: "callee_nodes ist jetzt eine Liste von Listen (Name-Steps).\nWir pr√ºfen zuerst lokale Definitionen, dann import_mapping."
                  start_line: 36
                  end_line: 66
                - identifier: backend.callgraph.CallGraph._make_full_name
                  name: _make_full_name
                  calls[0]:
                  called_by[0]:
                  args[3]: self,basename,class_name
                  docstring: null
                  start_line: 68
                  end_line: 71
                - identifier: backend.callgraph.CallGraph._current_caller
                  name: _current_caller
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 73
                  end_line: 76
                - identifier: backend.callgraph.CallGraph.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 78
                  end_line: 83
                - identifier: backend.callgraph.CallGraph.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 85
                  end_line: 88
                - identifier: backend.callgraph.CallGraph.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 90
                  end_line: 94
                - identifier: backend.callgraph.CallGraph.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 96
                  end_line: 107
                - identifier: backend.callgraph.CallGraph.visit_AsyncFunctionDef
                  name: visit_AsyncFunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 109
                  end_line: 110
                - identifier: backend.callgraph.CallGraph.visit_Call
                  name: visit_Call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 112
                  end_line: 123
                - identifier: backend.callgraph.CallGraph.visit_If
                  name: visit_If
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 125
                  end_line: 134
    "backend/converter.py":
      ast_nodes:
        imports[4]: logging,nbformat,base64,nbformat.reader.NotJSONError
        functions[5]:
          - mode: function_analysis
            identifier: backend.converter.wrap_cdata
            name: wrap_cdata
            args[1]: content
            docstring: Wraps content in CDATA tags.
            source_code: "def wrap_cdata(content):\n    \"\"\"Wraps content in CDATA tags.\"\"\"\n    return f\"<![CDATA[\\n{content}\\n]]>\""
            start_line: 8
            end_line: 10
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.converter.extract_output_content
            name: extract_output_content
            args[2]: outputs,image_list
            docstring: "Extracts text and handles images by decoding Base64 to bytes.\nReturns: A list of text strings or placeholders."
            source_code: "def extract_output_content(outputs, image_list):\n    \"\"\"\n    Extracts text and handles images by decoding Base64 to bytes.\n    Returns: A list of text strings or placeholders.\n    \"\"\"\n    extracted_xml_snippets = []\n    \n    for output in outputs:\n\n        if output.output_type in ('display_data', 'execute_result') and hasattr(output, 'data'):\n            data = output.data\n            \n            # Helper to process image\n            def process_image(mime_type):\n                if mime_type in data:\n                    try:\n                        b64_str = data[mime_type]\n                        b64_str = b64_str.replace('\\n', '')\n\n                        image_index = len(image_list)\n                        image_list.append({\n                            \"mime_type\": mime_type,\n                            \"data\": b64_str\n                        })\n\n                        return f'\\n<IMAGE_PLACEHOLDER index=\"{image_index}\" mime=\"{mime_type}\"/>\\n'\n                    except Exception as e:\n                        return f\"<ERROR>Could not decode image: {e}</ERROR>\"\n                return None\n\n            # Ignore jpeg if png is found\n            img_xml = process_image('image/png')\n            if not img_xml:\n                img_xml = process_image('image/jpeg')\n            \n            if img_xml:\n                extracted_xml_snippets.append(img_xml)\n            elif 'text/plain' in data:\n                extracted_xml_snippets.append(data['text/plain'])\n\n        elif output.output_type == 'stream':\n            extracted_xml_snippets.append(output.text)\n            \n        elif output.output_type == 'error':\n            extracted_xml_snippets.append(f\"{output.ename}: {output.evalue}\")\n\n    return extracted_xml_snippets"
            start_line: 12
            end_line: 58
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.converter.process_image
            name: process_image
            args[1]: mime_type
            docstring: null
            source_code: "def process_image(mime_type):\n                if mime_type in data:\n                    try:\n                        b64_str = data[mime_type]\n                        b64_str = b64_str.replace('\\n', '')\n\n                        image_index = len(image_list)\n                        image_list.append({\n                            \"mime_type\": mime_type,\n                            \"data\": b64_str\n                        })\n\n                        return f'\\n<IMAGE_PLACEHOLDER index=\"{image_index}\" mime=\"{mime_type}\"/>\\n'\n                    except Exception as e:\n                        return f\"<ERROR>Could not decode image: {e}</ERROR>\"\n                return None"
            start_line: 25
            end_line: 40
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.converter.convert_notebook_to_xml
            name: convert_notebook_to_xml
            args[1]: file_content
            docstring: null
            source_code: "def convert_notebook_to_xml(file_content):\n    try:\n        nb = nbformat.reads(file_content, as_version=4)\n    except NotJSONError:\n        return \"<ERROR>Could not parse file as JSON/Notebook</ERROR>\", []\n\n    xml_parts = []\n    extracted_images = [] \n\n    for cell in nb.cells:\n        if cell.cell_type == 'markdown':\n            cell_xml = f'<CELL type=\"markdown\">\\n{cell.source}\\n</CELL>'\n            xml_parts.append(cell_xml)\n\n        elif cell.cell_type == 'code':\n            source_code = wrap_cdata(cell.source)\n            xml_parts.append(f'<CELL type=\"code\">\\n{source_code}\\n</CELL>')\n\n            if hasattr(cell, 'outputs') and cell.outputs:\n                snippets = extract_output_content(cell.outputs, extracted_images)\n                \n                content = \"\\n\".join(snippets)\n                \n                if content.strip():\n                    wrapped_output = wrap_cdata(content)\n                    xml_parts.append(f'<CELL type=\"output\">\\n{wrapped_output}\\n</CELL>')\n\n    return \"\\n\\n\".join(xml_parts), extracted_images"
            start_line: 60
            end_line: 87
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.converter.process_repo_notebooks
            name: process_repo_notebooks
            args[1]: repo_files
            docstring: null
            source_code: "def process_repo_notebooks(repo_files):\n    notebook_files = [f for f in repo_files if f.path.endswith('.ipynb')]\n    logging.info(f\"Found {len(notebook_files)} notebooks.\")\n        \n    results = {}\n\n    for nb_file in notebook_files:\n        logging.info(f\"Processing: {nb_file.path}\")\n\n        xml_output, images = convert_notebook_to_xml(nb_file.content)\n        results[nb_file.path] = {\n            \"xml\": xml_output,\n            \"images\": images\n        }\n            \n    return results"
            start_line: 90
            end_line: 105
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "backend/getRepo.py":
      ast_nodes:
        imports[5]: tempfile,git.Repo,git.GitCommandError,logging,os
        functions[0]:
        classes[2]:
          - mode: class_analysis
            identifier: backend.getRepo.RepoFile
            name: RepoFile
            docstring: "Repr√§sentiert eine einzelne Datei in einem Git-Repository.\n\nDer Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tats√§chlichem Zugriff."
            source_code: "class RepoFile:\n    \"\"\"\n    Repr√§sentiert eine einzelne Datei in einem Git-Repository.\n    \n    Der Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tats√§chlichem Zugriff.\n    \"\"\"\n    def __init__(self, file_path, commit_tree):\n        \"\"\"\n        Initialisiert das RepoFile-Objekt.\n\n        Args:\n            file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n            commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt.\n        \"\"\"\n        self.path = file_path\n        self._tree = commit_tree\n        \n\n        self._blob = None\n        self._content = None\n        self._size = None\n\n    @property\n    def blob(self):\n        \"\"\"Lazy-l√§dt das Git-Blob-Objekt.\"\"\"\n        if self._blob is None:\n            try:\n                self._blob = self._tree[self.path]\n            except KeyError:\n                raise FileNotFoundError(f\"Datei '{self.path}' konnte im Commit-Tree nicht gefunden werden.\")\n        return self._blob\n\n    @property\n    def content(self):\n        \"\"\"Lazy-l√§dt und gibt den dekodierten Inhalt der Datei zur√ºck.\"\"\"\n        if self._content is None:\n            self._content = self.blob.data_stream.read().decode('utf-8', errors='ignore')\n        return self._content\n\n    @property\n    def size(self):\n        \"\"\"Lazy-l√§dt und gibt die Gr√∂√üe der Datei in Bytes zur√ºck.\"\"\"\n        if self._size is None:\n            self._size = self.blob.size\n        return self._size\n        \n    def analyze_word_count(self):\n        \"\"\"\n        Eine Beispiel-Analyse-Methode. Z√§hlt die W√∂rter im Dateiinhalt.\n        \"\"\"\n        return len(self.content.split())\n\n    def __repr__(self):\n        \"\"\"Gibt eine n√ºtzliche String-Repr√§sentation des Objekts zur√ºck.\"\"\"\n        return f\"<RepoFile(path='{self.path}')>\"\n    \n    def to_dict(self, include_content=False):\n        data = {\n            \"path\": self.path,\n            \"name\": os.path.basename(self.path),\n            \"size\": self.size,\n            \"type\": \"file\"\n        }\n        if include_content:\n            data[\"content\"] = self.content\n        return data"
            start_line: 7
            end_line: 72
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[7]:
                - identifier: backend.getRepo.RepoFile.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[3]: self,file_path,commit_tree
                  docstring: "Initialisiert das RepoFile-Objekt.\n\nArgs:\n    file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n    commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt."
                  start_line: 13
                  end_line: 27
                - identifier: backend.getRepo.RepoFile.blob
                  name: blob
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt das Git-Blob-Objekt.
                  start_line: 30
                  end_line: 37
                - identifier: backend.getRepo.RepoFile.content
                  name: content
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt und gibt den dekodierten Inhalt der Datei zur√ºck.
                  start_line: 40
                  end_line: 44
                - identifier: backend.getRepo.RepoFile.size
                  name: size
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Lazy-l√§dt und gibt die Gr√∂√üe der Datei in Bytes zur√ºck.
                  start_line: 47
                  end_line: 51
                - identifier: backend.getRepo.RepoFile.analyze_word_count
                  name: analyze_word_count
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Eine Beispiel-Analyse-Methode. Z√§hlt die W√∂rter im Dateiinhalt.
                  start_line: 53
                  end_line: 57
                - identifier: backend.getRepo.RepoFile.__repr__
                  name: __repr__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: Gibt eine n√ºtzliche String-Repr√§sentation des Objekts zur√ºck.
                  start_line: 59
                  end_line: 61
                - identifier: backend.getRepo.RepoFile.to_dict
                  name: to_dict
                  calls[0]:
                  called_by[0]:
                  args[2]: self,include_content
                  docstring: null
                  start_line: 63
                  end_line: 72
          - mode: class_analysis
            identifier: backend.getRepo.GitRepository
            name: GitRepository
            docstring: "Verwaltet ein Git-Repository, einschlie√ülich Klonen in ein tempor√§res\nVerzeichnis und Bereitstellung von RepoFile-Objekten."
            source_code: "class GitRepository:\n    \"\"\"\n    Verwaltet ein Git-Repository, einschlie√ülich Klonen in ein tempor√§res\n    Verzeichnis und Bereitstellung von RepoFile-Objekten.\n    \"\"\"\n    def __init__(self, repo_url):\n        self.repo_url = repo_url\n        self.temp_dir = tempfile.mkdtemp()\n        self.repo = None\n\n        self.files = []\n        \n        try:\n            logging.info(f\"Clone Repository {self.repo_url}...\")\n            self.repo = Repo.clone_from(self.repo_url, self.temp_dir)\n            self.latest_commit = self.repo.head.commit\n            self.commit_tree = self.latest_commit.tree\n            logging.info(\"Cloning successful.\")\n        except GitCommandError as e:\n            self.close()\n            raise RuntimeError(f\"Error cloning repository: {e}\") from e\n\n    def get_all_files(self):\n        \"\"\"\n        Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zur√ºck.\n\n        Returns:\n            list[RepoFile]: Eine Liste von RepoFile-Instanzen.\n        \"\"\"\n        file_paths = self.repo.git.ls_files().split('\\n')\n        self.files = [RepoFile(path, self.commit_tree) for path in file_paths if path]\n        return self.files\n\n    def close(self):\n        \"\"\"L√∂scht das tempor√§re Verzeichnis und dessen Inhalt.\"\"\"\n        if self.temp_dir:\n            print(f\"\\nL√∂sche tempor√§res Verzeichnis: {self.temp_dir}\")\n            self.temp_dir = None\n            \n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def get_file_tree(self, include_content=False):\n\n        if not self.files:\n            self.get_all_files()\n\n        tree = {\"name\": \"root\", \"type\": \"directory\", \"children\": []}\n\n        for file_obj in self.files:\n            parts = file_obj.path.split('/')\n            current_level = tree[\"children\"]\n            \n            # Iteriere durch die Ordnerstruktur\n            for part in parts[:-1]:\n                # Suche, ob Ordner schon existiert\n                found = next((item for item in current_level if item[\"name\"] == part and item[\"type\"] == \"directory\"), None)\n                if not found:\n                    new_dir = {\"name\": part, \"type\": \"directory\", \"children\": []}\n                    current_level.append(new_dir)\n                    current_level = new_dir[\"children\"]\n                else:\n                    current_level = found[\"children\"]\n            \n            # Datei am Ende hinzuf√ºgen\n            current_level.append(file_obj.to_dict(include_content=include_content))\n\n        return tree"
            start_line: 78
            end_line: 148
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[6]:
                - identifier: backend.getRepo.GitRepository.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,repo_url
                  docstring: null
                  start_line: 83
                  end_line: 98
                - identifier: backend.getRepo.GitRepository.get_all_files
                  name: get_all_files
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: "Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zur√ºck.\n\nReturns:\n    list[RepoFile]: Eine Liste von RepoFile-Instanzen."
                  start_line: 100
                  end_line: 109
                - identifier: backend.getRepo.GitRepository.close
                  name: close
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: L√∂scht das tempor√§re Verzeichnis und dessen Inhalt.
                  start_line: 111
                  end_line: 115
                - identifier: backend.getRepo.GitRepository.__enter__
                  name: __enter__
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 117
                  end_line: 118
                - identifier: backend.getRepo.GitRepository.__exit__
                  name: __exit__
                  calls[0]:
                  called_by[0]:
                  args[4]: self,exc_type,exc_val,exc_tb
                  docstring: null
                  start_line: 120
                  end_line: 121
                - identifier: backend.getRepo.GitRepository.get_file_tree
                  name: get_file_tree
                  calls[0]:
                  called_by[0]:
                  args[2]: self,include_content
                  docstring: null
                  start_line: 123
                  end_line: 148
    "backend/main.py":
      ast_nodes:
        imports[28]: json,math,logging,os,re,time,math,datetime.datetime,matplotlib.pyplot,datetime.datetime,pathlib.Path,dotenv.load_dotenv,getRepo.GitRepository,AST_Schema.ASTAnalyzer,MainLLM.MainLLM,basic_info.ProjektInfoExtractor,HelperLLM.LLMHelper,relationship_analyzer.ProjectAnalyzer,schemas.types.FunctionContextInput,schemas.types.FunctionAnalysisInput,schemas.types.ClassContextInput,schemas.types.ClassAnalysisInput,schemas.types.MethodContextInput,toon_format.encode,toon_format.count_tokens,toon_format.estimate_savings,toon_format.compare_formats,converter.process_repo_notebooks
        functions[7]:
          - mode: function_analysis
            identifier: backend.main.create_savings_chart
            name: create_savings_chart
            args[4]: json_tokens,toon_tokens,savings_percent,output_path
            docstring: Erstellt ein Balkendiagramm f√ºr den Token-Vergleich und speichert es.
            source_code: "def create_savings_chart(json_tokens, toon_tokens, savings_percent, output_path):\n    \"\"\"Erstellt ein Balkendiagramm f√ºr den Token-Vergleich und speichert es.\"\"\"\n    labels = ['JSON', 'TOON']\n    values = [json_tokens, toon_tokens]\n    colors = ['#ff9999', '#66b3ff']\n\n    plt.figure(figsize=(8, 6))\n    bars = plt.bar(labels, values, color=colors, width=0.5)\n\n    # Titel und Beschriftungen\n    plt.title(f'Token-Vergleich: {savings_percent:.2f}% Einsparung', fontsize=14)\n    plt.ylabel('Anzahl Token')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n\n    # Werte √ºber den Balken anzeigen\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width() / 2.0, height, \n                 f'{int(height):,}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n\n    # Speichern\n    plt.savefig(output_path)\n    plt.close()"
            start_line: 33
            end_line: 55
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.calculate_net_time
            name: calculate_net_time
            args[5]: start_time,end_time,total_items,batch_size,model_name
            docstring: Berechnet die Dauer abz√ºglich der Sleep-Zeiten f√ºr Rate-Limits.
            source_code: "def calculate_net_time(start_time, end_time, total_items, batch_size, model_name):\n    \"\"\"Berechnet die Dauer abz√ºglich der Sleep-Zeiten f√ºr Rate-Limits.\"\"\"\n    total_duration = end_time - start_time\n    \n    if not model_name.startswith(\"gemini-\"):\n        return total_duration\n\n    if total_items == 0:\n        return 0\n\n    num_batches = math.ceil(total_items / batch_size)\n    sleep_count = max(0, num_batches - 1)\n    total_sleep_time = sleep_count * 61\n    \n    net_time = total_duration - total_sleep_time\n    return max(0, net_time)"
            start_line: 57
            end_line: 72
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.main_workflow
            name: main_workflow
            args[4]: input,api_keys,model_names,status_callback
            docstring: null
            source_code: "def main_workflow(input, api_keys: dict, model_names: dict, status_callback=None):\n    \n    def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)\n\n    update_status(\"üîç Analysiere Input...\")\n    \n    user_input = input\n    \n    # API Key & Ollama Base URL aus Frontend holen\n    gemini_api_key = api_keys.get(\"gemini\")\n    openai_api_key = api_keys.get(\"gpt\")\n    scadsllm_api_key = api_keys.get(\"scadsllm\")\n    scadsllm_base_url = api_keys.get(\"scadsllm_base_url\")\n    ollama_base_url = api_keys.get(\"ollama\")\n    base_url = None\n\n    if model_names[\"helper\"].startswith(\"gpt-\"):\n        helper_api_key = openai_api_key\n    elif model_names[\"helper\"].startswith(\"gemini-\"):\n        helper_api_key = gemini_api_key\n    elif \"/\" in model_names[\"helper\"] or model_names[\"helper\"].startswith(\"alias-\") or any(x in model_names[\"helper\"] for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        helper_api_key = scadsllm_api_key\n        base_url = scadsllm_base_url\n    else:\n        helper_api_key = None\n        base_url = ollama_base_url\n    if model_names[\"main\"].startswith(\"gpt-\"):\n        main_api_key = openai_api_key\n    elif model_names[\"main\"].startswith(\"gemini-\"):\n        main_api_key = gemini_api_key\n    elif \"/\" in model_names[\"main\"] or model_names[\"main\"].startswith(\"alias-\") or any(x in model_names[\"main\"] for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        main_api_key = scadsllm_api_key\n        base_url = scadsllm_base_url\n    else:\n        main_api_key = None\n        base_url = ollama_base_url\n\n    # Standardeinstellungen f√ºr Modelle\n    helper_model = model_names.get(\"helper\", \"gpt-5-mini\")\n    main_model = model_names.get(\"main\", \"gpt-5.1\")\n\n    # Error Handling f√ºr fehlende API Keys\n    if not gemini_api_key and \"gemini\" in helper_model:\n        raise ValueError(\"Gemini API Key was not provided in api_keys dictionary.\")\n\n    # URL Extraktion\n    repo_url = None\n    url_pattern = r\"https?://(?:www\\.)?github\\.com/[^\\s]+\"\n    match = re.search(url_pattern, user_input)\n\n    if match:\n        repo_url = match.group(0)\n        logging.info(f\"Extracted repository URL: {repo_url}\")\n    else:\n        raise ValueError(\"Could not find a valid URL in the provided input.\")\n    \n    # Repo klonen und Dateien extrahieren\n    update_status(f\"‚¨áÔ∏è Klone Repository: {repo_url} ...\")\n    \n    repo_files = []\n    local_repo_path = \"\" \n\n    try: \n\n        with GitRepository(repo_url) as repo:\n            repo_files = repo.get_all_files()\n            local_repo_path = repo.temp_dir\n\n            logging.info(f\"Total files retrieved: {len(repo_files)}\")\n\n    except Exception as e:\n        logging.error(f\"Error cloning repository: {e}\")\n        raise \n\n\n    # Extrahiere Basic Infos\n    update_status(\"‚ÑπÔ∏è Extrahiere Basis-Informationen...\")\n    try:\n        info_extractor = ProjektInfoExtractor()\n        basic_project_info = info_extractor.extrahiere_info(dateien=repo_files, repo_url=repo_url)\n        logging.info(\"Basic project info extracted\")\n    except Exception as e:\n        logging.error(f\"Error extracting basic project info: {e}\")\n        basic_project_info = \"Could not extract basic info.\"\n\n    # Erstelle Repository Dateibaum\n    update_status(\"üå≤ Erstelle Repository Dateibaum...\")\n    try:\n        repo_file_tree = repo.get_file_tree()\n        logging.info(\"Repository file tree constructed\")\n    except Exception as e:\n        logging.error(f\"Error constructing repository file tree: {e}\")\n        repo_file_tree = \"Could not create file tree.\"\n\n    # Relationship Analyse durchf√ºhren\n    update_status(\"üîó Analysiere Beziehungen (Calls & Instanziierungen)...\")\n    try:\n        rel_analyzer = ProjectAnalyzer(project_root=local_repo_path)\n        rel_analyzer.analyze()\n        \n        raw_relationships = rel_analyzer.get_raw_relationships()\n        \n        logging.info(f\"Relationships analyzed.\")\n    except Exception as e:\n        logging.error(f\"Error in relationship analyzer: {e}\")\n        raw_relationships = {\"outgoing\": {}, \"incoming\": {}}\n\n    # Erstelle AST Schema\n    update_status(\"üå≥ Erstelle Abstract Syntax Tree (AST)...\")\n    try:        \n        ast_analyzer = ASTAnalyzer()   \n        ast_schema = ast_analyzer.analyze_repository(files=repo_files, repo=repo)\n        logging.info(\"AST schema created\")\n    except Exception as e:\n        logging.error(f\"Error retrieving repository files: {e}\")\n        raise\n\n    # Anreichern des AST Schemas mit Relationship Daten\n    update_status(\"‚ûï Reiche AST mit Beziehungsdaten an...\")            \n    try:   \n        ast_schema = ast_analyzer.merge_relationship_data(ast_schema, raw_relationships)\n        logging.info(\"AST schema created and enriched\")\n\n    except Exception as e:\n        logging.error(f\"Error processing repository: {e}\")\n        raise\n\n    # Vorbereitung der HelperLLM Eingaben\n    update_status(\"‚öôÔ∏è Bereite Daten f√ºr Helper LLM vor...\")\n    \n    helper_llm_function_input = []\n    helper_llm_class_input = []\n\n    try:\n        for filename, file_data in ast_schema['files'].items():\n            ast_nodes = file_data.get('ast_nodes', {})\n            imports = ast_nodes.get('imports', [])\n            functions = ast_nodes.get('functions', [])\n            classes = ast_nodes.get('classes', [])\n\n            for function in functions:\n                context = function.get('context', {})\n                \n                raw_called_by = context.get('called_by', [])\n                clean_called_by = [cb for cb in raw_called_by if isinstance(cb, dict)]\n\n                function_context = FunctionContextInput(\n                    calls = context.get('calls', []),\n                    called_by = clean_called_by \n                )\n                \n                function_input = FunctionAnalysisInput(\n                    mode = function.get('mode', 'function_analysis'),\n                    identifier = function.get('identifier'),\n                    source_code = function.get('source_code'),\n                    imports = imports,\n                    context = function_context\n                )\n                \n                helper_llm_function_input.append(function_input)\n\n            for _class in classes:\n                context = _class.get('context', {})\n                \n                method_context_inputs = []\n                for method in context.get('method_context', []):\n                    \n                    raw_method_called_by = method.get('called_by', [])\n                    clean_method_called_by = [cb for cb in raw_method_called_by if isinstance(cb, dict)]\n\n                    method_context_inputs.append(\n                        MethodContextInput(\n                            identifier=method.get('identifier'),\n                            calls=method.get('calls', []),\n                            called_by=clean_method_called_by, \n                            args=method.get('args', []),\n                            docstring=method.get('docstring')\n                        )\n                    )\n\n                raw_instantiated_by = context.get('instantiated_by', [])\n                clean_instantiated_by = [ib for ib in raw_instantiated_by if isinstance(ib, dict)]\n\n                class_context = ClassContextInput(\n                    dependencies = context.get('dependencies', []),\n                    instantiated_by = clean_instantiated_by, \n                    method_context = method_context_inputs\n                )\n\n                class_input = ClassAnalysisInput(\n                    mode = _class.get('mode', 'class_analysis'),\n                    identifier =_class.get('identifier'),\n                    source_code = _class.get('source_code'), \n                    imports = imports, \n                    context = class_context\n                )\n                \n                helper_llm_class_input.append(class_input)\n\n    except Exception as e:\n        logging.error(f\"Error preparing inputs for Helper LLM: {e}\")\n        raise\n    \n    # Initialisiere HelperLLM\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    \n    llm_helper = LLMHelper(\n        api_key=helper_api_key, \n        function_prompt_path=function_prompt_file, \n        class_prompt_path=class_prompt_file,\n        model_name=helper_model,\n        base_url=base_url,\n    )\n    \n    if ollama_base_url:\n        os.environ[\"OLLAMA_BASE_URL\"] = ollama_base_url\n\n    # Initialisiere Ergebniscontainer\n    analysis_results = {}\n    function_analysis_results = []\n    class_analysis_results = []\n\n    # Call HelperLLM f√ºr Funktionen\n    update_status(f\"ü§ñ Helper LLM: Analysiere {len(helper_llm_function_input)} Funktionen ({helper_model})...\")\n\n    try:\n        net_time_func = 0\n        if len(helper_llm_function_input) > 0:\n\n            logging.info(\"\\n--- Generating documentation for Functions ---\")\n            t_start_func = time.time()    \n            function_analysis_results = llm_helper.generate_for_functions(helper_llm_function_input)    \n            t_end_func = time.time()\n            net_time_func = calculate_net_time(t_start_func, t_end_func, len(helper_llm_function_input), llm_helper.batch_size, helper_model)\n\n        if len(function_analysis_results) != 0:\n            for doc in function_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"functions\" not in analysis_results:\n                        analysis_results[\"functions\"] = {}\n                    analysis_results[\"functions\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(f\"Failed to generate doc for a function\") \n    except Exception as e:\n        logging.error(f\"Error during Helper LLM function analysis: {e}\")\n        raise\n    \n\n    # Call HelperLLM f√ºr Klassen\n    try:\n        net_time_class = 0\n        if len(helper_llm_class_input) > 0:\n            # Rate Limit Sleep f√ºr Gemini Modelle\n            if llm_helper.model_name.startswith(\"gemini-\") & (len(helper_llm_function_input) > 0):\n                update_status(\"üí§ Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n                time.sleep(65)\n            \n            update_status(f\"ü§ñ Helper LLM: Analysiere {len(helper_llm_class_input)} Klassen ({helper_model})...\")\n            \n            logging.info(\"\\n--- Generating documentation for Classes ---\")\n            t_start_class = time.time()\n            class_analysis_results = llm_helper.generate_for_classes(helper_llm_class_input)\n            t_end_class = time.time()\n            net_time_class = calculate_net_time(t_start_class, t_end_class, len(helper_llm_class_input), llm_helper.batch_size, helper_model)\n\n        if len(class_analysis_results) != 0:\n            for doc in class_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"classes\" not in analysis_results:\n                        analysis_results[\"classes\"] = {}\n                    analysis_results[\"classes\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(f\"Failed to generate doc for a class\")\n    except Exception as e:\n        logging.error(f\"Error during Helper LLM class analysis: {e}\")\n        raise\n\n    total_helper_time = net_time_func + net_time_class\n\n    # MainLLM Input Vorbereitung\n    main_llm_input = {\n        \"basic_info\": basic_project_info,\n        \"file_tree\": repo_file_tree,\n        \"ast_schema\": ast_schema,\n        \"analysis_results\": analysis_results\n    }\n\n    # Speichern als JSON (Optional)\n    main_llm_input_json = json.dumps(main_llm_input, indent=2)\n    # with open(\"output.json\", \"w\", encoding=\"utf-8\") as f:\n    #     f.write(main_llm_input_json)\n    #     logging.info(\"JSON-Datei wurde gespeichert.\")\n\n    # Konvertiere Input in Toon Format\n    main_llm_input_toon = encode(main_llm_input)\n\n    #Speichern in TOON Format (Optional)\n    # with open(\"output.toon\", \"w\", encoding=\"utf-8\") as f:\n    #     f.write(main_llm_input_toon)\n    #     logging.info(\"output.toon erfolgreich gespeichert.\")\n    \n    # Token Evaluation\n    savings_data = None\n    try:\n        logging.info(\"--- Evaluierung der Token-Ersparnis ---\")\n        savings_data = estimate_savings(main_llm_input)\n        logging.info(f\"JSON Tokens: {savings_data['json_tokens']}\")\n        logging.info(f\"TOON Tokens: {savings_data['toon_tokens']}\")\n        logging.info(f\"Ersparnis:   {savings_data['savings_percent']:.2f}%\")\n        \n    except Exception as e:\n        logging.warning(f\"Token evaluation could not be performed: {e}\")    \n\n\n\n    prompt_file_mainllm = \"SystemPrompts/SystemPromptMainLLM.txt\"\n    prompt_file_mainllm_toon = \"SystemPrompts/SystemPromptMainLLMToon.txt\"\n    # MainLLM Ausf√ºhrung\n    main_llm = MainLLM(\n        api_key=main_api_key, \n        prompt_file_path=prompt_file_mainllm_toon,\n        model_name=main_model,\n        base_url=base_url,\n    )\n\n\n    # RPM Limit Sleep f√ºr Gemini Modelle\n    if llm_helper.model_name == main_llm.model_name and main_llm.model_name.startswith(\"gemini-\"):\n        time.sleep(65)\n        update_status(\"üí§ Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n\n    # Call MainLLM f√ºr finalen Report\n    update_status(f\"üß† Main LLM: Generiere finalen Report ({main_model})...\")\n    try:\n        total_main_time = 0\n        logging.info(\"\\n--- Generating Final Report ---\")\n        t_start_main = time.time()\n        final_report = main_llm.call_llm(main_llm_input_toon)\n        t_end_main = time.time()\n        total_main_time = t_end_main - t_start_main\n    except Exception as e:\n        logging.error(f\"Error during Main LLM final report generation: {e}\")\n        raise\n\n    \n    # --- Speichern der Ergebnisse ---\n    output_dir = \"result\"\n    stats_dir = \"Statistics\" # Neuer Ordner\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(stats_dir, exist_ok=True) # Stelle sicher, dass Statistics Ordner existiert\n\n    total_active_time = total_helper_time + total_main_time\n    \n    # Dateiname f√ºr Report\n    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M-%S\")\n    report_filename = f\"report_{timestamp}_Helper_{llm_helper.model_name}_MainLLM_{main_llm.model_name}.md\"\n    report_filepath = os.path.join(output_dir, report_filename)\n    \n    if final_report:\n        with open(report_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(final_report)\n        logging.info(f\"Final report saved to '{report_filepath}'.\")\n        \n        if savings_data:\n            try:\n                savings_filename = report_filename.replace(\"report_\", \"savings_\").replace(\".md\", \".png\")\n                savings_filepath = os.path.join(stats_dir, savings_filename)\n                \n                logging.info(f\"Erstelle Token-Chart: {savings_filepath}\")\n                create_savings_chart(\n                    json_tokens=savings_data['json_tokens'], \n                    toon_tokens=savings_data['toon_tokens'], \n                    savings_percent=savings_data['savings_percent'],\n                    output_path=savings_filepath\n                )\n            except Exception as chart_error:\n                logging.error(f\"Konnte Diagramm nicht erstellen: {chart_error}\")\n\n    else:\n        final_report = \"Error: Report generation failed or returned empty.\"\n\n    metrics = {\n        \"helper_time\": round(total_helper_time, 2),\n        \"main_time\": round(total_main_time, 2),\n        \"total_time\": round(total_active_time, 2),\n        \"helper_model\": helper_model,\n        \"main_model\": main_model,\n        \"json_tokens\": savings_data['json_tokens'] if savings_data else None,\n        \"toon_tokens\": savings_data['toon_tokens'] if savings_data else None,\n        \"savings_percent\": round(savings_data['savings_percent'], 2) if savings_data else None\n    \n    }\n\n    return {\n        \"report\": final_report,\n        \"metrics\": metrics\n    }"
            start_line: 75
            end_line: 477
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.update_status
            name: update_status
            args[1]: msg
            docstring: null
            source_code: "def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)"
            start_line: 77
            end_line: 80
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.notebook_workflow
            name: notebook_workflow
            args[4]: input,api_keys,model,status_callback
            docstring: null
            source_code: "def notebook_workflow(input, api_keys, model, status_callback=None):\n    t_start = time.time()\n    final_report = \"keine Notebooks gefunden\"\n    def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)\n\n    def gemini_payload(basic_info, nb_path, xml_content, images):\n        \n        intro_json = json.dumps({\n            \"basic_info\": basic_info,\n            \"current_notebook_path\": nb_path\n        }, indent=2)\n        \n        payload_content = []\n\n        payload_content.append({\n            \"type\": \"text\",\n            \"text\": f\"Context Information:\\n{intro_json}\\n\\nNotebook XML Structure:\\n\"\n        })\n\n        # Regex to find: <IMAGE_PLACEHOLDER index=\"0\" mime=\"image/png\"/>\n        pattern = r'(<IMAGE_PLACEHOLDER index=\"(\\d+)\" mime=\"([^\"]+)\"/>)'\n        last_pos = 0\n        \n        for match in re.finditer(pattern, xml_content):\n            text_segment = xml_content[last_pos:match.start()]\n            if text_segment.strip():\n                payload_content.append({\n                    \"type\": \"text\",\n                    \"text\": text_segment\n                })\n                \n            image_index = int(match.group(2))\n            mime_type = match.group(3)\n            \n            if image_index < len(images):\n                img_data = images[image_index]\n                b64_string = img_data['data']\n                \n                payload_content.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:{mime_type};base64,{b64_string}\"\n                    }\n                })\n                \n            last_pos = match.end()\n\n        # Add any remaining text after the last image\n        remaining_text = xml_content[last_pos:]\n        if remaining_text.strip():\n            payload_content.append({\n                \"type\": \"text\",\n                \"text\": remaining_text\n            })\n\n        return payload_content\n\n\n    base_url = None\n\n    if model.startswith(\"gpt-\"):\n        input_api_key = api_keys.get(\"gpt\")\n    elif model.startswith(\"gemini-\"):\n        input_api_key = api_keys.get(\"gemini\")\n    elif \"/\" in model or model.startswith(\"alias-\") or any(x in model for x in [\"DeepSeek\", \"Teuken\", \"Llama\", \"Qwen\", \"gpt-oss\", \"openGPT\"]):\n        input_api_key = api_keys.get(\"scadsllm\")\n        base_url = api_keys.get(\"scadsllm_base_url\")\n    else:\n        input_api_key = None\n        base_url = api_keys.get(\"ollama\")\n\n    update_status(\"üîç Analysiere Input...\")\n\n    # URL Extraktion\n    repo_url = None\n    url_pattern = r\"https?://(?:www\\.)?github\\.com/[^\\s]+\"\n    match = re.search(url_pattern, input)\n\n    if match:\n        repo_url = match.group(0)\n        logging.info(f\"Extracted repository URL: {repo_url}\")\n    else:\n        raise ValueError(\"Could not find a valid URL in the provided input.\")\n    \n    update_status(f\"‚¨áÔ∏è Klone Repository: {repo_url} ...\")\n\n    try: \n        with GitRepository(repo_url) as repo:\n            repo_files = repo.get_all_files()\n    except Exception as e:\n        logging.error(f\"Error cloning repository: {e}\")\n        raise\n    \n    update_status(f\"üîó Bereite Input vor...\")\n\n    # convert to XML\n    processed_data = process_repo_notebooks(repo_files)\n\n    \n    # Extrahiere Basic Infos\n    update_status(\"‚ÑπÔ∏è Extrahiere Basis-Informationen...\")\n    try:\n        info_extractor = ProjektInfoExtractor()\n        basic_project_info = info_extractor.extrahiere_info(dateien=repo_files, repo_url=repo_url)\n        logging.info(\"Basic project info extracted\")\n    except Exception as e:\n        logging.error(f\"Error extracting basic project info: {e}\")\n        basic_project_info = \"Could not extract basic info.\"\n\n\n\n    prompt_file_notebook_llm = \"SystemPrompts/SystemPromptNotebookLLM.txt\"\n    notebook_llm = MainLLM(\n        api_key=input_api_key, \n        prompt_file_path=prompt_file_notebook_llm,\n        model_name=model,\n        base_url=base_url,\n    )\n\n    notebook_reports = []\n    total_notebooks = len(processed_data)\n    \n    logging.info(f\"Starting sequential processing of {total_notebooks} notebooks...\")\n\n    # Iterate over each notebook file individually\n    for index, (nb_path, nb_data) in enumerate(processed_data.items(), 1):\n        \n        update_status(f\"üß† Generiere Report ({index}/{total_notebooks}): {os.path.basename(nb_path)}\")\n        \n        nb_xml = nb_data['xml']\n        nb_images = nb_data['images']\n\n        llm_payload = gemini_payload(basic_project_info, nb_path, nb_xml, nb_images)\n\n        try:\n            single_report = notebook_llm.call_llm(llm_payload)\n            if single_report and isinstance(single_report, str):\n                notebook_reports.append(single_report)\n            else:\n                logging.warning(f\"LLM lieferte kein Ergebnis f√ºr {nb_path}\")\n                notebook_reports.append(f\"## Analyse f√ºr {os.path.basename(nb_path)}\\nFehler: Das Modell lieferte keine Antwort.\")\n            \n        except Exception as e:\n            logging.error(f\"Error generating report for {nb_path}: {e}\")\n            notebook_reports.append(f\"# Error processing {nb_path}\\n\\nError: {str(e)}\\n\\n---\\n\")\n\n    # Concatenate all reports\n    final_report = \"\\n\\n<br>\\n<br>\\n<br>\\n\\n\".join([str(r) for r in notebook_reports if r is not None])\n\n    \n    # --- Speichern der Ergebnisse ---\n    output_dir = \"result\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Dateiname f√ºr Report\n    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M-%S\")\n    report_filename = f\"notebook_report_{timestamp}_NotebookLLM_{notebook_llm.model_name}.md\"\n    report_filepath = os.path.join(output_dir, report_filename)\n    \n    if final_report:\n        with open(report_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(final_report)\n        logging.info(f\"Final report saved to '{report_filepath}'.\")\n\n    # Am Ende der Funktion nach dem Speichern:\n    total_time = time.time() - t_start\n    \n    # Metriken f√ºr das Frontend bauen\n    metrics = {\n        \"helper_time\": \"0\", # Notebook-Workflow hat meist keinen separaten Helper\n        \"main_time\": round(total_time, 2),\n        \"total_time\": round(total_time, 2),\n        \"helper_model\": \"None\",\n        \"main_model\": model,\n        \"json_tokens\": 0,\n        \"toon_tokens\": 0,\n        \"savings_percent\": 0\n    }\n\n    return {\n        \"report\": final_report,\n        \"metrics\": metrics\n    }"
            start_line: 483
            end_line: 668
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.update_status
            name: update_status
            args[1]: msg
            docstring: null
            source_code: "def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)"
            start_line: 486
            end_line: 489
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: backend.main.gemini_payload
            name: gemini_payload
            args[4]: basic_info,nb_path,xml_content,images
            docstring: null
            source_code: "def gemini_payload(basic_info, nb_path, xml_content, images):\n        \n        intro_json = json.dumps({\n            \"basic_info\": basic_info,\n            \"current_notebook_path\": nb_path\n        }, indent=2)\n        \n        payload_content = []\n\n        payload_content.append({\n            \"type\": \"text\",\n            \"text\": f\"Context Information:\\n{intro_json}\\n\\nNotebook XML Structure:\\n\"\n        })\n\n        # Regex to find: <IMAGE_PLACEHOLDER index=\"0\" mime=\"image/png\"/>\n        pattern = r'(<IMAGE_PLACEHOLDER index=\"(\\d+)\" mime=\"([^\"]+)\"/>)'\n        last_pos = 0\n        \n        for match in re.finditer(pattern, xml_content):\n            text_segment = xml_content[last_pos:match.start()]\n            if text_segment.strip():\n                payload_content.append({\n                    \"type\": \"text\",\n                    \"text\": text_segment\n                })\n                \n            image_index = int(match.group(2))\n            mime_type = match.group(3)\n            \n            if image_index < len(images):\n                img_data = images[image_index]\n                b64_string = img_data['data']\n                \n                payload_content.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:{mime_type};base64,{b64_string}\"\n                    }\n                })\n                \n            last_pos = match.end()\n\n        # Add any remaining text after the last image\n        remaining_text = xml_content[last_pos:]\n        if remaining_text.strip():\n            payload_content.append({\n                \"type\": \"text\",\n                \"text\": remaining_text\n            })\n\n        return payload_content"
            start_line: 491
            end_line: 541
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "backend/relationship_analyzer.py":
      ast_nodes:
        imports[4]: ast,os,logging,collections.defaultdict
        functions[1]:
          - mode: function_analysis
            identifier: backend.relationship_analyzer.path_to_module
            name: path_to_module
            args[2]: filepath,project_root
            docstring: Wandelt einen Dateipfad in einen Python-Modulpfad um.
            source_code: "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    module_path = rel_path.replace(os.path.sep, '.')\n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n    return module_path"
            start_line: 6
            end_line: 18
            context:
              calls[0]:
              called_by[0]:
        classes[2]:
          - mode: class_analysis
            identifier: backend.relationship_analyzer.ProjectAnalyzer
            name: ProjectAnalyzer
            docstring: null
            source_code: "class ProjectAnalyzer:\n    \n    def __init__(self, project_root):\n        self.project_root = os.path.abspath(project_root)\n        self.definitions = {}\n        self.call_graph = defaultdict(list)\n        self.file_asts = {} \n        self.ignore_dirs = {'.git', '.venv', 'venv', '__pycache__', 'node_modules', 'dist', 'build', 'docs'}\n\n    def analyze(self):\n        py_files = self._find_py_files()\n        \n        for filepath in py_files:\n            self._collect_definitions(filepath)\n            \n        for filepath in py_files:\n            self._resolve_calls(filepath)\n            \n        self.file_asts.clear()\n        \n        return self.call_graph\n\n    def get_raw_relationships(self):\n\n        outgoing = defaultdict(set)\n        incoming = defaultdict(set)\n\n        for callee_id, callers_info_list in self.call_graph.items():\n            for info in callers_info_list:\n                caller_id = info['caller']\n                \n                if caller_id and callee_id:\n                    outgoing[caller_id].add(callee_id)\n                    incoming[callee_id].add(caller_id)\n        \n        return {\n            \"outgoing\": {k: sorted(list(v)) for k, v in outgoing.items()},\n            \"incoming\": {k: sorted(list(v)) for k, v in incoming.items()}\n        }\n\n    def _find_py_files(self):\n        py_files = []\n        for root, dirs, files in os.walk(self.project_root):\n            dirs[:] = [d for d in dirs if d not in self.ignore_dirs]\n            for file in files:\n                if file.endswith(\".py\"):\n                    py_files.append(os.path.join(root, file))\n        return py_files\n\n    def _collect_definitions(self, filepath):\n        try:\n            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n                source = f.read()\n                tree = ast.parse(source, filename=filepath)\n                \n            self.file_asts[filepath] = tree\n            module_path = path_to_module(filepath, self.project_root)\n            \n            for node in ast.walk(tree):\n                if isinstance(node, ast.FunctionDef):\n                    parent = self._get_parent(tree, node)\n                    if isinstance(parent, ast.ClassDef):\n                        path_name = f\"{module_path}.{parent.name}.{node.name}\"\n                        def_type = 'method'\n                    else:\n                        path_name = f\"{module_path}.{node.name}\"\n                        def_type = 'function'\n                    self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': def_type}\n                elif isinstance(node, ast.ClassDef):\n                    path_name = f\"{module_path}.{node.name}\"\n                    self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': 'class'}\n        except Exception as e:\n            logging.error(f\"Error collecting definitions in {filepath}: {e}\")\n            self.file_asts[filepath] = None\n\n    def _get_parent(self, tree, node):\n        for parent in ast.walk(tree):\n            for child in ast.iter_child_nodes(parent):\n                if child is node:\n                    return parent\n        return None\n\n    def _resolve_calls(self, filepath):\n        tree = self.file_asts.get(filepath)\n        if not tree:\n            return\n\n        try:\n            resolver = CallResolverVisitor(filepath, self.project_root, self.definitions)\n            resolver.visit(tree)\n            \n            for callee_pathname, caller_info_list in resolver.calls.items():\n                self.call_graph[callee_pathname].extend(caller_info_list)\n        except Exception as e:\n            logging.error(f\"Error resolving calls in {filepath}: {e}\")"
            start_line: 20
            end_line: 114
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[7]:
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[2]: self,project_root
                  docstring: null
                  start_line: 22
                  end_line: 27
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.analyze
                  name: analyze
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 29
                  end_line: 40
                - identifier: backend.relationship_analyzer.ProjectAnalyzer.get_raw_relationships
                  name: get_raw_relationships
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 42
                  end_line: 58
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._find_py_files
                  name: _find_py_files
                  calls[0]:
                  called_by[0]:
                  args[1]: self
                  docstring: null
                  start_line: 60
                  end_line: 67
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._collect_definitions
                  name: _collect_definitions
                  calls[0]:
                  called_by[0]:
                  args[2]: self,filepath
                  docstring: null
                  start_line: 69
                  end_line: 93
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._get_parent
                  name: _get_parent
                  calls[0]:
                  called_by[0]:
                  args[3]: self,tree,node
                  docstring: null
                  start_line: 95
                  end_line: 100
                - identifier: backend.relationship_analyzer.ProjectAnalyzer._resolve_calls
                  name: _resolve_calls
                  calls[0]:
                  called_by[0]:
                  args[2]: self,filepath
                  docstring: null
                  start_line: 102
                  end_line: 114
          - mode: class_analysis
            identifier: backend.relationship_analyzer.CallResolverVisitor
            name: CallResolverVisitor
            docstring: null
            source_code: "class CallResolverVisitor(ast.NodeVisitor):\n    def __init__(self, filepath, project_root, definitions):\n        self.filepath = filepath\n        self.module_path = path_to_module(filepath, project_root)\n        self.definitions = definitions\n        self.scope = {}\n        self.instance_types = {}\n        self.current_caller_name = self.module_path\n        self.current_class_name = None\n        self.calls = defaultdict(list)\n\n    def visit_ClassDef(self, node):\n        old_class_name = self.current_class_name\n        self.current_class_name = node.name\n        self.generic_visit(node)\n        self.current_class_name = old_class_name\n\n    def visit_FunctionDef(self, node):\n        old_caller = self.current_caller_name\n        \n        if self.current_class_name:\n            full_identifier = f\"{self.module_path}.{self.current_class_name}.{node.name}\"\n        else:\n            full_identifier = f\"{self.module_path}.{node.name}\"\n            \n        self.current_caller_name = full_identifier\n        self.generic_visit(node)\n        self.current_caller_name = old_caller\n\n    def visit_Call(self, node):\n        callee_pathname = self._resolve_call_qname(node.func)\n        if callee_pathname and callee_pathname in self.definitions:\n            \n            if self.current_caller_name == self.module_path:\n                caller_type = 'module'\n            elif '.<locals>.' in self.current_caller_name:\n                caller_type = 'local_function'\n            elif self.current_class_name:\n                caller_type = 'method'\n            else:\n                caller_type = 'function'\n            \n            caller_info = {\n                'file': os.path.basename(self.filepath),\n                'line': node.lineno,\n                'caller': self.current_caller_name,\n                'caller_type': caller_type \n            }\n            self.calls[callee_pathname].append(caller_info)\n        self.generic_visit(node)\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.scope[alias.asname or alias.name] = alias.name\n        self.generic_visit(node)\n        \n    def visit_ImportFrom(self, node):\n        module = node.module or ''\n        for alias in node.names:\n            name = alias.asname or alias.name\n            if node.level > 0:\n                base = self.module_path.split('.')\n                prefix = '.'.join(base[:-node.level])\n                full_module_path = f\"{prefix}.{module}\" if module else prefix\n            else:\n                full_module_path = module\n            self.scope[name] = f\"{full_module_path}.{alias.name}\"\n        self.generic_visit(node)\n\n    def visit_Assign(self, node):\n        if isinstance(node.value, ast.Call) and isinstance(node.value.func, ast.Name):\n            class_name = node.value.func.id\n            if class_name in self.scope:\n                qualified_class_name = self.scope[class_name]\n                if qualified_class_name in self.definitions:\n                    for target in node.targets:\n                        if isinstance(target, ast.Name):\n                            self.instance_types[target.id] = qualified_class_name\n        self.generic_visit(node)\n\n    def _resolve_call_qname(self, func_node):\n        if isinstance(func_node, ast.Name):\n            name = func_node.id\n            if name in self.scope:\n                return self.scope[name]\n            local_pathname = f\"{self.module_path}.{name}\"\n            if local_pathname in self.definitions:\n                return local_pathname\n        elif isinstance(func_node, ast.Attribute) and isinstance(func_node.value, ast.Name):\n            var_name = func_node.value.id\n            method_name = func_node.attr\n            if var_name in self.instance_types:\n                class_pathname = self.instance_types[var_name]\n                return f\"{class_pathname}.{method_name}\"\n            if var_name in self.scope:\n                module_pathname = self.scope[var_name]\n                return f\"{module_pathname}.{method_name}\"\n        return None"
            start_line: 117
            end_line: 214
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[8]:
                - identifier: backend.relationship_analyzer.CallResolverVisitor.__init__
                  name: __init__
                  calls[0]:
                  called_by[0]:
                  args[4]: self,filepath,project_root,definitions
                  docstring: null
                  start_line: 118
                  end_line: 126
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_ClassDef
                  name: visit_ClassDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 128
                  end_line: 132
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_FunctionDef
                  name: visit_FunctionDef
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 134
                  end_line: 144
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Call
                  name: visit_Call
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 146
                  end_line: 166
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Import
                  name: visit_Import
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 168
                  end_line: 171
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_ImportFrom
                  name: visit_ImportFrom
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 173
                  end_line: 184
                - identifier: backend.relationship_analyzer.CallResolverVisitor.visit_Assign
                  name: visit_Assign
                  calls[0]:
                  called_by[0]:
                  args[2]: self,node
                  docstring: null
                  start_line: 186
                  end_line: 195
                - identifier: backend.relationship_analyzer.CallResolverVisitor._resolve_call_qname
                  name: _resolve_call_qname
                  calls[0]:
                  called_by[0]:
                  args[2]: self,func_node
                  docstring: null
                  start_line: 197
                  end_line: 214
    "backend/scads_key_test.py":
      ast_nodes:
        imports[3]: os,dotenv.load_dotenv,openai.OpenAI
        functions[0]:
        classes[0]:
    "database/db.py":
      ast_nodes:
        imports[7]: datetime.datetime,pymongo.MongoClient,dotenv.load_dotenv,streamlit_authenticator,cryptography.fernet.Fernet,uuid,os
        functions[29]:
          - mode: function_analysis
            identifier: database.db.encrypt_text
            name: encrypt_text
            args[1]: text
            docstring: null
            source_code: "def encrypt_text(text: str) -> str:\n    if not text or not cipher_suite: return text\n    return cipher_suite.encrypt(text.strip().encode()).decode()"
            start_line: 28
            end_line: 30
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.decrypt_text
            name: decrypt_text
            args[1]: text
            docstring: null
            source_code: "def decrypt_text(text: str) -> str:\n    if not text or not cipher_suite: return text\n    try:\n        return cipher_suite.decrypt(text.strip().encode()).decode()\n    except Exception:\n        return text"
            start_line: 32
            end_line: 37
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.insert_user
            name: insert_user
            args[3]: username,name,password
            docstring: null
            source_code: "def insert_user(username: str, name: str, password: str):\n    user = {\n        \"_id\": username,\n        \"name\": name, \n        \"hashed_password\": stauth.Hasher.hash(password),\n        \"gemini_api_key\": \"\",\n        \"ollama_base_url\": \"\",\n        \"gpt_api_key\": \"\"\n    }\n    result = dbusers.insert_one(user)\n    return result.inserted_id"
            start_line: 43
            end_line: 53
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_all_users
            name: fetch_all_users
            args[0]:
            docstring: null
            source_code: "def fetch_all_users():\n    return list(dbusers.find())"
            start_line: 55
            end_line: 56
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_user
            name: fetch_user
            args[1]: username
            docstring: null
            source_code: "def fetch_user(username: str):\n    return dbusers.find_one({\"_id\": username})"
            start_line: 58
            end_line: 59
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_user_name
            name: update_user_name
            args[2]: username,new_name
            docstring: null
            source_code: "def update_user_name(username: str, new_name: str):\n    # Achtung: _id kann in Mongo nicht einfach so ge√§ndert werden. \n    # Hier wird nur das Name-Feld geupdated.\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"name\": new_name}})\n    return result.modified_count"
            start_line: 61
            end_line: 65
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_gemini_key
            name: update_gemini_key
            args[2]: username,gemini_api_key
            docstring: null
            source_code: "def update_gemini_key(username: str, gemini_api_key: str):\n    encrypted_key = encrypt_text(gemini_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"gemini_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 67
            end_line: 70
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_gpt_key
            name: update_gpt_key
            args[2]: username,gpt_api_key
            docstring: null
            source_code: "def update_gpt_key(username: str, gpt_api_key: str):\n    encrypted_key = encrypt_text(gpt_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"gpt_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 72
            end_line: 75
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_ollama_url
            name: update_ollama_url
            args[2]: username,ollama_base_url
            docstring: null
            source_code: "def update_ollama_url(username: str, ollama_base_url: str):\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"ollama_base_url\": ollama_base_url.strip()}})\n    return result.modified_count"
            start_line: 77
            end_line: 79
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_opensrc_key
            name: update_opensrc_key
            args[2]: username,opensrc_api_key
            docstring: null
            source_code: "def update_opensrc_key(username: str, opensrc_api_key: str):\n    encrypted_key = encrypt_text(opensrc_api_key.strip())\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"opensrc_api_key\": encrypted_key}})\n    return result.modified_count"
            start_line: 81
            end_line: 84
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_opensrc_url
            name: update_opensrc_url
            args[2]: username,opensrc_base_url
            docstring: null
            source_code: "def update_opensrc_url(username: str, opensrc_base_url: str):\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"opensrc_base_url\": opensrc_base_url.strip()}})\n    return result.modified_count"
            start_line: 86
            end_line: 88
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_gemini_key
            name: fetch_gemini_key
            args[1]: username
            docstring: null
            source_code: "def fetch_gemini_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"gemini_api_key\": 1, \"_id\": 0})\n    return user.get(\"gemini_api_key\") if user else None"
            start_line: 90
            end_line: 92
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_ollama_url
            name: fetch_ollama_url
            args[1]: username
            docstring: null
            source_code: "def fetch_ollama_url(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"ollama_base_url\": 1, \"_id\": 0})\n    return user.get(\"ollama_base_url\") if user else None"
            start_line: 94
            end_line: 96
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_gpt_key
            name: fetch_gpt_key
            args[1]: username
            docstring: null
            source_code: "def fetch_gpt_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"gpt_api_key\": 1, \"_id\": 0})\n    return user.get(\"gpt_api_key\") if user else None"
            start_line: 98
            end_line: 100
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_opensrc_key
            name: fetch_opensrc_key
            args[1]: username
            docstring: null
            source_code: "def fetch_opensrc_key(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"opensrc_api_key\": 1, \"_id\": 0})\n    return user.get(\"opensrc_api_key\") if user else None"
            start_line: 102
            end_line: 104
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_opensrc_url
            name: fetch_opensrc_url
            args[1]: username
            docstring: null
            source_code: "def fetch_opensrc_url(username: str):\n    user = dbusers.find_one({\"_id\": username}, {\"opensrc_base_url\": 1, \"_id\": 0})\n    return user.get(\"opensrc_base_url\") if user else None"
            start_line: 106
            end_line: 108
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.delete_user
            name: delete_user
            args[1]: username
            docstring: null
            source_code: "def delete_user(username: str):\n    return dbusers.delete_one({\"_id\": username}).deleted_count"
            start_line: 110
            end_line: 111
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.get_decrypted_api_keys
            name: get_decrypted_api_keys
            args[1]: username
            docstring: null
            source_code: "def get_decrypted_api_keys(username: str):\n    user = dbusers.find_one({\"_id\": username})\n    if not user: return None, None\n    gemini_plain = decrypt_text(user.get(\"gemini_api_key\", \"\"))\n    ollama_plain = user.get(\"ollama_base_url\", \"\")\n    gpt_plain = decrypt_text(user.get(\"gpt_api_key\", \"\"))\n    opensrc_plain = decrypt_text(user.get(\"opensrc_api_key\", \"\"))\n    opensrc_url = user.get(\"opensrc_base_url\", \"\")\n    return gemini_plain, ollama_plain, gpt_plain, opensrc_plain, opensrc_url"
            start_line: 113
            end_line: 121
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.insert_chat
            name: insert_chat
            args[2]: username,chat_name
            docstring: Erstellt einen neuen Chat-Eintrag.
            source_code: "def insert_chat(username: str, chat_name: str):\n    \"\"\"Erstellt einen neuen Chat-Eintrag.\"\"\"\n    chat = {\n        \"_id\": str(uuid.uuid4()),\n        \"username\": username,\n        \"chat_name\": chat_name,\n        \"created_at\": datetime.now()\n    }\n    result = dbchats.insert_one(chat)\n    return result.inserted_id"
            start_line: 127
            end_line: 136
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_chats_by_user
            name: fetch_chats_by_user
            args[1]: username
            docstring: Holt alle definierten Chats eines Users.
            source_code: "def fetch_chats_by_user(username: str):\n    \"\"\"Holt alle definierten Chats eines Users.\"\"\"\n    # Sortieren nach Erstellung macht Sinn\n    chats = list(dbchats.find({\"username\": username}).sort(\"created_at\", 1))\n    return chats"
            start_line: 138
            end_line: 142
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.check_chat_exists
            name: check_chat_exists
            args[2]: username,chat_name
            docstring: null
            source_code: "def check_chat_exists(username: str, chat_name: str):\n    return dbchats.find_one({\"username\": username, \"chat_name\": chat_name}) is not None"
            start_line: 144
            end_line: 145
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.rename_chat_fully
            name: rename_chat_fully
            args[3]: username,old_name,new_name
            docstring: Benennt einen Chat und alle zugeh√∂rigen Exchanges um.
            source_code: "def rename_chat_fully(username: str, old_name: str, new_name: str):\n    \"\"\"\n    Benennt einen Chat und alle zugeh√∂rigen Exchanges um.\n    \"\"\"\n    # 1. Chat-Eintrag umbenennen\n    result = dbchats.update_one(\n        {\"username\": username, \"chat_name\": old_name}, \n        {\"$set\": {\"chat_name\": new_name}}\n    )\n    \n    # 2. Alle Messages (Exchanges) umh√§ngen\n    dbexchanges.update_many(\n        {\"username\": username, \"chat_name\": old_name},\n        {\"$set\": {\"chat_name\": new_name}}\n    )\n    return result.modified_count"
            start_line: 148
            end_line: 163
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.insert_exchange
            name: insert_exchange
            args[13]: question,answer,feedback,username,chat_name,helper_used,main_used,total_time,helper_time,main_time,json_tokens,toon_tokens,savings_percent
            docstring: null
            source_code: "def insert_exchange(question: str, answer: str, feedback: str, username: str, chat_name: str, \n                    helper_used: str=\"\", main_used: str=\"\", total_time: str=\"\", helper_time: str=\"\", main_time: str=\"\",\n                    json_tokens=0, toon_tokens=0, savings_percent=0.0):\n    new_id = str(uuid.uuid4())\n    exchange = {\n        \"_id\": new_id,\n        \"question\": question,\n        \"answer\": answer,\n        \"feedback\": feedback,\n        \"feedback_message\": \"\",\n        \"chat_name\": chat_name,\n        \"username\": username,\n        \"helper_used\": helper_used,\n        \"main_used\": main_used,\n        \"total_time\": total_time,\n        \"helper_time\": helper_time,\n        \"main_time\": main_time,\n        \"json_tokens\": json_tokens,\n        \"toon_tokens\": toon_tokens,\n        \"savings_percent\": savings_percent,\n        \"created_at\": datetime.now()\n    }\n    try:\n        dbexchanges.insert_one(exchange)\n        return new_id\n    except Exception as e:\n        print(f\"DB Error: {e}\")\n        return None"
            start_line: 169
            end_line: 196
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_exchanges_by_user
            name: fetch_exchanges_by_user
            args[1]: username
            docstring: null
            source_code: "def fetch_exchanges_by_user(username: str):\n    # Sortieren nach Zeitstempel wichtig f√ºr die Anzeige\n    exchanges = list(dbexchanges.find({\"username\": username}).sort(\"created_at\", 1))\n    return exchanges"
            start_line: 198
            end_line: 201
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.fetch_exchanges_by_chat
            name: fetch_exchanges_by_chat
            args[2]: username,chat_name
            docstring: null
            source_code: "def fetch_exchanges_by_chat(username: str, chat_name: str):\n    exchanges = list(dbexchanges.find({\"username\": username, \"chat_name\": chat_name}).sort(\"created_at\", 1))\n    return exchanges"
            start_line: 203
            end_line: 205
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_exchange_feedback
            name: update_exchange_feedback
            args[2]: exchange_id,feedback
            docstring: null
            source_code: "def update_exchange_feedback(exchange_id, feedback: int):\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback\": feedback}})\n    return result.modified_count"
            start_line: 207
            end_line: 209
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.update_exchange_feedback_message
            name: update_exchange_feedback_message
            args[2]: exchange_id,feedback_message
            docstring: null
            source_code: "def update_exchange_feedback_message(exchange_id, feedback_message: str):\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback_message\": feedback_message}})\n    return result.modified_count"
            start_line: 211
            end_line: 213
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.delete_exchange_by_id
            name: delete_exchange_by_id
            args[1]: exchange_id
            docstring: null
            source_code: "def delete_exchange_by_id(exchange_id: str):\n    result = dbexchanges.delete_one({\"_id\": exchange_id})\n    return result.deleted_count"
            start_line: 215
            end_line: 217
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: database.db.delete_full_chat
            name: delete_full_chat
            args[2]: username,chat_name
            docstring: "L√∂scht den Chat UND alle zugeh√∂rigen Exchanges.\nDas sorgt f√ºr Konsistenz zwischen Frontend und Backend."
            source_code: "def delete_full_chat(username: str, chat_name: str):\n    \"\"\"\n    L√∂scht den Chat UND alle zugeh√∂rigen Exchanges.\n    Das sorgt f√ºr Konsistenz zwischen Frontend und Backend.\n    \"\"\"\n    # 1. Alle Nachrichten in diesem Chat l√∂schen\n    del_exchanges = dbexchanges.delete_many({\"username\": username, \"chat_name\": chat_name})\n    \n    # 2. Den Chat selbst aus der Chat-Liste l√∂schen\n    del_chat = dbchats.delete_one({\"username\": username, \"chat_name\": chat_name})\n    \n    return del_chat.deleted_count"
            start_line: 221
            end_line: 232
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "frontend/frontend.py":
      ast_nodes:
        imports[16]: numpy,datetime.datetime,time,pymongo.MongoClient,dotenv.load_dotenv,os,sys,urllib.parse.urlparse,logging,traceback,re,streamlit_mermaid.st_mermaid,backend.main,database.db,streamlit,streamlit_authenticator
        functions[12]:
          - mode: function_analysis
            identifier: frontend.frontend.clean_names
            name: clean_names
            args[1]: model_list
            docstring: null
            source_code: "def clean_names(model_list):\n    return [m.split(\"/\")[-1] for m in model_list]"
            start_line: 54
            end_line: 55
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.get_filtered_models
            name: get_filtered_models
            args[2]: source_list,category_name
            docstring: Filtert eine Liste basierend auf der gew√§hlten Kategorie.
            source_code: "def get_filtered_models(source_list, category_name):\n    \"\"\"Filtert eine Liste basierend auf der gew√§hlten Kategorie.\"\"\"\n    keywords = CATEGORY_KEYWORDS.get(category_name, [\"\"])\n    \n    if \"STANDARD\" in keywords:\n        # Nur Modelle zur√ºckgeben, die auch in der Standard-Liste sind\n        return [m for m in source_list if m in STANDARD_MODELS]\n    \n    filtered = []\n    for model in source_list:\n        # Pr√ºfen ob ein Keyword im Namen steckt\n        if any(k in model.lower() for k in keywords):\n            filtered.append(model)\n            \n    return filtered if filtered else source_list"
            start_line: 86
            end_line: 100
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.save_gemini_cb
            name: save_gemini_cb
            args[0]:
            docstring: null
            source_code: "def save_gemini_cb():\n    new_key = st.session_state.get(\"in_gemini_key\", \"\")\n    if new_key:\n        db.update_gemini_key(st.session_state[\"username\"], new_key)\n        st.session_state[\"in_gemini_key\"] = \"\"\n        st.toast(\"Gemini Key erfolgreich gespeichert! ‚úÖ\")"
            start_line: 143
            end_line: 148
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.save_ollama_cb
            name: save_ollama_cb
            args[0]:
            docstring: null
            source_code: "def save_ollama_cb():\n    new_url = st.session_state.get(\"in_ollama_url\", \"\")\n    if new_url:\n        db.update_ollama_url(st.session_state[\"username\"], new_url)\n        st.toast(\"Ollama URL gespeichert! ‚úÖ\")"
            start_line: 150
            end_line: 154
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.load_data_from_db
            name: load_data_from_db
            args[1]: username
            docstring: L√§dt Chats und Exchanges konsistent aus der DB.
            source_code: "def load_data_from_db(username: str):\n    \"\"\"L√§dt Chats und Exchanges konsistent aus der DB.\"\"\"\n    \n    # Nur laden, wenn neuer User oder noch nicht geladen\n    if \"loaded_user\" not in st.session_state or st.session_state.loaded_user != username:\n        st.session_state.chats = {}\n        \n        # 1. Erst die definierten Chats laden (damit auch leere Chats da sind)\n        db_chats = db.fetch_chats_by_user(username)\n        for c in db_chats:\n            c_name = c.get(\"chat_name\")\n            if c_name:\n                st.session_state.chats[c_name] = {\"exchanges\": []}\n\n        # 2. Dann die Exchanges laden und einsortieren\n        db_exchanges = db.fetch_exchanges_by_user(username)\n        for ex in db_exchanges:\n            c_name = ex.get(\"chat_name\", \"Unbenannt\")\n            \n            # Falls Exchanges existieren f√ºr Chats, die nicht in dbchats sind (Legacy Support)\n            if c_name not in st.session_state.chats:\n                st.session_state.chats[c_name] = {\"exchanges\": []}\n            \n            if \"feedback\" not in ex or ex[\"feedback\"] is None:\n                ex[\"feedback\"] = np.nan\n            \n            st.session_state.chats[c_name][\"exchanges\"].append(ex)\n\n        # 3. Default Chat erstellen, falls gar nichts existiert\n        if not st.session_state.chats:\n            initial_name = \"Chat 1\"\n            # Konsistent in DB anlegen\n            db.insert_chat(username, initial_name)\n            st.session_state.chats[initial_name] = {\"exchanges\": []}\n            st.session_state.active_chat = initial_name\n        else:\n            # Active Chat setzen, falls n√∂tig\n            if \"active_chat\" not in st.session_state or st.session_state.active_chat not in st.session_state.chats:\n                # Nimm den ersten verf√ºgbaren Chat\n                st.session_state.active_chat = list(st.session_state.chats.keys())[0]\n        \n        st.session_state.loaded_user = username"
            start_line: 160
            end_line: 201
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.handle_feedback_change
            name: handle_feedback_change
            args[2]: ex,val
            docstring: null
            source_code: "def handle_feedback_change(ex, val):\n    ex[\"feedback\"] = val\n    db.update_exchange_feedback(ex[\"_id\"], val)\n    st.rerun()"
            start_line: 207
            end_line: 210
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.handle_delete_exchange
            name: handle_delete_exchange
            args[2]: chat_name,ex
            docstring: null
            source_code: "def handle_delete_exchange(chat_name, ex):\n    db.delete_exchange_by_id(ex[\"_id\"])\n    if chat_name in st.session_state.chats:\n        if ex in st.session_state.chats[chat_name][\"exchanges\"]:\n            st.session_state.chats[chat_name][\"exchanges\"].remove(ex)\n    st.rerun()"
            start_line: 212
            end_line: 217
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.handle_delete_chat
            name: handle_delete_chat
            args[2]: username,chat_name
            docstring: null
            source_code: "def handle_delete_chat(username, chat_name):\n    # KONSISTENZ: Ruft jetzt delete_full_chat in DB auf\n    db.delete_full_chat(username, chat_name)\n    \n    # State bereinigen\n    if chat_name in st.session_state.chats:\n        del st.session_state.chats[chat_name]\n    \n    # Active Chat neu setzen\n    if len(st.session_state.chats) > 0:\n        st.session_state.active_chat = list(st.session_state.chats.keys())[0]\n    else:\n        # Wenn alles weg ist, neuen leeren Chat anlegen\n        new_name = \"Chat 1\"\n        db.insert_chat(username, new_name)\n        st.session_state.chats[new_name] = {\"exchanges\": []}\n        st.session_state.active_chat = new_name\n        \n    st.rerun()"
            start_line: 219
            end_line: 237
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.extract_repo_name
            name: extract_repo_name
            args[1]: text
            docstring: null
            source_code: "def extract_repo_name(text):\n    url_match = re.search(r'(https?://[^\\s]+)', text)\n    if url_match:\n        url = url_match.group(0)\n        parsed = urlparse(url)\n        path = parsed.path.strip(\"/\")\n        if path:\n            repo_name = path.split(\"/\")[-1]\n            if repo_name.endswith(\".git\"):\n                repo_name = repo_name[:-4]\n            return repo_name\n    return None"
            start_line: 243
            end_line: 254
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.stream_text_generator
            name: stream_text_generator
            args[1]: text
            docstring: null
            source_code: "def stream_text_generator(text):\n    for word in text.split(\" \"):\n        yield word + \" \"\n        time.sleep(0.01)"
            start_line: 256
            end_line: 259
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.render_text_with_mermaid
            name: render_text_with_mermaid
            args[2]: markdown_text,should_stream
            docstring: null
            source_code: "def render_text_with_mermaid(markdown_text, should_stream=False):\n    if not markdown_text:\n        return\n\n    parts = re.split(r\"```mermaid\\s+(.*?)\\s+```\", markdown_text, flags=re.DOTALL)\n\n    for i, part in enumerate(parts):\n        if i % 2 == 0:\n            if part.strip():\n                if should_stream:\n                    st.write_stream(stream_text_generator(part))\n                else:\n                    st.markdown(part)\n        else:\n            try:\n                st_mermaid(part, key=f\"mermaid_{hash(part)}_{i}\")\n            except Exception:\n                st.code(part, language=\"mermaid\")"
            start_line: 261
            end_line: 278
            context:
              calls[0]:
              called_by[0]:
          - mode: function_analysis
            identifier: frontend.frontend.render_exchange
            name: render_exchange
            args[2]: ex,current_chat_name
            docstring: null
            source_code: "def render_exchange(ex, current_chat_name):\n    st.chat_message(\"user\").write(ex[\"question\"])\n\n    with st.chat_message(\"assistant\"):\n        answer_text = ex.get(\"answer\", \"\")\n        is_error = answer_text.startswith(\"Fehler\") or answer_text.startswith(\"Error\")\n\n        # --- 1. TOOLBAR CONTAINER ---\n        # Nutzt die neuen Parameter horizontal=True und alignment\n        # border=True rahmt die Toolbar ein\n        # horizontal_alignment=\"right\" schiebt alles nach rechts\n        with st.container(horizontal=True, horizontal_alignment=\"right\"):\n            \n            if not is_error:\n                # 1. Status Text (wird jetzt links von den Buttons angezeigt, aber rechtsb√ºndig im Container)\n                st.write(\"hier die Antwort:\")\n                if ex.get(\"feedback\") == 1:\n                    st.caption(\"‚úÖ Hilfreich\")\n                elif ex.get(\"feedback\") == 0:\n                    st.caption(\"‚ùå Nicht hilfreich\")\n                \n                # 2. Die Buttons (einfach untereinander im Code, erscheinen nebeneinander im UI)\n                \n                # Like\n                type_primary_up = ex.get(\"feedback\") == 1\n                if st.button(\"üëç\", key=f\"up_{ex['_id']}\", type=\"primary\" if type_primary_up else \"secondary\", help=\"Hilfreich\"):\n                    handle_feedback_change(ex, 1)\n\n                # Dislike\n                type_primary_down = ex.get(\"feedback\") == 0\n                if st.button(\"üëé\", key=f\"down_{ex['_id']}\", type=\"primary\" if type_primary_down else \"secondary\", help=\"Nicht hilfreich\"):\n                    handle_feedback_change(ex, 0)\n\n                # Comment Popover\n                with st.popover(\"üí¨\", help=\"Notiz hinzuf√ºgen\"):\n                    msg = st.text_area(\"Notiz:\", value=ex.get(\"feedback_message\", \"\"), key=f\"txt_{ex['_id']}\")\n                    if st.button(\"Speichern\", key=f\"save_{ex['_id']}\"):\n                        ex[\"feedback_message\"] = msg\n                        db.update_exchange_feedback_message(ex[\"_id\"], msg)\n                        st.success(\"Gespeichert!\")\n                        time.sleep(0.5)\n                        st.rerun()\n\n                # Download\n                st.download_button(\n                    label=\"üì•\",\n                    data=ex[\"answer\"],\n                    file_name=f\"response_{ex['_id']}.md\",\n                    mime=\"text/markdown\",\n                    key=f\"dl_{ex['_id']}\",\n                    help=\"Als Markdown herunterladen\"\n                )\n\n                # Delete\n                if st.button(\"üóëÔ∏è\", key=f\"del_{ex['_id']}\", help=\"Nachricht l√∂schen\", type=\"secondary\"):\n                    handle_delete_exchange(current_chat_name, ex)\n\n            else:\n                # Fehlerfall\n                st.error(\"‚ö†Ô∏è Fehler\")\n                if st.button(\"üóëÔ∏è L√∂schen\", key=f\"del_err_{ex['_id']}\"):\n                    handle_delete_exchange(current_chat_name, ex)\n\n        # --- 2. CONTENT (Antwort) ---\n        with st.container(height=500, border=True):\n             render_text_with_mermaid(ex[\"answer\"], should_stream=False)"
            start_line: 284
            end_line: 349
            context:
              calls[0]:
              called_by[0]:
        classes[0]:
    "schemas/types.py":
      ast_nodes:
        imports[5]: typing.List,typing.Optional,typing.Literal,pydantic.BaseModel,pydantic.ValidationError
        functions[0]:
        classes[15]:
          - mode: class_analysis
            identifier: schemas.types.ParameterDescription
            name: ParameterDescription
            docstring: Describes a single parameter of a function.
            source_code: "class ParameterDescription(BaseModel):\n    \"\"\"Describes a single parameter of a function.\"\"\"\n    name: str\n    type: str\n    description: str"
            start_line: 6
            end_line: 10
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ReturnDescription
            name: ReturnDescription
            docstring: Describes the return value of a function.
            source_code: "class ReturnDescription(BaseModel):\n    \"\"\"Describes the return value of a function.\"\"\"\n    name: str\n    type: str\n    description: str"
            start_line: 12
            end_line: 16
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.UsageContext
            name: UsageContext
            docstring: Describes the calling context of a function.
            source_code: "class UsageContext(BaseModel):\n    \"\"\"Describes the calling context of a function.\"\"\"\n    calls: str\n    called_by: str"
            start_line: 18
            end_line: 21
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionDescription
            name: FunctionDescription
            docstring: Contains the detailed analysis of a function's purpose and signature.
            source_code: "class FunctionDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a function's purpose and signature.\"\"\"\n    overall: str\n    parameters: List[ParameterDescription]\n    returns: List[ReturnDescription]\n    usage_context: UsageContext"
            start_line: 23
            end_line: 28
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionAnalysis
            name: FunctionAnalysis
            docstring: The main model representing the entire JSON schema for a function.
            source_code: "class FunctionAnalysis(BaseModel):\n    \"\"\"The main model representing the entire JSON schema for a function.\"\"\"\n    identifier: str\n    description: FunctionDescription\n    error: Optional[str] = None"
            start_line: 30
            end_line: 34
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ConstructorDescription
            name: ConstructorDescription
            docstring: Describes the __init__ method of a class.
            source_code: "class ConstructorDescription(BaseModel):\n    \"\"\"Describes the __init__ method of a class.\"\"\"\n    description: str\n    parameters: List[ParameterDescription]"
            start_line: 39
            end_line: 42
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassContext
            name: ClassContext
            docstring: Describes the class's external dependencies and primary points of instantiation.
            source_code: "class ClassContext(BaseModel):\n    \"\"\"Describes the class's external dependencies and primary points of instantiation.\"\"\"\n    dependencies: str\n    instantiated_by: str"
            start_line: 44
            end_line: 47
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassDescription
            name: ClassDescription
            docstring: "Contains the detailed analysis of a class's purpose, constructor, and methods."
            source_code: "class ClassDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a class's purpose, constructor, and methods.\"\"\"\n    overall: str\n    init_method: ConstructorDescription\n    methods: List[FunctionAnalysis]\n    usage_context: ClassContext"
            start_line: 49
            end_line: 54
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassAnalysis
            name: ClassAnalysis
            docstring: The main model for the entire JSON schema for a class.
            source_code: "class ClassAnalysis(BaseModel):\n    \"\"\"The main model for the entire JSON schema for a class.\"\"\"\n    identifier: str\n    description: ClassDescription\n    error: Optional[str] = None"
            start_line: 56
            end_line: 60
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.CallInfo
            name: CallInfo
            docstring: "Represents a specific call event from the relationship analyzer.\nUsed in 'called_by' and 'instantiated_by' lists."
            source_code: "class CallInfo(BaseModel):\n    \"\"\"\n    Represents a specific call event from the relationship analyzer.\n    Used in 'called_by' and 'instantiated_by' lists.\n    \"\"\"\n    file: str\n    function: str  # Name des Aufrufers\n    mode: str      # z.B. 'method', 'function', 'module'\n    line: int"
            start_line: 65
            end_line: 73
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionContextInput
            name: FunctionContextInput
            docstring: Structured context for analyzing a function.
            source_code: "class FunctionContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a function.\"\"\"\n    calls: List[str]\n    called_by: List[CallInfo]"
            start_line: 78
            end_line: 81
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.FunctionAnalysisInput
            name: FunctionAnalysisInput
            docstring: The required input to generate a FunctionAnalysis object.
            source_code: "class FunctionAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a FunctionAnalysis object.\"\"\"\n    mode: Literal[\"function_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: FunctionContextInput"
            start_line: 83
            end_line: 89
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.MethodContextInput
            name: MethodContextInput
            docstring: Structured context for a classes methods
            source_code: "class MethodContextInput(BaseModel):\n    \"\"\"Structured context for a classes methods\"\"\"\n    identifier: str\n    calls: List[str]\n    called_by: List[CallInfo]\n    args: List[str]\n    docstring: Optional[str]"
            start_line: 94
            end_line: 100
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassContextInput
            name: ClassContextInput
            docstring: Structured context for analyzing a class.
            source_code: "class ClassContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a class.\"\"\"\n    dependencies: List[str]\n    instantiated_by: List[CallInfo]\n    method_context: List[MethodContextInput]"
            start_line: 102
            end_line: 106
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
          - mode: class_analysis
            identifier: schemas.types.ClassAnalysisInput
            name: ClassAnalysisInput
            docstring: The required input to generate a ClassAnalysis object.
            source_code: "class ClassAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a ClassAnalysis object.\"\"\"\n    mode: Literal[\"class_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: ClassContextInput"
            start_line: 108
            end_line: 114
            context:
              dependencies[0]:
              instantiated_by[0]:
              method_context[0]:
analysis_results:
  functions:
    backend.AST_Schema.path_to_module:
      identifier: backend.AST_Schema.path_to_module
      description:
        overall: "This function converts a given file path into its corresponding Python module path. It first attempts to determine the relative path of the file with respect to a specified project root. If a ValueError occurs during this process, it falls back to using the base name of the file. It then removes the '.py' extension if present and replaces operating system path separators with dots. Finally, it handles '__init__' modules by removing the trailing '.__init__' segment to yield the correct package path."
        parameters[2]{name,type,description}:
          filepath,str,The absolute or relative path to the file that needs to be converted.
          project_root,str,"The root directory of the project, used as a reference to calculate the relative path of the file."
        returns[1]{name,type,description}:
          module_path,str,"The converted Python module path string, representing the file's location within the project's module structure."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.File_Dependency.build_file_dependency_graph:
      identifier: backend.File_Dependency.build_file_dependency_graph
      description:
        overall: "This function constructs a directed graph representing file-level import dependencies within a given Abstract Syntax Tree (AST). It initializes a NetworkX directed graph, then uses a custom `FileDependencyGraph` visitor to traverse the provided AST and identify import relationships. The function iterates through the collected import dependencies, adding nodes for each caller and callee file, and creating directed edges to represent the import relationships. The resulting graph illustrates which files import other files within the repository context."
        parameters[3]{name,type,description}:
          filename,str,The name of the file being analyzed for dependencies.
          tree,AST,The Abstract Syntax Tree (AST) of the file to analyze.
          repo_root,str,"The root directory of the repository, used for resolving file paths and dependencies."
        returns[1]{name,type,description}:
          graph,nx.DiGraph,A directed graph where nodes represent files and edges indicate import dependencies (caller imports callee).
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.File_Dependency.build_repository_graph:
      identifier: backend.File_Dependency.build_repository_graph
      description:
        overall: "This function constructs a directed graph representing the dependencies between Python files and their internal components within a given Git repository. It iterates through all Python files, parses each into an Abstract Syntax Tree (AST), and then uses a helper function to build a dependency graph for that specific file. Finally, it aggregates these individual file graphs into a single global graph, which is then returned."
        parameters[1]{name,type,description}:
          repository,GitRepository,The GitRepository object containing the files to be analyzed for dependencies.
        returns[1]{name,type,description}:
          global_graph,networkx.DiGraph,"A NetworkX directed graph where nodes represent Python files or their internal components, and edges represent dependencies between them."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.File_Dependency.get_all_temp_files:
      identifier: backend.File_Dependency.get_all_temp_files
      description:
        overall: "This function is designed to locate and list all Python files within a specified directory and its subdirectories. It takes a string representing a directory path as input. The function first resolves the absolute path of the given directory. It then recursively searches this root path for all files that have a '.py' extension. Finally, it returns a list of these Python file paths, with each path represented relative to the initial root directory."
        parameters[1]{name,type,description}:
          directory,str,The string path to the root directory from which to start searching for Python files.
        returns[1]{name,type,description}:
          all_files,"list[Path]","A list of pathlib.Path objects, where each object represents a Python file found within the specified directory, relative to the root directory."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    backend.HelperLLM.main_orchestrator:
      identifier: backend.HelperLLM.main_orchestrator
      description:
        overall: "This function serves as a main orchestrator for testing the LLMHelper class, defining dummy data and a processing loop. It sets up pre-computed analysis inputs and outputs for several example functions (add_item, check_stock, generate_report) and a class (InventoryManager) using Pydantic models. It then instantiates an LLMHelper and simulates the generation of function documentation, logging the process and printing the final aggregated results."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by other functions in the provided context.
      error: null
    backend.callgraph.make_safe_dot:
      identifier: backend.callgraph.make_safe_dot
      description:
        overall: "This function takes a NetworkX directed graph and an output file path. It creates a copy of the input graph and relabels its nodes with simple, safe identifiers (e.g., \"n0\", \"n1\") to ensure compatibility with DOT file format requirements. The original node names are preserved by being added as a 'label' attribute to the new, safe nodes. Finally, the modified graph is written to the specified output path as a DOT file."
        parameters[2]{name,type,description}:
          graph,nx.DiGraph,The input NetworkX directed graph to be processed and saved.
          out_path,str,The file path where the DOT representation of the graph will be saved.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.callgraph.build_filtered_callgraph:
      identifier: backend.callgraph.build_filtered_callgraph
      description:
        overall: "This function constructs a directed call graph for a given Git repository. It first identifies all Python files within the repository and parses them into Abstract Syntax Trees (ASTs). Using a `CallGraph` visitor, it extracts all 'self-written' functions. Subsequently, it iterates through the parsed files again to build a global call graph, adding edges only between functions that are identified as 'self-written'. The resulting graph represents the call relationships exclusively among the repository's own functions."
        parameters[1]{name,type,description}:
          repo,GitRepository,The Git repository object from which to extract and analyze Python files.
        returns[1]{name,type,description}:
          global_graph,nx.DiGraph,A NetworkX directed graph representing the call relationships between 'self-written' functions found within the repository.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by other functions in the provided context.
      error: null
    backend.converter.wrap_cdata:
      identifier: backend.converter.wrap_cdata
      description:
        overall: "The `wrap_cdata` function is designed to encapsulate a given string `content` within XML CDATA tags. It constructs a new string by prepending \"<![CDATA[\\n\" and appending \"\\n]]>\" to the provided content. This process effectively escapes the content, preventing XML parsers from interpreting any special characters within it as markup. The function is straightforward, performing a simple string formatting operation."
        parameters[1]{name,type,description}:
          content,str,The string content that needs to be wrapped inside CDATA tags.
        returns[1]{name,type,description}:
          wrapped_content,str,"A new string with the input content enclosed within CDATA tags, including leading and trailing newlines."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.extract_output_content:
      identifier: backend.converter.extract_output_content
      description:
        overall: "The extract_output_content function processes a collection of output objects, typically originating from a notebook execution. Its primary goal is to extract textual content and handle embedded images by converting them into structured placeholders. It iterates through each output, identifying different types such as display data, execution results, streams, or errors. Images (PNG or JPEG) found within display or execution results are base64 decoded, stored in a provided image_list, and replaced with an XML-like placeholder in the output. Textual content from various output types is directly appended, and error messages are formatted. The function returns a list containing these extracted text snippets and image placeholders."
        parameters[2]{name,type,description}:
          outputs,"iterable[object]","An iterable of output objects, typically from a notebook execution, containing various types of output data such as display data, execution results, streams, or errors."
          image_list,"list[dict]","A mutable list used to store extracted image data as dictionaries, where each dictionary contains the 'mime_type' and Base64 encoded 'data' of an image."
        returns[1]{name,type,description}:
          extracted_xml_snippets,"list[str]","A list of strings, which can be plain text, stream output, formatted error messages, or XML-like placeholders for extracted images."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.process_image:
      identifier: backend.converter.process_image
      description:
        overall: "This function processes image data based on a given MIME type. It checks if the `mime_type` exists within an external `data` dictionary. If found, it retrieves a base64 encoded string, removes newline characters, and appends a dictionary containing the `mime_type` and processed data to an `image_list`. The function then returns a formatted placeholder string. It includes error handling for potential decoding issues, returning an error string if an exception occurs. If the `mime_type` is not present in the `data` dictionary, the function returns `None`."
        parameters[1]{name,type,description}:
          mime_type,str,"The MIME type of the image data to be processed, used as a key to retrieve data from an external `data` dictionary."
        returns[3]{name,type,description}:
          image_placeholder_string,str,"A formatted string representing an image placeholder, including the image's index and MIME type, if processing is successful."
          error_message_string,str,"An error message string indicating that the image could not be decoded, along with the exception details, if an error occurs during processing."
          None,None,Returns None if the specified `mime_type` is not found as a key in the external `data` dictionary.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.converter.convert_notebook_to_xml:
      identifier: backend.converter.convert_notebook_to_xml
      description:
        overall: "This function converts the content of a Jupyter Notebook, provided as a string, into a structured XML format. It first attempts to parse the input using nbformat.reads, returning an error if the content is not valid JSON. It then iterates through the notebook's cells, converting markdown and code cells into corresponding XML elements. Code cell outputs are processed to extract content and potential images, which are also wrapped in XML. The function returns the concatenated XML string and a list of any extracted images."
        parameters[1]{name,type,description}:
          file_content,str,"The raw content of a Jupyter Notebook file, expected to be a string in JSON format, which will be parsed."
        returns[2]{name,type,description}:
          xml_output,str,"A string containing the converted notebook content in XML format, or an error message if the input file cannot be parsed."
          extracted_images,list,A list of extracted image data or paths from the notebook outputs. This list is empty if no images are extracted or if an error occurs during parsing.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions in the provided context.
      error: null
    backend.converter.process_repo_notebooks:
      identifier: backend.converter.process_repo_notebooks
      description:
        overall: "This function processes an iterable of repository file objects, identifying those that represent Jupyter notebooks by checking their file paths. For each identified notebook, it attempts to convert its content into an XML format and extract any embedded images. The results of this conversion, including the XML output and extracted images, are then stored in a dictionary. This dictionary uses the notebook's file path as a key, providing a structured output of all processed notebooks. The function also logs information about the number of notebooks found and each one being processed."
        parameters[1]{name,type,description}:
          repo_files,"Iterable[object]",An iterable collection of file objects from a repository. Each object is expected to have a 'path' attribute (string) and a 'content' attribute (representing the notebook's raw content).
        returns[1]{name,type,description}:
          results,"Dict[str, Dict[str, Any]]","A dictionary where keys are the paths of the processed notebook files (string) and values are dictionaries containing the converted XML output ('xml': string) and any extracted images ('images': Any)."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.create_savings_chart:
      identifier: backend.main.create_savings_chart
      description:
        overall: "This function generates a bar chart to visually compare two token counts: 'JSON' tokens and 'TOON' tokens. It calculates and displays a savings percentage in the chart's title. The function then saves the generated chart to a specified output path and closes the plot to free up resources. It uses `matplotlib.pyplot` for plotting operations."
        parameters[4]{name,type,description}:
          json_tokens,int,The number of tokens representing the JSON format.
          toon_tokens,int,The number of tokens representing the TOON format.
          savings_percent,float,"The percentage of savings to display in the chart title, formatted to two decimal places."
          output_path,str,The file path where the generated bar chart image will be saved.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    backend.main.calculate_net_time:
      identifier: backend.main.calculate_net_time
      description:
        overall: "This function calculates the effective processing time by subtracting estimated sleep times from the total elapsed duration. It specifically applies this adjustment only if the `model_name` starts with \"gemini-\", indicating a need to account for rate-limiting pauses. For other models or if no items are processed, it returns the raw total duration or zero, respectively. The sleep time is estimated based on the number of batches and a fixed sleep duration per batch."
        parameters[5]{name,type,description}:
          start_time,numeric,"The starting time of the operation, typically a timestamp or datetime object."
          end_time,numeric,"The ending time of the operation, typically a timestamp or datetime object."
          total_items,int,The total number of items processed during the operation.
          batch_size,int,The number of items processed per batch.
          model_name,str,"The name of the model used for processing, which determines if sleep time adjustments are made."
        returns[1]{name,type,description}:
          net_time,float,"The calculated net processing time, adjusted for rate-limiting sleep times if applicable, or the total duration if no adjustment is made. Returns 0 if total_items is 0 or if the net time is negative after adjustment."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    backend.main.main_workflow:
      identifier: backend.main.main_workflow
      description:
        overall: "The main_workflow function orchestrates an end-to-end process for analyzing a GitHub repository and generating a comprehensive report using multiple Large Language Models (LLMs). It begins by extracting API keys and model configurations, then clones the specified repository. The function proceeds with detailed code analysis, including extracting basic project information, constructing a file tree, analyzing code relationships, and building an Abstract Syntax Tree (AST) schema, which is subsequently enriched. It then prepares and dispatches analysis tasks to a \"Helper LLM\" for individual functions and classes, followed by a \"Main LLM\" to synthesize a final report. The process includes error handling, status updates, token usage estimation, and saving the final report along with performance metrics."
        parameters[4]{name,type,description}:
          input,str,"The user input, expected to contain a GitHub repository URL."
          api_keys,dict,"A dictionary containing API keys for various LLM services (e.g., 'gemini', 'gpt', 'scadsllm') and base URLs."
          model_names,dict,"A dictionary specifying the names of the helper and main LLM models to be used (e.g., 'helper', 'main')."
          status_callback,callable | None,An optional callback function to provide real-time status updates during the workflow execution.
        returns[2]{name,type,description}:
          report,str,"The final generated report in markdown format, or an error message if generation failed."
          metrics,dict,"A dictionary containing performance metrics such as helper LLM time, main LLM time, total active time, model names, and token savings data."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.update_status:
      identifier: backend.main.update_status
      description:
        overall: "This function, `update_status`, is designed to handle and disseminate status messages. It takes a message as input and, if a `status_callback` function is available in its scope, it invokes that callback with the provided message. Additionally, it logs the message at the INFO level using the `logging` module. The primary purpose is to provide a standardized way to report progress or status updates, potentially to a UI or another system component, while also ensuring the message is recorded in logs."
        parameters[1]{name,type,description}:
          msg,str,The status message to be processed and logged.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.notebook_workflow:
      identifier: backend.main.notebook_workflow
      description:
        overall: "This function orchestrates a workflow to analyze Jupyter notebooks from a given GitHub repository using a Large Language Model (LLM). It first extracts a repository URL from the input, clones the repository, and processes its notebooks. It then extracts basic project information and iteratively generates reports for each notebook using an LLM, supporting various model providers. Finally, it concatenates all individual reports into a single markdown file and returns the report along with execution metrics."
        parameters[4]{name,type,description}:
          input,str,"The input string, which is expected to contain a GitHub repository URL."
          api_keys,dict,"A dictionary containing API keys for different LLM services (e.g., 'gpt', 'gemini', 'scadsllm', 'ollama')."
          model,str,"The name of the LLM model to be used for generating reports (e.g., 'gpt-4', 'gemini-pro')."
          status_callback,callable or None,An optional callback function that receives status messages during the workflow execution.
        returns[2]{name,type,description}:
          report,str,The final concatenated markdown report generated by the LLM for all processed notebooks.
          metrics,dict,"A dictionary containing performance metrics for the workflow, including execution times and model information."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.main.gemini_payload:
      identifier: backend.main.gemini_payload
      description:
        overall: "This function constructs a multimodal payload suitable for the Gemini API. It takes basic project information, a notebook path, XML content that may contain image placeholders, and a list of image data. The function first serializes the basic information and notebook path into a JSON string for context. It then processes the XML content, extracting text segments and replacing image placeholders with base64-encoded image data from the provided list. The final output is a list of dictionaries, each representing either a text segment or an image URL, formatted for a multimodal API request."
        parameters[4]{name,type,description}:
          basic_info,object,A dictionary or object containing basic project information to be included in the payload context.
          nb_path,str,"The file path of the current notebook, used for context information."
          xml_content,str,"The XML string content of the notebook, which may contain image placeholders."
          images,list,"A list of image data objects, where each object is expected to contain a 'data' key with a base64 string and a 'mime' type."
        returns[1]{name,type,description}:
          payload_content,"list[dict]","A list of dictionaries, where each dictionary represents a segment of the multimodal payload, either as 'text' or 'image_url' data."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    backend.relationship_analyzer.path_to_module:
      identifier: backend.relationship_analyzer.path_to_module
      description:
        overall: "This function transforms a given file system path into a Python module path string. It first determines the relative path of the file with respect to a specified project root, handling cases where the file might not be directly under the root. It then removes the '.py' extension if present and replaces directory separators with dots. Finally, it removes '.__init__' from the end of the module path if it exists."
        parameters[2]{name,type,description}:
          filepath,str,The full path to the Python file to be converted into a module path.
          project_root,str,"The root directory of the project, used as a reference to calculate the relative path for the module."
        returns[1]{name,type,description}:
          module_path,str,The converted Python module path string.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.encrypt_text:
      identifier: database.db.encrypt_text
      description:
        overall: "This function encrypts a given string using a pre-configured `cipher_suite` object. It first performs a check to see if the input `text` is empty or if the `cipher_suite` is not initialized; if either is true, it returns the original text without encryption. Otherwise, it prepares the text by stripping whitespace, encodes it into bytes, encrypts these bytes using the `cipher_suite`, and finally decodes the resulting encrypted bytes back into a string before returning."
        parameters[1]{name,type,description}:
          text,str,The string value to be encrypted.
        returns[1]{name,type,description}:
          encrypted_text,str,"The encrypted string, or the original string if encryption was skipped due to empty input or an unavailable cipher suite."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.decrypt_text:
      identifier: database.db.decrypt_text
      description:
        overall: "This function attempts to decrypt a given string using an external `cipher_suite` object. It first performs a guard clause, returning the original text if the input text is empty or if `cipher_suite` is not initialized. If conditions are met, it strips whitespace from the input, encodes it to bytes, and then attempts decryption. The decrypted bytes are then decoded back into a string. In case of any error during the decryption process, the function gracefully falls back to returning the original, unencrypted text."
        parameters[1]{name,type,description}:
          text,str,The string value to be decrypted.
        returns[1]{name,type,description}:
          decrypted_text,str,"The decrypted string if successful, otherwise the original string if decryption fails or conditions are not met."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.insert_user:
      identifier: database.db.insert_user
      description:
        overall: "This function creates a new user record in the database. It accepts a username, a display name, and a plain-text password. The provided password is first hashed using `stauth.Hasher.hash` for secure storage. A user document is then constructed, including the hashed password and initializing various API keys to empty strings. Finally, this document is inserted into the `dbusers` collection, and the function returns the `_id` of the newly inserted user document."
        parameters[3]{name,type,description}:
          username,str,"The unique identifier for the user, which will also serve as the document's `_id` in the database."
          name,str,The user's full or display name.
          password,str,"The plain-text password provided by the user, which will be hashed before storage."
        returns[1]{name,type,description}:
          inserted_id,str,"The `_id` of the newly inserted user document, which corresponds to the provided username."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_all_users:
      identifier: database.db.fetch_all_users
      description:
        overall: "This function is designed to retrieve all user records from a database collection. It executes a `find()` operation on the `dbusers` collection, which typically yields an iterable cursor containing all documents. The function then converts this iterable into a standard Python list. This list, comprising all fetched user documents, is subsequently returned by the function."
        parameters[0]:
        returns[1]{name,type,description}:
          users,"list[dict]","A list of all user documents found in the database collection, where each document is represented as a dictionary."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.fetch_user:
      identifier: database.db.fetch_user
      description:
        overall: "The `fetch_user` function is designed to retrieve a single user document from a database collection. It takes a username as input and uses it to query the `dbusers` collection, specifically matching it against the `_id` field. The function returns the first document that matches the provided username."
        parameters[1]{name,type,description}:
          username,str,"The unique identifier for the user to be fetched, which corresponds to the `_id` field in the database."
        returns[1]{name,type,description}:
          user_document,dict,"A dictionary representing the user document if a match is found, or `None` if no user with the given username exists."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_user_name:
      identifier: database.db.update_user_name
      description:
        overall: "This function updates the 'name' field for a specific user in the 'dbusers' collection. It takes the current username, which serves as the unique document identifier ('_id'), and a new name to be assigned. The function uses a MongoDB `update_one` operation to modify the document. It returns the count of documents that were successfully modified."
        parameters[2]{name,type,description}:
          username,str,"The unique identifier of the user, corresponding to the '_id' field in the database."
          new_name,str,The new name to set for the specified user.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_gemini_key:
      identifier: database.db.update_gemini_key
      description:
        overall: "This function updates a user's Gemini API key within a database. It takes a username and the new API key as input. The provided API key is first stripped of any leading/trailing whitespace and then encrypted. Finally, the function updates the 'gemini_api_key' field for the specified user in the 'dbusers' collection with the newly encrypted key. It returns an integer indicating the number of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          username,str,The username of the user whose Gemini API key is to be updated in the database.
          gemini_api_key,str,The new Gemini API key to be stored. This key will be stripped of whitespace and encrypted before storage.
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents modified by the update operation. Typically 1 if the user was found and updated, or 0 if the user was not found."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_gpt_key:
      identifier: database.db.update_gpt_key
      description:
        overall: "This function updates a user's GPT API key in the database. It first encrypts the provided API key after stripping any leading/trailing whitespace. Then, it uses the username to locate the corresponding user record and updates the 'gpt_api_key' field with the encrypted value. The function returns the count of modified documents."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose GPT API key needs to be updated.
          gpt_api_key,str,The new GPT API key to be stored for the user. It will be stripped of whitespace and encrypted before storage.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified in the database as a result of the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.update_ollama_url:
      identifier: database.db.update_ollama_url
      description:
        overall: "This function updates the Ollama base URL for a specific user in a database. It takes a username and a new Ollama base URL as input. The function uses `dbusers.update_one` to locate the user by their username (mapped to `_id`) and sets the `ollama_base_url` field with the provided URL, ensuring any leading or trailing whitespace is removed. It then returns the count of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose Ollama base URL needs to be updated. This is used as the `_id` in the database query.
          ollama_base_url,str,The new base URL for Ollama to be associated with the specified user. Leading and trailing whitespace will be removed before storage.
        returns[1]{name,type,description}:
          modified_count,int,"An integer representing the number of documents that were modified by the update operation. A value of 1 indicates success if the user existed and the URL was different, 0 if the user existed but the URL was the same, or if the user was not found."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_opensrc_key:
      identifier: database.db.update_opensrc_key
      description:
        overall: "This function is responsible for updating a user's Open Source API key in the database. It first encrypts the provided API key after removing any leading or trailing whitespace. Subsequently, it performs an update operation on the 'dbusers' collection, setting the 'opensrc_api_key' field for the specified username with the newly encrypted key. The function then returns the count of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose API key is to be updated.
          opensrc_api_key,str,The new Open Source API key to be encrypted and stored for the user.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified in the database by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    database.db.update_opensrc_url:
      identifier: database.db.update_opensrc_url
      description:
        overall: "This function updates the 'opensrc_base_url' for a specific user in a database collection. It takes a username and a new opensource base URL as input. The function performs an 'update_one' operation on the 'dbusers' collection, identifying the document by the provided username and setting the 'opensrc_base_url' field. The input URL is stripped of leading/trailing whitespace before being stored. The function returns the count of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          username,str,The unique identifier for the user whose opensource base URL needs to be updated. This is used as the '_id' in the database query.
          opensrc_base_url,str,The new base URL for opensource projects to be associated with the user. This string will be stripped of whitespace before being saved.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation. Typically 0 or 1 for an update_one operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_gemini_key:
      identifier: database.db.fetch_gemini_key
      description:
        overall: "The `fetch_gemini_key` function is designed to retrieve a user's Gemini API key from a database. It takes a username as input and queries a `dbusers` collection to locate the corresponding user document. The function then extracts the `gemini_api_key` field from the found user document. If a user is found and possesses a Gemini API key, that key is returned; otherwise, the function returns `None`."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose Gemini API key is to be fetched.
        returns[1]{name,type,description}:
          gemini_api_key,str | None,"The Gemini API key associated with the specified user, or None if the user is not found or does not have a key."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_ollama_url:
      identifier: database.db.fetch_ollama_url
      description:
        overall: "This function retrieves the Ollama base URL associated with a specific username from a database. It queries the `dbusers` collection, searching for a document where the `_id` matches the provided username. If a user document is found, it extracts the `ollama_base_url` field. If no user is found or the field is absent, the function returns None."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose Ollama base URL is to be fetched.
        returns[1]{name,type,description}:
          ollama_base_url,str | None,"The base URL for Ollama associated with the user, or None if the user is not found or the URL is not set."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_gpt_key:
      identifier: database.db.fetch_gpt_key
      description:
        overall: "This function retrieves the GPT API key for a specified user from a database. It queries the 'dbusers' collection using the provided username as the document ID. The function specifically fetches only the 'gpt_api_key' field. If a user is found, it returns their associated GPT API key; otherwise, it returns None."
        parameters[1]{name,type,description}:
          username,str,The unique identifier (username) of the user whose GPT API key is to be fetched.
        returns[1]{name,type,description}:
          gpt_api_key,str | None,"The GPT API key associated with the user, or None if the user is not found in the database."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_opensrc_key:
      identifier: database.db.fetch_opensrc_key
      description:
        overall: "This function retrieves the 'opensrc_api_key' for a specified username from a database. It queries the 'dbusers' collection to find a document matching the provided username by its '_id'. If a user document is found, it extracts the 'opensrc_api_key' field. The function returns this key if present, or 'None' if the user is not found or the key is missing."
        parameters[1]{name,type,description}:
          username,str,The username for which to retrieve the 'opensrc_api_key'.
        returns[1]{name,type,description}:
          opensrc_api_key,str | None,"The 'opensrc_api_key' associated with the username, or 'None' if the user is not found or the key does not exist."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_opensrc_url:
      identifier: database.db.fetch_opensrc_url
      description:
        overall: "This function retrieves the Open Source base URL for a specified user from a database. It queries the 'dbusers' collection, searching for a document where the '_id' field matches the provided 'username'. If a user document is found, the function attempts to extract the 'opensrc_base_url' field from it. The function returns the found URL as a string. If no user is found or the 'opensrc_base_url' field is absent, it returns None."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose Open Source base URL is to be fetched.
        returns[1]{name,type,description}:
          opensrc_base_url,str | None,"The Open Source base URL associated with the user, or None if the user is not found or the URL is not set."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.delete_user:
      identifier: database.db.delete_user
      description:
        overall: "This function is designed to remove a user record from a database collection. It takes a username as input and uses it as the unique identifier (`_id`) to locate and delete the corresponding document. The function returns the count of documents that were successfully deleted, which will typically be 0 or 1 for a single-document deletion operation."
        parameters[1]{name,type,description}:
          username,str,The unique identifier (username) of the user to be deleted from the database.
        returns[1]{name,type,description}:
          deleted_count,int,"The number of documents deleted by the operation (0 if no user was found, 1 if the user was successfully deleted)."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.get_decrypted_api_keys:
      identifier: database.db.get_decrypted_api_keys
      description:
        overall: "This function retrieves a user's API keys and base URLs from a database. It queries the 'dbusers' collection using the provided username. If the user is found, it decrypts specific API keys (Gemini, GPT, open-source) and retrieves base URLs (Ollama, open-source). If the user is not found, it returns a tuple of None values."
        parameters[1]{name,type,description}:
          username,str,The unique identifier for the user whose API keys and URLs are to be retrieved.
        returns[5]{name,type,description}:
          gemini_plain,str | None,"The decrypted Gemini API key, or None if the user is not found."
          ollama_plain,str | None,"The Ollama base URL, or None if the user is not found."
          gpt_plain,str | None,"The decrypted GPT API key, or None if the user is not found."
          opensrc_plain,str | None,"The decrypted open-source API key, or None if the user is not found."
          opensrc_url,str | None,"The open-source base URL, or None if the user is not found."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.insert_chat:
      identifier: database.db.insert_chat
      description:
        overall: "This function creates a new chat entry in a database. It constructs a chat document by generating a unique UUID for its identifier, assigning the provided username and chat name, and recording the current timestamp. This document is then inserted into the 'dbchats' collection. The function concludes by returning the unique identifier of the newly inserted chat."
        parameters[2]{name,type,description}:
          username,str,The username associated with the new chat entry.
          chat_name,str,The name of the chat to be created.
        returns[1]{name,type,description}:
          inserted_id,str,The unique identifier (UUID) of the newly created chat document in the database.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_chats_by_user:
      identifier: database.db.fetch_chats_by_user
      description:
        overall: "This function retrieves all chat records associated with a specific user from a database collection named `dbchats`. It queries the collection for documents where the 'username' field matches the provided input. The retrieved chat documents are then sorted in ascending order based on their 'created_at' timestamp. Finally, the function returns these sorted chat documents as a list."
        parameters[1]{name,type,description}:
          username,str,The username for which to fetch the associated chat records.
        returns[1]{name,type,description}:
          chats,list,"A list of chat documents belonging to the specified user, sorted by their creation date."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.check_chat_exists:
      identifier: database.db.check_chat_exists
      description:
        overall: This function checks for the existence of a specific chat within a database collection named `dbchats`. It takes a username and a chat name as input. The function queries the `dbchats` collection to find a document that matches both the provided username and chat name. It returns a boolean indicating whether such a chat exists.
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to be checked.
          chat_name,str,The name of the chat to be checked for existence.
        returns[1]{name,type,description}:
          chat_exists,bool,"True if a chat matching the given username and chat name is found in the database, False otherwise."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.rename_chat_fully:
      identifier: database.db.rename_chat_fully
      description:
        overall: "This function renames a chat and all its associated exchanges (messages) within the database. It first updates the chat entry in the `dbchats` collection, changing the `chat_name` from `old_name` to `new_name` for a given `username`. Subsequently, it updates all related exchange entries in the `dbexchanges` collection to reflect the `new_name`. The function returns the count of documents modified during the initial chat entry update."
        parameters[3]{name,type,description}:
          username,str,The username associated with the chat to be renamed.
          old_name,str,The current name of the chat.
          new_name,str,The new desired name for the chat.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents modified in the `dbchats` collection during the chat renaming operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    database.db.insert_exchange:
      identifier: database.db.insert_exchange
      description:
        overall: "This function records a chat exchange into a database. It generates a new unique identifier for the exchange using `uuid.uuid4()`. It then constructs a dictionary containing the question, answer, feedback, user details, chat name, and various performance metrics and token counts, along with a `created_at` timestamp. Finally, it attempts to insert this structured exchange document into the `dbexchanges` MongoDB collection. If the insertion is successful, it returns the new unique ID; otherwise, it catches any exceptions, prints the error, and returns `None`."
        parameters[13]{name,type,description}:
          question,str,The user's question in the chat exchange.
          answer,str,The answer provided in the chat exchange.
          feedback,str,Feedback related to the chat exchange.
          username,str,The username associated with the chat exchange.
          chat_name,str,The name of the chat session.
          helper_used,str,"Indicates if a helper was used, defaulting to an empty string."
          main_used,str,"Indicates if the main system was used, defaulting to an empty string."
          total_time,str,"The total time taken for the exchange, defaulting to an empty string."
          helper_time,str,"The time spent by the helper, defaulting to an empty string."
          main_time,str,"The time spent by the main system, defaulting to an empty string."
          json_tokens,int,"The number of JSON tokens used, defaulting to 0."
          toon_tokens,int,"The number of Toon tokens used, defaulting to 0."
          savings_percent,float,"The percentage of savings, defaulting to 0.0."
        returns[2]{name,type,description}:
          new_id,str,The unique identifier of the newly inserted exchange if the operation is successful.
          None,NoneType,Returns None if an error occurs during the database insertion process.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_exchanges_by_user:
      identifier: database.db.fetch_exchanges_by_user
      description:
        overall: "This function retrieves exchange records from a database based on a given username. It queries a collection, presumably `dbexchanges`, to find all entries where the 'username' field matches the input. The results are then sorted by their 'created_at' timestamp in ascending order. Finally, the function returns these sorted exchange records as a list."
        parameters[1]{name,type,description}:
          username,str,The username used to filter and retrieve specific exchange records.
        returns[1]{name,type,description}:
          exchanges,list,"A list of exchange records associated with the specified username, sorted by their creation timestamp."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.fetch_exchanges_by_chat:
      identifier: database.db.fetch_exchanges_by_chat
      description:
        overall: This function retrieves a list of exchanges from a database collection named `dbexchanges`. It filters the exchanges based on a provided username and chat name. The results are then sorted by their `created_at` timestamp in ascending order and returned as a list.
        parameters[2]{name,type,description}:
          username,str,The username used to filter the exchanges.
          chat_name,str,The name of the chat used to filter the exchanges.
        returns[1]{name,type,description}:
          exchanges,list,"A list of exchange documents (dictionaries) matching the specified username and chat name, sorted by creation time."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_exchange_feedback:
      identifier: database.db.update_exchange_feedback
      description:
        overall: "This function is responsible for updating the feedback value associated with a specific exchange record in a database. It takes an exchange identifier and an integer feedback value as input. The function uses the `dbexchanges.update_one` method to locate the document matching the provided `exchange_id` and sets its 'feedback' field to the new integer value. Finally, it returns the count of documents that were modified by this operation."
        parameters[2]{name,type,description}:
          exchange_id,Any,The unique identifier for the exchange record to be updated in the database.
          feedback,int,The integer value representing the feedback to be set for the specified exchange.
        returns[1]{name,type,description}:
          modified_count,int,"The number of documents that were modified by the update operation, typically 0 or 1."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.update_exchange_feedback_message:
      identifier: database.db.update_exchange_feedback_message
      description:
        overall: This function updates the feedback message associated with a specific exchange record in the database. It takes an `exchange_id` to identify the target record and a `feedback_message` string to set as the new value for the 'feedback_message' field. The update operation is performed using `dbexchanges.update_one`. The function returns the count of documents that were modified by this operation.
        parameters[2]{name,type,description}:
          exchange_id,Any,The unique identifier for the exchange record to be updated. Its specific type is inferred as an identifier for a database document.
          feedback_message,str,The new feedback message string to be set for the specified exchange record.
        returns[1]{name,type,description}:
          modified_count,int,The number of documents that were modified by the update operation.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.delete_exchange_by_id:
      identifier: database.db.delete_exchange_by_id
      description:
        overall: This function is responsible for deleting a single exchange record from the database. It takes an exchange identifier as input and uses it to locate and remove the corresponding document from the `dbexchanges` collection. The function then reports the number of documents that were successfully deleted.
        parameters[1]{name,type,description}:
          exchange_id,str,The unique string identifier of the exchange record to be deleted.
        returns[1]{name,type,description}:
          deleted_count,int,"The number of documents that were deleted, typically 0 or 1."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    database.db.delete_full_chat:
      identifier: database.db.delete_full_chat
      description:
        overall: "This function is designed to completely remove a specific chat and all its associated message exchanges from the database. It first deletes all messages (exchanges) linked to the given username and chat name. Subsequently, it deletes the chat entry itself. This two-step process ensures data consistency between frontend and backend representations. The function returns the count of chats that were deleted."
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to be deleted.
          chat_name,str,The name of the chat to be deleted.
        returns[1]{name,type,description}:
          deleted_count,int,The number of chat documents deleted from the database (typically 0 or 1 for delete_one operations).
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.clean_names:
      identifier: frontend.frontend.clean_names
      description:
        overall: "This function takes a list of strings, presumably representing model names or paths. It processes each string by splitting it using the '/' character as a delimiter and then extracts the last segment. The primary purpose is to simplify or 'clean' these names by removing any preceding path or URL components, returning a new list of these simplified names."
        parameters[1]{name,type,description}:
          model_list,"List[str]","A list of strings, where each string is expected to contain path-like or URL-like segments separated by '/'. These strings represent model names or identifiers that need to be cleaned."
        returns[1]{name,type,description}:
          cleaned_names,"List[str]",A new list containing the last segment of each input string after splitting by '/'. This effectively provides a 'cleaned' or base name for each item in the original list.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.get_filtered_models:
      identifier: frontend.frontend.get_filtered_models
      description:
        overall: "This function filters a given list of models based on a specified category name. It retrieves keywords associated with the category from a global `CATEGORY_KEYWORDS` mapping. If the category's keywords include \"STANDARD\", it returns only those models from the `source_list` that are also present in `STANDARD_MODELS`. Otherwise, it iterates through the `source_list` and collects models whose names contain any of the category's keywords. If any models match the keywords, the filtered list is returned; otherwise, the original `source_list` is returned."
        parameters[2]{name,type,description}:
          source_list,list,The initial list of models to be filtered.
          category_name,str,The name of the category used to determine filtering keywords.
        returns[1]{name,type,description}:
          filtered_models,list,"A list of models filtered according to the specified category, or the original `source_list` if no filters apply or no matches are found."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.save_gemini_cb:
      identifier: frontend.frontend.save_gemini_cb
      description:
        overall: "This function, `save_gemini_cb`, acts as a callback to save a user's Gemini API key. It retrieves a new key from the Streamlit session state. If a valid new key is found, it updates the user's Gemini key in the database using the `db.update_gemini_key` method, associating it with the username stored in the session state. Following a successful update, the temporary key is cleared from the session state, and a success toast notification is displayed to the user."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.save_ollama_cb:
      identifier: frontend.frontend.save_ollama_cb
      description:
        overall: "This function, `save_ollama_cb`, is responsible for saving a user-defined Ollama URL. It retrieves the potential new URL from the Streamlit session state, specifically from the key \"in_ollama_url\". If a non-empty URL is found, the function proceeds to update this URL in the database. It uses the current user's username, also obtained from the session state, to associate the URL with the correct user. Upon successful update, a confirmation toast message is displayed to the user."
        parameters[0]:
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is called by no other functions.
      error: null
    frontend.frontend.load_data_from_db:
      identifier: frontend.frontend.load_data_from_db
      description:
        overall: "This function is designed to load chat and exchange data for a specified user from the database into the Streamlit session state. It first checks if the data for the given username is already loaded to prevent redundant operations. If not loaded, it initializes the session state, fetches predefined chats, and then populates them with associated exchanges. The function also handles cases where exchanges might exist for chats not initially defined and ensures a default chat is created if no chats are found. Finally, it sets an active chat in the session state and marks the user's data as loaded."
        parameters[1]{name,type,description}:
          username,str,The username for whom to load chat and exchange data.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not called by any other functions.
      error: null
    frontend.frontend.handle_feedback_change:
      identifier: frontend.frontend.handle_feedback_change
      description:
        overall: "This function `handle_feedback_change` is designed to process and apply user feedback within an application context. It accepts an exchange object (`ex`) and a new feedback value (`val`). The function first updates the 'feedback' key within the provided `ex` object with the new `val`. Subsequently, it invokes a database utility, `db.update_exchange_feedback`, to persist this feedback change using the `_id` from the `ex` object. Finally, it triggers a rerun of the Streamlit application, likely to refresh the user interface and reflect the updated feedback."
        parameters[2]{name,type,description}:
          ex,dict,"A dictionary-like object representing an exchange or data record, expected to contain 'feedback' and '_id' keys."
          val,Any,"The new feedback value to be applied, which can be of any type."
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.handle_delete_exchange:
      identifier: frontend.frontend.handle_delete_exchange
      description:
        overall: "This function handles the deletion of a specific exchange. It first removes the exchange from the database using its ID. Subsequently, it updates the Streamlit session state by removing the exchange from the relevant chat's list of exchanges, if it exists. Finally, it triggers a Streamlit rerun to refresh the UI."
        parameters[2]{name,type,description}:
          chat_name,str,The name of the chat from which the exchange should be deleted.
          ex,dict,"The exchange object to be deleted, expected to contain an '_id' field for database deletion."
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions.
      error: null
    frontend.frontend.handle_delete_chat:
      identifier: frontend.frontend.handle_delete_chat
      description:
        overall: "This function handles the deletion of a specified chat for a given user. It first removes the chat from the database using `db.delete_full_chat`. Subsequently, it cleans up the Streamlit session state by removing the chat from `st.session_state.chats`. If other chats exist, the first one is set as the active chat; otherwise, a new default chat named 'Chat 1' is created, inserted into the database, and set as the active chat. Finally, it triggers a Streamlit rerun."
        parameters[2]{name,type,description}:
          username,str,The username associated with the chat to be deleted.
          chat_name,str,The name of the chat to be deleted.
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.extract_repo_name:
      identifier: frontend.frontend.extract_repo_name
      description:
        overall: "This function extracts a repository name from a given text string. It first attempts to find a URL within the text using a regular expression. If a URL is identified, it then parses the URL to isolate its path component. The last segment of this path is considered the potential repository name, with any '.git' suffix being removed. If a valid repository name is successfully extracted, it is returned; otherwise, the function returns None."
        parameters[1]{name,type,description}:
          text,str,The input string that may contain a URL from which to extract a repository name.
        returns[1]{name,type,description}:
          repo_name,str | None,"The extracted repository name as a string, or None if no URL is found or a repository name cannot be determined."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.stream_text_generator:
      identifier: frontend.frontend.stream_text_generator
      description:
        overall: "This function is a generator that takes a string of text and yields it word by word. It splits the input text by spaces and, for each word, yields the word followed by a space. A small delay of 0.01 seconds is introduced after yielding each word, simulating a streaming effect. This is useful for displaying text incrementally."
        parameters[1]{name,type,description}:
          text,str,The input string of text to be processed and streamed word by word.
        returns[1]{name,type,description}:
          word_chunk,str,"A single word from the input text, followed by a space, yielded sequentially."
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.render_text_with_mermaid:
      identifier: frontend.frontend.render_text_with_mermaid
      description:
        overall: "This function processes a given markdown text, identifying and rendering both standard markdown content and embedded Mermaid diagrams. It splits the input text based on ````mermaid` delimiters. Regular markdown sections are rendered using `st.markdown` or streamed via `st.write_stream` if `should_stream` is true. Mermaid diagram blocks are attempted to be rendered using `st_mermaid`, with a fallback to displaying the raw Mermaid code using `st.code` if an error occurs during rendering."
        parameters[2]{name,type,description}:
          markdown_text,str,"The input text, which may contain standard markdown and embedded Mermaid diagram syntax delimited by ````mermaid`."
          should_stream,bool,A flag indicating whether the standard markdown parts should be streamed using `st.write_stream` or rendered directly with `st.markdown`. Defaults to False.
        returns[1]{name,type,description}:
          None,None,The function does not explicitly return a value; it performs side effects by rendering content to a Streamlit application. It returns implicitly if `markdown_text` is empty.
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
    frontend.frontend.render_exchange:
      identifier: frontend.frontend.render_exchange
      description:
        overall: "This function is responsible for rendering a single exchange (a user question and an assistant's answer) within a chat interface. It first displays the user's question. Subsequently, it renders the assistant's response, which includes a dynamic toolbar for feedback, comments, download, and deletion, adapting its appearance based on whether the answer indicates an error. Finally, it displays the main content of the assistant's answer, potentially with Mermaid diagrams."
        parameters[2]{name,type,description}:
          ex,dict,"A dictionary-like object containing the exchange data, including the user's question, assistant's answer, feedback status, unique ID, and feedback message."
          current_chat_name,str,"The name of the current chat session, used when handling the deletion of an exchange."
        returns[0]:
        usage_context:
          calls: This function calls no other functions.
          called_by: This function is not explicitly called by any other functions in the provided context.
      error: null
  classes:
    backend.AST_Schema.ASTVisitor:
      identifier: backend.AST_Schema.ASTVisitor
      description:
        overall: "The ASTVisitor class extends ast.NodeVisitor to perform a structured traversal of a Python Abstract Syntax Tree (AST). Its main objective is to extract and organize metadata about imports, class definitions, and function definitions (both synchronous and asynchronous) found within a given source file. It compiles this information into a hierarchical schema, making it suitable for further analysis or documentation generation."
        init_method:
          description: "The constructor initializes the ASTVisitor with the source code, file path, and project root. It calculates the module path, sets up an empty schema dictionary to store extracted information (imports, functions, classes), and initializes a `_current_class` attribute to track the class being visited during traversal."
          parameters[3]{name,type,description}:
            source_code,str,The raw source code of the Python file to be analyzed.
            file_path,str,The absolute path to the Python file being processed.
            project_root,str,"The root directory of the project, used to determine the module path."
        methods[5]:
          - identifier: visit_Import
            description:
              overall: "This method is invoked by the AST traversal framework when an `ast.Import` node is encountered, representing a simple `import module` statement. It iterates through the imported aliases, extracting each module's name and appending it to the 'imports' list within the `self.schema` dictionary. After processing, it calls `self.generic_visit` to ensure continued traversal of any child nodes."
              parameters[1]{name,type,description}:
                node,ast.Import,The AST node representing an import statement.
              returns[0]:
              usage_context:
                calls: This method calls `self.schema.append` to add import names and `self.generic_visit` to continue AST traversal.
                called_by: This method is called by the `ast.NodeVisitor` framework when an `ast.Import` node is encountered during AST traversal.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method processes `ast.ImportFrom` nodes, which correspond to `from module import name` statements. It iterates through the imported names, constructs a full import string (e.g., `module.name`), and adds it to the `imports` list in `self.schema`. It then ensures further traversal of child nodes by calling `self.generic_visit`."
              parameters[1]{name,type,description}:
                node,ast.ImportFrom,The AST node representing a 'from ... import ...' statement.
              returns[0]:
              usage_context:
                calls: This method calls `self.schema.append` to add import names and `self.generic_visit` to continue AST traversal.
                called_by: This method is called by the `ast.NodeVisitor` framework when an `ast.ImportFrom` node is encountered during AST traversal.
            error: null
          - identifier: visit_ClassDef
            description:
              overall: "This method processes `ast.ClassDef` nodes, representing class definitions. It constructs a unique identifier for the class, extracts its name, docstring, source code segment, and line numbers. This information is then stored in a `class_info` dictionary, which is appended to the `classes` list in `self.schema`. It temporarily sets `self._current_class` to the newly created `class_info` to allow nested function definitions to be correctly associated with their parent class, before resetting it after visiting the class's children."
              parameters[1]{name,type,description}:
                node,ast.ClassDef,The AST node representing a class definition.
              returns[0]:
              usage_context:
                calls: "This method calls `path_to_module`, `ast.get_docstring`, `ast.get_source_segment`, and `self.generic_visit`."
                called_by: This method is called by the `ast.NodeVisitor` framework when an `ast.ClassDef` node is encountered during AST traversal.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method handles `ast.FunctionDef` nodes, representing regular function definitions. It distinguishes between methods (functions defined within a class) and standalone functions. If a class is currently being visited (`self._current_class` is set), it extracts method-specific information (identifier, name, arguments, docstring, line numbers) and appends it to the `method_context` of the current class. Otherwise, it extracts similar information for a standalone function and appends it to the `functions` list in `self.schema`. Finally, it calls `self.generic_visit` to continue traversal."
              parameters[1]{name,type,description}:
                node,ast.FunctionDef,The AST node representing a function definition.
              returns[0]:
              usage_context:
                calls: "This method calls `ast.get_docstring`, `ast.get_source_segment`, and `self.generic_visit`."
                called_by: This method is called by the `ast.NodeVisitor` framework when an `ast.FunctionDef` node is encountered during AST traversal.
            error: null
          - identifier: visit_AsyncFunctionDef
            description:
              overall: "This method is designed to process `ast.AsyncFunctionDef` nodes, which represent asynchronous function definitions. Its implementation simply delegates the processing to the `visit_FunctionDef` method, treating async functions identically to regular functions for the purpose of schema extraction."
              parameters[1]{name,type,description}:
                node,ast.AsyncFunctionDef,The AST node representing an asynchronous function definition.
              returns[0]:
              usage_context:
                calls: This method calls `self.visit_FunctionDef`.
                called_by: This method is called by the `ast.NodeVisitor` framework when an `ast.AsyncFunctionDef` node is encountered during AST traversal.
            error: null
        usage_context:
          dependencies: The class depends on the `ast` module for AST traversal functionalities and `path_to_module` for resolving module paths.
          instantiated_by: This class is not explicitly instantiated by any known components within the provided context.
      error: null
    backend.AST_Schema.ASTAnalyzer:
      identifier: backend.AST_Schema.ASTAnalyzer
      description:
        overall: "The ASTAnalyzer class is designed to process and analyze the Abstract Syntax Tree (AST) of Python code within a repository. Its primary functions include parsing Python files to extract structural information like functions, classes, and imports, and then integrating relationship data (like calls and instantiations) into this structured schema. It acts as a central component for building a comprehensive, interconnected view of a codebase's structure and dependencies."
        init_method:
          description: This constructor initializes an instance of the ASTAnalyzer class. It does not take any specific parameters beyond 'self' and performs no explicit setup or attribute assignments.
          parameters[0]:
        methods[2]:
          - identifier: merge_relationship_data
            description:
              overall: "This method integrates raw relationship data, specifically incoming and outgoing calls, into a structured full schema. It iterates through files, functions, and classes within the schema, populating their respective 'calls', 'called_by', and 'instantiated_by' contexts. For classes, it also identifies and lists external dependencies based on method calls that are not internal to the class."
              parameters[3]{name,type,description}:
                self,ASTAnalyzer,The instance of the class.
                full_schema,dict,"The comprehensive schema containing file, function, and class definitions."
                raw_relationships,dict,A dictionary containing raw incoming and outgoing call relationships.
              returns[1]{name,type,description}:
                full_schema,dict,The updated full schema with integrated relationship data.
              usage_context:
                calls: This method does not explicitly call other methods or functions within its implementation based on the provided context.
                called_by: This method is not explicitly called by other functions or methods in the provided context.
            error: null
          - identifier: analyze_repository
            description:
              overall: "This method processes a list of file objects from a Git repository to build a comprehensive Abstract Syntax Tree (AST) schema. It filters for Python files, parses their content using the 'ast' module, and then uses an 'ASTVisitor' to extract structured information about imports, functions, and classes. The method aggregates this data into a 'full_schema' dictionary, handling potential parsing errors gracefully."
              parameters[3]{name,type,description}:
                self,ASTAnalyzer,The instance of the class.
                files,list,"A list of file objects, each expected to have 'path' and 'content' attributes."
                repo,GitRepository,"An object representing the Git repository, though it is not directly used in the provided method body."
              returns[1]{name,type,description}:
                full_schema,dict,"A dictionary representing the AST schema of the analyzed repository, structured by file paths."
              usage_context:
                calls: This method does not explicitly call other methods or functions within its implementation based on the provided context.
                called_by: This method is not explicitly called by other functions or methods in the provided context.
            error: null
        usage_context:
          dependencies: This class does not have any explicit external dependencies listed in the provided context.
          instantiated_by: This class is not explicitly instantiated by other components in the provided context.
      error: null
    backend.File_Dependency.FileDependencyGraph:
      identifier: backend.File_Dependency.FileDependencyGraph
      description:
        overall: "The FileDependencyGraph class is an AST NodeVisitor designed to analyze Python source code files and build a graph of their import dependencies. It traverses the Abstract Syntax Tree of a given file, specifically looking for 'import' and 'from ... import' statements. It includes sophisticated logic to resolve relative imports and identify symbols exported from '__init__.py' files, accurately mapping dependencies within a repository."
        init_method:
          description: "This method initializes the FileDependencyGraph instance. It sets the `filename` (the file being analyzed) and the `repo_root` (the root directory of the repository) as instance attributes, which are crucial for resolving file paths and dependencies."
          parameters[2]{name,type,description}:
            filename,str,The name of the file for which dependencies are being analyzed.
            repo_root,str,"The root directory of the repository, used for resolving file paths."
        methods[5]:
          - identifier: _resolve_module_name
            description:
              overall: "This method resolves relative import statements, such as `from .. import name1, name2`. It calculates the correct base directory based on the import level and the current file's location within the repository. It then iterates through the imported names, checking if they correspond to existing module files or if they are explicitly exported symbols from `__init__.py` files, returning a list of successfully resolved names."
              parameters[1]{name,type,description}:
                node,ImportFrom,The AST `ImportFrom` node representing the relative import statement to be resolved.
              returns[1]{name,type,description}:
                resolved,"list[str]",A list of resolved module or symbol names that exist within the repository.
              usage_context:
                calls: This method does not make any calls to other functions or methods based on the provided context.
                called_by: This method does not appear to be called by other functions or methods based on the provided context.
            error: null
          - identifier: module_file_exists
            description:
              overall: "This function, nested within `_resolve_module_name`, checks if a given module `name` exists as a Python file (`.py`) or as a package (`__init__.py`) relative to a specified `rel_base` directory. It constructs the full path using the `repo_root_path` (which is defined in the outer `_resolve_module_name` scope) and the provided `rel_base` and `name` to verify file system existence."
              parameters[2]{name,type,description}:
                rel_base,Path,"The relative base path to check against, typically a directory within the repository."
                name,str,The name of the module or file to check for existence.
              returns[1]{name,type,description}:
                "",bool,"True if the module file or package `__init__.py` exists, False otherwise."
              usage_context:
                calls: This method does not make any calls to other functions or methods based on the provided context.
                called_by: This method does not appear to be called by other functions or methods based on the provided context.
            error: null
          - identifier: init_exports_symbol
            description:
              overall: "This function, nested within `_resolve_module_name`, determines if a specified `symbol` is explicitly exported or defined within an `__init__.py` file located at `rel_base`. It reads and parses the `__init__.py` file (using `repo_root_path` from the outer scope) to check for the symbol's presence in `__all__` lists or as a direct function, class, or assignment definition."
              parameters[2]{name,type,description}:
                rel_base,Path,The relative base path where the `__init__.py` file is expected.
                symbol,str,The symbol name to check for export or definition within the `__init__.py` file.
              returns[1]{name,type,description}:
                "",bool,"True if the `__init__.py` exists and exports/defines the symbol, False otherwise."
              usage_context:
                calls: This method does not make any calls to other functions or methods based on the provided context.
                called_by: This method does not appear to be called by other functions or methods based on the provided context.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method is part of the AST `NodeVisitor` pattern and is responsible for processing `Import` and `ImportFrom` nodes. It records direct import dependencies by adding the imported module's base name to the `import_dependencies` dictionary for the current `filename`. After processing, it calls `generic_visit` to continue the AST traversal."
              parameters[2]{name,type,description}:
                node,Import | ImportFrom,The AST node representing an import statement (either `Import` or `ImportFrom`).
                base_name,str | None,"An optional base name for the module, typically provided when resolving `ImportFrom` nodes to specify the module's final part."
              returns[0]:
              usage_context:
                calls: This method does not make any calls to other functions or methods based on the provided context.
                called_by: This method does not appear to be called by other functions or methods based on the provided context.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method specifically handles `ImportFrom` AST nodes. If the import has a direct module name (e.g., `from a.b.c import d`), it extracts the last part of the module name and calls `visit_Import` to record the dependency. If it's a relative import (e.g., `from .. import name`), it attempts to resolve the actual module names using `_resolve_module_name` and then records those resolved dependencies via `visit_Import`. It also ensures the generic AST traversal continues."
              parameters[1]{name,type,description}:
                node,ImportFrom,"The AST `ImportFrom` node to process, representing a 'from ... import ...' statement."
              returns[0]:
              usage_context:
                calls: This method does not make any calls to other functions or methods based on the provided context.
                called_by: This method does not appear to be called by other functions or methods based on the provided context.
            error: null
        usage_context:
          dependencies: The class does not have any explicit external dependencies listed in the provided context.
          instantiated_by: The class is not explicitly instantiated by any known components within the provided context.
      error: null
    backend.HelperLLM.LLMHelper:
      identifier: backend.HelperLLM.LLMHelper
      description:
        overall: "The LLMHelper class provides a centralized interface for interacting with various Large Language Models (LLMs) to generate structured documentation for Python functions and classes. It handles the configuration of different LLM providers (Gemini, OpenAI, Ollama, custom APIs) and manages system prompts for specific analysis tasks. The class implements batch processing with rate limiting and robust error handling, ensuring efficient and reliable generation of FunctionAnalysis and ClassAnalysis outputs."
        init_method:
          description: "The constructor initializes the LLMHelper by validating the API key and loading system prompts from specified file paths. It configures the LLM model based on `model_name`, setting up `function_llm` and `class_llm` for structured output and a `raw_llm` for general use. It also determines the batch size for API calls by invoking `_configure_batch_settings`."
          parameters[5]{name,type,description}:
            api_key,str,"The API key for the chosen LLM service (e.g., Gemini, OpenAI)."
            function_prompt_path,str,Path to the file containing the system prompt for function analysis.
            class_prompt_path,str,Path to the file containing the system prompt for class analysis.
            model_name,str,"The name of the LLM model to use, defaulting to 'gemini-2.0-flash-lite'."
            base_url,str | None,Optional base URL for custom LLM endpoints like Ollama or SCADSLLM.
        methods[3]:
          - identifier: _configure_batch_settings
            description:
              overall: "This private method sets the `batch_size` attribute of the instance based on the provided `model_name`. It uses a series of conditional statements to assign different batch sizes optimized for various LLM models, including specific Gemini versions, Llama3, GPT models, and custom APIs. If the model name is unrecognized, it defaults to a conservative batch size of 2 and logs a warning."
              parameters[1]{name,type,description}:
                model_name,str,The name of the LLM model for which to configure batch settings.
              returns[0]:
              usage_context:
                calls: "This method does not explicitly call other methods or functions within its source code, beyond standard Python operations."
                called_by: This method is called by the `__init__` constructor of the `LLMHelper` class to set up the appropriate batch size for the chosen LLM model.
            error: null
          - identifier: generate_for_functions
            description:
              overall: "This method takes a list of `FunctionAnalysisInput` objects, converts them into JSON payloads, and then constructs a list of conversation messages for the LLM. It processes these conversations in batches, using the `function_llm` to generate structured function analysis. The method includes error handling for batch calls and implements a waiting period between batches to respect API rate limits. It returns a list of `FunctionAnalysis` objects, with `None` for any failed generations."
              parameters[1]{name,type,description}:
                function_inputs,"List[FunctionAnalysisInput]","A list of input objects, each containing data required for analyzing a function."
              returns[1]{name,type,description}:
                None,"List[Optional[FunctionAnalysis]]","A list of `FunctionAnalysis` objects, where each object represents the structured analysis of a function, or `None` if an error occurred during its generation."
              usage_context:
                calls: "This method calls `json.dumps` to serialize input, `logging.info` and `logging.error` for logging, `self.function_llm.batch` to interact with the LLM, and `time.sleep` for rate limiting."
                called_by: "The input `method_context` does not specify any callers for this method, implying it is called externally or by other high-level orchestration logic."
            error: null
          - identifier: generate_for_classes
            description:
              overall: "This method is responsible for generating structured documentation for a batch of classes using an LLM. It takes a list of `ClassAnalysisInput` objects, converts them into JSON, and prepares them as conversations for the `class_llm`. The method iterates through these conversations in batches, making calls to the LLM, handling potential errors, and enforcing rate limits with a waiting period. It returns a list of `ClassAnalysis` objects, with `None` for any failed generations."
              parameters[1]{name,type,description}:
                class_inputs,"List[ClassAnalysisInput]","A list of input objects, each containing data required for analyzing a class."
              returns[1]{name,type,description}:
                None,"List[Optional[ClassAnalysis]]","A list of `ClassAnalysis` objects, where each object represents the structured analysis of a class, or `None` if an error occurred during its generation."
              usage_context:
                calls: "This method calls `json.dumps` to serialize input, `logging.info` and `logging.error` for logging, `self.class_llm.batch` to interact with the LLM, and `time.sleep` for rate limiting."
                called_by: "The input `method_context` does not specify any callers for this method, implying it is called externally or by other high-level orchestration logic."
            error: null
        usage_context:
          dependencies: "The class depends on external libraries for LLM interaction (e.g., `langchain_google_genai`, `langchain_ollama`, `langchain_openai`), JSON serialization (`json`), logging (`logging`), time management (`time`), and Pydantic schemas (`schemas.types`)."
          instantiated_by: The specific instantiation points for this class are not provided in the current context.
      error: null
    backend.MainLLM.MainLLM:
      identifier: backend.MainLLM.MainLLM
      description:
        overall: "The MainLLM class serves as a versatile interface for interacting with various Large Language Models (LLMs). It abstracts away the specifics of different LLM providers (Gemini, OpenAI-compatible, Ollama) by dynamically instantiating the appropriate client based on configuration. Its primary responsibilities include loading a system prompt, managing LLM model selection, and providing methods for both single-shot and streaming interactions with the chosen LLM, ensuring robust error handling."
        init_method:
          description: "The constructor initializes the MainLLM instance by setting up the system prompt from a file and configuring the underlying Large Language Model (LLM) client. It supports various LLM providers like Gemini, OpenAI-compatible APIs (e.g., SCADSLLM), and Ollama, dynamically selecting the client based on the `model_name` and `base_url`. It also performs validation for the API key and prompt file path."
          parameters[4]{name,type,description}:
            api_key,str,The API key required for authenticating with the chosen LLM service.
            prompt_file_path,str,The file path to the system prompt that will guide the LLM's behavior.
            model_name,str,"The name of the LLM model to use, defaulting to 'gemini-2.5-pro'. This determines which LLM client (Gemini, OpenAI, Ollama) is instantiated."
            base_url,str,"An optional base URL for custom LLM endpoints, used primarily for Ollama or other OpenAI-compatible services if SCADSLLM_URL is not set."
        methods[2]:
          - identifier: call_llm
            description:
              overall: "This method sends a user input along with the pre-configured system prompt to the underlying LLM and retrieves a single, complete response. It constructs a list of messages, logs the call, invokes the LLM, and returns the content of the LLM's response. Error handling is included to catch exceptions during the LLM invocation."
              parameters[1]{name,type,description}:
                user_input,str,The user's query or message to be sent to the LLM.
              returns[2]{name,type,description}:
                content,str,The textual content of the LLM's response.
                None,None,Returns None if an error occurs during the LLM call.
              usage_context:
                calls: "This method calls SystemMessage, HumanMessage, self.llm.invoke, logging.info, and logging.error."
                called_by: This method is not explicitly called by other methods within the provided context.
            error: null
          - identifier: stream_llm
            description:
              overall: "This method interacts with the LLM to get a streaming response, yielding chunks of content as they become available. It prepares the messages with the system prompt and user input, then iterates through the LLM's stream. Each chunk's content is yielded, allowing for real-time display of the LLM's output. It also includes error handling to yield an error message if the streaming process fails."
              parameters[1]{name,type,description}:
                user_input,str,The user's query or message for which a streaming LLM response is requested.
              returns[2]{name,type,description}:
                chunk.content,str,Yields individual content chunks from the LLM's streaming response.
                error_message,str,Yields an error message string if an exception occurs during the streaming call.
              usage_context:
                calls: "This method calls SystemMessage, HumanMessage, self.llm.stream, logging.info, and logging.error."
                called_by: This method is not explicitly called by other methods within the provided context.
            error: null
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies within the provided context.
          instantiated_by: This class is not explicitly shown to be instantiated by other components within the provided context.
      error: null
    backend.basic_info.ProjektInfoExtractor:
      identifier: backend.basic_info.ProjektInfoExtractor
      description:
        overall: "The ProjektInfoExtractor class is designed to extract comprehensive project information from common project files such as README, pyproject.toml, and requirements.txt. It initializes an internal dictionary with placeholder values, which is then populated by parsing these files. The class prioritizes information from pyproject.toml for dependencies and description, falling back to requirements.txt and README.md as needed. It provides a structured output containing project overview details, installation instructions, and dependencies."
        init_method:
          description: "The constructor initializes the ProjektInfoExtractor instance by setting a constant `INFO_NICHT_GEFUNDEN` to \"Information not found\". It also sets up a nested dictionary `self.info` which acts as a container for extracted project details, pre-populating all fields with the \"Information not found\" placeholder."
          parameters[0]:
        methods[7]:
          - identifier: _clean_content
            description:
              overall: "This private helper method processes a given string content by removing null bytes (\\x00). Null bytes can appear due to encoding errors, such as reading a UTF-16 encoded file as UTF-8. The method ensures that the content is clean before further parsing, returning an empty string if the input content is empty."
              parameters[1]{name,type,description}:
                content,str,The string content to be cleaned.
              returns[1]{name,type,description}:
                cleaned_content,str,The content string with null bytes removed.
              usage_context:
                calls: This method does not explicitly call other functions or methods.
                called_by: "This method is called by _parse_readme, _parse_toml, and _parse_requirements to clean file contents before parsing."
            error: null
          - identifier: _finde_datei
            description:
              overall: "This private helper method searches for a specific file within a list of provided files. It takes a list of filename patterns and a list of file objects, iterating through them to find a match. The search is case-insensitive, checking if a file's path ends with any of the given patterns. It returns the matching file object if found, otherwise None."
              parameters[2]{name,type,description}:
                patterns,"List[str]","A list of string patterns (e.g., file extensions or full filenames) to search for."
                dateien,"List[Any]","A list of file-like objects, each expected to have a `path` attribute."
              returns[1]{name,type,description}:
                found_file,"Optional[Any]","The file object that matches one of the patterns, or None if no match is found."
              usage_context:
                calls: This method does not explicitly call other functions or methods.
                called_by: "This method is called by extrahiere_info to locate specific project configuration files like README, pyproject.toml, and requirements.txt."
            error: null
          - identifier: _extrahiere_sektion_aus_markdown
            description:
              overall: "This private helper method extracts text content located under a specific Markdown heading (level 2, ##) within a given string. It constructs a regular expression pattern based on a list of keywords to find the heading. Once the heading is matched, it captures all content following it until the next level 2 heading or the end of the document. The method returns the stripped content of the section or None if no matching section is found."
              parameters[2]{name,type,description}:
                inhalt,str,The Markdown content string to parse.
                keywords,"List[str]","A list of keywords that could form the Markdown heading (e.g., \"Features\", \"Installation\")."
              returns[1]{name,type,description}:
                section_content,"Optional[str]","The extracted text content under the matched Markdown heading, or None."
              usage_context:
                calls: "This method calls re.escape, re.compile, and re.search for regular expression operations."
                called_by: "This method is called by _parse_readme to extract specific sections like \"Key Features\", \"Tech Stack\", \"Status\", \"Installation\", and \"Quick Start\" from README files."
            error: null
          - identifier: _parse_readme
            description:
              overall: "This private method parses the content of a README file to extract various project information. It first cleans the content using _clean_content. It then attempts to find the project title and a general description using regular expressions. Subsequently, it utilizes _extrahiere_sektion_aus_markdown to extract specific sections like \"Key Features\", \"Tech Stack\", \"Status\", \"Installation instructions\", and \"Quick Start Guide\" based on predefined keywords. The extracted information is stored in the self.info dictionary."
              parameters[1]{name,type,description}:
                inhalt,str,The raw string content of the README file.
              returns[0]:
              usage_context:
                calls: "This method calls _clean_content to preprocess the input, re.search for title and description extraction, and _extrahiere_sektion_aus_markdown for section-specific content."
                called_by: This method is called by extrahiere_info to process the content of a found README file.
            error: null
          - identifier: _parse_toml
            description:
              overall: "This private method parses the content of a pyproject.toml file to extract project metadata. It first cleans the content using _clean_content. It checks for the availability of the tomllib module and prints a warning if it's not installed. If tomllib is available, it attempts to load the TOML content, extract the project name, description, and dependencies from the [project] table, and store them in self.info. It includes error handling for TOMLDecodeError."
              parameters[1]{name,type,description}:
                inhalt,str,The raw string content of the pyproject.toml file.
              returns[0]:
              usage_context:
                calls: "This method calls _clean_content for content preprocessing, tomllib.loads to parse TOML data, and data.get to access dictionary elements. It also uses print for warnings."
                called_by: This method is called by extrahiere_info to process the content of a found pyproject.toml file.
            error: null
          - identifier: _parse_requirements
            description:
              overall: "This private method parses the content of a requirements.txt file to extract project dependencies. It first cleans the content using _clean_content. It only proceeds to extract dependencies if the dependencies field in self.info is still set to the default \"Information not found\" placeholder, giving priority to pyproject.toml if it was parsed first. It splits the content into lines, filters out empty lines and comments, and stores the remaining lines as dependencies in self.info."
              parameters[1]{name,type,description}:
                inhalt,str,The raw string content of the requirements.txt file.
              returns[0]:
              usage_context:
                calls: This method calls _clean_content for content preprocessing.
                called_by: This method is called by extrahiere_info to process the content of a found requirements.txt file.
            error: null
          - identifier: extrahiere_info
            description:
              overall: "This public method orchestrates the entire information extraction process from various project files. It first uses _finde_datei to locate README, pyproject.toml, and requirements.txt files within a provided list of file objects. It then parses these files in a prioritized order: pyproject.toml first, then requirements.txt, and finally README.md. After parsing, it formats the extracted dependencies and attempts to derive a project title from the repo_url if no title was found in the files. Finally, it returns the complete self.info dictionary containing all gathered project details."
              parameters[2]{name,type,description}:
                dateien,"List[Any]","A list of file-like objects, each expected to have `path` and `content` attributes."
                repo_url,str,"The URL of the repository, used as a fallback for the project title."
              returns[1]{name,type,description}:
                project_info,"Dict[str, Any]",A dictionary containing all extracted project information.
              usage_context:
                calls: "This method calls _finde_datei to locate files, _parse_toml, _parse_requirements, and _parse_readme to parse their contents. It also uses os.path.basename and str.removesuffix for URL processing."
                called_by: "This method is the primary public interface of the ProjektInfoExtractor class, intended to be called externally to initiate the information extraction."
            error: null
        usage_context:
          dependencies: "The class relies on the re module for regular expression operations, the os module for path manipulation, and the tomllib module for parsing TOML files. It also uses typing for type hints."
          instantiated_by: The instantiation points for this class are not specified in the provided context.
      error: null
    backend.callgraph.CallGraph:
      identifier: backend.callgraph.CallGraph
      description:
        overall: "The CallGraph class is an AST NodeVisitor designed to construct a call graph for a given Python source file. It traverses the Abstract Syntax Tree (AST) of a file, identifying function definitions, class definitions, imports, and function calls. The class maintains internal state to track the current function and class context, resolve fully qualified names for functions and methods, and record call relationships in a directed graph."
        init_method:
          description: "The constructor initializes the CallGraph instance with the filename being analyzed. It sets up various internal data structures such as dictionaries for local definitions, import mappings, and edges, along with a NetworkX directed graph to store the call relationships. It also initializes state variables for tracking the current function and class during AST traversal."
          parameters[1]{name,type,description}:
            filename,str,The path to the Python source file being analyzed.
        methods[11]:
          - identifier: _recursive_call
            description:
              overall: "This private helper method recursively extracts the name components from an AST node representing a callable. It handles `ast.Call`, `ast.Name`, and `ast.Attribute` nodes to build a list of strings that represent the dotted path of the callable, such as ['pkg', 'mod', 'Class', 'method']. This is crucial for identifying the full name of a function or method being called."
              parameters[1]{name,type,description}:
                node,ast.AST,"The AST node to analyze, typically an ast.Call, ast.Name, or ast.Attribute."
              returns[1]{name,type,description}:
                parts,"list[str]",A list of strings representing the components of the callable's name.
              usage_context:
                calls: This method recursively calls itself to process nested AST nodes.
                called_by: This method is called by _resolve_all_callee_names and visit_Call.
            error: null
          - identifier: _resolve_all_callee_names
            description:
              overall: "This private method takes a list of potential callee name components and resolves them to their fully qualified names. It first checks against local definitions (functions/methods defined within the current scope), then against the `import_mapping` to resolve imported names. If neither applies, it constructs a full name based on the current filename and class context. This ensures that each callee is identified uniquely within the project."
              parameters[1]{name,type,description}:
                callee_nodes,"list[list[str]]","A list where each inner list contains name components of a potential callee (e.g., [['module', 'function'], ['Class', 'method']])."
              returns[1]{name,type,description}:
                resolved,"list[str]",A list of fully qualified names (strings) for the resolved callees.
              usage_context:
                calls: This method does not explicitly call other methods.
                called_by: This method is called by visit_Call.
            error: null
          - identifier: _make_full_name
            description:
              overall: "This private helper method constructs a fully qualified name for a function or method. It combines the filename, an optional class name, and the base name of the function/method. This standardization is essential for creating unique identifiers for nodes in the call graph, ensuring that functions are correctly distinguished across different files and classes."
              parameters[2]{name,type,description}:
                basename,str,The base name of the function or method.
                class_name,str | None,"The name of the class if the function is a method, otherwise None."
              returns[1]{name,type,description}:
                full_name,str,The fully qualified name of the function or method.
              usage_context:
                calls: This method does not explicitly call other methods.
                called_by: This method is called by visit_FunctionDef.
            error: null
          - identifier: _current_caller
            description:
              overall: "This private method determines the identifier for the current calling context. If a function is currently being visited (i.e., `self.current_function` is set), its full name is returned. Otherwise, it returns a placeholder string indicating the global scope of the current file, or a generic global scope if the filename is not available. This is used to identify the source of a function call."
              parameters[0]:
              returns[1]{name,type,description}:
                caller_identifier,str,A string representing the fully qualified name of the current function or the global scope identifier.
              usage_context:
                calls: This method does not explicitly call other methods.
                called_by: This method is called by visit_Call.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method is an AST visitor for `ast.Import` nodes. It processes `import module_name as alias` statements, extracting the module name and its alias (if present). It then populates the `self.import_mapping` dictionary, associating the alias (or original name) with the actual module name. After processing, it calls `generic_visit` to continue traversing the AST."
              parameters[1]{name,type,description}:
                node,ast.Import,The AST node representing an import statement.
              returns[0]:
              usage_context:
                calls: This method calls self.generic_visit to continue AST traversal.
                called_by: This method is called by the ast.NodeVisitor mechanism during AST traversal.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method is an AST visitor for `ast.ImportFrom` nodes. It handles `from module import name as alias` statements. It extracts the module name and then iterates through the imported names, mapping each alias (or original name) to the module it originates from. This mapping is stored in `self.import_mapping` to aid in resolving fully qualified names during call analysis."
              parameters[1]{name,type,description}:
                node,ast.ImportFrom,The AST node representing a 'from ... import ...' statement.
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other methods.
                called_by: This method is called by the ast.NodeVisitor mechanism during AST traversal.
            error: null
          - identifier: visit_ClassDef
            description:
              overall: "This method is an AST visitor for `ast.ClassDef` nodes. When a class definition is encountered, it temporarily updates `self.current_class` to the name of the current class. This context is crucial for correctly forming fully qualified names of methods defined within that class. After visiting the class's body using `generic_visit`, it restores the previous `self.current_class` to maintain correct scope."
              parameters[1]{name,type,description}:
                node,ast.ClassDef,The AST node representing a class definition.
              returns[0]:
              usage_context:
                calls: This method calls self.generic_visit to continue AST traversal.
                called_by: This method is called by the ast.NodeVisitor mechanism during AST traversal.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method is an AST visitor for `ast.FunctionDef` nodes, processing regular function definitions. It constructs the full qualified name of the function using `_make_full_name`, stores it in `local_defs` for resolution, and updates `self.current_function` to reflect the current scope. It adds the function as a node to the `self.graph` and then traverses its body. Finally, it adds the function to `function_set` and restores the previous `self.current_function`."
              parameters[1]{name,type,description}:
                node,ast.FunctionDef,The AST node representing a function definition.
              returns[0]:
              usage_context:
                calls: "This method calls self._make_full_name, self.graph.add_node, and self.generic_visit."
                called_by: This method is called by the ast.NodeVisitor mechanism during AST traversal and by visit_AsyncFunctionDef.
            error: null
          - identifier: visit_AsyncFunctionDef
            description:
              overall: "This method is an AST visitor for `ast.AsyncFunctionDef` nodes, handling asynchronous function definitions. For the purpose of call graph construction, it delegates its processing directly to `visit_FunctionDef`. This ensures that asynchronous functions are treated similarly to regular functions in terms of name resolution and graph node creation, simplifying the overall logic."
              parameters[1]{name,type,description}:
                node,ast.AsyncFunctionDef,The AST node representing an asynchronous function definition.
              returns[0]:
              usage_context:
                calls: This method calls self.visit_FunctionDef.
                called_by: This method is called by the ast.NodeVisitor mechanism during AST traversal.
            error: null
          - identifier: visit_Call
            description:
              overall: "This method is an AST visitor for `ast.Call` nodes, which represent function calls. It first identifies the `caller` using `_current_caller` and then extracts the `callee`'s name components using `_recursive_call`. These components are then resolved to a fully qualified name using `_resolve_all_callee_names`. Finally, it records the call relationship by adding the resolved callee to the `self.edges` dictionary under the caller's identifier, before continuing AST traversal."
              parameters[1]{name,type,description}:
                node,ast.Call,The AST node representing a function call.
              returns[0]:
              usage_context:
                calls: "This method calls self._current_caller, self._recursive_call, self._resolve_all_callee_names, and self.generic_visit."
                called_by: This method is called by the ast.NodeVisitor mechanism during AST traversal.
            error: null
          - identifier: visit_If
            description:
              overall: "This method is an AST visitor for `ast.If` nodes. It includes special handling for the `if __name__ == \"__main__\":` block, which is common in Python scripts. When such a block is detected, it temporarily sets `self.current_function` to \"<main_block>\" to correctly attribute calls made within this entry point. For all other `if` statements, it simply continues the generic AST traversal."
              parameters[1]{name,type,description}:
                node,ast.If,The AST node representing an if statement.
              returns[0]:
              usage_context:
                calls: This method calls self.generic_visit to continue AST traversal.
                called_by: This method is called by the ast.NodeVisitor mechanism during AST traversal.
            error: null
        usage_context:
          dependencies: This class does not explicitly list its functional dependencies in the provided context.
          instantiated_by: This class is not explicitly shown to be instantiated by any other components in the provided context.
      error: null
    backend.getRepo.RepoFile:
      identifier: backend.getRepo.RepoFile
      description:
        overall: "The RepoFile class represents a single file within a Git repository, providing mechanisms for lazy loading its content and metadata. It encapsulates the file's path and its associated Git tree, allowing for efficient access to its blob, content, and size only when needed. The class also offers utility methods for converting file data to a dictionary and performing basic analysis like word counting."
        init_method:
          description: "The __init__ method initializes a RepoFile object by storing the file's path and the Git commit_tree from which it originates. It also sets up internal attributes (_blob, _content, _size) to None, indicating that these properties will be loaded lazily upon first access."
          parameters[2]{name,type,description}:
            file_path,str,The path to the file within the repository.
            commit_tree,git.Tree,The Tree-object of the commit from which the file originates.
        methods[6]:
          - identifier: blob
            description:
              overall: "This property provides lazy loading of the Git blob object corresponding to the file path within the associated commit tree. It checks if the _blob attribute is already set; if not, it attempts to retrieve the blob from the _tree using the file path. A FileNotFoundError is raised if the file cannot be found in the commit tree."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",git.Blob,The Git blob object for the file.
              usage_context:
                calls: This method does not explicitly call other methods or functions within its source code.
                called_by: This method is not explicitly called by other functions or methods within the provided context.
            error: null
          - identifier: content
            description:
              overall: "This property provides lazy loading of the file's content, decoding it as UTF-8. It first ensures the _content attribute is not None; if it is, it accesses the blob property (which itself is lazy-loaded) and reads its data stream, decoding it with error handling."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",str,The decoded content of the file.
              usage_context:
                calls: This method does not explicitly call other methods or functions within its source code.
                called_by: This method is not explicitly called by other functions or methods within the provided context.
            error: null
          - identifier: size
            description:
              overall: "This property provides lazy loading of the file's size in bytes. It checks if the _size attribute is None; if so, it retrieves the size from the blob property. This ensures the size is only computed or fetched when explicitly requested."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",int,The size of the file in bytes.
              usage_context:
                calls: This method does not explicitly call other methods or functions within its source code.
                called_by: This method is not explicitly called by other functions or methods within the provided context.
            error: null
          - identifier: analyze_word_count
            description:
              overall: "This method serves as an example analysis function, calculating the total number of words in the file's content. It accesses the content property, splits the string by whitespace, and returns the length of the resulting list, effectively counting the words."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",int,The total word count of the file content.
              usage_context:
                calls: This method does not explicitly call other methods or functions within its source code.
                called_by: This method is not explicitly called by other functions or methods within the provided context.
            error: null
          - identifier: __repr__
            description:
              overall: "This special method provides a developer-friendly string representation of the RepoFile object. It returns a formatted string indicating the class name and the file's path, which is useful for debugging and logging."
              parameters[0]:
              returns[1]{name,type,description}:
                "null",str,"A string representation of the RepoFile object, including its path."
              usage_context:
                calls: This method does not explicitly call other methods or functions within its source code.
                called_by: This method is not explicitly called by other functions or methods within the provided context.
            error: null
          - identifier: to_dict
            description:
              overall: "This method converts the RepoFile object into a dictionary representation, providing structured access to its metadata. It includes the file's path, its base name, size, and type. Optionally, it can include the file's content if the 'include_content' parameter is set to True."
              parameters[1]{name,type,description}:
                include_content,bool,"If True, the file's content will be included in the dictionary."
              returns[1]{name,type,description}:
                data,dict,"A dictionary representation of the file, optionally including its content."
              usage_context:
                calls: This method does not explicitly call other methods or functions within its source code.
                called_by: This method is not explicitly called by other functions or methods within the provided context.
            error: null
        usage_context:
          dependencies: This class does not have explicit external dependencies listed in the provided context.
          instantiated_by: This class is not explicitly instantiated by other components listed in the provided context.
      error: null
    backend.getRepo.GitRepository:
      identifier: backend.getRepo.GitRepository
      description:
        overall: "The GitRepository class provides a robust mechanism for interacting with Git repositories programmatically. It handles the cloning of a remote repository into a temporary local directory, offers methods to retrieve all files as RepoFile objects, and can construct a hierarchical representation of the repository's file structure. Furthermore, it implements the context manager protocol, ensuring proper cleanup of the temporary directory upon exiting a with statement, making it suitable for temporary and secure repository access."
        init_method:
          description: "The __init__ method initializes a GitRepository instance by cloning a specified Git repository URL into a temporary directory. It sets up instance attributes such as the repository URL, the path to the temporary directory, the GitPython Repo object, and the latest commit and its tree. If cloning fails, it cleans up the temporary directory and raises a RuntimeError."
          parameters[1]{name,type,description}:
            repo_url,str,The URL of the Git repository to be cloned.
        methods[5]:
          - identifier: get_all_files
            description:
              overall: "This method retrieves a list of all files present in the cloned Git repository. It uses the `git.ls_files()` command to get file paths, then processes these paths to create `RepoFile` objects, which are stored in the `self.files` attribute and returned. The `RepoFile` objects are initialized with the file path and the commit tree for content access."
              parameters[0]:
              returns[1]{name,type,description}:
                None,"list[RepoFile]",A list of RepoFile instances representing all files in the repository.
              usage_context:
                calls: This method calls `self.repo.git.ls_files()` and instantiates `RepoFile` objects.
                called_by: This method is called by `get_file_tree`.
            error: null
          - identifier: close
            description:
              overall: "This method is responsible for cleaning up the temporary directory where the Git repository was cloned. It checks if `self.temp_dir` is set and, if so, prints a message indicating the deletion and then sets `self.temp_dir` to `None`. This method is crucial for resource management and preventing leftover temporary files, although the actual directory deletion logic is not explicitly shown in the provided code."
              parameters[0]:
              returns[0]:
              usage_context:
                calls: This method does not explicitly call other methods or functions.
                called_by: This method is called by `__init__` in case of a cloning error and by `__exit__`.
            error: null
          - identifier: __enter__
            description:
              overall: "This special method makes the `GitRepository` class compatible with the context manager protocol (e.g., `with GitRepository(...) as repo:`). When entering a `with` statement, this method is called and simply returns the instance of the `GitRepository` itself, allowing it to be bound to the `as` variable."
              parameters[0]:
              returns[1]{name,type,description}:
                None,GitRepository,The instance of the GitRepository class itself.
              usage_context:
                calls: This method does not call any other functions or methods.
                called_by: This method is implicitly called when the `GitRepository` object is used in a `with` statement.
            error: null
          - identifier: __exit__
            description:
              overall: "This special method is part of the context manager protocol. It is automatically invoked when exiting a `with` statement, regardless of whether an exception occurred. Its primary responsibility is to ensure that resources are properly cleaned up by calling the `close` method of the instance."
              parameters[3]{name,type,description}:
                exc_type,type,"The type of the exception that caused the context to be exited, or None if no exception occurred."
                exc_val,Exception,"The exception instance that caused the context to be exited, or None."
                exc_tb,traceback,"A traceback object encapsulating the call stack at the point where the exception originally occurred, or None."
              returns[0]:
              usage_context:
                calls: This method calls `self.close()`.
                called_by: This method is implicitly called when exiting a `with` statement that uses a `GitRepository` object.
            error: null
          - identifier: get_file_tree
            description:
              overall: "This method constructs a hierarchical dictionary representation of the repository's file structure, similar to a file system tree. It first ensures that all files have been loaded by calling `get_all_files()` if `self.files` is empty. Then, it iterates through each `RepoFile` object, splitting its path to build nested dictionary structures representing directories and files. Each file is added as a dictionary, optionally including its content based on the `include_content` parameter."
              parameters[1]{name,type,description}:
                include_content,bool,A flag indicating whether the content of each file should be included in its dictionary representation. Defaults to False.
              returns[1]{name,type,description}:
                None,dict,A dictionary representing the hierarchical file tree of the repository.
              usage_context:
                calls: This method calls `self.get_all_files()` and `file_obj.to_dict()`.
                called_by: This method is not explicitly called by other methods within the provided class definition.
            error: null
        usage_context:
          dependencies: "The class depends on `tempfile` for temporary directory creation, `git.Repo` and `git.GitCommandError` from the `git` library for Git operations, and `logging` for informational messages. It also implicitly depends on a `RepoFile` class, which is not defined in the provided source but is used."
          instantiated_by: This class is not explicitly instantiated by other components within the provided context.
      error: null
    backend.relationship_analyzer.ProjectAnalyzer:
      identifier: backend.relationship_analyzer.ProjectAnalyzer
      description:
        overall: "The ProjectAnalyzer class is designed to perform static analysis on a Python project to build a comprehensive call graph. It identifies all Python files, extracts definitions of functions, methods, and classes, and then resolves calls between these entities. The class provides methods to initiate the analysis, retrieve the processed call graph, and present raw relationships in a structured format."
        init_method:
          description: "The constructor initializes the ProjectAnalyzer instance by setting the project root, and establishing empty data structures for storing definitions, the call graph, and file ASTs. It also defines a set of directories to ignore during file traversal, such as version control directories and build outputs."
          parameters[1]{name,type,description}:
            project_root,string,The root directory of the project to be analyzed.
        methods[6]:
          - identifier: analyze
            description:
              overall: "This method orchestrates the entire project analysis workflow. It first identifies all Python files within the project, then iterates through them to collect all function, method, and class definitions. Subsequently, it iterates again to resolve all function and method calls, building a comprehensive call graph. Finally, it clears the cached ASTs to free memory and returns the generated call graph."
              parameters[0]:
              returns[1]{name,type,description}:
                call_graph,defaultdict,"A dictionary representing the call graph of the project, mapping callee identifiers to a list of caller information."
              usage_context:
                calls: "This method calls `_find_py_files` to locate Python files, `_collect_definitions` to gather definitions from each file, and `_resolve_calls` to establish call relationships."
                called_by: This method is not called by any other functions or methods in the provided context.
            error: null
          - identifier: get_raw_relationships
            description:
              overall: "This method processes the internal `call_graph` to generate a structured representation of outgoing and incoming relationships. It iterates through the `call_graph` to build two separate dictionaries: `outgoing` which maps a caller to the set of entities it calls, and `incoming` which maps a callee to the set of entities that call it. The results are then sorted and converted to lists for a consistent output format."
              parameters[0]:
              returns[1]{name,type,description}:
                relationships,dict,"A dictionary containing two keys, \"outgoing\" and \"incoming\", each mapping entity identifiers to sorted lists of related entity identifiers."
              usage_context:
                calls: This method does not call any other functions or methods.
                called_by: This method is not called by any other functions or methods in the provided context.
            error: null
          - identifier: _find_py_files
            description:
              overall: "This private helper method is responsible for traversing the project directory structure to locate all Python source files. It uses `os.walk` to recursively explore directories, actively pruning directories specified in `self.ignore_dirs` to avoid unnecessary processing. It collects the absolute paths of all files ending with \".py\"."
              parameters[0]:
              returns[1]{name,type,description}:
                py_files,"list[str]",A list of absolute file paths to Python source files found within the project root.
              usage_context:
                calls: This method calls `os.walk` to traverse the file system and `os.path.join` to construct file paths.
                called_by: This method is called by `analyze`.
            error: null
          - identifier: _collect_definitions
            description:
              overall: "This private method reads the content of a given Python file, parses it into an Abstract Syntax Tree (AST), and then walks the AST to identify function, method, and class definitions. For each definition found, it constructs a unique path name and stores metadata (file path, line number, type) in `self.definitions`. It also caches the AST in `self.file_asts` for later use. Error handling is included for file processing."
              parameters[1]{name,type,description}:
                filepath,str,The absolute path to the Python file to be analyzed for definitions.
              returns[0]:
              usage_context:
                calls: "This method calls `open` to read the file, `ast.parse` to create the AST, `ast.walk` to traverse the AST, `_get_parent` to determine if a function is a method, `path_to_module` to convert a file path to a module path, and `logging.error` for error reporting."
                called_by: This method is called by `analyze`.
            error: null
          - identifier: _get_parent
            description:
              overall: "This private utility method searches for the immediate parent AST node of a given child node within a larger AST. It iterates through all nodes in the provided `tree` and, for each node, checks its direct children to see if any match the `node` parameter. If a match is found, the parent node is returned."
              parameters[2]{name,type,description}:
                tree,ast.AST,The root AST node or subtree to search within.
                node,ast.AST,The child AST node for which to find the parent.
              returns[1]{name,type,description}:
                parent_node,ast.AST | None,"The parent AST node if found, otherwise None."
              usage_context:
                calls: This method calls `ast.walk` to traverse the AST and `ast.iter_child_nodes` to get children of a node.
                called_by: This method is called by `_collect_definitions`.
            error: null
          - identifier: _resolve_calls
            description:
              overall: "This private method takes a file's AST (retrieved from `self.file_asts`) and uses a `CallResolverVisitor` to identify all function and method calls within that file. It then aggregates the discovered call relationships into the class's `self.call_graph` attribute, extending the list of callers for each callee. Error handling is included for the call resolution process."
              parameters[1]{name,type,description}:
                filepath,str,The absolute path to the Python file whose calls need to be resolved.
              returns[0]:
              usage_context:
                calls: "This method calls `self.file_asts.get` to retrieve the AST, instantiates `CallResolverVisitor`, calls `resolver.visit` to perform the AST traversal and call resolution, and `logging.error` for error reporting."
                called_by: This method is called by `analyze`.
            error: null
        usage_context:
          dependencies: "The class depends on `os` for file system operations, `ast` for parsing Python code into Abstract Syntax Trees, `logging` for error reporting, and `collections.defaultdict` for its internal call graph representation. It also implicitly depends on `path_to_module` and `CallResolverVisitor` which are not defined in the provided source but are used."
          instantiated_by: This class is not instantiated by any other functions or methods in the provided context.
      error: null
    backend.relationship_analyzer.CallResolverVisitor:
      identifier: backend.relationship_analyzer.CallResolverVisitor
      description:
        overall: "The CallResolverVisitor class extends `ast.NodeVisitor` to systematically traverse the Abstract Syntax Tree (AST) of a Python file. Its core function is to identify and record all function and method calls, mapping each callee to its respective callers. It achieves this by managing import scopes, tracking instance types for method resolution, and maintaining context (current class and caller) during traversal to construct fully qualified names for all involved entities."
        init_method:
          description: "The constructor initializes the visitor with essential context such as the file path, project root, and a dictionary of known definitions. It sets up internal state variables including `module_path`, `scope` for import resolution, `instance_types` to track object types, and `current_caller_name` and `current_class_name` to maintain the current context during AST traversal. A `defaultdict` named `calls` is also initialized to store the discovered call relationships."
          parameters[3]{name,type,description}:
            filepath,str,The absolute path to the Python source file currently being analyzed.
            project_root,str,"The root directory of the entire project, used to derive the module path from the file path."
            definitions,dict,"A dictionary containing all known fully qualified definitions (functions, classes, methods) within the project, used to validate resolved call targets."
        methods[7]:
          - identifier: visit_ClassDef
            description:
              overall: "This method is invoked by the `ast.NodeVisitor` when a class definition (`ast.ClassDef`) is encountered during AST traversal. It temporarily updates the `current_class_name` attribute to the name of the class being visited, which is crucial for correctly forming fully qualified names for methods defined within that class. After processing the class's children, it restores the `current_class_name` to its previous state, ensuring proper context management."
              parameters[1]{name,type,description}:
                node,ast.ClassDef,The AST node representing the class definition.
              returns[0]:
              usage_context:
                calls: This method calls `self.generic_visit(node)` to continue the AST traversal into the class's body.
                called_by: This method is implicitly called by the `ast.NodeVisitor` framework when traversing an AST and encountering a `ClassDef` node.
            error: null
          - identifier: visit_FunctionDef
            description:
              overall: "This method is triggered when an `ast.FunctionDef` node (representing a function or method definition) is encountered. It constructs the full qualified identifier for the current function, taking into account whether it is a method within a class or a top-level function. The `self.current_caller_name` is then updated to this identifier before recursively visiting the function's body, and subsequently restored to its original value upon exiting the function's scope."
              parameters[1]{name,type,description}:
                node,ast.FunctionDef,The AST node representing the function or method definition.
              returns[0]:
              usage_context:
                calls: This method calls `self.generic_visit(node)` to continue the AST traversal into the function's body.
                called_by: This method is implicitly called by the `ast.NodeVisitor` framework when traversing an AST and encountering a `FunctionDef` node.
            error: null
          - identifier: visit_Call
            description:
              overall: "This method processes `ast.Call` nodes, which represent function or method invocations. It attempts to resolve the fully qualified name of the called entity using the `_resolve_call_qname` helper. If the callee is successfully resolved and exists within the `definitions`, the call is recorded. The recorded information includes the caller's file, line number, full identifier, and its type (e.g., module, method, function), which is then appended to the `self.calls` dictionary, indexed by the callee's qualified name."
              parameters[1]{name,type,description}:
                node,ast.Call,The AST node representing a function or method call.
              returns[0]:
              usage_context:
                calls: This method calls `self._resolve_call_qname(node.func)` to determine the qualified name of the called function and `self.generic_visit(node)` to continue the AST traversal.
                called_by: This method is implicitly called by the `ast.NodeVisitor` framework when traversing an AST and encountering a `Call` node.
            error: null
          - identifier: visit_Import
            description:
              overall: "This method handles `ast.Import` nodes, which correspond to `import module` statements. It iterates through each imported name and its potential alias, storing the mapping from the local name (alias or original name) to the full module name in the `self.scope` dictionary. This scope is essential for later resolving qualified names of entities imported directly. After processing the imports, it continues the AST traversal to other nodes."
              parameters[1]{name,type,description}:
                node,ast.Import,The AST node representing an import statement.
              returns[0]:
              usage_context:
                calls: This method calls `self.generic_visit(node)` to continue the AST traversal.
                called_by: This method is implicitly called by the `ast.NodeVisitor` framework when traversing an AST and encountering an `Import` node.
            error: null
          - identifier: visit_ImportFrom
            description:
              overall: "This method processes `ast.ImportFrom` nodes, which represent `from module import name` statements. It first determines the full module path, correctly handling relative imports based on `node.level`. For each name imported, it constructs its fully qualified path and stores this mapping in `self.scope`, associating the local name (alias or original) with its global qualified path. This ensures accurate resolution of imported functions or classes during subsequent analysis. The AST traversal then continues."
              parameters[1]{name,type,description}:
                node,ast.ImportFrom,The AST node representing a 'from ... import ...' statement.
              returns[0]:
              usage_context:
                calls: This method calls `self.generic_visit(node)` to continue the AST traversal.
                called_by: This method is implicitly called by the `ast.NodeVisitor` framework when traversing an AST and encountering an `ImportFrom` node.
            error: null
          - identifier: visit_Assign
            description:
              overall: "This method handles `ast.Assign` nodes, specifically focusing on assignments where the assigned value is a constructor call (e.g., `variable = ClassName()`). If such a pattern is detected, and the class name can be resolved through `self.scope` and is present in `self.definitions`, it records the qualified class name as the type for the assigned variable's identifier in `self.instance_types`. This mapping is crucial for resolving method calls on instances later in the analysis. The AST traversal then proceeds."
              parameters[1]{name,type,description}:
                node,ast.Assign,The AST node representing an assignment statement.
              returns[0]:
              usage_context:
                calls: This method calls `self.generic_visit(node)` to continue the AST traversal.
                called_by: This method is implicitly called by the `ast.NodeVisitor` framework when traversing an AST and encountering an `Assign` node.
            error: null
          - identifier: _resolve_call_qname
            description:
              overall: "This private helper method is responsible for resolving the fully qualified name (QName) of a function or method represented by an AST node. It handles two primary scenarios: direct function calls or imported names (`ast.Name`), and method calls on objects or module attributes (`ast.Attribute`). For `ast.Name` nodes, it checks `self.scope` and local module definitions. For `ast.Attribute` nodes, it uses `self.instance_types` to resolve instance methods or `self.scope` for module-level attributes. It returns the resolved QName as a string or `None` if resolution fails."
              parameters[1]{name,type,description}:
                func_node,ast.expr,"The AST node representing the function or method being called (e.g., an `ast.Name` or `ast.Attribute` node)."
              returns[1]{name,type,description}:
                name,str | None,"The fully qualified name of the called function or method, or `None` if it cannot be resolved."
              usage_context:
                calls: "This method does not explicitly call other methods within the provided source code, but it accesses `self.scope`, `self.definitions`, and `self.instance_types` for resolution."
                called_by: This method is called by `self.visit_Call` to resolve the qualified name of a function or method being invoked.
            error: null
        usage_context:
          dependencies: "The class relies on `ast` for AST traversal, `os` for path manipulation, and `collections.defaultdict` for storing call relationships. It also implicitly depends on a `path_to_module` utility function, which is not provided in the source but is used in `__init__`."
          instantiated_by: The instantiation points for this class are not provided in the context.
      error: null
    schemas.types.ParameterDescription:
      identifier: schemas.types.ParameterDescription
      description:
        overall: "The ParameterDescription class serves as a structured data model, inheriting from Pydantic's BaseModel, to encapsulate the essential details of a function parameter. It defines three core attributes: name, type, and description, ensuring that parameter information is consistently represented and validated. This class is primarily used for defining schemas and data structures where function parameters need to be formally described."
        init_method:
          description: "This class, being a Pydantic BaseModel, automatically generates an __init__ method that initializes its instance attributes (name, type, description) directly from the provided arguments. It ensures type validation upon instantiation."
          parameters[3]{name,type,description}:
            name,str,The name of the parameter.
            type,str,The type hint or inferred type of the parameter.
            description,str,A brief explanation of the parameter's purpose.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list external functional dependencies within the provided context.
          instantiated_by: The specific locations where this class is instantiated are not provided in the current context.
      error: null
    schemas.types.ReturnDescription:
      identifier: schemas.types.ReturnDescription
      description:
        overall: "The ReturnDescription class is a Pydantic BaseModel designed to structure and validate information about a function's return value. It serves as a data model to consistently represent the name, type, and a textual description of what a function returns. This class ensures that return value metadata is consistently formatted and easily consumable by other systems."
        init_method:
          description: "This class, being a Pydantic BaseModel, automatically generates an `__init__` method. It initializes instances with `name`, `type`, and `description` attributes, enforcing their string types based on the model's field definitions."
          parameters[3]{name,type,description}:
            name,str,The name or identifier of the return value.
            type,str,The Python type hint or description of the return value's type.
            description,str,A detailed explanation of what the return value represents or contains.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list external functional dependencies in the provided context.
          instantiated_by: The specific locations where this class is instantiated are not provided in the current context.
      error: null
    schemas.types.UsageContext:
      identifier: schemas.types.UsageContext
      description:
        overall: The UsageContext class is a Pydantic BaseModel designed to encapsulate information about how a function or method interacts with other parts of a system. It provides a structured way to describe what an entity calls and by what other entities it is called. This model is used to provide context for understanding the functional relationships within a codebase.
        init_method:
          description: "This class, being a Pydantic BaseModel, has an implicitly generated constructor. It initializes an instance by accepting values for `calls` and `called_by`, which are then stored as instance attributes, ensuring type validation upon instantiation."
          parameters[2]{name,type,description}:
            calls,str,"A string describing the functions, methods, or classes that this entity calls."
            called_by,str,A string describing the functions or methods that call this entity.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in the provided context.
          instantiated_by: The specific points of instantiation for this class are not provided in the current context.
      error: null
    schemas.types.FunctionDescription:
      identifier: schemas.types.FunctionDescription
      description:
        overall: "The `FunctionDescription` class is a Pydantic BaseModel designed to provide a structured and comprehensive analysis of a Python function. It encapsulates various aspects of a function, including its high-level purpose, detailed descriptions of its parameters, its return values, and its operational context within a larger system. This model facilitates the standardized representation of function metadata for automated processing and documentation."
        init_method:
          description: "As a Pydantic BaseModel, the `__init__` method for `FunctionDescription` is implicitly generated. It handles the instantiation of the class by validating and assigning values to its defined fields: `overall`, `parameters`, `returns`, and `usage_context`. This constructor ensures that instances conform to the specified data types and structure."
          parameters[4]{name,type,description}:
            overall,str,"A concise, high-level summary describing the function's purpose and its implementation details."
            parameters,"List[ParameterDescription]",A list containing detailed descriptions for each parameter accepted by the function.
            returns,"List[ReturnDescription]",A list containing detailed descriptions of the values or types that the function returns.
            usage_context,UsageContext,"An object providing context on how the function is used, including what it calls and where it is called from."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies.
          instantiated_by: The specific locations where this class is instantiated are not provided in the current context.
      error: null
    schemas.types.FunctionAnalysis:
      identifier: schemas.types.FunctionAnalysis
      description:
        overall: "The FunctionAnalysis class is a Pydantic BaseModel designed to represent a comprehensive analysis of a single function. It encapsulates the function's unique identifier, a detailed description of its purpose and signature, and an optional field for any errors encountered during its analysis. This model serves as a structured data container for machine-readable function analysis."
        init_method:
          description: "This class inherits from Pydantic's BaseModel, meaning its constructor is implicitly generated by Pydantic. It initializes instances by validating and assigning values to its defined fields: `identifier`, `description`, and an optional `error` field."
          parameters[3]{name,type,description}:
            identifier,str,The unique name or identifier of the function being analyzed.
            description,FunctionDescription,"An object containing a detailed breakdown of the function's purpose, parameters, returns, and usage context."
            error,"Optional[str]",An optional string field to capture any error messages related to the function's analysis. Defaults to None.
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies.
          instantiated_by: The specific points of instantiation for this class are not provided in the context.
      error: null
    schemas.types.ConstructorDescription:
      identifier: schemas.types.ConstructorDescription
      description:
        overall: "This class is a Pydantic BaseModel designed to provide a structured description of a class's `__init__` method. It encapsulates a textual summary of the constructor's purpose and a list of detailed descriptions for each of its parameters. Its primary role is to serve as a schema for representing constructor metadata, facilitating consistent data handling and validation within a larger system."
        init_method:
          description: "The `__init__` method for `ConstructorDescription` is implicitly generated by Pydantic. It initializes an instance of this model by assigning values to its `description` (a string) and `parameters` (a list of `ParameterDescription` objects) based on the provided arguments, ensuring type validation."
          parameters[2]{name,type,description}:
            description,str,A string providing a high-level summary of the constructor's purpose.
            parameters,"List[ParameterDescription]","A list of `ParameterDescription` objects, each detailing a parameter accepted by the constructor."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly depend on other components within the provided context.
          instantiated_by: This class is not explicitly shown to be instantiated by other components within the provided context.
      error: null
    schemas.types.ClassContext:
      identifier: schemas.types.ClassContext
      description:
        overall: "The `ClassContext` class is a Pydantic model designed to encapsulate information regarding a class's external relationships. It specifically defines fields to describe the class's dependencies and the locations or entities responsible for its instantiation, providing a structured way to represent this contextual metadata."
        init_method:
          description: "This class does not define an explicit `__init__` method. As a Pydantic `BaseModel`, its initialization is handled automatically by Pydantic, which validates and assigns values to its declared fields (`dependencies` and `instantiated_by`) upon object creation."
          parameters[0]:
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in the provided context.
          instantiated_by: The provided context does not specify where this class is instantiated.
      error: null
    schemas.types.ClassDescription:
      identifier: schemas.types.ClassDescription
      description:
        overall: "The ClassDescription class is a Pydantic BaseModel designed to encapsulate a comprehensive analysis of another Python class. It serves as a structured data container, holding a high-level summary of the analyzed class, details about its constructor, a list of analyses for each of its methods, and information regarding its external dependencies and instantiation points. This schema is crucial for organizing and standardizing the output of a class analysis process."
        init_method:
          description: "This class is a Pydantic BaseModel and does not define an explicit `__init__` method. Initialization is handled automatically by Pydantic, which validates and assigns values to its fields (`overall`, `init_method`, `methods`, `usage_context`) upon instantiation based on the provided arguments."
          parameters[0]:
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies in the provided context.
          instantiated_by: The provided context does not specify where this class is instantiated.
      error: null
    schemas.types.ClassAnalysis:
      identifier: schemas.types.ClassAnalysis
      description:
        overall: "The ClassAnalysis class serves as the main Pydantic model for structuring the comprehensive analysis of a Python class. It encapsulates the class's identifier, a detailed description object, and an optional error field to indicate any issues during analysis. This model is fundamental for generating structured data about a class's structure and behavior."
        init_method:
          description: "As a Pydantic BaseModel, the `__init__` method for ClassAnalysis is implicitly generated by Pydantic. It handles the validation and assignment of the `identifier`, `description`, and `error` fields based on the provided arguments during instantiation."
          parameters[0]:
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies.
          instantiated_by: The instantiation points for this class are not specified in the provided context.
      error: null
    schemas.types.CallInfo:
      identifier: schemas.types.CallInfo
      description:
        overall: "The CallInfo class is a Pydantic BaseModel designed to represent a specific call event within a relationship analysis system. It acts as a structured data container, encapsulating key details about a call, such as the originating file, the name of the calling function, the mode of the call (e.g., method, function), and the line number where the call occurs. This class is intended for use in lists like 'called_by' and 'instantiated_by' to provide detailed context about code interactions."
        init_method:
          description: "The CallInfo class, being a Pydantic BaseModel, automatically generates its `__init__` method. This constructor initializes the instance attributes based on the type-hinted fields defined in the class, ensuring data validation and type enforcement upon instantiation."
          parameters[4]{name,type,description}:
            file,str,The file path where the call event originates.
            function,str,The name of the function or method that performed the call.
            mode,str,"The classification of the call, such as 'method', 'function', or 'module'."
            line,int,The specific line number within the file where the call occurs.
        methods[0]:
        usage_context:
          dependencies: "This class does not explicitly list any external functional dependencies beyond its base class, Pydantic's BaseModel."
          instantiated_by: The specific instantiation points for this class are not provided in the current context.
      error: null
    schemas.types.FunctionContextInput:
      identifier: schemas.types.FunctionContextInput
      description:
        overall: "The `FunctionContextInput` class is a Pydantic BaseModel designed to encapsulate structured context information for analyzing a function. It serves as a data container, holding details about what a function calls and by whom it is called. This model provides a standardized format for representing a function's interaction within a larger system, ensuring data integrity through Pydantic's validation."
        init_method:
          description: "The `__init__` method, implicitly provided by Pydantic's BaseModel, initializes an instance of `FunctionContextInput` by accepting values for its `calls` and `called_by` fields. It ensures that the provided data conforms to the specified types, `List[str]` for calls and `List[CallInfo]` for called_by, validating the input upon instantiation."
          parameters[2]{name,type,description}:
            calls,"List[str]","A list of strings representing the identifiers of other functions, methods, or classes that this function calls."
            called_by,"List[CallInfo]","A list of `CallInfo` objects, detailing the entities that invoke this function."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly depend on other components within the provided context.
          instantiated_by: This class is not explicitly instantiated by other components within the provided context.
      error: null
    schemas.types.FunctionAnalysisInput:
      identifier: schemas.types.FunctionAnalysisInput
      description:
        overall: "The FunctionAnalysisInput class is a Pydantic BaseModel designed to define the structured input required for performing a function analysis. It acts as a data contract, ensuring that all necessary information, such as the analysis mode, the function's identifier, its source code, imports, and additional context, is consistently provided. This model facilitates robust data validation and clear communication between system components involved in code analysis."
        init_method:
          description: "As a Pydantic BaseModel, the FunctionAnalysisInput class implicitly handles its initialization. The constructor is generated by Pydantic and accepts keyword arguments corresponding to its defined fields: `mode`, `identifier`, `source_code`, `imports`, and `context`. These arguments are then validated against their respective types and assigned as instance attributes, ensuring the object's state conforms to the schema."
          parameters[0]:
        methods[0]:
        usage_context:
          dependencies: "The class `FunctionAnalysisInput` depends on Pydantic's `BaseModel` for its core functionality and data validation, and on standard Python typing modules like `Literal` and `List` for type hints."
          instantiated_by: "The class `FunctionAnalysisInput` is not explicitly shown to be instantiated by any other components in the provided context. It is likely instantiated in parts of the system responsible for preparing or receiving input data for function analysis, such as API endpoints or data processing pipelines."
      error: null
    schemas.types.MethodContextInput:
      identifier: schemas.types.MethodContextInput
      description:
        overall: "The MethodContextInput class is a Pydantic BaseModel designed to encapsulate structured contextual information about a specific method. It serves as a data transfer object (DTO) to hold details such as the method's unique identifier, a list of entities it calls, a list of entities that call it, its arguments, and its docstring. This model is crucial for organizing and passing around comprehensive data related to a method's operational context within a larger system."
        init_method:
          description: "This class, being a Pydantic BaseModel, automatically generates an __init__ method. It initializes an instance of MethodContextInput by accepting values for its defined fields: identifier, calls, called_by, args, and docstring. These fields store structured information about a method's context."
          parameters[5]{name,type,description}:
            identifier,str,A unique string identifying the method.
            calls,"List[str]","A list of identifiers for other methods, classes, or functions that this method calls."
            called_by,"List[CallInfo]",A list of CallInfo objects indicating where this method is called from.
            args,"List[str]",A list of string representations of the method's arguments.
            docstring,"Optional[str]","The docstring of the method, if available."
        methods[0]:
        usage_context:
          dependencies: The class does not explicitly list any external functional dependencies in the provided context.
          instantiated_by: The provided context does not specify any explicit locations where this class is instantiated.
      error: null
    schemas.types.ClassContextInput:
      identifier: schemas.types.ClassContextInput
      description:
        overall: "The ClassContextInput class is a Pydantic BaseModel designed to provide a structured representation of the context surrounding a Python class for analysis. It serves as a data container, holding information about the class's external dependencies, where it is instantiated, and detailed context for each of its methods. This model facilitates a comprehensive understanding of a class's operational environment and internal structure."
        init_method:
          description: "This class does not explicitly define an __init__ method. It inherits from pydantic.BaseModel, and its constructor is implicitly generated by Pydantic, which initializes instance attributes based on the provided type hints."
          parameters[3]{name,type,description}:
            dependencies,"List[str]",A list of external dependencies that the class relies upon.
            instantiated_by,"List[CallInfo]",A list of CallInfo objects indicating the locations or components that instantiate this class.
            method_context,"List[MethodContextInput]","A list of MethodContextInput objects, each providing specific context for a method within the class."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly declare external functional dependencies within its definition.
          instantiated_by: This class is not explicitly shown to be instantiated by other components in the provided context.
      error: null
    schemas.types.ClassAnalysisInput:
      identifier: schemas.types.ClassAnalysisInput
      description:
        overall: "The `ClassAnalysisInput` class is a Pydantic BaseModel designed to define the structured input required for generating a `ClassAnalysis` object. It serves as a data transfer object (DTO) or schema validator, ensuring that all necessary components for a class analysis, such as the mode of operation, class identifier, source code, imports, and contextual information, are provided in a consistent format. This class is fundamental for standardizing the input to an AI system performing code analysis."
        init_method:
          description: "The `ClassAnalysisInput` class inherits from `pydantic.BaseModel` and does not define an explicit `__init__` method. Pydantic automatically generates a constructor based on the defined type-hinted fields. This allows instances of `ClassAnalysisInput` to be created by passing keyword arguments corresponding to its fields, which Pydantic then validates and assigns as instance attributes."
          parameters[5]{name,type,description}:
            mode,"Literal[\"class_analysis\"]","Specifies the operation mode, which must be 'class_analysis' to indicate the type of analysis being requested."
            identifier,str,The unique name or identifier of the class that is to be analyzed.
            source_code,str,"The raw source code of the entire class definition, including its methods and docstrings."
            imports,"List[str]","A list of import statements found in the source file, which may be relevant to the class or its methods."
            context,ClassContextInput,"Additional contextual information relevant to the class analysis, such as dependencies and instantiation points."
        methods[0]:
        usage_context:
          dependencies: This class does not explicitly list any external functional dependencies within the provided context.
          instantiated_by: The specific instantiation points for this class are not provided in the current context.
      error: null