{
  "files": {
    "backend/AST_Schema.py": {
      "ast_nodes": {
        "imports": [
          "ast",
          "networkx",
          "os",
          "callgraph.build_callGraph"
        ],
        "functions": [
          {
            "mode": "function_analysis",
            "identifier": "backend.AST_Schema.path_to_module",
            "name": "path_to_module",
            "args": [
              "filepath",
              "project_root"
            ],
            "docstring": "Wandelt einen Dateipfad in einen Python-Modulpfad um.",
            "source_code": "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    try:\n        rel_path = os.path.relpath(filepath, project_root)\n    except ValueError:\n        rel_path = os.path.basename(filepath)\n\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    \n    module_path = rel_path.replace(os.path.sep, '.')\n    \n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n        \n    return module_path",
            "start_line": 7,
            "end_line": 22,
            "context": {
              "calls": [
                "backend/AST_Schema.py::basename",
                "backend/AST_Schema.py::endswith",
                "backend/AST_Schema.py::relpath",
                "backend/AST_Schema.py::replace"
              ],
              "called_by": [
                {
                  "file": "AST_Schema.py",
                  "function": "__init__",
                  "mode": "method",
                  "line": 29
                }
              ]
            }
          }
        ],
        "classes": [
          {
            "mode": "class_analysis",
            "identifier": "backend.AST_Schema.ASTVisitor",
            "name": "ASTVisitor",
            "docstring": null,
            "source_code": "class ASTVisitor(ast.NodeVisitor):\n    def __init__(self, source_code: str, file_path: str, project_root: str):\n        self.source_code = source_code\n        self.file_path = file_path\n        self.project_root = project_root\n        self.module_path = path_to_module(self.file_path, self.project_root)\n        self.schema = {\"imports\": [], \"functions\": [], \"classes\": []}\n        self._current_class = None\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        for alias in node.names:\n            self.schema[\"imports\"].append(f\"{node.module}.{alias.name}\")\n        self.generic_visit(node)\n\n    def visit_ClassDef(self, node):\n        class_identifier = f\"{self.module_path}.{node.name}\"\n\n        class_info = {\n            \"mode\": \"class_analysis\",\n            \"identifier\": class_identifier,\n            \"name\": node.name,\n            \"docstring\": ast.get_docstring(node),\n            \"source_code\": ast.get_source_segment(self.source_code, node),\n            \"start_line\": node.lineno,\n            \"end_line\": node.end_lineno,\n            \"context\": {\n                \"dependencies\": [],\n                \"instantiated_by\": [],\n                \"method_context\": []\n            },   \n        }\n        self.schema[\"classes\"].append(class_info)\n        \n        self._current_class = class_info \n        self.generic_visit(node)\n        self._current_class = None\n\n    def visit_FunctionDef(self, node):\n        if self._current_class:\n            method_identifier = f\"{self._current_class['identifier']}.{node.name}\"\n            method_context_info = {\n                \"identifier\": method_identifier,\n                \"name\": node.name,\n                \"calls\": [],\n                \"called_by\": [],\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n            }\n            self._current_class[\"context\"][\"method_context\"].append(method_context_info)\n        else:\n            func_identifier = f\"{self.module_path}.{node.name}\"\n            func_info = {\n                \"mode\": \"function_analysis\",\n                \"identifier\": func_identifier,\n                \"name\": node.name,\n                \"args\": [arg.arg for arg in node.args.args],\n                \"docstring\": ast.get_docstring(node),\n                \"source_code\": ast.get_source_segment(self.source_code, node),\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n                \"context\": {\n                    \"calls\": [],\n                    \"called_by\": []\n                }\n            }\n            self.schema[\"functions\"].append(func_info)\n            \n        self.generic_visit(node)\n    \n    def visit_AsyncFunctionDef(self, node):\n        self.visit_FunctionDef(node)",
            "start_line": 24,
            "end_line": 101,
            "context": {
              "dependencies": [],
              "instantiated_by": [
                {
                  "file": "AST_Schema.py",
                  "function": "analyze_repository",
                  "mode": "method",
                  "line": 189
                }
              ],
              "method_context": [
                {
                  "identifier": "backend.AST_Schema.ASTVisitor.__init__",
                  "name": "__init__",
                  "calls": [
                    "backend/AST_Schema.py::ASTVisitor::path_to_module"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "source_code",
                    "file_path",
                    "project_root"
                  ],
                  "docstring": null,
                  "start_line": 25,
                  "end_line": 31
                },
                {
                  "identifier": "backend.AST_Schema.ASTVisitor.visit_Import",
                  "name": "visit_Import",
                  "calls": [
                    "backend/AST_Schema.py::ASTVisitor::append",
                    "backend/AST_Schema.py::ASTVisitor::generic_visit"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "node"
                  ],
                  "docstring": null,
                  "start_line": 33,
                  "end_line": 36
                },
                {
                  "identifier": "backend.AST_Schema.ASTVisitor.visit_ImportFrom",
                  "name": "visit_ImportFrom",
                  "calls": [
                    "backend/AST_Schema.py::ASTVisitor::append",
                    "backend/AST_Schema.py::ASTVisitor::generic_visit"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "node"
                  ],
                  "docstring": null,
                  "start_line": 38,
                  "end_line": 41
                },
                {
                  "identifier": "backend.AST_Schema.ASTVisitor.visit_ClassDef",
                  "name": "visit_ClassDef",
                  "calls": [
                    "backend/AST_Schema.py::ASTVisitor::append",
                    "backend/AST_Schema.py::ASTVisitor::generic_visit",
                    "backend/AST_Schema.py::ASTVisitor::get_docstring",
                    "backend/AST_Schema.py::ASTVisitor::get_source_segment"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "node"
                  ],
                  "docstring": null,
                  "start_line": 43,
                  "end_line": 64
                },
                {
                  "identifier": "backend.AST_Schema.ASTVisitor.visit_FunctionDef",
                  "name": "visit_FunctionDef",
                  "calls": [
                    "backend/AST_Schema.py::ASTVisitor::append",
                    "backend/AST_Schema.py::ASTVisitor::generic_visit",
                    "backend/AST_Schema.py::ASTVisitor::get_docstring",
                    "backend/AST_Schema.py::ASTVisitor::get_source_segment"
                  ],
                  "called_by": [
                    "backend/AST_Schema.py::ASTVisitor::visit_AsyncFunctionDef"
                  ],
                  "args": [
                    "self",
                    "node"
                  ],
                  "docstring": null,
                  "start_line": 66,
                  "end_line": 98
                },
                {
                  "identifier": "backend.AST_Schema.ASTVisitor.visit_AsyncFunctionDef",
                  "name": "visit_AsyncFunctionDef",
                  "calls": [
                    "backend/AST_Schema.py::ASTVisitor::visit_FunctionDef"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "node"
                  ],
                  "docstring": null,
                  "start_line": 100,
                  "end_line": 101
                }
              ]
            }
          },
          {
            "mode": "class_analysis",
            "identifier": "backend.AST_Schema.ASTAnalyzer",
            "name": "ASTAnalyzer",
            "docstring": null,
            "source_code": "class ASTAnalyzer:\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def _enrich_schema_with_callgraph(schema: dict, call_graph: nx.DiGraph, filename: str):\n        for func in schema[\"functions\"]:\n            func_name_key = f\"{filename}::{func['name']}\"\n            if func_name_key in call_graph:\n                func['context']['calls'] = sorted(list(call_graph.successors(func_name_key)))\n                func['context']['called_by'] = sorted(list(call_graph.predecessors(func_name_key)))\n\n        for cls in schema[\"classes\"]:\n            for method_context in cls[\"context\"][\"method_context\"]:\n                func_name_key = f\"{filename}::{cls['name']}::{method_context['name']}\"\n                if func_name_key in call_graph:\n                    calls = sorted(list(call_graph.successors(func_name_key)))\n                    called_by = sorted(list(call_graph.predecessors(func_name_key)))\n                    \n                    method_context['calls'] = calls\n                    method_context['called_by'] = called_by\n\n    def analyze_repository(self, files: list) -> dict:\n        full_schema = {\n            \"files\": {}\n        }\n\n        all_paths = [file_obj.path for file_obj in files]\n        if not all_paths:\n            return full_schema\n        \n        project_root = os.path.commonpath(all_paths)\n        if os.path.isfile(project_root):\n            project_root = os.path.dirname(project_root)\n\n        for file_obj in files:\n            if not file_obj.path.endswith('.py'):\n                continue\n            \n            file_content = file_obj.content\n            if not file_content.strip():\n                continue\n\n            try:\n                tree = ast.parse(file_content)\n                \n                visitor = ASTVisitor(\n                    source_code=file_content, \n                    file_path=file_obj.path, \n                    project_root=project_root\n                )\n                visitor.visit(tree)\n                file_schema_nodes = visitor.schema\n\n                call_graph = build_callGraph(tree, filename=file_obj.path)\n\n                self._enrich_schema_with_callgraph(\n                    file_schema_nodes, \n                    call_graph, \n                    file_obj.path\n                )\n\n                if file_schema_nodes[\"imports\"] or file_schema_nodes[\"functions\"] or file_schema_nodes[\"classes\"]:\n                    full_schema[\"files\"][file_obj.path] = {\n                        \"ast_nodes\": file_schema_nodes\n                    }\n\n            except (SyntaxError, ValueError) as e:\n                print(f\"Warnung: Konnte Datei '{file_obj.path}' nicht parsen. Fehler: {e}\")\n\n        return full_schema",
            "start_line": 103,
            "end_line": 174,
            "context": {
              "dependencies": [],
              "instantiated_by": [
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 133
                }
              ],
              "method_context": [
                {
                  "identifier": "backend.AST_Schema.ASTAnalyzer.__init__",
                  "name": "__init__",
                  "calls": [],
                  "called_by": [],
                  "args": [
                    "self"
                  ],
                  "docstring": null,
                  "start_line": 105,
                  "end_line": 106
                },
                {
                  "identifier": "backend.AST_Schema.ASTAnalyzer._enrich_schema_with_callgraph",
                  "name": "_enrich_schema_with_callgraph",
                  "calls": [
                    "backend/AST_Schema.py::ASTAnalyzer::list",
                    "backend/AST_Schema.py::ASTAnalyzer::predecessors",
                    "backend/AST_Schema.py::ASTAnalyzer::sorted",
                    "backend/AST_Schema.py::ASTAnalyzer::successors"
                  ],
                  "called_by": [
                    "backend/AST_Schema.py::ASTAnalyzer::analyze_repository"
                  ],
                  "args": [
                    "schema",
                    "call_graph",
                    "filename"
                  ],
                  "docstring": null,
                  "start_line": 109,
                  "end_line": 124
                },
                {
                  "identifier": "backend.AST_Schema.ASTAnalyzer.analyze_repository",
                  "name": "analyze_repository",
                  "calls": [
                    "backend/AST_Schema.py::ASTAnalyzer::ASTVisitor",
                    "backend/AST_Schema.py::ASTAnalyzer::_enrich_schema_with_callgraph",
                    "backend/AST_Schema.py::ASTAnalyzer::build_callGraph",
                    "backend/AST_Schema.py::ASTAnalyzer::commonpath",
                    "backend/AST_Schema.py::ASTAnalyzer::dirname",
                    "backend/AST_Schema.py::ASTAnalyzer::endswith",
                    "backend/AST_Schema.py::ASTAnalyzer::isfile",
                    "backend/AST_Schema.py::ASTAnalyzer::parse",
                    "backend/AST_Schema.py::ASTAnalyzer::print",
                    "backend/AST_Schema.py::ASTAnalyzer::strip",
                    "backend/AST_Schema.py::ASTAnalyzer::visit"
                  ],
                  "called_by": [
                    {
                      "file": "main.py",
                      "function": "main_workflow",
                      "mode": "function",
                      "line": 134
                    }
                  ],
                  "args": [
                    "self",
                    "files"
                  ],
                  "docstring": null,
                  "start_line": 126,
                  "end_line": 174
                }
              ]
            }
          }
        ]
      }
    },
    "backend/File_Dependency.py": {
      "ast_nodes": {
        "imports": [
          "networkx",
          "os",
          "ast.Assign",
          "ast.AST",
          "ast.ClassDef",
          "ast.FunctionDef",
          "ast.Import",
          "ast.ImportFrom",
          "ast.Name",
          "ast.NodeVisitor",
          "ast.literal_eval",
          "ast.parse",
          "ast.walk",
          "keyword.iskeyword",
          "pathlib.Path",
          "getRepo.GitRepository",
          "backend.callgraph.make_safe_dot",
          "collections.defaultdict"
        ],
        "functions": [
          {
            "mode": "function_analysis",
            "identifier": "backend.File_Dependency.build_file_dependency_graph",
            "name": "build_file_dependency_graph",
            "args": [
              "filename",
              "tree",
              "repo_root"
            ],
            "docstring": null,
            "source_code": "def build_file_dependency_graph(filename: str, tree: AST, repo_root: str) -> nx.DiGraph:\n    graph = nx.DiGraph()\n\n    tree_visitor = FileDependencyGraph(filename, repo_root)\n    tree_visitor.visit(tree)\n\n    for caller, callees in tree_visitor.import_dependencies.items():\n        graph.add_node(caller)\n        graph.add_nodes_from(callees)\n        for callee in callees:\n            graph.add_edge(caller, callee)\n    \n    return graph",
            "start_line": 156,
            "end_line": 168,
            "context": {
              "calls": [
                "backend/File_Dependency.py::DiGraph",
                "backend/File_Dependency.py::FileDependencyGraph",
                "backend/File_Dependency.py::add_edge",
                "backend/File_Dependency.py::add_node",
                "backend/File_Dependency.py::add_nodes_from",
                "backend/File_Dependency.py::items",
                "backend/File_Dependency.py::visit"
              ],
              "called_by": [
                {
                  "file": "File_Dependency.py",
                  "function": "build_repository_graph",
                  "mode": "function",
                  "line": 180
                }
              ]
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "backend.File_Dependency.build_repository_graph",
            "name": "build_repository_graph",
            "args": [
              "repository"
            ],
            "docstring": null,
            "source_code": "def build_repository_graph(repository: GitRepository) -> nx.DiGraph:\n    all_files = repository.get_all_files()\n    repo_root = repository.temp_dir\n    global_graph = nx.DiGraph()\n\n    for file in all_files: \n        if not file.path.endswith(\".py\"):\n            continue\n        filename = str(os.path.basename(file.path)).removesuffix(\".py\")\n        tree = parse(file.content)\n        graph = build_file_dependency_graph(filename, tree, repo_root)\n        \n        for node in graph.nodes:\n            global_graph.add_node(node)\n\n        for caller, callee in graph.edges:\n            if callee:\n                global_graph.add_node(callee)\n                global_graph.add_edge(caller, callee)\n    \n    return global_graph",
            "start_line": 170,
            "end_line": 190,
            "context": {
              "calls": [
                "backend/File_Dependency.py::DiGraph",
                "backend/File_Dependency.py::add_edge",
                "backend/File_Dependency.py::add_node",
                "backend/File_Dependency.py::basename",
                "backend/File_Dependency.py::build_file_dependency_graph",
                "backend/File_Dependency.py::endswith",
                "backend/File_Dependency.py::get_all_files",
                "backend/File_Dependency.py::parse",
                "backend/File_Dependency.py::removesuffix",
                "backend/File_Dependency.py::str"
              ],
              "called_by": [
                {
                  "file": "File_Dependency.py",
                  "function": "backend.File_Dependency",
                  "mode": "module",
                  "line": 236
                }
              ]
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "backend.File_Dependency.get_all_temp_files",
            "name": "get_all_temp_files",
            "args": [
              "directory"
            ],
            "docstring": null,
            "source_code": "def get_all_temp_files(directory: str) -> list[Path]:\n    root_path = Path(directory).resolve()\n    all_files = [file.relative_to(root_path) for file in root_path.rglob(\"*.py\")]\n\n    return all_files",
            "start_line": 192,
            "end_line": 196,
            "context": {
              "calls": [
                "backend/File_Dependency.py::Path",
                "backend/File_Dependency.py::relative_to",
                "backend/File_Dependency.py::resolve",
                "backend/File_Dependency.py::rglob"
              ],
              "called_by": [
                {
                  "file": "File_Dependency.py",
                  "function": "_resolve_module_name",
                  "mode": "method",
                  "line": 44
                }
              ]
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "backend.File_Dependency.nx_to_mermaid_with_folders",
            "name": "nx_to_mermaid_with_folders",
            "args": [
              "G"
            ],
            "docstring": null,
            "source_code": "def nx_to_mermaid_with_folders(G: nx.DiGraph):\n    # Ordner → Liste der Dateien\n    folder_map = defaultdict(list)\n    for node in G.nodes:\n        parts = node.split(\"/\")\n        folder = \"/\".join(parts[:-1])  # alles außer Datei = Ordner\n        folder_map[folder].append(parts[-1])  # nur der Dateiname als Knoten\n\n    lines = [\"graph TD\"]\n\n    # Subgraphs erstellen\n    for folder, files in folder_map.items():\n        if folder:  # nur wenn es einen Ordner gibt\n            lines.append(f\"    subgraph {folder.replace('/', '_')}\")\n            for f in files:\n                node_id = f\"{folder}/{f}\".replace('/', '_')\n                lines.append(f'        {node_id}[\"{f}\"]')\n            lines.append(\"    end\")\n        else:\n            # Dateien im Root\n            for f in files:\n                node_id = f.replace('/', '_')\n                lines.append(f'    {node_id}[\"{f}\"]')\n\n    # Kanten\n    for caller, callee in G.edges:\n        caller_id = caller.replace('/', '_')\n        callee_id = callee.replace('/', '_')\n        lines.append(f'    {caller_id} --> {callee_id}')\n\n    return \"\\n\".join(lines)",
            "start_line": 200,
            "end_line": 230,
            "context": {
              "calls": [
                "backend/File_Dependency.py::append",
                "backend/File_Dependency.py::defaultdict",
                "backend/File_Dependency.py::items",
                "backend/File_Dependency.py::join",
                "backend/File_Dependency.py::replace",
                "backend/File_Dependency.py::split"
              ],
              "called_by": [
                {
                  "file": "File_Dependency.py",
                  "function": "backend.File_Dependency",
                  "mode": "module",
                  "line": 238
                }
              ]
            }
          }
        ],
        "classes": [
          {
            "mode": "class_analysis",
            "identifier": "backend.File_Dependency.FileDependencyGraph",
            "name": "FileDependencyGraph",
            "docstring": null,
            "source_code": "class FileDependencyGraph(NodeVisitor):\n\n    import_dependencies: dict[str, set[str]] = {}\n    def __init__(self, filename: str, repo_root):\n        \"\"\"\n        Initialisiert den File Dependency Graphen\n\n        Args:\n\n        \"\"\"\n        self.filename = filename\n        self.repo_root = repo_root\n\n    def _resolve_module_name(self, node: ImportFrom) -> list[str]:\n        \"\"\"\n        Löst relative Imports der Form `from .. import name1, name2` auf.\n        Liefert die Liste der tatsächlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\n        Wirft ImportError, wenn nichts aufgelöst werden konnte.\n        \"\"\"\n        level_depth = node.level\n        names = [alias.name for alias in node.names]  \n        all_files = get_all_temp_files(self.repo_root)  \n\n        # Suche die aktuelle Datei (self.filename -> modulname ohne .py oder auch Pfad)\n        candidates = [p for p in all_files if p.stem == Path(self.filename).stem or p.name == f\"{self.filename}.py\"]\n\n        if not candidates:\n            raise ImportError(f\"Kann aktuelle Datei '{self.filename}' im Repo nicht finden.\")\n\n        # Wähle die plausibelste Datei (flachster Pfad)\n        candidates.sort(key=lambda p: len(p.parts))\n        current_rel_path = candidates[0]  # z.B. package/subpkg/module.py\n\n        base_dir = current_rel_path.parent\n        if level_depth <= 0:\n            raise ImportError(\"Erwarteter relativer Import (level >= 1).\")\n        for _ in range(level_depth - 1):\n            if base_dir == base_dir.parent and len(base_dir.parts) == 0:\n                raise ImportError(f\"Relative Import-Ebene ({level_depth}) zu groß für Datei '{current_rel_path}'.\")\n            base_dir = base_dir.parent\n\n        repo_root_path = Path(self.repo_root).resolve()\n        resolved: list[str] = []\n\n        def module_file_exists(rel_base: Path, name: str) -> bool:\n            file_path = repo_root_path / rel_base / f\"{name}.py\"\n            pkg_init = repo_root_path / rel_base / name / \"__init__.py\"\n            return file_path.exists() or pkg_init.exists()\n\n        def init_exports_symbol(rel_base: Path, symbol: str) -> bool:\n            \"\"\"\n            Prüft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\n            oder als Name (funktion/klasse/assign) definiert ist.\n            \"\"\"\n            init_path = repo_root_path / rel_base / \"__init__.py\"\n            if not init_path.exists():\n                return False\n            try:\n                src = init_path.read_text(encoding=\"utf-8\")\n                mod = parse(src, filename=str(init_path))\n            except Exception:\n                return False\n\n            for node_ in walk(mod):\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == \"__all__\":\n                            try:\n                                value = literal_eval(node_.value)\n                                if isinstance(value, (list, tuple)) and symbol in value:\n                                    return True\n                            except Exception:\n                                pass\n                if isinstance(node_, (FunctionDef, ClassDef)) and node_.name == symbol:\n                    return True\n                if isinstance(node_, Assign):\n                    for target in node_.targets:\n                        if isinstance(target, Name) and target.id == symbol:\n                            return True\n            return False\n\n        for name in names:\n            if not name.isidentifier() or iskeyword(name):\n                continue\n\n            if module_file_exists(base_dir, name):\n                resolved.append(name)\n                continue\n            if init_exports_symbol(base_dir, name):\n                resolved.append(name)\n                continue\n\n        resolved = sorted(set(resolved))\n\n        if not resolved:\n            raise ImportError(\n                f\"Kein passendes Modul/Symbol für relative Import-Auflösung gefunden \"\n                f\"(level={level_depth}, names={names}, base_dir={base_dir})\"\n            )\n\n        return resolved\n    \n    def visit_Import(self, node: Import | ImportFrom, base_name: str | None = None):\n        for alias in node.names:\n        \n            if self.filename not in self.import_dependencies:\n                self.import_dependencies[self.filename] = set()\n\n            if base_name:\n                self.import_dependencies[self.filename].add(base_name)\n            else:\n                self.import_dependencies[self.filename].add(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node: ImportFrom):\n        \"\"\"\n        Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\n        also c, und dieser wird als callee für den caller, das File, gesetzt.\n        \"\"\"\n        module_name = node.module\n        if module_name:\n            module_base = module_name.split(\".\")[-1]\n            self.visit_Import(node, module_base)\n        else:\n            try:\n                resolved = self._resolve_module_name(node)\n                for base in resolved:\n                    self.visit_Import(node, base)\n            except ImportError as e:\n                print(f\"Auflösung eines relativen Imports fehlgeschlagen: {e}\")\n\n        self.generic_visit(node)",
            "start_line": 23,
            "end_line": 154,
            "context": {
              "dependencies": [],
              "instantiated_by": [
                {
                  "file": "File_Dependency.py",
                  "function": "build_file_dependency_graph",
                  "mode": "function",
                  "line": 159
                }
              ],
              "method_context": [
                {
                  "identifier": "backend.File_Dependency.FileDependencyGraph.__init__",
                  "name": "__init__",
                  "calls": [],
                  "called_by": [],
                  "args": [
                    "self",
                    "filename",
                    "repo_root"
                  ],
                  "docstring": "Initialisiert den File Dependency Graphen\n\nArgs:",
                  "start_line": 26,
                  "end_line": 34
                },
                {
                  "identifier": "backend.File_Dependency.FileDependencyGraph._resolve_module_name",
                  "name": "_resolve_module_name",
                  "calls": [
                    "backend/File_Dependency.py::FileDependencyGraph::ImportError",
                    "backend/File_Dependency.py::FileDependencyGraph::Path",
                    "backend/File_Dependency.py::FileDependencyGraph::get_all_temp_files",
                    "backend/File_Dependency.py::FileDependencyGraph::len",
                    "backend/File_Dependency.py::FileDependencyGraph::range",
                    "backend/File_Dependency.py::FileDependencyGraph::resolve",
                    "backend/File_Dependency.py::FileDependencyGraph::sort"
                  ],
                  "called_by": [
                    "backend/File_Dependency.py::FileDependencyGraph::visit_ImportFrom"
                  ],
                  "args": [
                    "self",
                    "node"
                  ],
                  "docstring": "Löst relative Imports der Form `from .. import name1, name2` auf.\nLiefert die Liste der tatsächlich existierenden Modul-/Symbolnamen (z.B. [\"foo\",\"bar\"]).\nWirft ImportError, wenn nichts aufgelöst werden konnte.",
                  "start_line": 36,
                  "end_line": 123
                },
                {
                  "identifier": "backend.File_Dependency.FileDependencyGraph.module_file_exists",
                  "name": "module_file_exists",
                  "calls": [
                    "backend/File_Dependency.py::FileDependencyGraph::exists"
                  ],
                  "called_by": [
                    "<backend/File_Dependency.py>"
                  ],
                  "args": [
                    "rel_base",
                    "name"
                  ],
                  "docstring": null,
                  "start_line": 67,
                  "end_line": 70
                },
                {
                  "identifier": "backend.File_Dependency.FileDependencyGraph.init_exports_symbol",
                  "name": "init_exports_symbol",
                  "calls": [
                    "backend/File_Dependency.py::FileDependencyGraph::exists",
                    "backend/File_Dependency.py::FileDependencyGraph::isinstance",
                    "backend/File_Dependency.py::FileDependencyGraph::literal_eval",
                    "backend/File_Dependency.py::FileDependencyGraph::parse",
                    "backend/File_Dependency.py::FileDependencyGraph::read_text",
                    "backend/File_Dependency.py::FileDependencyGraph::str",
                    "backend/File_Dependency.py::FileDependencyGraph::walk"
                  ],
                  "called_by": [
                    "<backend/File_Dependency.py>"
                  ],
                  "args": [
                    "rel_base",
                    "symbol"
                  ],
                  "docstring": "Prüft, ob rel_base/__init__.py existiert und symbol entweder in __all__ ist\noder als Name (funktion/klasse/assign) definiert ist.",
                  "start_line": 72,
                  "end_line": 102
                },
                {
                  "identifier": "backend.File_Dependency.FileDependencyGraph.visit_Import",
                  "name": "visit_Import",
                  "calls": [
                    "backend/File_Dependency.py::FileDependencyGraph::add",
                    "backend/File_Dependency.py::FileDependencyGraph::generic_visit",
                    "backend/File_Dependency.py::FileDependencyGraph::set"
                  ],
                  "called_by": [
                    "backend/File_Dependency.py::FileDependencyGraph::visit_ImportFrom"
                  ],
                  "args": [
                    "self",
                    "node",
                    "base_name"
                  ],
                  "docstring": null,
                  "start_line": 125,
                  "end_line": 135
                },
                {
                  "identifier": "backend.File_Dependency.FileDependencyGraph.visit_ImportFrom",
                  "name": "visit_ImportFrom",
                  "calls": [
                    "backend/File_Dependency.py::FileDependencyGraph::_resolve_module_name",
                    "backend/File_Dependency.py::FileDependencyGraph::generic_visit",
                    "backend/File_Dependency.py::FileDependencyGraph::print",
                    "backend/File_Dependency.py::FileDependencyGraph::split",
                    "backend/File_Dependency.py::FileDependencyGraph::visit_Import"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "node"
                  ],
                  "docstring": "Wenn der Import die Form from a.b.c import d besitzt, wird der letzte Teil des Moduls genommen,\nalso c, und dieser wird als callee für den caller, das File, gesetzt.",
                  "start_line": 137,
                  "end_line": 154
                }
              ]
            }
          }
        ]
      }
    },
    "backend/HelperLLM.py": {
      "ast_nodes": {
        "imports": [
          "os",
          "json",
          "logging",
          "time",
          "typing.List",
          "typing.Dict",
          "typing.Any",
          "typing.Optional",
          "typing.Union",
          "dotenv.load_dotenv",
          "langchain_google_genai.ChatGoogleGenerativeAI",
          "langchain_ollama.ChatOllama",
          "langchain_openai.ChatOpenAI",
          "langchain.messages.HumanMessage",
          "langchain.messages.SystemMessage",
          "langchain.messages.AIMessage",
          "pydantic.ValidationError",
          "schemas.types.FunctionAnalysis",
          "schemas.types.ClassAnalysis",
          "schemas.types.FunctionAnalysisInput",
          "schemas.types.FunctionContextInput",
          "schemas.types.ClassAnalysisInput",
          "schemas.types.ClassContextInput"
        ],
        "functions": [
          {
            "mode": "function_analysis",
            "identifier": "backend.HelperLLM.main_orchestrator",
            "name": "main_orchestrator",
            "args": [],
            "docstring": "Dummy Data and processing loop for testing the LLMHelper class.\nThis version is syntactically correct and logically matches the Pydantic models.",
            "source_code": "def main_orchestrator():\n    \"\"\"\n    Dummy Data and processing loop for testing the LLMHelper class.\n    This version is syntactically correct and logically matches the Pydantic models.\n    \"\"\"\n    \n    # --- Step 1: Define the pre-computed analysis for each method ---\n    \n    # Example Input 1: For the 'add_item' function\n    add_item_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"source_code\": \"\"\"def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\\\"\\\"\\\"\n        if not isinstance(quantity, int) or quantity <= 0:\n            raise ValueError(\"Quantity must be a positive integer.\")\n    \n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n        print(f\"Added {quantity} of {item_name}. New total: {self.inventory[item_name]}\")\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"process_shipment\", \"restock_api_endpoint\"]\n        }\n    })\n\n    # Example Input 2: For the 'check_stock' function\n    check_stock_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"source_code\": \"\"\"def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Retrieves the current stock quantity for a given item.\\\"\\\"\\\"\n        stock_level = self.inventory.get(item_name, 0)\n        return stock_level\"\"\",\n        \"imports\": [],\n        \"context\": {\n            \"calls\": [\"self.inventory.get\"],\n            \"called_by\": [\"fulfill_order\", \"ui_display_handler\"]\n        }\n    })\n\n    # Example Input 3: For the 'generate_report' function\n    generate_report_input = FunctionAnalysisInput.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"source_code\": \"\"\"def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a timestamped string summary of the current inventory state.\\\"\\\"\\\"\n        timestamp = datetime.now()\n        report_header = \"Inventory Report as of: {ts}\\\\n\".format(ts=timestamp)\n        report_lines = [report_header]\n        \n        for item, quantity in self.inventory.items():\n            line = \"- {name}: {qty}\".format(name=item, qty=quantity)\n            report_lines.append(line)\n            \n        return \"\\\\n\".join(report_lines)\"\"\",\n        \"imports\": [\"from datetime import datetime\"],\n        \"context\": {\n            \"calls\": [\"datetime.now\", \"str.format\"],\n            \"called_by\": [\"daily_cron_job\", \"admin_dashboard_export\"]\n        }\n    })\n\n    add_item_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"add_item\",\n        \"description\": {\n            \"overall\": \"Adds a specified quantity of an item to the inventory. If the item already exists, its quantity is increased.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to add.\"},\n                {\"name\": \"quantity\", \"type\": \"int\", \"description\": \"The number of units to add. Must be a positive integer.\"}\n            ],\n            \"returns\": [],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"process_shipment and restock_api_endpoint\" }\n        },\n        \"error\": None\n    })\n\n    check_stock_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"check_stock\",\n        \"description\": {\n            \"overall\": \"Retrieves the current stock quantity for a given item.\",\n            \"parameters\": [\n                {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"},\n                {\"name\": \"item_name\", \"type\": \"str\", \"description\": \"The name or ID of the item to check.\"}\n            ],\n            \"returns\": [\n                {\"name\": \"stock_level\", \"type\": \"int\", \"description\": \"The quantity of the item in stock. Returns 0 if the item is not found.\"}\n            ],\n            \"usage_context\": { \"calls\": \"self.inventory.get\", \"called_by\": \"fulfill_order and ui_display_handler\" }\n        },\n        \"error\": None\n    })\n\n    generate_report_analysis = FunctionAnalysis.model_validate({\n        \"mode\": \"function_analysis\",\n        \"identifier\": \"generate_report\",\n        \"description\": {\n            \"overall\": \"Generates a timestamped string summary of the current inventory state.\",\n            \"parameters\": [ {\"name\": \"self\", \"type\": \"InventoryManager\", \"description\": \"The instance of the class.\"} ],\n            \"returns\": [\n                {\"name\": \"report\", \"type\": \"str\", \"description\": \"A formatted string detailing all items and their quantities.\"}\n            ],\n            \"usage_context\": { \"calls\": \"datetime.now and str.format\", \"called_by\": \"daily_cron_job and admin_dashboard_export\" }\n        },\n        \"error\": None\n    })\n\n    class_input = ClassAnalysisInput(\n        mode=\"class_analysis\",\n        identifier=\"InventoryManager\",\n        # NOTE: The indentation of this string is now fixed. It starts at column 0.\n        source_code=\"\"\"class InventoryManager:\n    \\\"\\\"\\\"Manages stock levels for products in a warehouse.\\\"\\\"\\\"\n    def __init__(self, warehouse_id: str):\n        self.warehouse_id = warehouse_id\n        self.inventory = {}  # item_name: quantity\n\n    def add_item(self, item_name: str, quantity: int):\n        \\\"\\\"\\\"Adds an item to the inventory.\\\"\\\"\\\"\n        current_quantity = self.inventory.get(item_name, 0)\n        self.inventory[item_name] = current_quantity + quantity\n\n    def check_stock(self, item_name: str) -> int:\n        \\\"\\\"\\\"Checks the stock of a specific item.\\\"\\\"\\\"\n        return self.inventory.get(item_name, 0)\n\n    def generate_report(self) -> str:\n        \\\"\\\"\\\"Generates a summary report of the inventory.\\\"\\\"\\\"\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        report_lines = [f\"Inventory Report for {self.warehouse_id} at {timestamp}\"]\n        if not self.inventory:\n            report_lines.append(\"Inventory is empty.\")\n        else:\n            for item, quantity in self.inventory.items():\n                report_lines.append(f\"- {item}: {quantity}\")\n        return \"\\\\n\".join(report_lines)\n\"\"\",\n        imports=[\"from datetime import datetime\"],\n        context=ClassContextInput(\n            dependencies=[\"datetime\"],\n            instantiated_by=[\"main_app_startup\", \"warehouse_worker_script\"],\n            methods_analysis=[\n                add_item_analysis,\n                check_stock_analysis,\n                generate_report_analysis\n            ]\n        )\n    )\n\n    \n    # The helper methods expect a LIST of inputs, so we wrap our single object in a list.\n    input = [add_item_input, check_stock_input, generate_report_input]\n    analysis = [add_item_analysis, check_stock_analysis, generate_report_analysis] \n\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    llm_helper = LLMHelper(api_key=GEMINI_API_KEY, function_prompt_path=function_prompt_file, class_prompt_path=class_prompt_file)\n\n    # This will be the final JSON containing all documentation\n    final_documentation = {}\n\n    # The analysis of the methods is already provided in the context.\n    logging.info(\"\\n--- Generating documentation for classes ---\")\n    \n    # The `generate_for_classes` method returns a list of results.\n    analysis_results = llm_helper.generate_for_functions(input)\n\n    # --- Step 4: Process the results ---\n    \n    # We loop through the list of results (even though there's only one in this case).\n    for doc in analysis_results:\n        if doc:\n            logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n            if \"classes\" not in final_documentation:\n                final_documentation[\"classes\"] = {}\n            # Use .model_dump() to convert the Pydantic object back to a dict for JSON serialization\n            final_documentation[\"classes\"][doc.identifier] = doc.model_dump() \n        else:\n            logging.warning(f\"Failed to generate doc for a class\")\n\n    # --- Step 5: Display the final aggregated result ---\n    logging.info(\"\\n--- Final Generated Documentation ---\")\n    print(json.dumps(final_documentation, indent=2))",
            "start_line": 205,
            "end_line": 391,
            "context": {
              "calls": [
                "backend/HelperLLM.py::ClassAnalysisInput",
                "backend/HelperLLM.py::ClassContextInput",
                "backend/HelperLLM.py::LLMHelper",
                "backend/HelperLLM.py::dumps",
                "backend/HelperLLM.py::generate_for_functions",
                "backend/HelperLLM.py::info",
                "backend/HelperLLM.py::model_dump",
                "backend/HelperLLM.py::model_validate",
                "backend/HelperLLM.py::print",
                "backend/HelperLLM.py::warning"
              ],
              "called_by": [
                {
                  "file": "HelperLLM.py",
                  "function": "backend.HelperLLM",
                  "mode": "module",
                  "line": 397
                }
              ]
            }
          }
        ],
        "classes": [
          {
            "mode": "class_analysis",
            "identifier": "backend.HelperLLM.LLMHelper",
            "name": "LLMHelper",
            "docstring": "A class to interact with Google Gemini for generating code snippet documentation.\nIt centralizes API interaction, error handling, and validates I/O using Pydantic.",
            "source_code": "class LLMHelper:\n    \"\"\"\n    A class to interact with Google Gemini for generating code snippet documentation.\n    It centralizes API interaction, error handling, and validates I/O using Pydantic.\n    \"\"\"\n    def __init__(self, api_key: str, function_prompt_path: str, class_prompt_path: str, model_name: str = \"gemini-2.0-flash-lite\", ollama_base_url: str = None):\n        if not api_key:\n            raise ValueError(\"Gemini API Key must be set.\")\n        \n        try:\n            with open(function_prompt_path, 'r', encoding='utf-8') as f:\n                self.function_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Function system prompt file not found at: {function_prompt_path}\")\n            raise\n\n        # Handle the second file\n        try:\n            with open(class_prompt_path, 'r', encoding='utf-8') as f:\n                self.class_system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"Class system prompt file not found at: {class_prompt_path}\")\n            raise\n        \n        # Batch-Size config\n        self.model_name = model_name\n        self._configure_batch_settings(model_name)\n\n        if model_name.startswith(\"gemini-\"):\n            base_llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=0.3, \n            )\n        \n        elif model_name.startswith(\"gpt-\"):\n            base_llm = ChatOpenAI(\n                model_name=model_name,\n                api_key=api_key,\n                temperature=0.3,\n            )\n\n        else:\n            target_url = ollama_base_url if ollama_base_url else OLLAMA_BASE_URL\n            base_llm = ChatOllama(\n                model=model_name,\n                temperature=0.3,\n                base_url=target_url,\n            )\n\n        self.function_llm = base_llm.with_structured_output(FunctionAnalysis, method=\"json_schema\")\n        self.class_llm = base_llm.with_structured_output(ClassAnalysis, method=\"json_schema\")\n\n        self.raw_llm = base_llm\n\n        logging.info(f\"LLMHelper initialized with model '{model_name}'. Batch Size: {self.batch_size}\")\n\n    def _configure_batch_settings(self, model_name: str):\n\n        if model_name == \"gemini-2.0-flash-lite\":\n            self.batch_size = 30\n        \n        elif model_name == \"gemini-flash-latest\":\n            self.batch_size = 10\n            \n        elif model_name == \"gemini-2.5-pro\":\n            self.batch_size = 2\n        \n        elif model_name == \"gemini-2.0-flash\":\n            self.batch_size = 15\n\n        elif model_name == \"gemini-2.5-flash-lite\":\n            self.batch_size = 15\n\n        elif model_name == \"llama3\":\n            self.batch_size = 50\n\n        elif model_name == \"gpt-5.1\":\n            self.batch_size = 500\n\n        elif model_name == \"gpt-5-mini\":\n            self.batch_size = 500\n            \n        else:\n            logging.warning(f\"Unknown model '{model_name}', using conservative defaults.\")\n            self.batch_size = 2\n\n    def generate_for_functions(self, function_inputs: List[FunctionAnalysisInput]) -> List[Optional[FunctionAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of functions.\"\"\"\n\n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 61\n\n        if not function_inputs:\n            return []\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(function_input.model_dump(), indent=2) \n            for function_input in function_inputs\n        ]\n       \n        conversations = [[SystemMessage(content=self.function_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_functions = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n        \n            try:\n                batch_results = self.function_llm.batch(batch_conversations)\n                all_validated_functions.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                all_validated_functions.extend([None] * len(batch_conversations))\n            \n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_functions\n        \n            \n    \n\n    def generate_for_classes(self, class_inputs: List[ClassAnalysisInput]) -> List[Optional[ClassAnalysis]]:\n        \"\"\"Generates and validates documentation for a batch of classes.\"\"\"\n        if not class_inputs:\n            return []\n        \n        BATCH_SIZE = self.batch_size\n        WAITING_TIME = 61\n\n        # Create a list of JSON payloads from the input models\n        json_payloads = [\n            json.dumps(class_input.model_dump(), indent=2)\n            for class_input in class_inputs\n        ]\n\n        conversations = [[SystemMessage(content=self.class_system_prompt), HumanMessage(content=payload)] for payload in json_payloads]\n\n        all_validated_classes = []\n        total_items = len(conversations)\n\n        for i in range(0, total_items, BATCH_SIZE):\n\n            batch_conversations = conversations[i:i + BATCH_SIZE]\n            logging.info(f\"Calling LLM {self.model_name} API for Batch {i // BATCH_SIZE + 1} (Items {i+1} to {min(i + BATCH_SIZE, total_items)} of {total_items})...\")            \n\n            try:\n                batch_results = self.class_llm.batch(batch_conversations)\n                all_validated_classes.extend(batch_results)\n                logging.info(\"Batch call successful.\")\n\n            except Exception as e:\n                logging.error(f\"An error occurred during batch {i // BATCH_SIZE + 1}: {e}\")\n                # Falls ein Fehler auftritt, füllen wir die Liste mit None auf, um die Reihenfolge zu wahren\n                all_validated_classes.extend([None] * len(batch_conversations))\n\n            if i + BATCH_SIZE < total_items:\n                logging.info(f\"Waiting {WAITING_TIME} seconds to respect rate limits...\")\n                time.sleep(WAITING_TIME)\n\n        return all_validated_classes",
            "start_line": 30,
            "end_line": 199,
            "context": {
              "dependencies": [],
              "instantiated_by": [
                {
                  "file": "HelperLLM.py",
                  "function": "main_orchestrator",
                  "mode": "function",
                  "line": 365
                },
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 223
                }
              ],
              "method_context": [
                {
                  "identifier": "backend.HelperLLM.LLMHelper.__init__",
                  "name": "__init__",
                  "calls": [
                    "backend/HelperLLM.py::LLMHelper::ChatGoogleGenerativeAI",
                    "backend/HelperLLM.py::LLMHelper::ChatOllama",
                    "backend/HelperLLM.py::LLMHelper::ChatOpenAI",
                    "backend/HelperLLM.py::LLMHelper::ValueError",
                    "backend/HelperLLM.py::LLMHelper::_configure_batch_settings",
                    "backend/HelperLLM.py::LLMHelper::error",
                    "backend/HelperLLM.py::LLMHelper::info",
                    "backend/HelperLLM.py::LLMHelper::open",
                    "backend/HelperLLM.py::LLMHelper::read",
                    "backend/HelperLLM.py::LLMHelper::startswith",
                    "backend/HelperLLM.py::LLMHelper::with_structured_output"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "api_key",
                    "function_prompt_path",
                    "class_prompt_path",
                    "model_name",
                    "ollama_base_url"
                  ],
                  "docstring": null,
                  "start_line": 35,
                  "end_line": 85
                },
                {
                  "identifier": "backend.HelperLLM.LLMHelper._configure_batch_settings",
                  "name": "_configure_batch_settings",
                  "calls": [
                    "backend/HelperLLM.py::LLMHelper::warning"
                  ],
                  "called_by": [
                    "backend/HelperLLM.py::LLMHelper::__init__"
                  ],
                  "args": [
                    "self",
                    "model_name"
                  ],
                  "docstring": null,
                  "start_line": 87,
                  "end_line": 115
                },
                {
                  "identifier": "backend.HelperLLM.LLMHelper.generate_for_functions",
                  "name": "generate_for_functions",
                  "calls": [
                    "backend/HelperLLM.py::LLMHelper::HumanMessage",
                    "backend/HelperLLM.py::LLMHelper::SystemMessage",
                    "backend/HelperLLM.py::LLMHelper::batch",
                    "backend/HelperLLM.py::LLMHelper::dumps",
                    "backend/HelperLLM.py::LLMHelper::error",
                    "backend/HelperLLM.py::LLMHelper::extend",
                    "backend/HelperLLM.py::LLMHelper::info",
                    "backend/HelperLLM.py::LLMHelper::len",
                    "backend/HelperLLM.py::LLMHelper::min",
                    "backend/HelperLLM.py::LLMHelper::model_dump",
                    "backend/HelperLLM.py::LLMHelper::range",
                    "backend/HelperLLM.py::LLMHelper::sleep"
                  ],
                  "called_by": [
                    {
                      "file": "main.py",
                      "function": "main_workflow",
                      "mode": "function",
                      "line": 248
                    }
                  ],
                  "args": [
                    "self",
                    "function_inputs"
                  ],
                  "docstring": "Generates and validates documentation for a batch of functions.",
                  "start_line": 117,
                  "end_line": 156
                },
                {
                  "identifier": "backend.HelperLLM.LLMHelper.generate_for_classes",
                  "name": "generate_for_classes",
                  "calls": [
                    "backend/HelperLLM.py::LLMHelper::HumanMessage",
                    "backend/HelperLLM.py::LLMHelper::SystemMessage",
                    "backend/HelperLLM.py::LLMHelper::batch",
                    "backend/HelperLLM.py::LLMHelper::dumps",
                    "backend/HelperLLM.py::LLMHelper::error",
                    "backend/HelperLLM.py::LLMHelper::extend",
                    "backend/HelperLLM.py::LLMHelper::info",
                    "backend/HelperLLM.py::LLMHelper::len",
                    "backend/HelperLLM.py::LLMHelper::min",
                    "backend/HelperLLM.py::LLMHelper::model_dump",
                    "backend/HelperLLM.py::LLMHelper::range",
                    "backend/HelperLLM.py::LLMHelper::sleep"
                  ],
                  "called_by": [
                    {
                      "file": "main.py",
                      "function": "main_workflow",
                      "mode": "function",
                      "line": 279
                    }
                  ],
                  "args": [
                    "self",
                    "class_inputs"
                  ],
                  "docstring": "Generates and validates documentation for a batch of classes.",
                  "start_line": 161,
                  "end_line": 199
                }
              ]
            }
          }
        ]
      }
    },
    "backend/MainLLM.py": {
      "ast_nodes": {
        "imports": [
          "os",
          "logging",
          "sys",
          "dotenv.load_dotenv",
          "langchain_google_genai.ChatGoogleGenerativeAI",
          "langchain_ollama.ChatOllama",
          "langchain_openai.ChatOpenAI",
          "langchain.messages.HumanMessage",
          "langchain.messages.SystemMessage"
        ],
        "functions": [],
        "classes": [
          {
            "mode": "class_analysis",
            "identifier": "backend.MainLLM.MainLLM",
            "name": "MainLLM",
            "docstring": "Hauptklasse für die Interaktion mit dem LLM.",
            "source_code": "class MainLLM:\n    \"\"\"\n    Hauptklasse für die Interaktion mit dem LLM.\n    \"\"\"\n    def __init__(self, api_key: str, prompt_file_path: str, model_name: str = \"gemini-2.5-pro\", ollama_base_url: str = None):\n        if not api_key:\n            raise ValueError(\"Gemini API Key must be set.\")\n        \n        try:\n            with open(prompt_file_path, 'r', encoding='utf-8') as f:\n                self.system_prompt = f.read()\n        except FileNotFoundError:\n            logging.error(f\"System prompt file not found at: {prompt_file_path}\")\n            raise\n        \n        self.model_name = model_name\n\n        if model_name.startswith(\"gemini-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n\n        elif model_name.startswith(\"gpt-\"):\n            self.llm = ChatGoogleGenerativeAI(\n                model=model_name,\n                api_key=api_key,\n                temperature=1.0, \n            )\n\n        else:\n            target_url = ollama_base_url if ollama_base_url else OLLAMA_BASE_URL\n            self.llm = ChatOllama(\n                model=model_name,\n                temperature=1.0,\n                base_url=target_url,\n            )\n\n        logging.info(f\"Main LLM initialized with model '{model_name}'.\")\n    \n    def call_llm(self, user_input: str):\n\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with HelperLLM input...\")\n\n        try:\n            response = self.llm.invoke(messages)\n            logging.info(\"LLM call successful.\")\n            return response.content\n        except Exception as e:\n            logging.error(f\"Error during LLM call: {e}\")\n            return None\n\n    def stream_llm(self, user_input: str):\n        messages = [\n            SystemMessage(content=self.system_prompt),\n            HumanMessage(content=user_input)\n        ]\n        logging.info(\"Calling LLM with 'stream'...\")\n\n        try:\n            stream_iterator = self.llm.stream(messages)\n            \n            for chunk in stream_iterator:\n                yield chunk.content\n        except Exception as e:\n            error_message = f\"\\n--- Error during LLM stream call: {e} ---\"\n            logging.error(error_message)\n            yield error_message",
            "start_line": 20,
            "end_line": 92,
            "context": {
              "dependencies": [],
              "instantiated_by": [
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 308
                }
              ],
              "method_context": [
                {
                  "identifier": "backend.MainLLM.MainLLM.__init__",
                  "name": "__init__",
                  "calls": [
                    "backend/MainLLM.py::MainLLM::ChatGoogleGenerativeAI",
                    "backend/MainLLM.py::MainLLM::ChatOllama",
                    "backend/MainLLM.py::MainLLM::ValueError",
                    "backend/MainLLM.py::MainLLM::error",
                    "backend/MainLLM.py::MainLLM::info",
                    "backend/MainLLM.py::MainLLM::open",
                    "backend/MainLLM.py::MainLLM::read",
                    "backend/MainLLM.py::MainLLM::startswith"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "api_key",
                    "prompt_file_path",
                    "model_name",
                    "ollama_base_url"
                  ],
                  "docstring": null,
                  "start_line": 24,
                  "end_line": 59
                },
                {
                  "identifier": "backend.MainLLM.MainLLM.call_llm",
                  "name": "call_llm",
                  "calls": [
                    "backend/MainLLM.py::MainLLM::HumanMessage",
                    "backend/MainLLM.py::MainLLM::SystemMessage",
                    "backend/MainLLM.py::MainLLM::error",
                    "backend/MainLLM.py::MainLLM::info",
                    "backend/MainLLM.py::MainLLM::invoke"
                  ],
                  "called_by": [
                    {
                      "file": "main.py",
                      "function": "main_workflow",
                      "mode": "function",
                      "line": 327
                    }
                  ],
                  "args": [
                    "self",
                    "user_input"
                  ],
                  "docstring": null,
                  "start_line": 61,
                  "end_line": 75
                },
                {
                  "identifier": "backend.MainLLM.MainLLM.stream_llm",
                  "name": "stream_llm",
                  "calls": [
                    "backend/MainLLM.py::MainLLM::HumanMessage",
                    "backend/MainLLM.py::MainLLM::SystemMessage",
                    "backend/MainLLM.py::MainLLM::error",
                    "backend/MainLLM.py::MainLLM::info",
                    "backend/MainLLM.py::MainLLM::stream"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "user_input"
                  ],
                  "docstring": null,
                  "start_line": 77,
                  "end_line": 92
                }
              ]
            }
          }
        ]
      }
    },
    "backend/basic_info.py": {
      "ast_nodes": {
        "imports": [
          "re",
          "os",
          "tomllib",
          "typing.List",
          "typing.Dict",
          "typing.Any",
          "typing.Optional"
        ],
        "functions": [],
        "classes": [
          {
            "mode": "class_analysis",
            "identifier": "backend.basic_info.ProjektInfoExtractor",
            "name": "ProjektInfoExtractor",
            "docstring": "Extrahiert grundlegende Projektinformationen aus gängigen Projektdateien\nwie README, pyproject.toml und requirements.txt.",
            "source_code": "class ProjektInfoExtractor:\n    \"\"\"\n    Extrahiert grundlegende Projektinformationen aus gängigen Projektdateien\n    wie README, pyproject.toml und requirements.txt.\n    \"\"\"\n    def __init__(self):\n        self.INFO_NICHT_GEFUNDEN = \"Information not found\"\n        # Initialisiert die Struktur mit Platzhaltern\n        self.info = {\n            \"projekt_uebersicht\": {\n                \"titel\": self.INFO_NICHT_GEFUNDEN,\n                \"beschreibung\": self.INFO_NICHT_GEFUNDEN,\n                \"aktueller_status\": self.INFO_NICHT_GEFUNDEN,\n                \"key_features\": self.INFO_NICHT_GEFUNDEN,\n                \"tech_stack\": self.INFO_NICHT_GEFUNDEN,\n            },\n            \"installation\": {\n                \"dependencies\": self.INFO_NICHT_GEFUNDEN,\n                \"setup_anleitung\": self.INFO_NICHT_GEFUNDEN,\n                \"quick_start_guide\": self.INFO_NICHT_GEFUNDEN,\n            }\n        }\n\n    def _finde_datei(self, patterns: List[str], dateien: List[Any]) -> Optional[Any]:\n        \"\"\"Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht.\"\"\"\n        for datei in dateien:\n            for pattern in patterns:\n                if datei.path.lower().endswith(pattern.lower()):\n                    return datei\n        return None\n\n    def _extrahiere_sektion_aus_markdown(self, inhalt: str, keywords: List[str]) -> Optional[str]:\n        \"\"\"\n        Extrahiert den Text unter einer Markdown-Überschrift (##).\n        \n        Args:\n            inhalt (str): Der gesamte Markdown-Text.\n            keywords (list): Eine Liste von alternativen Schlüsselwörtern für den Titel \n                             der Sektion (z.B. [\"Installation\", \"Setup\"]).\n        \n        Returns:\n            str: Der extrahierte Textabschnitt oder None.\n        \"\"\"\n        # Erstellt ein Regex-Pattern, das auf jedes der Schlüsselwörter reagiert\n        keyword_pattern = \"|\".join(re.escape(k) for k in keywords)\n        \n        # Sucht nach \"## Schlüsselwort\" und erfasst alles bis zur nächsten \"##\" oder dem Dateiende\n        pattern = re.compile(\n            rf\"##\\s*({keyword_pattern})\\s*\\n(.*?)(?=\\n##|\\Z)\",\n            re.IGNORECASE | re.DOTALL\n        )\n        match = pattern.search(inhalt)\n        if match:\n            return match.group(2).strip()\n        return None\n\n    def _parse_readme(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer README-Datei.\"\"\"\n        if self.info[\"projekt_uebersicht\"][\"titel\"] == self.INFO_NICHT_GEFUNDEN:\n            title_match = re.search(r\"^\\s*#\\s*(.*)\", inhalt)\n            if title_match:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = title_match.group(1).strip()\n\n        # Beschreibung (Fallback)\n        # Nimmt den Text nach dem Titel bis zur nächsten Überschrift\n        if self.info[\"projekt_uebersicht\"][\"beschreibung\"] == self.INFO_NICHT_GEFUNDEN:\n            desc_match = re.search(r\"^\\s*#\\s*.*\\n+([^#\\n].*)\", inhalt, re.DOTALL)\n            if desc_match:\n                 self.info[\"projekt_uebersicht\"][\"beschreibung\"] = desc_match.group(1).strip().split('\\n\\n')[0]\n\n\n        # Key Features\n        features = self._extrahiere_sektion_aus_markdown(inhalt, [\"Features\", \"Key Features\", \"Merkmale\"])\n        if features:\n            self.info[\"projekt_uebersicht\"][\"key_features\"] = features\n            \n        # Tech Stack\n        tech_stack = self._extrahiere_sektion_aus_markdown(inhalt, [\"Tech Stack\", \"Technology\", \"Technologien\"])\n        if tech_stack:\n            self.info[\"projekt_uebersicht\"][\"tech_stack\"] = tech_stack\n            \n        # Status\n        status = self._extrahiere_sektion_aus_markdown(inhalt, [\"Status\", \"Current Status\"])\n        if status:\n            self.info[\"projekt_uebersicht\"][\"aktueller_status\"] = status\n\n        # Setup-Anleitung\n        setup = self._extrahiere_sektion_aus_markdown(inhalt, [\"Installation\", \"Setup\", \"Getting Started\"])\n        if setup:\n            self.info[\"installation\"][\"setup_anleitung\"] = setup\n            \n        # Quick Start\n        quick_start = self._extrahiere_sektion_aus_markdown(inhalt, [\"Quick Start\", \"Schnellstart\"])\n        if quick_start:\n            self.info[\"installation\"][\"quick_start_guide\"] = quick_start\n\n    def _parse_toml(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer pyproject.toml-Datei.\"\"\"\n        if not tomllib:\n            print(\"Warnung: 'tomli' ist nicht installiert. pyproject.toml kann nicht analysiert werden.\")\n            return\n            \n        try:\n            data = tomllib.loads(inhalt)\n            project_data = data.get(\"project\", {})\n            \n            if \"name\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"titel\"] = project_data[\"name\"]\n            if \"description\" in project_data:\n                self.info[\"projekt_uebersicht\"][\"beschreibung\"] = project_data[\"description\"]\n            if \"dependencies\" in project_data:\n                # Überschreibt 'dependencies' immer, da toml als primäre Quelle gilt\n                self.info[\"installation\"][\"dependencies\"] = project_data[\"dependencies\"]\n        except tomllib.TOMLDecodeError as e:\n            print(f\"Warnung: Fehler beim Parsen der pyproject.toml: {e}\")\n\n    def _parse_requirements(self, inhalt: str):\n        \"\"\"Parst den Inhalt einer requirements.txt-Datei.\"\"\"\n        # Nur füllen, wenn noch keine Dependencies aus toml gefunden wurden\n        if self.info[\"installation\"][\"dependencies\"] == self.INFO_NICHT_GEFUNDEN:\n            lines = inhalt.splitlines()\n            dependencies = [\n                line.strip() for line in lines \n                if line.strip() and not line.strip().startswith('#')\n            ]\n            if dependencies:\n                self.info[\"installation\"][\"dependencies\"] = dependencies\n\n    def extrahiere_info(self, dateien: List[Any], repo_url: str) -> Dict[str, Any]:\n        \"\"\"\n        Orchestriert die Extraktion von Informationen aus einer Liste von RepoFile-Objekten.\n        \n        Die Reihenfolge der Verarbeitung ist wichtig, um Prioritäten zu setzen:\n        1. pyproject.toml (höchste Priorität für Metadaten)\n        2. requirements.txt (Fallback für Dependencies)\n        3. README (für beschreibende Texte und als Fallback)\n        4. Titel wird am Ende basierend auf der URL überschrieben.\n        \"\"\"\n        # 1. Relevante Dateien finden\n        readme_datei = self._finde_datei([\"readme.md\", \"readme.rst\", \"readme.txt\", \"readme\"], dateien)\n        toml_datei = self._finde_datei([\"pyproject.toml\"], dateien)\n        req_datei = self._finde_datei([\"requirements.txt\"], dateien)\n\n        # 2. Dateien parsen (mit Priorisierung)\n        if toml_datei:\n            self._parse_toml(toml_datei.content)\n\n        if req_datei:\n            self._parse_requirements(req_datei.content)\n            \n        if readme_datei:\n            self._parse_readme(readme_datei.content)\n            \n        # 3. Finale Formatierung der Dependencies\n        deps = self.info[\"installation\"][\"dependencies\"]\n        if isinstance(deps, list):\n            if not deps:\n                self.info[\"installation\"][\"dependencies\"] = self.INFO_NICHT_GEFUNDEN\n            else:\n                self.info[\"installation\"][\"dependencies\"] = \"\\n\".join(f\"- {dep}\" for dep in deps)\n        \n        repo_name = os.path.basename(repo_url.removesuffix('.git'))\n        self.info[\"projekt_uebersicht\"][\"titel\"] = f\"{repo_name} documentation\"\n\n        return self.info",
            "start_line": 8,
            "end_line": 172,
            "context": {
              "dependencies": [],
              "instantiated_by": [
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 114
                }
              ],
              "method_context": [
                {
                  "identifier": "backend.basic_info.ProjektInfoExtractor.__init__",
                  "name": "__init__",
                  "calls": [],
                  "called_by": [],
                  "args": [
                    "self"
                  ],
                  "docstring": null,
                  "start_line": 13,
                  "end_line": 29
                },
                {
                  "identifier": "backend.basic_info.ProjektInfoExtractor._finde_datei",
                  "name": "_finde_datei",
                  "calls": [
                    "backend/basic_info.py::ProjektInfoExtractor::endswith",
                    "backend/basic_info.py::ProjektInfoExtractor::lower"
                  ],
                  "called_by": [
                    "backend/basic_info.py::ProjektInfoExtractor::extrahiere_info"
                  ],
                  "args": [
                    "self",
                    "patterns",
                    "dateien"
                  ],
                  "docstring": "Sucht case-insensitiv nach einer Datei, die einem der Muster entspricht.",
                  "start_line": 31,
                  "end_line": 37
                },
                {
                  "identifier": "backend.basic_info.ProjektInfoExtractor._extrahiere_sektion_aus_markdown",
                  "name": "_extrahiere_sektion_aus_markdown",
                  "calls": [
                    "backend/basic_info.py::ProjektInfoExtractor::compile",
                    "backend/basic_info.py::ProjektInfoExtractor::escape",
                    "backend/basic_info.py::ProjektInfoExtractor::group",
                    "backend/basic_info.py::ProjektInfoExtractor::join",
                    "backend/basic_info.py::ProjektInfoExtractor::search",
                    "backend/basic_info.py::ProjektInfoExtractor::strip"
                  ],
                  "called_by": [
                    "backend/basic_info.py::ProjektInfoExtractor::_parse_readme"
                  ],
                  "args": [
                    "self",
                    "inhalt",
                    "keywords"
                  ],
                  "docstring": "Extrahiert den Text unter einer Markdown-Überschrift (##).\n\nArgs:\n    inhalt (str): Der gesamte Markdown-Text.\n    keywords (list): Eine Liste von alternativen Schlüsselwörtern für den Titel \n                     der Sektion (z.B. [\"Installation\", \"Setup\"]).\n\nReturns:\n    str: Der extrahierte Textabschnitt oder None.",
                  "start_line": 39,
                  "end_line": 62
                },
                {
                  "identifier": "backend.basic_info.ProjektInfoExtractor._parse_readme",
                  "name": "_parse_readme",
                  "calls": [
                    "backend/basic_info.py::ProjektInfoExtractor::_extrahiere_sektion_aus_markdown",
                    "backend/basic_info.py::ProjektInfoExtractor::group",
                    "backend/basic_info.py::ProjektInfoExtractor::search",
                    "backend/basic_info.py::ProjektInfoExtractor::split",
                    "backend/basic_info.py::ProjektInfoExtractor::strip"
                  ],
                  "called_by": [
                    "backend/basic_info.py::ProjektInfoExtractor::extrahiere_info"
                  ],
                  "args": [
                    "self",
                    "inhalt"
                  ],
                  "docstring": "Parst den Inhalt einer README-Datei.",
                  "start_line": 64,
                  "end_line": 102
                },
                {
                  "identifier": "backend.basic_info.ProjektInfoExtractor._parse_toml",
                  "name": "_parse_toml",
                  "calls": [
                    "backend/basic_info.py::ProjektInfoExtractor::get",
                    "backend/basic_info.py::ProjektInfoExtractor::loads",
                    "backend/basic_info.py::ProjektInfoExtractor::print"
                  ],
                  "called_by": [
                    "backend/basic_info.py::ProjektInfoExtractor::extrahiere_info"
                  ],
                  "args": [
                    "self",
                    "inhalt"
                  ],
                  "docstring": "Parst den Inhalt einer pyproject.toml-Datei.",
                  "start_line": 104,
                  "end_line": 122
                },
                {
                  "identifier": "backend.basic_info.ProjektInfoExtractor._parse_requirements",
                  "name": "_parse_requirements",
                  "calls": [
                    "backend/basic_info.py::ProjektInfoExtractor::splitlines",
                    "backend/basic_info.py::ProjektInfoExtractor::startswith",
                    "backend/basic_info.py::ProjektInfoExtractor::strip"
                  ],
                  "called_by": [
                    "backend/basic_info.py::ProjektInfoExtractor::extrahiere_info"
                  ],
                  "args": [
                    "self",
                    "inhalt"
                  ],
                  "docstring": "Parst den Inhalt einer requirements.txt-Datei.",
                  "start_line": 124,
                  "end_line": 134
                },
                {
                  "identifier": "backend.basic_info.ProjektInfoExtractor.extrahiere_info",
                  "name": "extrahiere_info",
                  "calls": [
                    "backend/basic_info.py::ProjektInfoExtractor::_finde_datei",
                    "backend/basic_info.py::ProjektInfoExtractor::_parse_readme",
                    "backend/basic_info.py::ProjektInfoExtractor::_parse_requirements",
                    "backend/basic_info.py::ProjektInfoExtractor::_parse_toml",
                    "backend/basic_info.py::ProjektInfoExtractor::basename",
                    "backend/basic_info.py::ProjektInfoExtractor::isinstance",
                    "backend/basic_info.py::ProjektInfoExtractor::join",
                    "backend/basic_info.py::ProjektInfoExtractor::removesuffix"
                  ],
                  "called_by": [
                    {
                      "file": "main.py",
                      "function": "main_workflow",
                      "mode": "function",
                      "line": 115
                    }
                  ],
                  "args": [
                    "self",
                    "dateien",
                    "repo_url"
                  ],
                  "docstring": "Orchestriert die Extraktion von Informationen aus einer Liste von RepoFile-Objekten.\n\nDie Reihenfolge der Verarbeitung ist wichtig, um Prioritäten zu setzen:\n1. pyproject.toml (höchste Priorität für Metadaten)\n2. requirements.txt (Fallback für Dependencies)\n3. README (für beschreibende Texte und als Fallback)\n4. Titel wird am Ende basierend auf der URL überschrieben.",
                  "start_line": 136,
                  "end_line": 172
                }
              ]
            }
          }
        ]
      }
    },
    "backend/callgraph.py": {
      "ast_nodes": {
        "imports": [
          "ast",
          "networkx",
          "os",
          "typing.Dict",
          "getRepo.RepoFile",
          "getRepo.GitRepository",
          "getRepo.GitRepository",
          "basic_info.ProjektInfoExtractor",
          "os"
        ],
        "functions": [
          {
            "mode": "function_analysis",
            "identifier": "backend.callgraph.build_callGraph",
            "name": "build_callGraph",
            "args": [
              "tree",
              "filename"
            ],
            "docstring": "Erstellt einen Call-Graphen aus einem gegebenen Python-AST.\n\nDer Graph ist ein gerichteter Graph (networkx.DiGraph), in dem:\n  - Knoten: Funktionen, Methoden und der globale Scope (<module>) bzw. <main_block> für `if __name__ == \"__main__\"`-Code\n  - Kanten: Funktions-/Methodenaufrufe zwischen diesen Knoten\n\nArgs:\n    tree (ast.AST): Der AST der zu analysierenden Python-Datei.\n    filename (str, optional): Der Name der analysierten Datei, z. B. `\"main.py\"` oder `\"src/utils.py\"`.\n\nReturns:\n    nx.DiGraph: Der vollständige Call-Graph.",
            "source_code": "def build_callGraph(tree: ast.AST, filename: str) -> nx.DiGraph:\n    \"\"\" \n    Erstellt einen Call-Graphen aus einem gegebenen Python-AST.\n\n    Der Graph ist ein gerichteter Graph (networkx.DiGraph), in dem:\n      - Knoten: Funktionen, Methoden und der globale Scope (<module>) bzw. <main_block> für `if __name__ == \"__main__\"`-Code\n      - Kanten: Funktions-/Methodenaufrufe zwischen diesen Knoten\n\n    Args:\n        tree (ast.AST): Der AST der zu analysierenden Python-Datei.\n        filename (str, optional): Der Name der analysierten Datei, z. B. `\"main.py\"` oder `\"src/utils.py\"`.\n\n    Returns:\n        nx.DiGraph: Der vollständige Call-Graph.\n    \"\"\"\n    visitor = CallGraph(filename)\n    visitor.visit(tree)\n    graph = visitor.graph\n\n# add all edges from dictionary to the graph at once\n    for caller, callees in visitor.edges.items():\n        for callee in callees:\n            # if callee in visitor.function_set:\n                graph.add_edge(caller, callee)\n\n    return graph",
            "start_line": 150,
            "end_line": 175,
            "context": {
              "calls": [
                "backend/callgraph.py::CallGraph",
                "backend/callgraph.py::add_edge",
                "backend/callgraph.py::items",
                "backend/callgraph.py::visit"
              ],
              "called_by": [
                {
                  "file": "AST_Schema.py",
                  "function": "analyze_repository",
                  "mode": "method",
                  "line": 197
                },
                {
                  "file": "callgraph.py",
                  "function": "build_global_callgraph",
                  "mode": "function",
                  "line": 211
                }
              ]
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "backend.callgraph.graph_to_adj_list",
            "name": "graph_to_adj_list",
            "args": [
              "graph"
            ],
            "docstring": "Konvertiert einen networkx.DiGraph in eine Adjazenzliste (Dict),\ndie JSON-serialisierbar ist.\n\nArgs:\n    graph (nx.DiGraph): Der zu konvertierende Call-Graph.\n\nReturns:\n    Dict[str, List[str]]: Eine Adjazenzliste, bei der jeder Schlüssel\n                          ein aufrufender Knoten (caller) und der Wert\n                          eine Liste der aufgerufenen Knoten (callees) ist.",
            "source_code": "def graph_to_adj_list(graph: nx.DiGraph) -> Dict[str, list[str]]:\n    \"\"\"\n    Konvertiert einen networkx.DiGraph in eine Adjazenzliste (Dict),\n    die JSON-serialisierbar ist.\n\n    Args:\n        graph (nx.DiGraph): Der zu konvertierende Call-Graph.\n\n    Returns:\n        Dict[str, List[str]]: Eine Adjazenzliste, bei der jeder Schlüssel\n                              ein aufrufender Knoten (caller) und der Wert\n                              eine Liste der aufgerufenen Knoten (callees) ist.\n    \"\"\"\n    adj_list = {}\n    # Wir sortieren die Knoten für eine konsistente Ausgabe\n    for node in sorted(list(graph.nodes())):\n        # Wir holen alle Nachfolger (aufgerufene Funktionen) und sortieren sie ebenfalls\n        successors = sorted(list(graph.successors(node)))\n        if successors:  # Nur Knoten aufnehmen, die auch wirklich andere aufrufen\n            adj_list[node] = successors\n    return adj_list",
            "start_line": 178,
            "end_line": 198,
            "context": {
              "calls": [
                "backend/callgraph.py::list",
                "backend/callgraph.py::nodes",
                "backend/callgraph.py::sorted",
                "backend/callgraph.py::successors"
              ],
              "called_by": []
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "backend.callgraph.build_global_callgraph",
            "name": "build_global_callgraph",
            "args": [
              "repo"
            ],
            "docstring": null,
            "source_code": "def build_global_callgraph(repo: GitRepository)-> nx.DiGraph:\n    all_files = repository.get_all_files()\n    global_graph = nx.DiGraph()\n\n    for file in all_files: \n        if not file.path.endswith(\".py\"):\n            continue\n        filename = str(os.path.basename(file.path)).removesuffix(\".py\")\n        tree = ast.parse(file.content)\n        graph = build_callGraph(tree, filename)\n        \n        for node in graph.nodes:\n            global_graph.add_node(node)\n            \n        for caller, callee in graph.edges:\n            if callee:\n                global_graph.add_node(callee)\n                global_graph.add_edge(caller, callee)\n    \n    return global_graph",
            "start_line": 202,
            "end_line": 221,
            "context": {
              "calls": [
                "backend/callgraph.py::DiGraph",
                "backend/callgraph.py::add_edge",
                "backend/callgraph.py::add_node",
                "backend/callgraph.py::basename",
                "backend/callgraph.py::build_callGraph",
                "backend/callgraph.py::endswith",
                "backend/callgraph.py::get_all_files",
                "backend/callgraph.py::parse",
                "backend/callgraph.py::removesuffix",
                "backend/callgraph.py::str"
              ],
              "called_by": [
                {
                  "file": "callgraph.py",
                  "function": "backend.callgraph",
                  "mode": "module",
                  "line": 262
                }
              ]
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "backend.callgraph.make_safe_dot",
            "name": "make_safe_dot",
            "args": [
              "graph",
              "out_path"
            ],
            "docstring": null,
            "source_code": "def make_safe_dot(graph: nx.DiGraph, out_path: str):\n    mapping = {}\n    H = graph.copy()\n    for i, n in enumerate(list(graph.nodes())):\n        safe = f\"n{i}\"           \n        mapping[n] = safe\n\n    H = nx.relabel_nodes(H, mapping)\n    for orig, safe in mapping.items():\n        H.nodes[safe][\"label\"] = orig\n\n    nx.drawing.nx_pydot.write_dot(H, out_path)",
            "start_line": 225,
            "end_line": 236,
            "context": {
              "calls": [
                "backend/callgraph.py::copy",
                "backend/callgraph.py::enumerate",
                "backend/callgraph.py::items",
                "backend/callgraph.py::list",
                "backend/callgraph.py::nodes",
                "backend/callgraph.py::relabel_nodes",
                "backend/callgraph.py::write_dot"
              ],
              "called_by": [
                {
                  "file": "callgraph.py",
                  "function": "backend.callgraph",
                  "mode": "module",
                  "line": 263
                }
              ]
            }
          }
        ],
        "classes": [
          {
            "mode": "class_analysis",
            "identifier": "backend.callgraph.CallGraph",
            "name": "CallGraph",
            "docstring": "Visitor, der Funktionsaufrufe im AST sammelt und Kanten für den Call-Graph erstellt.",
            "source_code": "class CallGraph(ast.NodeVisitor):\n    \"\"\"\n    Visitor, der Funktionsaufrufe im AST sammelt und Kanten für den Call-Graph erstellt.\n    \"\"\"\n    def __init__(self, filename: str):\n        \"\"\"\n        Initialisiert den Visitor mit einem Platzhalter für die aktuelle Funktion.\n        \"\"\"\n        self.filename = filename\n        self.current_function = None\n        self.current_class = None\n\n        self.graph = nx.DiGraph()\n        self.import_mapping: dict[str, str] = {}\n        self.function_set: set[str] = set()\n        self.edges: Dict[str, set[str]] = {}\n\n    def _recursive_call(self, node) -> list[str]:\n        all_calls = []\n        if isinstance(node, ast.Call):\n            return self._recursive_call(node.func)\n        elif isinstance(node, ast.Name):\n            all_calls.append(node.id)\n            return all_calls\n        elif isinstance(node, ast.Attribute):\n            all_calls.append(node.attr)\n            return all_calls\n        return all_calls\n            \n    def _resolve_all_callee_names(self, callee_nodes: list[str]) -> list[str]:\n        resolved_callees = []\n        for raw_callee in callee_nodes:\n            if not self.current_class:\n                resolved_callee = f\"{self.filename}::{raw_callee}\"\n            else:\n                resolved_callee = f\"{self.filename}::{self.current_class}::{raw_callee}\"\n            resolved_callees.append(resolved_callee)\n        return resolved_callees\n\n    def _make_full_name(self, basename: str, class_name: str | None = None)-> str:\n        if class_name:\n            return f\"{self.filename}::{class_name}::{basename}\"\n        return f\"{self.filename}::{basename}\"\n    \n    def _current_caller(self)-> str:\n        if self.current_function:\n            return self.current_function\n        return f\"<{self.filename}>\" if self.filename else\"<global-scope>\"\n\n    # def visit_Import(self, node):\n    #     for alias in node.names:\n    #         module_name = alias.name\n    #         module_asname = (alias.asname if alias.asname else module_name)\n    #         self.import_mapping[module_asname] = module_name\n    #     self.generic_visit(node)\n\n    # def visit_ImportFrom(self, node):\n    #     module_name = node.module\n    #     level_depth = node.level\n        \n    #     module_base = module_name.split(\".\")[0]\n    #     for alias in node.names:\n    #         import_name = (alias.asname if alias.asname else alias.name)\n    #         self.import_mapping[import_name] = module_base\n    #     # TODO: level depth und relative import From resolven\n\n    def visit_ClassDef(self, node: ast.ClassDef):\n        prev_class = self.current_class\n        self.current_class = node.name\n        for function in node.body:\n            self.visit(function)\n        self.current_class = prev_class\n        \n    def visit_FunctionDef(self, node):\n        \"\"\" \n        Besucht eine normale Funktionsdefinition.\n\n        Setzt `self.current_function`, erstellt den Knoten im Graphen und \n        traversiert rekursiv den Funktionskörper.\n        \"\"\"\n        if self.current_class:\n            self.current_function = self._make_full_name(node.name, class_name=self.current_class)\n        else:\n            self.current_function = self._make_full_name(node.name) \n        self.graph.add_node(self.current_function)\n        self.generic_visit(node)\n        self.function_set.add(self.current_function)\n        self.current_function = None\n\n    def visit_AsyncFunctionDef(self, node):\n        \"\"\"\n        Besucht eine asynchrone Funktionsdefinition (`async def`).\n\n        Funktioniert analog zu `visit_FunctionDef`.\n        \"\"\"\n        self.visit_FunctionDef(node)\n\n    def visit_Call(self, node):\n        \"\"\"\n        Besucht einen Funktions- oder Methodenaufruf und behandelt komplexe Fälle\n        mit einer detaillierten Warnung, anstatt abzustürzen.\n        \"\"\"             \n        caller = self._current_caller()\n        raw_callees: list[str] = []\n        try:\n            # benutze die Helferfunktion, um Namen zu extrahieren\n            raw_callees = self._recursive_call(node)\n            # falls _recursive_call ein leeres array oder None zurückgibt, sicherstellen, dass es list ist\n            if raw_callees is None:\n                raw_callees = []\n\n            resolved_callees = self._resolve_all_callee_names(raw_callees)\n\n            # sicherstellen, dass caller als Key existiert und ein Set ist\n            if caller not in self.edges:\n                self.edges[caller] = set()\n\n            for resolved_callee in resolved_callees:\n                if resolved_callee:\n                    self.edges[caller].add(resolved_callee)\n        except Exception as e:\n            print(f\"Unerwarteter Fehler bei der Verarbeitung eines Funktionsaufrufs: {e} in Datei {self.filename}\")\n        self.generic_visit(node)\n\n    def visit_If(self, node):\n        \"\"\"\n        Prüft auf `if __name__ == \"__main__\"`-Blöcke, um globale Aufrufe innerhalb\n        dieses Blocks separat als <main_block> im Call-Graphen darzustellen.\n\n        Alle Funktionsaufrufe innerhalb des Blocks werden dann von <main_block> aus\n        als Caller-Knoten erfasst.\n        \"\"\"\n        if (isinstance(node.test, ast.Compare) and\n            isinstance(node.test.left, ast.Name) and \n            node.test.left.id == \"__name__\"):\n            caller_backup = self.current_function\n            self.current_function = \"<main_block>\"\n            self.generic_visit(node)\n            self.current_function = caller_backup\n        else:\n            self.generic_visit(node)",
            "start_line": 8,
            "end_line": 148,
            "context": {
              "dependencies": [],
              "instantiated_by": [
                {
                  "file": "callgraph.py",
                  "function": "build_callGraph",
                  "mode": "function",
                  "line": 165
                }
              ],
              "method_context": [
                {
                  "identifier": "backend.callgraph.CallGraph.__init__",
                  "name": "__init__",
                  "calls": [
                    "backend/callgraph.py::CallGraph::DiGraph",
                    "backend/callgraph.py::CallGraph::set"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "filename"
                  ],
                  "docstring": "Initialisiert den Visitor mit einem Platzhalter für die aktuelle Funktion.",
                  "start_line": 12,
                  "end_line": 23
                },
                {
                  "identifier": "backend.callgraph.CallGraph._recursive_call",
                  "name": "_recursive_call",
                  "calls": [
                    "backend/callgraph.py::CallGraph::_recursive_call",
                    "backend/callgraph.py::CallGraph::append",
                    "backend/callgraph.py::CallGraph::isinstance"
                  ],
                  "called_by": [
                    "backend/callgraph.py::CallGraph::_recursive_call",
                    "backend/callgraph.py::CallGraph::visit_Call"
                  ],
                  "args": [
                    "self",
                    "node"
                  ],
                  "docstring": null,
                  "start_line": 25,
                  "end_line": 35
                },
                {
                  "identifier": "backend.callgraph.CallGraph._resolve_all_callee_names",
                  "name": "_resolve_all_callee_names",
                  "calls": [
                    "backend/callgraph.py::CallGraph::append"
                  ],
                  "called_by": [
                    "backend/callgraph.py::CallGraph::visit_Call"
                  ],
                  "args": [
                    "self",
                    "callee_nodes"
                  ],
                  "docstring": null,
                  "start_line": 37,
                  "end_line": 45
                },
                {
                  "identifier": "backend.callgraph.CallGraph._make_full_name",
                  "name": "_make_full_name",
                  "calls": [],
                  "called_by": [
                    "backend/callgraph.py::CallGraph::visit_FunctionDef"
                  ],
                  "args": [
                    "self",
                    "basename",
                    "class_name"
                  ],
                  "docstring": null,
                  "start_line": 47,
                  "end_line": 50
                },
                {
                  "identifier": "backend.callgraph.CallGraph._current_caller",
                  "name": "_current_caller",
                  "calls": [],
                  "called_by": [
                    "backend/callgraph.py::CallGraph::visit_Call"
                  ],
                  "args": [
                    "self"
                  ],
                  "docstring": null,
                  "start_line": 52,
                  "end_line": 55
                },
                {
                  "identifier": "backend.callgraph.CallGraph.visit_ClassDef",
                  "name": "visit_ClassDef",
                  "calls": [
                    "backend/callgraph.py::CallGraph::visit"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "node"
                  ],
                  "docstring": null,
                  "start_line": 74,
                  "end_line": 79
                },
                {
                  "identifier": "backend.callgraph.CallGraph.visit_FunctionDef",
                  "name": "visit_FunctionDef",
                  "calls": [
                    "backend/callgraph.py::CallGraph::_make_full_name",
                    "backend/callgraph.py::CallGraph::add",
                    "backend/callgraph.py::CallGraph::add_node",
                    "backend/callgraph.py::CallGraph::generic_visit"
                  ],
                  "called_by": [
                    "backend/callgraph.py::CallGraph::visit_AsyncFunctionDef"
                  ],
                  "args": [
                    "self",
                    "node"
                  ],
                  "docstring": "Besucht eine normale Funktionsdefinition.\n\nSetzt `self.current_function`, erstellt den Knoten im Graphen und \ntraversiert rekursiv den Funktionskörper.",
                  "start_line": 81,
                  "end_line": 95
                },
                {
                  "identifier": "backend.callgraph.CallGraph.visit_AsyncFunctionDef",
                  "name": "visit_AsyncFunctionDef",
                  "calls": [
                    "backend/callgraph.py::CallGraph::visit_FunctionDef"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "node"
                  ],
                  "docstring": "Besucht eine asynchrone Funktionsdefinition (`async def`).\n\nFunktioniert analog zu `visit_FunctionDef`.",
                  "start_line": 97,
                  "end_line": 103
                },
                {
                  "identifier": "backend.callgraph.CallGraph.visit_Call",
                  "name": "visit_Call",
                  "calls": [
                    "backend/callgraph.py::CallGraph::_current_caller",
                    "backend/callgraph.py::CallGraph::_recursive_call",
                    "backend/callgraph.py::CallGraph::_resolve_all_callee_names",
                    "backend/callgraph.py::CallGraph::add",
                    "backend/callgraph.py::CallGraph::generic_visit",
                    "backend/callgraph.py::CallGraph::print",
                    "backend/callgraph.py::CallGraph::set"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "node"
                  ],
                  "docstring": "Besucht einen Funktions- oder Methodenaufruf und behandelt komplexe Fälle\nmit einer detaillierten Warnung, anstatt abzustürzen.",
                  "start_line": 105,
                  "end_line": 130
                },
                {
                  "identifier": "backend.callgraph.CallGraph.visit_If",
                  "name": "visit_If",
                  "calls": [
                    "backend/callgraph.py::CallGraph::generic_visit",
                    "backend/callgraph.py::CallGraph::isinstance"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "node"
                  ],
                  "docstring": "Prüft auf `if __name__ == \"__main__\"`-Blöcke, um globale Aufrufe innerhalb\ndieses Blocks separat als <main_block> im Call-Graphen darzustellen.\n\nAlle Funktionsaufrufe innerhalb des Blocks werden dann von <main_block> aus\nals Caller-Knoten erfasst.",
                  "start_line": 132,
                  "end_line": 148
                }
              ]
            }
          }
        ]
      }
    },
    "backend/getRepo.py": {
      "ast_nodes": {
        "imports": [
          "tempfile",
          "shutil",
          "git.Repo",
          "git.GitCommandError",
          "logging",
          "os"
        ],
        "functions": [],
        "classes": [
          {
            "mode": "class_analysis",
            "identifier": "backend.getRepo.RepoFile",
            "name": "RepoFile",
            "docstring": "Repräsentiert eine einzelne Datei in einem Git-Repository.\n\nDer Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tatsächlichem Zugriff.",
            "source_code": "class RepoFile:\n    \"\"\"\n    Repräsentiert eine einzelne Datei in einem Git-Repository.\n    \n    Der Inhalt der Datei wird \"lazy\" geladen, d.h. erst bei tatsächlichem Zugriff.\n    \"\"\"\n    def __init__(self, file_path, commit_tree):\n        \"\"\"\n        Initialisiert das RepoFile-Objekt.\n\n        Args:\n            file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n            commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt.\n        \"\"\"\n        self.path = file_path\n        self._tree = commit_tree\n        \n        # Attribute für Lazy Loading (werden erst bei Bedarf gefüllt)\n        self._blob = None\n        self._content = None\n        self._size = None\n\n    @property\n    def blob(self):\n        \"\"\"Lazy-lädt das Git-Blob-Objekt.\"\"\"\n        if self._blob is None:\n            try:\n                self._blob = self._tree[self.path]\n            except KeyError:\n                raise FileNotFoundError(f\"Datei '{self.path}' konnte im Commit-Tree nicht gefunden werden.\")\n        return self._blob\n\n    @property\n    def content(self):\n        \"\"\"Lazy-lädt und gibt den dekodierten Inhalt der Datei zurück.\"\"\"\n        if self._content is None:\n            self._content = self.blob.data_stream.read().decode('utf-8', errors='ignore')\n        return self._content\n\n    @property\n    def size(self):\n        \"\"\"Lazy-lädt und gibt die Größe der Datei in Bytes zurück.\"\"\"\n        if self._size is None:\n            self._size = self.blob.size\n        return self._size\n        \n    def analyze_word_count(self):\n        \"\"\"\n        Eine Beispiel-Analyse-Methode. Zählt die Wörter im Dateiinhalt.\n        \"\"\"\n        return len(self.content.split())\n\n    def __repr__(self):\n        \"\"\"Gibt eine nützliche String-Repräsentation des Objekts zurück.\"\"\"\n        return f\"<RepoFile(path='{self.path}')>\"\n    \n    def to_dict(self, include_content=False):\n        data = {\n            \"path\": self.path,\n            \"name\": os.path.basename(self.path),\n            \"size\": self.size,\n            \"type\": \"file\"\n        }\n        if include_content:\n            data[\"content\"] = self.content\n        return data",
            "start_line": 10,
            "end_line": 75,
            "context": {
              "dependencies": [],
              "instantiated_by": [
                {
                  "file": "getRepo.py",
                  "function": "get_all_files",
                  "mode": "method",
                  "line": 111
                }
              ],
              "method_context": [
                {
                  "identifier": "backend.getRepo.RepoFile.__init__",
                  "name": "__init__",
                  "calls": [],
                  "called_by": [],
                  "args": [
                    "self",
                    "file_path",
                    "commit_tree"
                  ],
                  "docstring": "Initialisiert das RepoFile-Objekt.\n\nArgs:\n    file_path (str): Der Pfad zur Datei innerhalb des Repositories.\n    commit_tree (git.Tree): Das Tree-Objekt des Commits, aus dem die Datei stammt.",
                  "start_line": 16,
                  "end_line": 30
                },
                {
                  "identifier": "backend.getRepo.RepoFile.blob",
                  "name": "blob",
                  "calls": [
                    "backend/getRepo.py::RepoFile::FileNotFoundError"
                  ],
                  "called_by": [],
                  "args": [
                    "self"
                  ],
                  "docstring": "Lazy-lädt das Git-Blob-Objekt.",
                  "start_line": 33,
                  "end_line": 40
                },
                {
                  "identifier": "backend.getRepo.RepoFile.content",
                  "name": "content",
                  "calls": [
                    "backend/getRepo.py::RepoFile::decode",
                    "backend/getRepo.py::RepoFile::read"
                  ],
                  "called_by": [],
                  "args": [
                    "self"
                  ],
                  "docstring": "Lazy-lädt und gibt den dekodierten Inhalt der Datei zurück.",
                  "start_line": 43,
                  "end_line": 47
                },
                {
                  "identifier": "backend.getRepo.RepoFile.size",
                  "name": "size",
                  "calls": [],
                  "called_by": [],
                  "args": [
                    "self"
                  ],
                  "docstring": "Lazy-lädt und gibt die Größe der Datei in Bytes zurück.",
                  "start_line": 50,
                  "end_line": 54
                },
                {
                  "identifier": "backend.getRepo.RepoFile.analyze_word_count",
                  "name": "analyze_word_count",
                  "calls": [
                    "backend/getRepo.py::RepoFile::len",
                    "backend/getRepo.py::RepoFile::split"
                  ],
                  "called_by": [],
                  "args": [
                    "self"
                  ],
                  "docstring": "Eine Beispiel-Analyse-Methode. Zählt die Wörter im Dateiinhalt.",
                  "start_line": 56,
                  "end_line": 60
                },
                {
                  "identifier": "backend.getRepo.RepoFile.__repr__",
                  "name": "__repr__",
                  "calls": [],
                  "called_by": [],
                  "args": [
                    "self"
                  ],
                  "docstring": "Gibt eine nützliche String-Repräsentation des Objekts zurück.",
                  "start_line": 62,
                  "end_line": 64
                },
                {
                  "identifier": "backend.getRepo.RepoFile.to_dict",
                  "name": "to_dict",
                  "calls": [
                    "backend/getRepo.py::RepoFile::basename"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "include_content"
                  ],
                  "docstring": null,
                  "start_line": 66,
                  "end_line": 75
                }
              ]
            }
          },
          {
            "mode": "class_analysis",
            "identifier": "backend.getRepo.GitRepository",
            "name": "GitRepository",
            "docstring": "Verwaltet ein Git-Repository, einschließlich Klonen in ein temporäres\nVerzeichnis und Bereitstellung von RepoFile-Objekten.",
            "source_code": "class GitRepository:\n    \"\"\"\n    Verwaltet ein Git-Repository, einschließlich Klonen in ein temporäres\n    Verzeichnis und Bereitstellung von RepoFile-Objekten.\n    \"\"\"\n    def __init__(self, repo_url):\n        self.repo_url = repo_url\n        self.temp_dir = tempfile.mkdtemp()\n        self.repo = None\n\n        self.files = []\n        \n        try:\n            logging.info(f\"Clone Repository {self.repo_url}...\")\n            self.repo = Repo.clone_from(self.repo_url, self.temp_dir)\n            self.latest_commit = self.repo.head.commit\n            self.commit_tree = self.latest_commit.tree\n            logging.info(\"Cloning successful.\")\n        except GitCommandError as e:\n            self.close()\n            raise RuntimeError(f\"Error cloning repository: {e}\") from e\n\n    def get_all_files(self):\n        \"\"\"\n        Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zurück.\n\n        Returns:\n            list[RepoFile]: Eine Liste von RepoFile-Instanzen.\n        \"\"\"\n        file_paths = self.repo.git.ls_files().split('\\n')\n        self.files = [RepoFile(path, self.commit_tree) for path in file_paths if path]\n        return self.files\n\n    def close(self):\n        \"\"\"Löscht das temporäre Verzeichnis und dessen Inhalt.\"\"\"\n        if self.temp_dir:\n            print(f\"\\nLösche temporäres Verzeichnis: {self.temp_dir}\")\n            #shutil.rmtree(self.temp_dir)\n            self.temp_dir = None\n            \n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def get_file_tree(self, include_content=False):\n\n        if not self.files:\n            self.get_all_files()\n\n        tree = {\"name\": \"root\", \"type\": \"directory\", \"children\": []}\n\n        for file_obj in self.files:\n            parts = file_obj.path.split('/')\n            current_level = tree[\"children\"]\n            \n            # Iteriere durch die Ordnerstruktur\n            for part in parts[:-1]:\n                # Suche, ob Ordner schon existiert\n                found = next((item for item in current_level if item[\"name\"] == part and item[\"type\"] == \"directory\"), None)\n                if not found:\n                    new_dir = {\"name\": part, \"type\": \"directory\", \"children\": []}\n                    current_level.append(new_dir)\n                    current_level = new_dir[\"children\"]\n                else:\n                    current_level = found[\"children\"]\n            \n            # Datei am Ende hinzufügen\n            current_level.append(file_obj.to_dict(include_content=include_content))\n\n        return tree",
            "start_line": 81,
            "end_line": 152,
            "context": {
              "dependencies": [],
              "instantiated_by": [
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 96
                }
              ],
              "method_context": [
                {
                  "identifier": "backend.getRepo.GitRepository.__init__",
                  "name": "__init__",
                  "calls": [
                    "backend/getRepo.py::GitRepository::RuntimeError",
                    "backend/getRepo.py::GitRepository::clone_from",
                    "backend/getRepo.py::GitRepository::close",
                    "backend/getRepo.py::GitRepository::info",
                    "backend/getRepo.py::GitRepository::mkdtemp"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "repo_url"
                  ],
                  "docstring": null,
                  "start_line": 86,
                  "end_line": 101
                },
                {
                  "identifier": "backend.getRepo.GitRepository.get_all_files",
                  "name": "get_all_files",
                  "calls": [
                    "backend/getRepo.py::GitRepository::RepoFile",
                    "backend/getRepo.py::GitRepository::ls_files",
                    "backend/getRepo.py::GitRepository::split"
                  ],
                  "called_by": [
                    "backend/getRepo.py::GitRepository::get_file_tree"
                  ],
                  "args": [
                    "self"
                  ],
                  "docstring": "Gibt eine Liste aller Dateien im Repository als RepoFile-Objekte zurück.\n\nReturns:\n    list[RepoFile]: Eine Liste von RepoFile-Instanzen.",
                  "start_line": 103,
                  "end_line": 112
                },
                {
                  "identifier": "backend.getRepo.GitRepository.close",
                  "name": "close",
                  "calls": [
                    "backend/getRepo.py::GitRepository::print"
                  ],
                  "called_by": [
                    "backend/getRepo.py::GitRepository::__exit__",
                    "backend/getRepo.py::GitRepository::__init__"
                  ],
                  "args": [
                    "self"
                  ],
                  "docstring": "Löscht das temporäre Verzeichnis und dessen Inhalt.",
                  "start_line": 114,
                  "end_line": 119
                },
                {
                  "identifier": "backend.getRepo.GitRepository.__enter__",
                  "name": "__enter__",
                  "calls": [],
                  "called_by": [],
                  "args": [
                    "self"
                  ],
                  "docstring": null,
                  "start_line": 121,
                  "end_line": 122
                },
                {
                  "identifier": "backend.getRepo.GitRepository.__exit__",
                  "name": "__exit__",
                  "calls": [
                    "backend/getRepo.py::GitRepository::close"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "exc_type",
                    "exc_val",
                    "exc_tb"
                  ],
                  "docstring": null,
                  "start_line": 124,
                  "end_line": 125
                },
                {
                  "identifier": "backend.getRepo.GitRepository.get_file_tree",
                  "name": "get_file_tree",
                  "calls": [
                    "backend/getRepo.py::GitRepository::append",
                    "backend/getRepo.py::GitRepository::get_all_files",
                    "backend/getRepo.py::GitRepository::next",
                    "backend/getRepo.py::GitRepository::split",
                    "backend/getRepo.py::GitRepository::to_dict"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "include_content"
                  ],
                  "docstring": null,
                  "start_line": 127,
                  "end_line": 152
                }
              ]
            }
          }
        ]
      }
    },
    "backend/main.py": {
      "ast_nodes": {
        "imports": [
          "logging",
          "os",
          "re",
          "json",
          "time",
          "math",
          "datetime.datetime",
          "dotenv.load_dotenv",
          "getRepo.GitRepository",
          "getRepo.RepoFile",
          "AST_Schema.ASTAnalyzer",
          "MainLLM.MainLLM",
          "basic_info.ProjektInfoExtractor",
          "HelperLLM.LLMHelper",
          "schemas.types.FunctionContextInput",
          "schemas.types.FunctionAnalysisInput",
          "schemas.types.ClassContextInput",
          "schemas.types.ClassAnalysisInput"
        ],
        "functions": [
          {
            "mode": "function_analysis",
            "identifier": "backend.main.calculate_net_time",
            "name": "calculate_net_time",
            "args": [
              "start_time",
              "end_time",
              "total_items",
              "batch_size",
              "model_name"
            ],
            "docstring": "Berechnet die Dauer abzüglich der Sleep-Zeiten für Rate-Limits.",
            "source_code": "def calculate_net_time(start_time, end_time, total_items, batch_size, model_name):\n    \"\"\"Berechnet die Dauer abzüglich der Sleep-Zeiten für Rate-Limits.\"\"\"\n    total_duration = end_time - start_time\n    \n    if not model_name.startswith(\"gemini-\"):\n        return total_duration\n\n    if total_items == 0:\n        return 0\n\n    num_batches = math.ceil(total_items / batch_size)\n    sleep_count = max(0, num_batches - 1)\n    total_sleep_time = sleep_count * 61\n    \n    net_time = total_duration - total_sleep_time\n    return max(0, net_time)",
            "start_line": 22,
            "end_line": 37,
            "context": {
              "calls": [
                "backend/main.py::ceil",
                "backend/main.py::max",
                "backend/main.py::startswith"
              ],
              "called_by": [
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 250
                },
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 281
                }
              ]
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "backend.main.main_workflow",
            "name": "main_workflow",
            "args": [
              "input",
              "api_keys",
              "model_names",
              "status_callback"
            ],
            "docstring": null,
            "source_code": "def main_workflow(input, api_keys: dict, model_names: dict, status_callback=None):\n    \n    def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)\n\n    update_status(\"🔍 Analysiere Input...\")\n    \n    user_input = input\n    \n    # API Key & Ollama Base URL aus Frontend holen\n    gemini_api_key = api_keys.get(\"gemini\")\n    openai_api_key = api_keys.get(\"gpt\")\n    ollama_base_url = api_keys.get(\"ollama\")\n\n    if model_names[\"helper\"].startswith(\"gpt-\"):\n        helper_api_key = openai_api_key\n    elif model_names[\"helper\"].startswith(\"gemini-\"):\n        helper_api_key = gemini_api_key\n    if model_names[\"main\"].startswith(\"gpt-\"):\n        api_key = openai_api_key\n    elif model_names[\"main\"].startswith(\"gemini-\"):\n        api_key = gemini_api_key\n\n    # Standardeinstellungen für Modelle\n    helper_model = model_names.get(\"helper\", \"gpt-5-mini\")\n    main_model = model_names.get(\"main\", \"gpt-5.1\")\n\n    # Error Handling für fehlende API Keys\n    if not gemini_api_key and \"gemini\" in helper_model:\n        raise ValueError(\"Gemini API Key was not provided in api_keys dictionary.\")\n\n    # 2. URL Extraktion\n    repo_url = None\n    url_pattern = r\"https?://[^\\s]+\"\n    match = re.search(url_pattern, user_input)\n\n    if match:\n        repo_url = match.group(0)\n        logging.info(f\"Extracted repository URL: {repo_url}\")\n    else:\n        raise ValueError(\"Could not find a valid URL in the provided input.\")\n    \n\n    update_status(f\"⬇️ Klone Repository: {repo_url} ...\")\n    \n    repo_files = []\n    try: \n        with GitRepository(repo_url) as repo:\n            repo_files = repo.get_all_files()\n            logging.info(f\"Total files retrieved: {len(repo_files)}\")\n    except Exception as e:\n        logging.error(f\"Error cloning repository: {e}\")\n        raise        \n\n    # Extrahiere Basic Infos\n    update_status(\"ℹ️ Extrahiere Basis-Informationen...\")\n    try:\n        info_extractor = ProjektInfoExtractor()\n        basic_project_info = info_extractor.extrahiere_info(dateien=repo_files, repo_url=repo_url)\n        logging.info(\"Basic project info extracted\")\n    except Exception as e:\n        logging.error(f\"Error extracting basic project info: {e}\")\n        \n\n    # Erstelle Repository Dateibaum\n    update_status(\"🌲 Erstelle Repository Dateibaum...\")\n    try:\n        repo_file_tree = repo.get_file_tree()\n        logging.info(\"Repository file tree constructed\")\n    except Exception as e:\n        logging.error(f\"Error constructing repository file tree: {e}\")\n        \n\n    # Erstelle AST Schema\n    update_status(\"🌳 Erstelle Abstract Syntax Tree (AST)...\")\n    try:        \n        ast_analyzer = ASTAnalyzer()   \n        ast_schema = ast_analyzer.analyze_repository(files=repo_files)\n        print(json.dumps(ast_schema, indent=2))\n        logging.info(\"AST schema created\")\n    except Exception as e:\n        logging.error(f\"Error retrieving repository files: {e}\")\n        raise\n\n    # 4. HelperLLM Input Vorbereitung\n    update_status(\"⚙️ Bereite Daten für Helper LLM vor...\")\n    \n    helper_llm_function_input = []\n    helper_llm_class_input = []\n\n    try:\n        for filename, file_data in ast_schema['files'].items():\n            ast_nodes = file_data.get('ast_nodes', {})\n            imports = ast_nodes.get('imports', [])\n            functions = ast_nodes.get('functions', [])\n            classes = ast_nodes.get('classes', [])\n\n            for function in functions:\n                context = function.get('context', {})\n                function_context = FunctionContextInput(\n                    calls = context.get('calls', []),\n                    called_by = context.get('called_by', [])\n                )\n                \n                function_input = FunctionAnalysisInput(\n                    mode = function.get('mode', 'function_analysis'),\n                    identifier = function.get('identifier'),\n                    source_code = function.get('source_code'),\n                    imports = imports,\n                    context = function_context\n                )\n                \n                helper_llm_function_input.append(function_input)\n    except Exception as e:\n        logging.error(f\"Error preparing function inputs for Helper LLM: {e}\")\n        raise\n    \n    try:\n        for _class in classes:\n            context = _class.get('context', {})\n            class_context = ClassContextInput(\n                dependencies = context.get('dependencies', []),\n                instantiated_by = context.get('instantiated_by', []),\n                method_context = context.get('method_context', [])\n            )\n\n            class_input = ClassAnalysisInput(\n                mode = _class.get('mode', 'class_analysis'),\n                identifier =_class.get('identifier'),\n                source_code = _class.get('source_code'), \n                imports = imports, \n                context = class_context\n            )\n            \n            helper_llm_class_input.append(class_input)\n    except Exception as e:\n        logging.error(f\"Error preparing class inputs for Helper LLM: {e}\")\n        raise\n    \n    logging.info(f\"Functions: {len(helper_llm_function_input)}, Classes: {len(helper_llm_class_input)}\")\n    \n    # Initialisiere HelperLLM\n    function_prompt_file = 'SystemPrompts/SystemPromptFunctionHelperLLM.txt'\n    class_prompt_file = 'SystemPrompts/SystemPromptClassHelperLLM.txt'\n    \n    llm_helper = LLMHelper(\n        api_key=helper_api_key, \n        function_prompt_path=function_prompt_file, \n        class_prompt_path=class_prompt_file,\n        model_name=helper_model,\n        ollama_base_url=ollama_base_url,\n    )\n    \n    if ollama_base_url:\n        os.environ[\"OLLAMA_BASE_URL\"] = ollama_base_url\n\n    # Initialisiere Ergebniscontainer\n    analysis_results = {}\n    function_analysis_results = []\n    class_analysis_results = []\n\n    # Call HelperLLM für Funktionen\n    update_status(f\"🤖 Helper LLM: Analysiere {len(helper_llm_function_input)} Funktionen ({helper_model})...\")\n\n    try:\n        net_time_func = 0\n        if len(helper_llm_function_input) > 0:\n\n            logging.info(\"\\n--- Generating documentation for Functions ---\")\n            t_start_func = time.time()    \n            function_analysis_results = llm_helper.generate_for_functions(helper_llm_function_input)    \n            t_end_func = time.time()\n            net_time_func = calculate_net_time(t_start_func, t_end_func, len(helper_llm_function_input), llm_helper.batch_size, helper_model)\n\n        if len(function_analysis_results) != 0:\n            for doc in function_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"functions\" not in analysis_results:\n                        analysis_results[\"functions\"] = {}\n                    analysis_results[\"functions\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(f\"Failed to generate doc for a function\") \n    except Exception as e:\n        logging.error(f\"Error during Helper LLM function analysis: {e}\")\n        raise\n    \n\n    # Call HelperLLM für Klassen\n    try:\n        net_time_class = 0\n        if len(helper_llm_class_input) > 0:\n            # Rate Limit Sleep für Gemini Modelle\n            if llm_helper.model_name.startswith(\"gemini-\") & (len(helper_llm_function_input) > 0):\n                time.sleep(61)\n                update_status(\"💤 Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n            \n            update_status(f\"🤖 Helper LLM: Analysiere {len(helper_llm_class_input)} Klassen ({helper_model})...\")\n            \n            logging.info(\"\\n--- Generating documentation for Classes ---\")\n            t_start_class = time.time()\n            class_analysis_results = llm_helper.generate_for_classes(helper_llm_class_input)\n            t_end_class = time.time()\n            net_time_class = calculate_net_time(t_start_class, t_end_class, len(helper_llm_class_input), llm_helper.batch_size, helper_model)\n\n        if len(class_analysis_results) != 0:\n            for doc in class_analysis_results:\n                if doc:\n                    logging.info(f\"Successfully generated doc for: {doc.identifier}\")\n                    if \"classes\" not in analysis_results:\n                        analysis_results[\"classes\"] = {}\n                    analysis_results[\"classes\"][doc.identifier] = doc.model_dump() \n                else:\n                    logging.warning(f\"Failed to generate doc for a class\")\n    except Exception as e:\n        logging.error(f\"Error during Helper LLM class analysis: {e}\")\n        raise\n\n    total_helper_time = net_time_func + net_time_class\n\n    # MainLLM Input Vorbereitung\n    main_llm_input = {\n        \"basic_info\": basic_project_info,\n        \"file_tree\": repo_file_tree,\n        \"ast_schema\": ast_schema,\n        \"analysis_results\": analysis_results\n    }\n    main_llm_input_json = json.dumps(main_llm_input, indent=2)\n    \n    # MainLLM Ausführung\n    main_llm = MainLLM(\n        api_key=api_key, \n        prompt_file_path=\"SystemPrompts/SystemPromptMainLLM.txt\",\n        model_name=main_model,\n        ollama_base_url=ollama_base_url,\n    )\n\n\n    # RPM Limit Sleep für Gemini Modelle\n    if llm_helper.model_name == main_llm.model_name and main_llm.model_name.startswith(\"gemini-\"):\n        time.sleep(61)\n        update_status(\"💤 Wartezeit eingelegt, um Rate Limits einzuhalten...\")\n\n    # Call MainLLM für finalen Report\n    update_status(f\"🧠 Main LLM: Generiere finalen Report ({main_model})...\")\n    try:\n        total_main_time = 0\n        logging.info(\"\\n--- Generating Final Report ---\")\n        t_start_main = time.time()\n        final_report = main_llm.call_llm(main_llm_input_json)\n        #for token in main_llm.stream_llm(main_llm_input_json):\n        #    full_response += token    \n        #    final_report = full_response\n        t_end_main = time.time()\n        total_main_time = t_end_main - t_start_main\n    except Exception as e:\n        logging.error(f\"Error during Main LLM final report generation: {e}\")\n        raise\n\n    \n    # Speichern\n    output_dir = \"result\"\n    os.makedirs(output_dir, exist_ok=True)  \n    total_active_time = total_helper_time + total_main_time\n    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M-%S\")\n    \n    report_filename = f\"report_{timestamp}_Helper_{llm_helper.model_name}_MainLLM_{main_llm.model_name}.md\"\n    report_filepath = os.path.join(output_dir, report_filename)\n    \n    if final_report:\n        with open(report_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(final_report)\n        logging.info(f\"Final report saved to '{report_filepath}'.\")\n    else:\n        final_report = \"Error: Report generation failed or returned empty.\"\n\n    metrics = {\n        \"helper_time\": round(total_helper_time, 2),\n        \"main_time\": round(total_main_time, 2),\n        \"total_time\": round(total_active_time, 2),\n        \"helper_model\": helper_model,\n        \"main_model\": main_model\n    }\n\n    return {\n        \"report\": final_report,\n        \"metrics\": metrics\n    }",
            "start_line": 40,
            "end_line": 329,
            "context": {
              "calls": [],
              "called_by": [
                {
                  "file": "Frontend.py",
                  "function": "frontend.Frontend",
                  "mode": "module",
                  "line": 308
                },
                {
                  "file": "main.py",
                  "function": "backend.main",
                  "mode": "module",
                  "line": 369
                }
              ]
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "backend.main.update_status",
            "name": "update_status",
            "args": [
              "msg"
            ],
            "docstring": null,
            "source_code": "def update_status(msg):\n        if status_callback:\n            status_callback(msg)\n        logging.info(msg)",
            "start_line": 42,
            "end_line": 45,
            "context": {
              "calls": [
                "backend/main.py::info",
                "backend/main.py::status_callback"
              ],
              "called_by": [
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 48
                },
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 86
                },
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 113
                },
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 118
                },
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 122
                },
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 132
                },
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 137
                },
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 163
                },
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 240
                },
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 273
                },
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 275
                },
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 319
                },
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 322
                }
              ]
            }
          }
        ],
        "classes": []
      }
    },
    "backend/relationship_analyzer.py": {
      "ast_nodes": {
        "imports": [
          "ast",
          "os",
          "logging",
          "collections.defaultdict"
        ],
        "functions": [
          {
            "mode": "function_analysis",
            "identifier": "backend.relationship_analyzer.path_to_module",
            "name": "path_to_module",
            "args": [
              "filepath",
              "project_root"
            ],
            "docstring": "Wandelt einen Dateipfad in einen Python-Modulpfad um.",
            "source_code": "def path_to_module(filepath, project_root):\n    \"\"\"Wandelt einen Dateipfad in einen Python-Modulpfad um.\"\"\"\n    rel_path = os.path.relpath(filepath, project_root)\n    if rel_path.endswith('.py'):\n        rel_path = rel_path[:-3]\n    module_path = rel_path.replace(os.path.sep, '.')\n    if module_path.endswith('.__init__'):\n        return module_path[:-9]\n    return module_path",
            "start_line": 6,
            "end_line": 14,
            "context": {
              "calls": [
                "backend/relationship_analyzer.py::endswith",
                "backend/relationship_analyzer.py::relpath",
                "backend/relationship_analyzer.py::replace"
              ],
              "called_by": [
                {
                  "file": "relationship_analyzer.py",
                  "function": "_collect_definitions",
                  "mode": "method",
                  "line": 68
                },
                {
                  "file": "relationship_analyzer.py",
                  "function": "__init__",
                  "mode": "method",
                  "line": 147
                }
              ]
            }
          }
        ],
        "classes": [
          {
            "mode": "class_analysis",
            "identifier": "backend.relationship_analyzer.ProjectAnalyzer",
            "name": "ProjectAnalyzer",
            "docstring": null,
            "source_code": "class ProjectAnalyzer:\n    \n    def __init__(self, project_root):\n        self.project_root = os.path.abspath(project_root)\n        self.definitions = {}\n        self.call_graph = defaultdict(list)\n\n    def analyze(self):\n        py_files = self._find_py_files()\n        for filepath in py_files:\n            self._collect_definitions(filepath)\n        for filepath in py_files:\n            self._resolve_calls(filepath)\n        return self.get_formatted_results()\n\n    def _find_py_files(self):\n        py_files = []\n        for root, _, files in os.walk(self.project_root): # _ wird ignoriert aber für Struktur benötigt\n            for file in files:\n                if file.endswith(\".py\"):\n                    py_files.append(os.path.join(root, file))\n        return py_files\n\n    def _collect_definitions(self, filepath):\n        try:\n            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n                source = f.read()\n                tree = ast.parse(source, filename=filepath)\n                module_path = path_to_module(filepath, self.project_root)\n                \n                for node in ast.walk(tree):\n                    if isinstance(node, ast.FunctionDef):\n                        parent = self._get_parent(tree, node)\n                        if isinstance(parent, ast.ClassDef):\n                            path_name = f\"{module_path}.{parent.name}.{node.name}\"\n                            def_type = 'method'\n                        else:\n                            path_name = f\"{module_path}.{node.name}\"\n                            def_type = 'function'\n                        self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': def_type}\n                    elif isinstance(node, ast.ClassDef):\n                        path_name = f\"{module_path}.{node.name}\"\n                        self.definitions[path_name] = {'file': filepath, 'line': node.lineno, 'type': 'class'}\n        except Exception as e:\n            logging.error(f\"Error collecting definitions in {filepath}: {e}\")\n            \n    def _get_parent(self, tree, node):\n        for parent in ast.walk(tree):\n            for child in ast.iter_child_nodes(parent):\n                if child is node:\n                    return parent\n        return None\n\n    def _resolve_calls(self, filepath):\n        try:\n            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n                source = f.read()\n                tree = ast.parse(source, filename=filepath)\n                resolver = CallResolverVisitor(filepath, self.project_root, self.definitions)\n                resolver.visit(tree)\n                for callee_pathname, caller_info in resolver.calls.items():\n                    self.call_graph[callee_pathname].extend(caller_info)\n        except Exception as e:\n            logging.error(f\"Error resolving calls in {filepath}: {e}\")\n\n    def get_formatted_results(self):\n        output_list = []\n        \n        for callee_pathname, calls in self.call_graph.items():\n            if callee_pathname in self.definitions:\n                def_info = self.definitions[callee_pathname]\n                \n                definition_dict = {\n                    \"identifier\": callee_pathname,\n                    \"mode\": def_info.get('type', 'unknown'),\n                    \"origin\": os.path.basename(def_info['file']),\n                    \"origin_line\": def_info['line'],\n                    \"called_by\": [] \n                }\n                \n                unique_calls = []\n                for call in calls:\n                    caller_dict = {\n                        \"file\": call['file'],\n                        \"function\": call['caller'],\n                        \"mode\": call.get('caller_type', 'unknown'),\n                        \"line\": call['line']\n                    }\n                    if caller_dict not in unique_calls:\n                        unique_calls.append(caller_dict)\n\n                if unique_calls:\n                    definition_dict[\"called_by\"] = sorted(unique_calls, key=lambda x: (x['file'], x['line']))\n                    output_list.append(definition_dict)\n                    \n        return output_list",
            "start_line": 16,
            "end_line": 111,
            "context": {
              "dependencies": [],
              "instantiated_by": [
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 124
                }
              ],
              "method_context": [
                {
                  "identifier": "backend.relationship_analyzer.ProjectAnalyzer.__init__",
                  "name": "__init__",
                  "calls": [
                    "backend/relationship_analyzer.py::ProjectAnalyzer::abspath",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::defaultdict"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "project_root"
                  ],
                  "docstring": null,
                  "start_line": 18,
                  "end_line": 21
                },
                {
                  "identifier": "backend.relationship_analyzer.ProjectAnalyzer.analyze",
                  "name": "analyze",
                  "calls": [
                    "backend/relationship_analyzer.py::ProjectAnalyzer::_collect_definitions",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::_find_py_files",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::_resolve_calls",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::get_formatted_results"
                  ],
                  "called_by": [
                    {
                      "file": "main.py",
                      "function": "main_workflow",
                      "mode": "function",
                      "line": 125
                    }
                  ],
                  "args": [
                    "self"
                  ],
                  "docstring": null,
                  "start_line": 23,
                  "end_line": 29
                },
                {
                  "identifier": "backend.relationship_analyzer.ProjectAnalyzer._find_py_files",
                  "name": "_find_py_files",
                  "calls": [
                    "backend/relationship_analyzer.py::ProjectAnalyzer::append",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::endswith",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::join",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::walk"
                  ],
                  "called_by": [
                    "backend/relationship_analyzer.py::ProjectAnalyzer::analyze"
                  ],
                  "args": [
                    "self"
                  ],
                  "docstring": null,
                  "start_line": 31,
                  "end_line": 37
                },
                {
                  "identifier": "backend.relationship_analyzer.ProjectAnalyzer._collect_definitions",
                  "name": "_collect_definitions",
                  "calls": [
                    "backend/relationship_analyzer.py::ProjectAnalyzer::_get_parent",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::error",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::isinstance",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::open",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::parse",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::path_to_module",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::read",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::walk"
                  ],
                  "called_by": [
                    "backend/relationship_analyzer.py::ProjectAnalyzer::analyze"
                  ],
                  "args": [
                    "self",
                    "filepath"
                  ],
                  "docstring": null,
                  "start_line": 39,
                  "end_line": 60
                },
                {
                  "identifier": "backend.relationship_analyzer.ProjectAnalyzer._get_parent",
                  "name": "_get_parent",
                  "calls": [
                    "backend/relationship_analyzer.py::ProjectAnalyzer::iter_child_nodes",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::walk"
                  ],
                  "called_by": [
                    "backend/relationship_analyzer.py::ProjectAnalyzer::_collect_definitions"
                  ],
                  "args": [
                    "self",
                    "tree",
                    "node"
                  ],
                  "docstring": null,
                  "start_line": 62,
                  "end_line": 67
                },
                {
                  "identifier": "backend.relationship_analyzer.ProjectAnalyzer._resolve_calls",
                  "name": "_resolve_calls",
                  "calls": [
                    "backend/relationship_analyzer.py::ProjectAnalyzer::CallResolverVisitor",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::error",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::extend",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::items",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::open",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::parse",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::read",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::visit"
                  ],
                  "called_by": [
                    "backend/relationship_analyzer.py::ProjectAnalyzer::analyze"
                  ],
                  "args": [
                    "self",
                    "filepath"
                  ],
                  "docstring": null,
                  "start_line": 69,
                  "end_line": 79
                },
                {
                  "identifier": "backend.relationship_analyzer.ProjectAnalyzer.get_formatted_results",
                  "name": "get_formatted_results",
                  "calls": [
                    "backend/relationship_analyzer.py::ProjectAnalyzer::append",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::basename",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::get",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::items",
                    "backend/relationship_analyzer.py::ProjectAnalyzer::sorted"
                  ],
                  "called_by": [
                    "backend/relationship_analyzer.py::ProjectAnalyzer::analyze"
                  ],
                  "args": [
                    "self"
                  ],
                  "docstring": null,
                  "start_line": 81,
                  "end_line": 111
                }
              ]
            }
          },
          {
            "mode": "class_analysis",
            "identifier": "backend.relationship_analyzer.CallResolverVisitor",
            "name": "CallResolverVisitor",
            "docstring": null,
            "source_code": "class CallResolverVisitor(ast.NodeVisitor):\n    def __init__(self, filepath, project_root, definitions):\n        self.filepath = filepath\n        self.module_path = path_to_module(filepath, project_root)\n        self.definitions = definitions\n        self.scope = {}\n        self.instance_types = {}\n        self.current_caller_name = self.module_path\n        self.current_class_name = None\n        self.calls = defaultdict(list)\n\n    def visit_ClassDef(self, node):\n        old_class_name, self.current_class_name = self.current_class_name, node.name\n        self.generic_visit(node)\n        self.current_class_name = old_class_name\n\n    def visit_FunctionDef(self, node):\n        old_caller_name, self.current_caller_name = self.current_caller_name, node.name\n        self.generic_visit(node)\n        self.current_caller_name = old_caller_name\n\n    def visit_Call(self, node):\n        callee_pathname = self._resolve_call_qname(node.func)\n        if callee_pathname and callee_pathname in self.definitions:\n            if self.current_caller_name == self.module_path:\n                caller_type = 'module'\n            elif self.current_class_name:\n                caller_type = 'method'\n            else:\n                caller_type = 'function'\n            \n            caller_info = {\n                'file': os.path.basename(self.filepath),\n                'line': node.lineno,\n                'caller': self.current_caller_name,\n                'caller_type': caller_type \n            }\n            self.calls[callee_pathname].append(caller_info)\n        self.generic_visit(node)\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.scope[alias.asname or alias.name] = alias.name\n        self.generic_visit(node)\n        \n    def visit_ImportFrom(self, node):\n        module = node.module or ''\n        for alias in node.names:\n            name = alias.asname or alias.name\n            if node.level > 0:\n                base = self.module_path.split('.')\n                prefix = '.'.join(base[:-node.level])\n                full_module_path = f\"{prefix}.{module}\" if module else prefix\n            else:\n                full_module_path = module\n            self.scope[name] = f\"{full_module_path}.{alias.name}\"\n        self.generic_visit(node)\n\n    def visit_Assign(self, node):\n        if isinstance(node.value, ast.Call) and isinstance(node.value.func, ast.Name):\n            class_name = node.value.func.id\n            if class_name in self.scope:\n                qualified_class_name = self.scope[class_name]\n                if qualified_class_name in self.definitions:\n                    for target in node.targets:\n                        if isinstance(target, ast.Name):\n                            self.instance_types[target.id] = qualified_class_name\n        self.generic_visit(node)\n\n    def _resolve_call_qname(self, func_node):\n        if isinstance(func_node, ast.Name):\n            name = func_node.id\n            if name in self.scope:\n                return self.scope[name]\n            local_pathname = f\"{self.module_path}.{name}\"\n            if local_pathname in self.definitions:\n                return local_pathname\n        elif isinstance(func_node, ast.Attribute) and isinstance(func_node.value, ast.Name):\n            var_name = func_node.value.id\n            method_name = func_node.attr\n            if var_name in self.instance_types:\n                class_pathname = self.instance_types[var_name]\n                return f\"{class_pathname}.{method_name}\"\n            if var_name in self.scope:\n                module_pathname = self.scope[var_name]\n                return f\"{module_pathname}.{method_name}\"\n        return None",
            "start_line": 113,
            "end_line": 199,
            "context": {
              "dependencies": [],
              "instantiated_by": [
                {
                  "file": "relationship_analyzer.py",
                  "function": "_resolve_calls",
                  "mode": "method",
                  "line": 103
                }
              ],
              "method_context": [
                {
                  "identifier": "backend.relationship_analyzer.CallResolverVisitor.__init__",
                  "name": "__init__",
                  "calls": [
                    "backend/relationship_analyzer.py::CallResolverVisitor::defaultdict",
                    "backend/relationship_analyzer.py::CallResolverVisitor::path_to_module"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "filepath",
                    "project_root",
                    "definitions"
                  ],
                  "docstring": null,
                  "start_line": 114,
                  "end_line": 122
                },
                {
                  "identifier": "backend.relationship_analyzer.CallResolverVisitor.visit_ClassDef",
                  "name": "visit_ClassDef",
                  "calls": [
                    "backend/relationship_analyzer.py::CallResolverVisitor::generic_visit"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "node"
                  ],
                  "docstring": null,
                  "start_line": 124,
                  "end_line": 127
                },
                {
                  "identifier": "backend.relationship_analyzer.CallResolverVisitor.visit_FunctionDef",
                  "name": "visit_FunctionDef",
                  "calls": [
                    "backend/relationship_analyzer.py::CallResolverVisitor::generic_visit"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "node"
                  ],
                  "docstring": null,
                  "start_line": 129,
                  "end_line": 132
                },
                {
                  "identifier": "backend.relationship_analyzer.CallResolverVisitor.visit_Call",
                  "name": "visit_Call",
                  "calls": [
                    "backend/relationship_analyzer.py::CallResolverVisitor::_resolve_call_qname",
                    "backend/relationship_analyzer.py::CallResolverVisitor::append",
                    "backend/relationship_analyzer.py::CallResolverVisitor::basename",
                    "backend/relationship_analyzer.py::CallResolverVisitor::generic_visit"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "node"
                  ],
                  "docstring": null,
                  "start_line": 134,
                  "end_line": 151
                },
                {
                  "identifier": "backend.relationship_analyzer.CallResolverVisitor.visit_Import",
                  "name": "visit_Import",
                  "calls": [
                    "backend/relationship_analyzer.py::CallResolverVisitor::generic_visit"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "node"
                  ],
                  "docstring": null,
                  "start_line": 153,
                  "end_line": 156
                },
                {
                  "identifier": "backend.relationship_analyzer.CallResolverVisitor.visit_ImportFrom",
                  "name": "visit_ImportFrom",
                  "calls": [
                    "backend/relationship_analyzer.py::CallResolverVisitor::generic_visit",
                    "backend/relationship_analyzer.py::CallResolverVisitor::join",
                    "backend/relationship_analyzer.py::CallResolverVisitor::split"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "node"
                  ],
                  "docstring": null,
                  "start_line": 158,
                  "end_line": 169
                },
                {
                  "identifier": "backend.relationship_analyzer.CallResolverVisitor.visit_Assign",
                  "name": "visit_Assign",
                  "calls": [
                    "backend/relationship_analyzer.py::CallResolverVisitor::generic_visit",
                    "backend/relationship_analyzer.py::CallResolverVisitor::isinstance"
                  ],
                  "called_by": [],
                  "args": [
                    "self",
                    "node"
                  ],
                  "docstring": null,
                  "start_line": 171,
                  "end_line": 180
                },
                {
                  "identifier": "backend.relationship_analyzer.CallResolverVisitor._resolve_call_qname",
                  "name": "_resolve_call_qname",
                  "calls": [
                    "backend/relationship_analyzer.py::CallResolverVisitor::isinstance"
                  ],
                  "called_by": [
                    "backend/relationship_analyzer.py::CallResolverVisitor::visit_Call"
                  ],
                  "args": [
                    "self",
                    "func_node"
                  ],
                  "docstring": null,
                  "start_line": 182,
                  "end_line": 199
                }
              ]
            }
          }
        ]
      }
    },
    "database/db.py": {
      "ast_nodes": {
        "imports": [
          "datetime.datetime",
          "pymongo.MongoClient",
          "dotenv.load_dotenv",
          "streamlit_authenticator",
          "cryptography.fernet.Fernet",
          "streamlit",
          "os"
        ],
        "functions": [
          {
            "mode": "function_analysis",
            "identifier": "database.db.encrypt_text",
            "name": "encrypt_text",
            "args": [
              "text"
            ],
            "docstring": null,
            "source_code": "def encrypt_text(text: str) -> str:\n    if not text or not cipher_suite: return text\n    return cipher_suite.encrypt(text.encode()).decode()",
            "start_line": 31,
            "end_line": 33,
            "context": {
              "calls": [
                "database/db.py::decode",
                "database/db.py::encode",
                "database/db.py::encrypt"
              ],
              "called_by": [
                {
                  "file": "db.py",
                  "function": "update_gemini_key",
                  "mode": "function",
                  "line": 71
                }
              ]
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "database.db.decrypt_text",
            "name": "decrypt_text",
            "args": [
              "text"
            ],
            "docstring": null,
            "source_code": "def decrypt_text(text: str) -> str:\n    if not text or not cipher_suite: return text\n    try:\n        return cipher_suite.decrypt(text.encode()).decode()\n    except Exception:\n        \n        return text",
            "start_line": 35,
            "end_line": 41,
            "context": {
              "calls": [
                "database/db.py::decode",
                "database/db.py::decrypt",
                "database/db.py::encode"
              ],
              "called_by": [
                {
                  "file": "db.py",
                  "function": "get_decrypted_api_keys",
                  "mode": "function",
                  "line": 100
                }
              ]
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "database.db.insert_user",
            "name": "insert_user",
            "args": [
              "username",
              "name",
              "password"
            ],
            "docstring": null,
            "source_code": "def insert_user(username: str, name: str,  password: str):\n    # Insert a new user into the database\n    user = {\n            \"_id\": username,\n            \"name\":name, \"hashed_password\": stauth.hasher.hash(password),\n            \"gemini_api_key\": \"\", \n            \"ollama_base_url\": \"\"\n    }\n    result = dbusers.insert_one(user)\n    return result.inserted_id",
            "start_line": 47,
            "end_line": 56,
            "context": {
              "calls": [
                "database/db.py::hash",
                "database/db.py::insert_one"
              ],
              "called_by": []
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "database.db.fetch_all_users",
            "name": "fetch_all_users",
            "args": [],
            "docstring": null,
            "source_code": "def fetch_all_users():\n    # Fetch all users from the database\n    users = list(dbusers.find())\n    return users",
            "start_line": 59,
            "end_line": 62,
            "context": {
              "calls": [
                "database/db.py::find",
                "database/db.py::list"
              ],
              "called_by": [
                {
                  "file": "Frontend.py",
                  "function": "frontend.Frontend",
                  "mode": "module",
                  "line": 159
                }
              ]
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "database.db.fetch_user",
            "name": "fetch_user",
            "args": [
              "username"
            ],
            "docstring": null,
            "source_code": "def fetch_user(username: str):\n    # Fetch a single user by username\n    user = dbusers.find_one({\"_id\": username})\n    return user",
            "start_line": 64,
            "end_line": 67,
            "context": {
              "calls": [
                "database/db.py::find_one"
              ],
              "called_by": []
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "database.db.update_gemini_key",
            "name": "update_gemini_key",
            "args": [
              "username",
              "gemini_api_key"
            ],
            "docstring": null,
            "source_code": "def update_gemini_key(username: str, gemini_api_key: str ):\n    # Update the Gemini API key for a user\n    encrypted_key = encrypt_text(gemini_api_key)\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"gemini_api_key\": encrypted_key}})\n    return result.modified_count",
            "start_line": 69,
            "end_line": 73,
            "context": {
              "calls": [
                "database/db.py::encrypt_text",
                "database/db.py::update_one"
              ],
              "called_by": [
                {
                  "file": "Frontend.py",
                  "function": "frontend.Frontend",
                  "mode": "module",
                  "line": 249
                }
              ]
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "database.db.update_ollama_url",
            "name": "update_ollama_url",
            "args": [
              "username",
              "ollama_base_url"
            ],
            "docstring": null,
            "source_code": "def update_ollama_url(username: str, ollama_base_url: str ):\n    # Update the Ollama Base URL for a user\n    result = dbusers.update_one({\"_id\": username}, {\"$set\": {\"ollama_base_url\": ollama_base_url}})\n    return result.modified_count",
            "start_line": 75,
            "end_line": 78,
            "context": {
              "calls": [
                "database/db.py::update_one"
              ],
              "called_by": [
                {
                  "file": "Frontend.py",
                  "function": "frontend.Frontend",
                  "mode": "module",
                  "line": 268
                }
              ]
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "database.db.fetch_gemini_key",
            "name": "fetch_gemini_key",
            "args": [
              "username"
            ],
            "docstring": null,
            "source_code": "def fetch_gemini_key(username: str):\n    # Fetch the Gemini API key for a user\n    user = dbusers.find_one({\"_id\": username}, {\"gemini_api_key\": 1, \"_id\": 0})\n    return user.get(\"gemini_api_key\")",
            "start_line": 80,
            "end_line": 83,
            "context": {
              "calls": [
                "database/db.py::find_one",
                "database/db.py::get"
              ],
              "called_by": []
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "database.db.fetch_ollama_url",
            "name": "fetch_ollama_url",
            "args": [
              "username"
            ],
            "docstring": null,
            "source_code": "def fetch_ollama_url(username: str):\n    # Fetch the Ollama Base URL for a user\n    user = dbusers.find_one({\"_id\": username}, {\"ollama_base_url\": 1, \"_id\": 0})\n    return user.get(\"ollama_base_url\")",
            "start_line": 85,
            "end_line": 88,
            "context": {
              "calls": [
                "database/db.py::find_one",
                "database/db.py::get"
              ],
              "called_by": []
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "database.db.delete_user",
            "name": "delete_user",
            "args": [
              "username"
            ],
            "docstring": null,
            "source_code": "def delete_user(username: str):\n    # Delete a user from the database\n    result = dbusers.delete_one({\"_id\": username})\n    return result.deleted_count",
            "start_line": 90,
            "end_line": 93,
            "context": {
              "calls": [
                "database/db.py::delete_one"
              ],
              "called_by": []
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "database.db.get_decrypted_api_keys",
            "name": "get_decrypted_api_keys",
            "args": [
              "username"
            ],
            "docstring": "Holt User und entschlüsselt die Keys direkt",
            "source_code": "def get_decrypted_api_keys(username: str):\n    \"\"\"Holt User und entschlüsselt die Keys direkt\"\"\"\n    user = dbusers.find_one({\"_id\": username})\n    if not user: return None, None\n    \n    gemini_plain = decrypt_text(user.get(\"gemini_api_key\", \"\"))\n    ollama_plain = user.get(\"ollama_base_url\", \"\")\n    return gemini_plain, ollama_plain",
            "start_line": 95,
            "end_line": 102,
            "context": {
              "calls": [
                "database/db.py::decrypt_text",
                "database/db.py::find_one",
                "database/db.py::get"
              ],
              "called_by": [
                {
                  "file": "Frontend.py",
                  "function": "frontend.Frontend",
                  "mode": "module",
                  "line": 233
                },
                {
                  "file": "Frontend.py",
                  "function": "frontend.Frontend",
                  "mode": "module",
                  "line": 295
                }
              ]
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "database.db.insert_exchange",
            "name": "insert_exchange",
            "args": [
              "question",
              "answer",
              "feedback",
              "username",
              "chat_name",
              "helper_used",
              "main_used",
              "total_time",
              "helper_time",
              "main_time"
            ],
            "docstring": null,
            "source_code": "def insert_exchange(question: str, answer: str, feedback: str, username: str, chat_name: str, helper_used: str=\"\", main_used: str=\"\", total_time: str=\"\", helper_time: str=\"\", main_time: str=\"\"):\n    # Insert a new exchange into the database\n    exchange = {\n        \"question\": question,\n        \"answer\": answer,\n        \"feedback\": feedback,\n        \"feedback_message\": \"\",\n        \"chat_name\": chat_name,\n        \"username\": username,\n        \"helper_used\": helper_used,\n        \"main_used\": main_used,\n        \"total_time\": total_time,\n        \"helper_time\": helper_time,\n        \"main_time\": main_time,\n        \"created_at\":  datetime.now()\n    }\n    result = dbexchanges.insert_one(exchange)\n    return result.inserted_id",
            "start_line": 108,
            "end_line": 125,
            "context": {
              "calls": [
                "database/db.py::insert_one",
                "database/db.py::now"
              ],
              "called_by": [
                {
                  "file": "Frontend.py",
                  "function": "frontend.Frontend",
                  "mode": "module",
                  "line": 336
                }
              ]
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "database.db.fetch_exchanges_by_user",
            "name": "fetch_exchanges_by_user",
            "args": [
              "username"
            ],
            "docstring": null,
            "source_code": "def fetch_exchanges_by_user(username: str):\n    exchanges = list(dbexchanges.find({\"username\": username}))\n    return exchanges",
            "start_line": 127,
            "end_line": 129,
            "context": {
              "calls": [
                "database/db.py::find",
                "database/db.py::list"
              ],
              "called_by": [
                {
                  "file": "Frontend.py",
                  "function": "load_data_from_db",
                  "mode": "function",
                  "line": 29
                }
              ]
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "database.db.fetch_exchanges_by_chat",
            "name": "fetch_exchanges_by_chat",
            "args": [
              "username",
              "chat_name"
            ],
            "docstring": null,
            "source_code": "def fetch_exchanges_by_chat(username: str, chat_name: str):\n    exchanges = list(dbexchanges.find({\"username\": username, \"chat_name\": chat_name}))\n    return exchanges",
            "start_line": 131,
            "end_line": 133,
            "context": {
              "calls": [
                "database/db.py::find",
                "database/db.py::list"
              ],
              "called_by": []
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "database.db.update_exchange_feedback",
            "name": "update_exchange_feedback",
            "args": [
              "exchange_id",
              "feedback"
            ],
            "docstring": null,
            "source_code": "def update_exchange_feedback(exchange_id, feedback: int):\n    # Update the feedback for a specific exchange\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback\": feedback}})\n    return result.modified_count",
            "start_line": 135,
            "end_line": 138,
            "context": {
              "calls": [
                "database/db.py::update_one"
              ],
              "called_by": [
                {
                  "file": "Frontend.py",
                  "function": "handle_feedback_change",
                  "mode": "function",
                  "line": 53
                }
              ]
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "database.db.update_exchange_feedback_message",
            "name": "update_exchange_feedback_message",
            "args": [
              "exchange_id",
              "feedback_message"
            ],
            "docstring": null,
            "source_code": "def update_exchange_feedback_message(exchange_id, feedback_message: str):\n    # Update the feedback message for a specific exchange\n    result = dbexchanges.update_one({\"_id\": exchange_id}, {\"$set\": {\"feedback_message\": feedback_message}})\n    return result.modified_count",
            "start_line": 140,
            "end_line": 143,
            "context": {
              "calls": [
                "database/db.py::update_one"
              ],
              "called_by": [
                {
                  "file": "Frontend.py",
                  "function": "render_exchange",
                  "mode": "function",
                  "line": 129
                }
              ]
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "database.db.delete_chats_by_user",
            "name": "delete_chats_by_user",
            "args": [
              "username",
              "chat_name"
            ],
            "docstring": null,
            "source_code": "def delete_chats_by_user(username: str, chat_name: str):\n    # Delete all exchanges for a specific user and chat\n    result = dbexchanges.delete_many({\"username\": username, \"chat_name\": chat_name})\n    return result.deleted_count",
            "start_line": 145,
            "end_line": 148,
            "context": {
              "calls": [
                "database/db.py::delete_many"
              ],
              "called_by": [
                {
                  "file": "Frontend.py",
                  "function": "handle_delete_chat",
                  "mode": "function",
                  "line": 68
                }
              ]
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "database.db.delete_exchange_by_id",
            "name": "delete_exchange_by_id",
            "args": [
              "exchange_id"
            ],
            "docstring": null,
            "source_code": "def delete_exchange_by_id(exchange_id: str):\n    # Delete a specific exchange by its ID\n    result = dbexchanges.delete_one({\"_id\": exchange_id})\n    return result.deleted_count",
            "start_line": 150,
            "end_line": 153,
            "context": {
              "calls": [
                "database/db.py::delete_one"
              ],
              "called_by": [
                {
                  "file": "Frontend.py",
                  "function": "handle_delete_exchange",
                  "mode": "function",
                  "line": 60
                }
              ]
            }
          }
        ],
        "classes": []
      }
    },
    "frontend/Frontend.py": {
      "ast_nodes": {
        "imports": [
          "numpy",
          "datetime.datetime",
          "time",
          "pymongo.MongoClient",
          "dotenv.load_dotenv",
          "os",
          "sys",
          "logging",
          "traceback",
          "re",
          "streamlit_mermaid.st_mermaid",
          "backend.main",
          "database.db",
          "streamlit",
          "streamlit_authenticator"
        ],
        "functions": [
          {
            "mode": "function_analysis",
            "identifier": "frontend.Frontend.load_data_from_db",
            "name": "load_data_from_db",
            "args": [
              "username"
            ],
            "docstring": "Lädt existierende Chats aus der DB in den Session State",
            "source_code": "def load_data_from_db(username: str):\n    \"\"\"Lädt existierende Chats aus der DB in den Session State\"\"\"\n    if \"data_loaded\" not in st.session_state:\n        st.session_state.chats = {}\n        db_exchanges = db.fetch_exchanges_by_user(username)\n        for ex in db_exchanges:\n            c_name = ex.get(\"chat_name\", \"Unbenannt\")\n            if c_name not in st.session_state.chats:\n                st.session_state.chats[c_name] = {\"exchanges\": []}\n            if \"feedback\" not in ex or ex[\"feedback\"] is None:\n                ex[\"feedback\"] = np.nan\n            st.session_state.chats[c_name][\"exchanges\"].append(ex)\n            \n        if not st.session_state.chats:\n            st.session_state.chats[\"Chat 1\"] = {\"exchanges\": []}\n            st.session_state.active_chat = \"Chat 1\"\n        else:\n            first_chat = list(st.session_state.chats.keys())[0]\n            if \"active_chat\" not in st.session_state:\n                st.session_state.active_chat = first_chat\n        \n        st.session_state.data_loaded = True",
            "start_line": 25,
            "end_line": 46,
            "context": {
              "calls": [
                "frontend/Frontend.py::append",
                "frontend/Frontend.py::fetch_exchanges_by_user",
                "frontend/Frontend.py::get",
                "frontend/Frontend.py::keys",
                "frontend/Frontend.py::list"
              ],
              "called_by": [
                {
                  "file": "Frontend.py",
                  "function": "frontend.Frontend",
                  "mode": "module",
                  "line": 193
                }
              ]
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "frontend.Frontend.handle_feedback_change",
            "name": "handle_feedback_change",
            "args": [
              "ex",
              "val"
            ],
            "docstring": "Update Feedback in State und DB",
            "source_code": "def handle_feedback_change(ex, val):\n    \"\"\"Update Feedback in State und DB\"\"\"\n    ex[\"feedback\"] = val\n    # DB Update\n    db.update_exchange_feedback(ex[\"_id\"], val)\n    st.rerun()",
            "start_line": 49,
            "end_line": 54,
            "context": {
              "calls": [
                "frontend/Frontend.py::rerun",
                "frontend/Frontend.py::update_exchange_feedback"
              ],
              "called_by": [
                {
                  "file": "Frontend.py",
                  "function": "render_exchange",
                  "mode": "function",
                  "line": 117
                },
                {
                  "file": "Frontend.py",
                  "function": "render_exchange",
                  "mode": "function",
                  "line": 122
                }
              ]
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "frontend.Frontend.handle_delete_exchange",
            "name": "handle_delete_exchange",
            "args": [
              "chat_name",
              "ex"
            ],
            "docstring": "Löscht Exchange aus State und DB",
            "source_code": "def handle_delete_exchange(chat_name, ex):\n    \"\"\"Löscht Exchange aus State und DB\"\"\"\n    # DB Delete\n    db.delete_exchange_by_id(ex[\"_id\"])\n    # State Delete\n    st.session_state.chats[chat_name][\"exchanges\"].remove(ex)\n    st.rerun()",
            "start_line": 57,
            "end_line": 63,
            "context": {
              "calls": [
                "frontend/Frontend.py::delete_exchange_by_id",
                "frontend/Frontend.py::remove",
                "frontend/Frontend.py::rerun"
              ],
              "called_by": [
                {
                  "file": "Frontend.py",
                  "function": "render_exchange",
                  "mode": "function",
                  "line": 146
                }
              ]
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "frontend.Frontend.handle_delete_chat",
            "name": "handle_delete_chat",
            "args": [
              "username",
              "chat_name"
            ],
            "docstring": "Löscht kompletten Chat",
            "source_code": "def handle_delete_chat(username, chat_name):\n    \"\"\"Löscht kompletten Chat\"\"\"\n    # DB Delete\n    db.delete_chats_by_user(username, chat_name)\n    # State Delete\n    del st.session_state.chats[chat_name]\n    \n    # Neuen aktiven Chat setzen oder Default erstellen\n    if len(st.session_state.chats) > 0:\n        st.session_state.active_chat = list(st.session_state.chats.keys())[0]\n    else:\n        st.session_state.chats[\"Chat 1\"] = {\"exchanges\": []}\n        st.session_state.active_chat = \"Chat 1\"\n    st.rerun()",
            "start_line": 65,
            "end_line": 78,
            "context": {
              "calls": [
                "frontend/Frontend.py::delete_chats_by_user",
                "frontend/Frontend.py::keys",
                "frontend/Frontend.py::len",
                "frontend/Frontend.py::list",
                "frontend/Frontend.py::rerun"
              ],
              "called_by": [
                {
                  "file": "Frontend.py",
                  "function": "frontend.Frontend",
                  "mode": "module",
                  "line": 219
                }
              ]
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "frontend.Frontend.render_text_with_mermaid",
            "name": "render_text_with_mermaid",
            "args": [
              "markdown_text"
            ],
            "docstring": "Splittet den Text bei ```mermaid Blöcken und rendert Diagramme grafisch.",
            "source_code": "def render_text_with_mermaid(markdown_text):\n    \"\"\"\n    Splittet den Text bei ```mermaid Blöcken und rendert Diagramme grafisch.\n    \"\"\"\n    if not markdown_text:\n        return\n\n    # Regex: Findet alles zwischen ```mermaid und ```\n    parts = re.split(r\"```mermaid\\s+(.*?)\\s+```\", markdown_text, flags=re.DOTALL)\n\n    for i, part in enumerate(parts):\n        # Gerade Indizes = Text\n        if i % 2 == 0:\n            if part.strip():\n                st.markdown(part)\n        # Ungerade Indizes = Mermaid Code\n        else:\n            try:\n                st_mermaid(part, key=f\"mermaid_{hash(part)}_{i}\")\n            except Exception:\n                st.code(part, language=\"mermaid\")",
            "start_line": 81,
            "end_line": 101,
            "context": {
              "calls": [
                "frontend/Frontend.py::code",
                "frontend/Frontend.py::enumerate",
                "frontend/Frontend.py::hash",
                "frontend/Frontend.py::markdown",
                "frontend/Frontend.py::split",
                "frontend/Frontend.py::st_mermaid",
                "frontend/Frontend.py::strip"
              ],
              "called_by": [
                {
                  "file": "Frontend.py",
                  "function": "render_exchange",
                  "mode": "function",
                  "line": 156
                },
                {
                  "file": "Frontend.py",
                  "function": "frontend.Frontend",
                  "mode": "module",
                  "line": 333
                }
              ]
            }
          },
          {
            "mode": "function_analysis",
            "identifier": "frontend.Frontend.render_exchange",
            "name": "render_exchange",
            "args": [
              "ex",
              "current_chat_name"
            ],
            "docstring": "Anzeige einer Nachricht mit Toolbar für Feedback, Download, Nachricht und Löschen.",
            "source_code": "def render_exchange(ex, current_chat_name):\n    \"\"\"\n    Anzeige einer Nachricht mit Toolbar für Feedback, Download, Nachricht und Löschen.\n    \"\"\"\n    st.chat_message(\"user\").write(ex[\"question\"])\n    \n    with st.chat_message(\"assistant\"):\n        # Layout: Buttons kompakt links\n        cols = st.columns([3, 3, 3, 3, 3, 15])\n        \n        with cols[0]:\n            type_primary = ex.get(\"feedback\") == 1\n            if st.button(\"👍\", key=f\"up_{ex['_id']}\", type=\"primary\" if type_primary else \"secondary\", help=\"Positiv\"):\n                handle_feedback_change(ex, 1)\n\n        with cols[1]:\n            type_primary = ex.get(\"feedback\") == 0\n            if st.button(\"👎\", key=f\"down_{ex['_id']}\", type=\"primary\" if type_primary else \"secondary\", help=\"Negativ\"):\n                handle_feedback_change(ex, 0)\n\n        with cols[2]:\n            with st.popover(\"💬\", help=\"Feedback schreiben\"):\n                msg = st.text_area(\"Feedback Nachricht:\", value=ex.get(\"feedback_message\", \"\"), key=f\"txt_{ex['_id']}\")\n                if st.button(\"Speichern\", key=f\"save_{ex['_id']}\"):\n                    ex[\"feedback_message\"] = msg\n                    db.update_exchange_feedback_message(ex[\"_id\"], msg)\n                    st.success(\"Gespeichert!\")\n                    time.sleep(1)\n                    st.rerun()\n\n        with cols[3]:\n            st.download_button(\n                label=\"📥\",\n                data=ex[\"answer\"],\n                file_name=f\"response_{ex['_id']}.md\",\n                mime=\"text/markdown\",\n                key=f\"dl_{ex['_id']}\",\n                help=\"Download Markdown\"\n            )\n\n        with cols[4]:\n            if st.button(\"🗑️\", key=f\"del_{ex['_id']}\", help=\"Nachricht löschen\"):\n                handle_delete_exchange(current_chat_name, ex)\n\n        with cols[5]:\n             if ex.get(\"feedback\") == 1:\n                 st.caption(\"Positiv bewertet\")\n             elif ex.get(\"feedback\") == 0:\n                 st.caption(\"Negativ bewertet\")\n\n        # Inhalt Scrollbar\n        with st.container(height=500):\n             render_text_with_mermaid(ex[\"answer\"])",
            "start_line": 104,
            "end_line": 156,
            "context": {
              "calls": [
                "frontend/Frontend.py::button",
                "frontend/Frontend.py::caption",
                "frontend/Frontend.py::chat_message",
                "frontend/Frontend.py::columns",
                "frontend/Frontend.py::container",
                "frontend/Frontend.py::download_button",
                "frontend/Frontend.py::get",
                "frontend/Frontend.py::handle_delete_exchange",
                "frontend/Frontend.py::handle_feedback_change",
                "frontend/Frontend.py::popover",
                "frontend/Frontend.py::render_text_with_mermaid",
                "frontend/Frontend.py::rerun",
                "frontend/Frontend.py::sleep",
                "frontend/Frontend.py::success",
                "frontend/Frontend.py::text_area",
                "frontend/Frontend.py::update_exchange_feedback_message",
                "frontend/Frontend.py::write"
              ],
              "called_by": [
                {
                  "file": "Frontend.py",
                  "function": "frontend.Frontend",
                  "mode": "module",
                  "line": 285
                }
              ]
            }
          }
        ],
        "classes": []
      }
    },
    "schemas/types.py": {
      "ast_nodes": {
        "imports": [
          "typing.List",
          "typing.Optional",
          "typing.Literal",
          "pydantic.BaseModel",
          "pydantic.ValidationError"
        ],
        "functions": [],
        "classes": [
          {
            "mode": "class_analysis",
            "identifier": "schemas.types.ParameterDescription",
            "name": "ParameterDescription",
            "docstring": "Describes a single parameter of a function.",
            "source_code": "class ParameterDescription(BaseModel):\n    \"\"\"Describes a single parameter of a function.\"\"\"\n    name: str\n    type: str\n    description: str",
            "start_line": 6,
            "end_line": 10,
            "context": {
              "dependencies": [],
              "instantiated_by": [],
              "method_context": []
            }
          },
          {
            "mode": "class_analysis",
            "identifier": "schemas.types.ReturnDescription",
            "name": "ReturnDescription",
            "docstring": "Describes the return value of a function.",
            "source_code": "class ReturnDescription(BaseModel):\n    \"\"\"Describes the return value of a function.\"\"\"\n    name: str\n    type: str\n    description: str",
            "start_line": 12,
            "end_line": 16,
            "context": {
              "dependencies": [],
              "instantiated_by": [],
              "method_context": []
            }
          },
          {
            "mode": "class_analysis",
            "identifier": "schemas.types.UsageContext",
            "name": "UsageContext",
            "docstring": "Describes the calling context of a function.",
            "source_code": "class UsageContext(BaseModel):\n    \"\"\"Describes the calling context of a function.\"\"\"\n    calls: str\n    called_by: str",
            "start_line": 18,
            "end_line": 21,
            "context": {
              "dependencies": [],
              "instantiated_by": [],
              "method_context": []
            }
          },
          {
            "mode": "class_analysis",
            "identifier": "schemas.types.FunctionDescription",
            "name": "FunctionDescription",
            "docstring": "Contains the detailed analysis of a function's purpose and signature.",
            "source_code": "class FunctionDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a function's purpose and signature.\"\"\"\n    overall: str\n    parameters: List[ParameterDescription]\n    returns: List[ReturnDescription]\n    usage_context: UsageContext",
            "start_line": 23,
            "end_line": 28,
            "context": {
              "dependencies": [],
              "instantiated_by": [],
              "method_context": []
            }
          },
          {
            "mode": "class_analysis",
            "identifier": "schemas.types.FunctionAnalysis",
            "name": "FunctionAnalysis",
            "docstring": "The main model representing the entire JSON schema for a function.",
            "source_code": "class FunctionAnalysis(BaseModel):\n    \"\"\"The main model representing the entire JSON schema for a function.\"\"\"\n    identifier: str\n    description: FunctionDescription\n    error: Optional[str] = None",
            "start_line": 30,
            "end_line": 34,
            "context": {
              "dependencies": [],
              "instantiated_by": [],
              "method_context": []
            }
          },
          {
            "mode": "class_analysis",
            "identifier": "schemas.types.ConstructorDescription",
            "name": "ConstructorDescription",
            "docstring": "Describes the __init__ method of a class.",
            "source_code": "class ConstructorDescription(BaseModel):\n    \"\"\"Describes the __init__ method of a class.\"\"\"\n    description: str\n    parameters: List[ParameterDescription]",
            "start_line": 39,
            "end_line": 42,
            "context": {
              "dependencies": [],
              "instantiated_by": [],
              "method_context": []
            }
          },
          {
            "mode": "class_analysis",
            "identifier": "schemas.types.ClassContext",
            "name": "ClassContext",
            "docstring": "Describes the class's external dependencies and primary points of instantiation.",
            "source_code": "class ClassContext(BaseModel):\n    \"\"\"Describes the class's external dependencies and primary points of instantiation.\"\"\"\n    dependencies: str\n    instantiated_by: str",
            "start_line": 44,
            "end_line": 47,
            "context": {
              "dependencies": [],
              "instantiated_by": [],
              "method_context": []
            }
          },
          {
            "mode": "class_analysis",
            "identifier": "schemas.types.ClassDescription",
            "name": "ClassDescription",
            "docstring": "Contains the detailed analysis of a class's purpose, constructor, and methods.",
            "source_code": "class ClassDescription(BaseModel):\n    \"\"\"Contains the detailed analysis of a class's purpose, constructor, and methods.\"\"\"\n    overall: str\n    init_method: ConstructorDescription\n    methods: List[FunctionAnalysis]\n    usage_context: ClassContext",
            "start_line": 49,
            "end_line": 54,
            "context": {
              "dependencies": [],
              "instantiated_by": [],
              "method_context": []
            }
          },
          {
            "mode": "class_analysis",
            "identifier": "schemas.types.ClassAnalysis",
            "name": "ClassAnalysis",
            "docstring": "The main model for the entire JSON schema for a class.",
            "source_code": "class ClassAnalysis(BaseModel):\n    \"\"\"The main model for the entire JSON schema for a class.\"\"\"\n    identifier: str\n    description: ClassDescription\n    error: Optional[str] = None",
            "start_line": 56,
            "end_line": 60,
            "context": {
              "dependencies": [],
              "instantiated_by": [],
              "method_context": []
            }
          },
          {
            "mode": "class_analysis",
            "identifier": "schemas.types.FunctionContextInput",
            "name": "FunctionContextInput",
            "docstring": "Structured context for analyzing a function.",
            "source_code": "class FunctionContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a function.\"\"\"\n    calls: List[str]\n    called_by: List[str]",
            "start_line": 65,
            "end_line": 68,
            "context": {
              "dependencies": [],
              "instantiated_by": [
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 177
                }
              ],
              "method_context": []
            }
          },
          {
            "mode": "class_analysis",
            "identifier": "schemas.types.FunctionAnalysisInput",
            "name": "FunctionAnalysisInput",
            "docstring": "The required input to generate a FunctionAnalysis object.",
            "source_code": "class FunctionAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a FunctionAnalysis object.\"\"\"\n    mode: Literal[\"function_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: FunctionContextInput",
            "start_line": 70,
            "end_line": 76,
            "context": {
              "dependencies": [],
              "instantiated_by": [
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 182
                }
              ],
              "method_context": []
            }
          },
          {
            "mode": "class_analysis",
            "identifier": "schemas.types.MethodContextInput",
            "name": "MethodContextInput",
            "docstring": "Structured context for a classes methods",
            "source_code": "class MethodContextInput(BaseModel):\n    \"\"\"Structured context for a classes methods\"\"\"\n    identifier: str\n    calls: List[str]\n    called_by: List[str]\n    args: List[str]\n    docstring: Optional[str]",
            "start_line": 81,
            "end_line": 87,
            "context": {
              "dependencies": [],
              "instantiated_by": [],
              "method_context": []
            }
          },
          {
            "mode": "class_analysis",
            "identifier": "schemas.types.ClassContextInput",
            "name": "ClassContextInput",
            "docstring": "Structured context for analyzing a class.",
            "source_code": "class ClassContextInput(BaseModel):\n    \"\"\"Structured context for analyzing a class.\"\"\"\n    dependencies: List[str]\n    instantiated_by: List[str]\n    method_context: List[MethodContextInput]",
            "start_line": 88,
            "end_line": 92,
            "context": {
              "dependencies": [],
              "instantiated_by": [
                {
                  "file": "HelperLLM.py",
                  "function": "main_orchestrator",
                  "mode": "function",
                  "line": 347
                },
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 198
                }
              ],
              "method_context": []
            }
          },
          {
            "mode": "class_analysis",
            "identifier": "schemas.types.ClassAnalysisInput",
            "name": "ClassAnalysisInput",
            "docstring": "The required input to generate a ClassAnalysis object.",
            "source_code": "class ClassAnalysisInput(BaseModel):\n    \"\"\"The required input to generate a ClassAnalysis object.\"\"\"\n    mode: Literal[\"class_analysis\"]\n    identifier: str\n    source_code: str\n    imports: List[str]\n    context: ClassContextInput",
            "start_line": 94,
            "end_line": 100,
            "context": {
              "dependencies": [],
              "instantiated_by": [
                {
                  "file": "HelperLLM.py",
                  "function": "main_orchestrator",
                  "mode": "function",
                  "line": 316
                },
                {
                  "file": "main.py",
                  "function": "main_workflow",
                  "mode": "function",
                  "line": 204
                }
              ],
              "method_context": []
            }
          }
        ]
      }
    }
  }
}