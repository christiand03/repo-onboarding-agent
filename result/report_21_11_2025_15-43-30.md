# Project Documentation: repo-onboarding-agent documentation

## 1. Project Overview
- **Description:** This project is an automated documentation agent designed to analyze Git repositories. It clones a given repository, parses the Python source code to build an Abstract Syntax Tree (AST), and constructs a call graph to map code interactions. The agent leverages Large Language Models (LLMs) to analyze individual functions and classes, and then synthesizes this information into a comprehensive, human-readable Markdown report.
- **Key Features:**
  - Automated Git repository cloning and file processing.
  - Code analysis using Abstract Syntax Trees (AST) and call graph generation.
  - Integration with Google Gemini LLMs for deep code analysis.
  - Structured analysis of classes, methods, and functions.
  - Generation of a cohesive Markdown documentation file.
- **Tech Stack:**
  - langchain
  - langchain-core
  - langchain-google-genai
  - google-generativeai
  - python-dotenv
  - pydantic
  - regex
  - networkx
  - gitPython

*   **Repository Structure:**
    ```mermaid
    graph LR;
        subgraph root;
            direction LR;
            A0(".env.example");
            A1(".gitignore");
            A2("analysis_output.json");
            A3("readme.md");
            A4("requirements.txt");
        end;
        root --> SystemPrompts;
        subgraph SystemPrompts;
            direction LR;
            B0("SystemPromptClassHelperLLM.txt");
            B1("SystemPromptFunctionHelperLLM.txt");
            B2("SystemPromptHelperLLM.txt");
            B3("SystemPromptMainLLM.txt");
        end;
        root --> backend;
        subgraph backend;
            direction LR;
            C0("AST_Schema.py");
            C1("HelperLLM.py");
            C2("MainLLM.py");
            C3("basic_info.py");
            C4("callgraph.py");
            C5("getRepo.py");
            C6("main.py");
            C7("tools.py");
        end;
        root --> frontend;
        subgraph frontend;
            direction LR;
            D0("Frontend.py");
        end;
        root --> notizen;
        subgraph notizen;
            direction LR;
            E0("Report Agenda.txt");
            E1("Zwischenpraesentation Agenda.txt");
            E2("doc_bestandteile.md");
            E3("notizen.md");
            E4("paul_notizen.md");
            E5("praesentation_notizen.md");
            E6("technische_notizen.md");
        end;
        notizen --> grafiken;
        subgraph grafiken;
            direction LR;
            F0("AST.dot");
            F1("Frontend.dot");
            F2("HelperLLM.dot");
            F3("HelperLLM.png");
            F4("MainLLM.dot");
            F5("agent.dot");
            F6("basic_info.dot");
            F7("callgraph.dot");
            F8("getRepo.dot");
            F9("graph_AST.png");
            F10("graph_AST2.png");
            F11("graph_AST3.png");
            F12("main.dot");
            F13("tools.dot");
            F14("types.dot");
        end;
        root --> result;
        subgraph result;
            direction LR;
            G0("report_14_11_2025_14-52-36.md");
            G1("report_14_11_2025_15-21-53.md");
            G2("report_14_11_2025_15-26-24.md");
            G3("result_2025-11-11_12-30-53.md");
            G4("result_2025-11-11_12-43-51.md");
            G5("result_2025-11-11_12-45-37.md");
        end;
        root --> schemas;
        subgraph schemas;
            direction LR;
            H0("types.py");
        end;
    ```

## 2. Installation
### Dependencies
- langchain 
- langchain-core 
- langchain-google-genai 
- google-generativeai 
- python-dotenv 
- pydantic 
- regex 
- networkx 
- gitPython 

As a `requirements.txt` file is present, you can install all dependencies with the following command:
```bash
pip install -r requirements.txt
```
### Setup Guide
1.  Clone the repository: `git clone https://github.com/christiand03/repo-onboarding-agent.git`
2.  Navigate to the project directory: `cd repo-onboarding-agent`
3.  Install the required dependencies: `pip install -r requirements.txt`
4.  Create a `.env` file by copying the `.env.example` template.
5.  Add your Google Gemini API key to the `.env` file.

### Quick Startup
To run the main analysis workflow, execute the main script in the backend:
```bash
python backend/main.py
```

## 3. Use Cases & Commands
The primary use case of this project is to generate technical documentation for a Python-based Git repository automatically. The user provides a repository URL, and the system performs a multi-stage analysis to produce a detailed Markdown report.

**Primary Command:**

The main entry point for the application is `backend/main.py`. The script currently has a repository URL hardcoded. To analyze a different repository, you must modify the `user_input` variable within the `main_workflow` function in that file.

```bash
# Execute the main analysis and documentation generation pipeline
python backend/main.py
```

## 4. Architecture
The Mermaid Syntax to visualize Graphs is not set up yet and will be added
but if there is mermaid syntax in your input json display it here

## 5. Code Analysis
### File: `backend/AST_Schema.py`
#### Class: `ASTVisitor`
*   **Summary:** The `ASTVisitor` class extends `ast.NodeVisitor` to traverse and analyze Python Abstract Syntax Trees (ASTs). It extracts information about imports, classes, and functions, storing the extracted data in a structured schema. The class is designed to parse source code and build a representation of its structure.
*   **Instantiation:** The class is not instantiated in the provided context.
*   **Dependencies:** The class depends on the `ast` module.
*   **Constructor:**
    *   *Description:* The constructor initializes the `ASTVisitor` with the source code and sets up an empty schema to store extracted information. It also initializes a variable to keep track of the current class being processed.
    *   *Parameters:*
        - **source_code** (`str`): The source code of the Python file to be analyzed.
*   **Methods:**
    *   **`visit_Import`**
        *   *Signature:* `def visit_Import(self, node)`
        *   *Description:* The `visit_Import` method is designed to process `Import` nodes in the AST. It extracts the names of imported modules and appends them to the schema's imports list. The method then calls `generic_visit` to continue traversing the AST.
        *   *Parameters:*
            - **node** (`ast.Import`): The AST node representing an import statement.
        *   *Returns:*
            *Analysis data not available for this component.*
    *   **`visit_ImportFrom`**
        *   *Signature:* `def visit_ImportFrom(self, node)`
        *   *Description:* The `visit_ImportFrom` method processes `ImportFrom` nodes in the AST. It extracts the names of imported objects from a module and adds them to the schema's imports list. It then calls `generic_visit` to continue traversing the AST.
        *   *Parameters:*
            - **node** (`ast.ImportFrom`): The AST node representing an import from statement.
        *   *Returns:*
            *Analysis data not available for this component.*
    *   **`visit_ClassDef`**
        *   *Signature:* `def visit_ClassDef(self, node)`
        *   *Description:* The `visit_ClassDef` method handles `ClassDef` nodes in the AST. It extracts class information, including the class name, docstring, source code segment, start line, and end line, and appends it to the schema's classes list. It also sets the `_current_class` attribute to the current class being processed and calls `generic_visit` to traverse the class's contents. Finally, it resets `_current_class` to `None` after processing the class.
        *   *Parameters:*
            - **node** (`ast.ClassDef`): The AST node representing a class definition.
        *   *Returns:*
            *Analysis data not available for this component.*
    *   **`visit_FunctionDef`**
        *   *Signature:* `def visit_FunctionDef(self, node)`
        *   *Description:* The `visit_FunctionDef` method processes `FunctionDef` nodes in the AST. If the method is inside a class, it extracts method context information and appends it to the `method_context` list of the current class. Otherwise, it extracts function information and appends it to the schema's functions list. It then calls `generic_visit` to continue traversing the AST.
        *   *Parameters:*
            - **node** (`ast.FunctionDef`): The AST node representing a function definition.
        *   *Returns:*
            *Analysis data not available for this component.*
    *   **`visit_AsyncFunctionDef`**
        *   *Signature:* `def visit_AsyncFunctionDef(self, node)`
        *   *Description:* The `visit_AsyncFunctionDef` method processes `AsyncFunctionDef` nodes by calling the `visit_FunctionDef` method. This indicates that asynchronous function definitions are handled in the same way as regular function definitions.
        *   *Parameters:*
            - **node** (`ast.AsyncFunctionDef`): The AST node representing an asynchronous function definition.
        *   *Returns:*
            *Analysis data not available for this component.*

#### Class: `ASTAnalyzer`
*   **Summary:** The ASTAnalyzer class is designed to parse Python files, extract their Abstract Syntax Trees (ASTs), and enrich the extracted schema with call graph information. It analyzes a list of files, extracts information about functions, classes, and imports, and then uses a call graph to determine function call relationships. The class provides a structured way to analyze Python codebases and understand their structure.
*   **Instantiation:** This class is not instantiated by any other classes or functions.
*   **Dependencies:** The class depends on the ast, networkx, and callgraph.build_callGraph modules.
*   **Constructor:**
    *   *Description:* The constructor initializes the ASTAnalyzer class. It does not take any parameters and performs no specific setup.
    *   *Parameters:*
        *Analysis data not available for this component.*
*   **Methods:**
    *   **`_enrich_schema_with_callgraph`**
        *   *Signature:* `def _enrich_schema_with_callgraph(schema, call_graph, filename)`
        *   *Description:* This static method enriches a given schema with call graph information. It iterates through the functions and classes in the schema and updates their context with 'calls' and 'called_by' information derived from the call graph. This method is crucial for understanding the dependencies and call relationships within the analyzed code.
        *   *Parameters:*
            - **schema** (`dict`): A dictionary representing the schema of the code, containing information about functions and classes.
            - **call_graph** (`nx.DiGraph`): A directed graph representing the call graph of the code.
            - **filename** (`str`): The name of the file being analyzed.
        *   *Returns:*
            *Analysis data not available for this component.*
    *   **`analyze_repository`**
        *   *Signature:* `def analyze_repository(self, files)`
        *   *Description:* This method analyzes a list of files to extract their ASTs and build a schema. It iterates through the files, parses the Python code, creates an AST, and uses an ASTVisitor to extract information. It then enriches the schema with call graph data and returns a dictionary containing the extracted information for each file. The method handles potential syntax errors during parsing.
        *   *Parameters:*
            - **files** (`list`): A list of file objects to analyze.
        *   *Returns:*
            - **return** (`dict`): A dictionary containing the extracted schema for each file.

---
### File: `backend/HelperLLM.py`
#### Class: `LLMHelper`
*   **Summary:** The `LLMHelper` class is designed to interact with Google Gemini to generate documentation for code snippets. It handles API interactions, error management, and input/output validation using Pydantic models. It takes an API key, prompt file paths, and a model name during initialization, and provides methods to generate documentation for both functions and classes in batches.
*   **Instantiation:** This class is not instantiated in the provided context.
*   **Dependencies:** The class depends on `os`, `json`, `logging`, `typing`, `dotenv`, `langchain_google_genai`, `langchain`, `pydantic`, and custom schemas for types.
*   **Constructor:**
    *   *Description:* The constructor initializes the LLMHelper with the Gemini API key, paths to function and class system prompts, and the model name. It reads the system prompts from the provided file paths, sets up the Gemini LLM with structured output for function and class analysis, and logs the initialization.
    *   *Parameters:*
        - **api_key** (`str`): The API key for accessing the Google Gemini API.
        - **function_prompt_path** (`str`): The file path to the function system prompt.
        - **class_prompt_path** (`str`): The file path to the class system prompt.
        - **model_name** (`str`): The name of the Gemini model to use. Defaults to 'gemini-flash-latest'.
*   **Methods:**
    *   **`generate_for_functions`**
        *   *Signature:* `def generate_for_functions(self, function_inputs)`
        *   *Description:* This method generates and validates documentation for a batch of functions using the Gemini API. It takes a list of `FunctionAnalysisInput` objects, converts them into JSON payloads, and sends them to the Gemini API. The method then processes the API responses, returning a list of validated `FunctionAnalysis` objects or a list of `None` values in case of an error.
        *   *Parameters:*
            - **function_inputs** (`List[FunctionAnalysisInput]`): A list of `FunctionAnalysisInput` objects, each representing the input for a function's documentation generation.
        *   *Returns:*
            - **validated_functions** (`List[Optional[FunctionAnalysis]]`): A list of validated `FunctionAnalysis` objects, or `None` if an error occurred.
    *   **`generate_for_classes`**
        *   *Signature:* `def generate_for_classes(self, class_inputs)`
        *   *Description:* This method generates and validates documentation for a batch of classes using the Gemini API. It takes a list of `ClassAnalysisInput` objects, converts them into JSON payloads, and sends them to the Gemini API. The method then processes the API responses, returning a list of validated `ClassAnalysis` objects or a list of `None` values in case of an error.
        *   *Parameters:*
            - **class_inputs** (`List[ClassAnalysisInput]`): A list of `ClassAnalysisInput` objects, each representing the input for a class's documentation generation.
        *   *Returns:*
            - **validated_classes** (`List[Optional[ClassAnalysis]]`): A list of validated `ClassAnalysis` objects, or `None` if an error occurred.

#### Function: `main_orchestrator`
*   **Signature:** `def main_orchestrator()`
*   **Description:** This function serves as a test orchestrator for the LLMHelper class. It defines inputs for function analysis, including 'add_item', 'check_stock', and 'generate_report'. It then validates these inputs and corresponding analysis results using Pydantic models. Finally, it uses the LLMHelper to generate documentation for the functions and prints the final documentation in JSON format.
*   **Parameters:**
    *Analysis data not available for this component.*
*   **Returns:**
    *Analysis data not available for this component.*
*   **Usage:**
    *   **Calls:** `backend/HelperLLM.py::ClassAnalysisInput`, `backend/HelperLLM.py::ClassContextInput`, `backend/HelperLLM.py::LLMHelper`, `backend/HelperLLM.py::dumps`, `backend/HelperLLM.py::generate_for_functions`, `backend/HelperLLM.py::info`, `backend/HelperLLM.py::model_dump`, `backend/HelperLLM.py::model_validate`, `backend/HelperLLM.py::print`, and `backend/HelperLLM.py::warning`
    *   **Called By:** `<main_block>`

---
### File: `backend/MainLLM.py`
#### Class: `MainLLM`
*   **Summary:** The `MainLLM` class serves as the primary interface for interacting with a Large Language Model (LLM). It initializes the LLM with an API key, a system prompt loaded from a file, and a specified model name. It provides methods for calling the LLM with user input, both for standard requests and for streaming responses.
*   **Instantiation:** The class is not instantiated by any other classes or functions in the provided context.
*   **Dependencies:** The class depends on `langchain_google_genai.ChatGoogleGenerativeAI`, `langchain.messages.HumanMessage`, `langchain.messages.SystemMessage`, `logging`, and `os` modules.
*   **Constructor:**
    *   *Description:* The constructor initializes the `MainLLM` object. It validates the API key, reads the system prompt from a specified file, and initializes a `ChatGoogleGenerativeAI` instance with the provided API key, model name, and a temperature setting. It raises a `ValueError` if the API key is not provided and handles potential `FileNotFoundError` exceptions during prompt file loading.
    *   *Parameters:*
        - **api_key** (`str`): The API key for accessing the LLM.
        - **prompt_file_path** (`str`): The file path to the system prompt.
        - **model_name** (`str`): The name of the LLM model to use. Defaults to 'gemini-2.5-pro'.
*   **Methods:**
    *   **`call_llm`**
        *   *Signature:* `def call_llm(self, user_input)`
        *   *Description:* The `call_llm` method sends a user input to the LLM and retrieves a single response. It constructs a message list with the system prompt and user input, calls the LLM's `invoke` method, and returns the response content. It includes error handling to catch exceptions during the LLM call and returns `None` if an error occurs.
        *   *Parameters:*
            - **user_input** (`str`): The user's input to be sent to the LLM.
        *   *Returns:*
            - **content** (`str`): The content of the LLM's response, or None if an error occurred.
    *   **`stream_llm`**
        *   *Signature:* `def stream_llm(self, user_input)`
        *   *Description:* The `stream_llm` method sends a user input to the LLM and streams the response in chunks. It constructs a message list with the system prompt and user input, calls the LLM's `stream` method, and yields the response content chunk by chunk. It includes error handling to catch exceptions during the LLM stream call and yields an error message if an error occurs.
        *   *Parameters:*
            - **user_input** (`str`): The user's input to be sent to the LLM.
        *   *Returns:*
            - **content** (`str`): Yields the content of the LLM's response in chunks, or an error message if an error occurred.

---
### File: `backend/basic_info.py`
#### Class: `ProjektInfoExtractor`
*   **Summary:** The `ProjektInfoExtractor` class extracts project information from common project files like README, pyproject.toml, and requirements.txt. It initializes a dictionary to store extracted information, providing methods to parse different file types and extract relevant details such as title, description, dependencies, and setup instructions. The class prioritizes information from pyproject.toml, then requirements.txt, and finally README files, with a fallback title derived from the repository URL.
*   **Instantiation:** This class is not instantiated in the provided context.
*   **Dependencies:** The class depends on `re`, `os`, `tomllib`, `typing.List`, `typing.Dict`, `typing.Any`, and `typing.Optional`.
*   **Constructor:**
    *   *Description:* The constructor initializes an `info` dictionary with placeholders for project overview and installation details. It also defines a constant `INFO_NICHT_GEFUNDEN` to represent missing information.
    *   *Parameters:*
        - **self** (`ProjektInfoExtractor`): Represents the instance of the class.
*   **Methods:**
    *   **`_finde_datei`**
        *   *Signature:* `def _finde_datei(self, patterns, dateien)`
        *   *Description:* The `_finde_datei` method searches for a file within a list of files, matching against a list of patterns. It iterates through the provided files and patterns, comparing the lowercase file path ending with the lowercase pattern. If a match is found, the method returns the matching file; otherwise, it returns `None`.
        *   *Parameters:*
            - **patterns** (`List[str]`): A list of file name patterns to search for.
            - **dateien** (`List[Any]`): A list of files to search within.
        *   *Returns:*
            - **datei** (`Any`): The matching file, or None if no match is found.
    *   **`_extrahiere_sektion_aus_markdown`**
        *   *Signature:* `def _extrahiere_sektion_aus_markdown(self, inhalt, keywords)`
        *   *Description:* The `_extrahiere_sektion_aus_markdown` method extracts text from a Markdown file based on a given heading. It constructs a regular expression pattern using provided keywords to match headings (##) and captures the content until the next heading or the end of the file. The extracted content is then returned, or `None` if no match is found.
        *   *Parameters:*
            - **inhalt** (`str`): The entire Markdown text content.
            - **keywords** (`list`): A list of alternative keywords for the section title.
        *   *Returns:*
            - **str** (`str`): The extracted text section, or None if not found.
    *   **`_parse_readme`**
        *   *Signature:* `def _parse_readme(self, inhalt)`
        *   *Description:* The `_parse_readme` method parses the content of a README file to extract project information. It searches for the title, description, key features, tech stack, current status, setup instructions, and quick start guide using regular expressions and the `_extrahiere_sektion_aus_markdown` method. The extracted information is then stored in the `self.info` dictionary.
        *   *Parameters:*
            - **inhalt** (`str`): The content of the README file.
        *   *Returns:*
            *Analysis data not available for this component.*
    *   **`_parse_toml`**
        *   *Signature:* `def _parse_toml(self, inhalt)`
        *   *Description:* The `_parse_toml` method parses a pyproject.toml file to extract project metadata. It uses the `tomllib` library to load the TOML content and extracts the project name, description, and dependencies. If `tomllib` is not installed, it prints a warning. The extracted information is then stored in the `self.info` dictionary.
        *   *Parameters:*
            - **inhalt** (`str`): The content of the pyproject.toml file.
        *   *Returns:*
            *Analysis data not available for this component.*
    *   **`_parse_requirements`**
        *   *Signature:* `def _parse_requirements(self, inhalt)`
        *   *Description:* The `_parse_requirements` method parses a requirements.txt file to extract project dependencies. It splits the content into lines, filters out empty lines and comments, and stores the dependencies in the `self.info` dictionary, but only if dependencies haven't already been found in the toml file.
        *   *Parameters:*
            - **inhalt** (`str`): The content of the requirements.txt file.
        *   *Returns:*
            *Analysis data not available for this component.*
    *   **`extrahiere_info`**
        *   *Signature:* `def extrahiere_info(self, dateien, repo_url)`
        *   *Description:* The `extrahiere_info` method orchestrates the extraction of project information from a list of files and a repository URL. It prioritizes parsing pyproject.toml, then requirements.txt, and finally README files. It uses helper methods to find and parse these files. After parsing, it formats the dependencies and sets the project title based on the repository URL.
        *   *Parameters:*
            - **dateien** (`List[Any]`): A list of file objects to extract information from.
            - **repo_url** (`str`): The URL of the repository.
        *   *Returns:*
            - **Dict[str, Any]** (`Dict[str, Any]`): A dictionary containing the extracted project information.

---
### File: `backend/callgraph.py`
#### Class: `CallGraph`
*   **Summary:** The `CallGraph` class is an AST (Abstract Syntax Tree) visitor designed to collect function calls and create edges for a call graph. It traverses the AST of Python code, identifying function definitions, calls, and imports to build a representation of the code's call structure.
*   **Instantiation:** This class is not instantiated in the provided context.
*   **Dependencies:** The class has no explicit dependencies.
*   **Constructor:**
    *   *Description:* The constructor initializes the visitor with attributes to track the current function, the current class, import mappings, a set of functions, and the edges of the call graph.
    *   *Parameters:*
        *Analysis data not available for this component.*
*   **Methods:**
    *   **`_recursive_call`**
        *   *Signature:* `def _recursive_call(self, node)`
        *   *Description:* This method recursively traverses an AST node to extract function call names. It handles different node types like `ast.Call`, `ast.Name`, and `ast.Attribute` to identify the called functions or methods. The method returns a list of strings representing the names of the called functions.
        *   *Parameters:*
            - **node** (`ast.AST`): The AST node to analyze for function calls.
        *   *Returns:*
            - **all_calls** (`list[str]`): A list of strings representing the names of the called functions.
    *   **`_resolve_all_callee_names`**
        *   *Signature:* `def _resolve_all_callee_names(self, callee_nodes)`
        *   *Description:* This method resolves the fully qualified names of the callees by combining the filename, class name (if applicable), and the raw callee name. It iterates through a list of raw callee names and constructs the full name based on the current context (class and file).
        *   *Parameters:*
            - **callee_nodes** (`list[str]`): A list of raw callee names to resolve.
        *   *Returns:*
            - **resolved_callees** (`list[str]`): A list of fully qualified callee names.
    *   **`_make_full_name`**
        *   *Signature:* `def _make_full_name(self, basename, class_name)`
        *   *Description:* This method constructs a fully qualified name for a function or method, including the filename, class name (if available), and the base name of the function. It returns the formatted full name as a string.
        *   *Parameters:*
            - **basename** (`str`): The base name of the function.
            - **class_name** (`str | None`): The name of the class, if the function is a method; otherwise, None.
        *   *Returns:*
            - **return** (`str`): The fully qualified name of the function.
    *   **`_current_caller`**
        *   *Signature:* `def _current_caller(self)`
        *   *Description:* This method returns the name of the current caller function. If a current function is set, it returns the function's name; otherwise, it returns the filename or '<global-scope>' if no filename is available.
        *   *Parameters:*
            *Analysis data not available for this component.*
        *   *Returns:*
            - **return** (`str`): The name of the current caller function.
    *   **`visit_Import`**
        *   *Signature:* `def visit_Import(self, node)`
        *   *Description:* This method visits import statements and populates the `import_mapping` dictionary with module aliases and their corresponding module names. It extracts the module name and alias from the import node and stores them in the mapping.
        *   *Parameters:*
            - **node** (`ast.Import`): The AST node representing the import statement.
        *   *Returns:*
            *Analysis data not available for this component.*
    *   **`visit_ImportFrom`**
        *   *Signature:* `def visit_ImportFrom(self, node)`
        *   *Description:* This method visits `import from` statements and updates the `import_mapping` with the imported names and their corresponding module base. It extracts the module name, level, and imported names, and stores them in the mapping.
        *   *Parameters:*
            - **node** (`ast.ImportFrom`): The AST node representing the import from statement.
        *   *Returns:*
            *Analysis data not available for this component.*
    *   **`visit_ClassDef`**
        *   *Signature:* `def visit_ClassDef(self, node)`
        *   *Description:* This method visits class definitions and updates the `current_class` attribute to track the current class being visited. It iterates through the class body and calls the `visit` method on each element. It restores the previous class after visiting the class definition.
        *   *Parameters:*
            - **node** (`ast.ClassDef`): The AST node representing the class definition.
        *   *Returns:*
            *Analysis data not available for this component.*
    *   **`visit_FunctionDef`**
        *   *Signature:* `def visit_FunctionDef(self, node)`
        *   *Description:* This method visits regular function definitions. It sets the `current_function`, creates a node in the graph, and recursively traverses the function body. It uses `_make_full_name` to create the full function name and adds a node to the graph.
        *   *Parameters:*
            - **node** (`ast.FunctionDef`): The AST node representing the function definition.
        *   *Returns:*
            *Analysis data not available for this component.*
    *   **`visit_AsyncFunctionDef`**
        *   *Signature:* `def visit_AsyncFunctionDef(self, node)`
        *   *Description:* This method visits asynchronous function definitions (`async def`). It calls `visit_FunctionDef` to handle the asynchronous function definition, effectively reusing the logic for regular function definitions.
        *   *Parameters:*
            - **node** (`ast.AsyncFunctionDef`): The AST node representing the asynchronous function definition.
        *   *Returns:*
            *Analysis data not available for this component.*
    *   **`visit_Call`**
        *   *Signature:* `def visit_Call(self, node)`
        *   *Description:* This method visits function or method calls and handles complex cases with detailed warnings. It extracts the caller and callee names, resolves the callee names using `_resolve_all_callee_names`, and adds an edge to the call graph. It uses helper methods to extract and resolve function names.
        *   *Parameters:*
            - **node** (`ast.Call`): The AST node representing the function call.
        *   *Returns:*
            *Analysis data not available for this component.*
    *   **`visit_If`**
        *   *Signature:* `def visit_If(self, node)`
        *   *Description:* This method checks for `if __name__ == '__main__'` blocks and represents global calls within this block separately as `<main_block>` in the call graph. It handles function calls within this block by setting the current function to `<main_block>` and then restoring the original caller.
        *   *Parameters:*
            - **node** (`ast.If`): The AST node representing the if statement.
        *   *Returns:*
            *Analysis data not available for this component.*

#### Function: `build_callGraph`
*   **Signature:** `def build_callGraph(tree, filename, file_content)`
*   **Description:** This function constructs a call graph from a given Python Abstract Syntax Tree (AST). It uses a custom AST visitor to traverse the tree, identify function calls, and build a directed graph (networkx.DiGraph). The graph's nodes represent functions, methods, and scopes, while edges represent function calls. The function handles both regular and asynchronous function definitions, class definitions, and import statements to accurately represent the call structure. It also considers `if __name__ == "__main__"` blocks, representing calls within this block as originating from a `<main_block>` node.
*   **Parameters:**
    - **tree** (`ast.AST`): The AST of the Python file to analyze.
    - **filename** (`str | None`): The name of the analyzed file, e.g., "main.py" or "src/utils.py".
    - **file_content** (`str | None`): The content of the file.
*   **Returns:**
    - **graph** (`nx.DiGraph`): The complete call graph.
*   **Usage:**
    *   **Calls:** The function calls `backend/callgraph.py::DiGraph` to create the call graph.
    *   **Called By:** The function is called by `<main_block>`.

#### Function: `graph_to_adj_list`
*   **Signature:** `def graph_to_adj_list(graph)`
*   **Description:** This function converts a directed graph represented by a `networkx.DiGraph` object into an adjacency list, which is a dictionary where keys are nodes and values are lists of their successors. It iterates through the sorted nodes of the graph. For each node, it retrieves its successors, sorts them, and adds the node and its successors to the adjacency list if successors exist. The resulting adjacency list is JSON-serializable.
*   **Parameters:**
    - **graph** (`nx.DiGraph`): The directed graph to convert.
*   **Returns:**
    - **adj_list** (`Dict[str, list[str]]`): An adjacency list where each key is a caller node and the value is a list of callee nodes.
*   **Usage:**
    *   **Calls:** The function calls `list`, `nodes`, `sorted`, and `successors` methods from the `networkx` library.
    *   **Called By:** This function is not called by any other function in the provided context.

#### Function: `build_global_callgraph`
*   **Signature:** `def build_global_callgraph(all_repo_files)`
*   **Description:** The function `build_global_callgraph` is designed to construct a global call graph, likely representing the relationships between functions within a software repository. It accepts a list of `RepoFile` objects as input, presumably containing information about the files in the repository. The function's implementation is currently a `NotImplementedError`, indicating that the actual call graph construction logic is not yet defined. The return type is specified as a `nx.DiGraph`, suggesting the use of the NetworkX library to represent the call graph as a directed graph.
*   **Parameters:**
    - **all_repo_files** (`list[RepoFile]`): A list of RepoFile objects, likely representing the files in the repository.
*   **Returns:**
    - **return** (`nx.DiGraph`): A directed graph representing the global call graph.
*   **Usage:**
    *   **Calls:** The function does not call any other functions.
    *   **Called By:** The function is not called by any other functions.

#### Function: `make_safe_dot`
*   **Signature:** `def make_safe_dot(graph, out_path)`
*   **Description:** The function `make_safe_dot` takes a directed graph and an output path as input, and generates a DOT file. It first creates a copy of the input graph and renames the nodes to safe identifiers (e.g., "n0", "n1") to avoid special characters. Then, it sets the original node names as labels for the safe identifiers. Finally, it writes the modified graph to a DOT file at the specified output path.
*   **Parameters:**
    - **graph** (`nx.DiGraph`): The input directed graph.
    - **out_path** (`str`): The file path to write the DOT file to.
*   **Returns:**
    *Analysis data not available for this component.*
*   **Usage:**
    *   **Calls:** The function calls `graph.copy`, `list`, `enumerate`, `graph.nodes`, `nx.relabel_nodes`, and `nx.drawing.nx_pydot.write_dot` to manipulate and write the graph.
    *   **Called By:** The function is called from the main block.

---
### File: `backend/getRepo.py`
#### Class: `RepoFile`
*   **Summary:** The `RepoFile` class represents a single file within a Git repository. It provides lazy loading of file content and size to optimize performance. The class encapsulates the file's path, content, size, and provides methods for accessing these properties, including a method to analyze word count and generate a dictionary representation.
*   **Instantiation:** This class is not instantiated by any other classes or functions.
*   **Dependencies:** The class has no external dependencies.
*   **Constructor:**
    *   *Description:* The constructor initializes a `RepoFile` object with the file path and the commit tree. It also initializes attributes for lazy loading of the blob, content, and size.
    *   *Parameters:*
        - **file_path** (`str`): The path to the file within the repository.
        - **commit_tree** (`git.Tree`): The Tree-Object of the commit, from which the file originates.
*   **Methods:**
    *   **`blob`**
        *   *Signature:* `def blob(self)`
        *   *Description:* The `blob` method lazily loads the Git blob object. It retrieves the blob from the commit tree using the file path. If the blob is not found, it raises a `FileNotFoundError`.
        *   *Parameters:*
            *Analysis data not available for this component.*
        *   *Returns:*
            - **blob** (`git.Blob`): The Git blob object representing the file's content.
    *   **`content`**
        *   *Signature:* `def content(self)`
        *   *Description:* The `content` method lazily loads and returns the decoded content of the file. It reads the file content from the blob object and decodes it using UTF-8 encoding, ignoring potential errors.
        *   *Parameters:*
            *Analysis data not available for this component.*
        *   *Returns:*
            - **content** (`str`): The decoded content of the file.
    *   **`size`**
        *   *Signature:* `def size(self)`
        *   *Description:* The `size` method lazily loads and returns the size of the file in bytes. It retrieves the size from the blob object.
        *   *Parameters:*
            *Analysis data not available for this component.*
        *   *Returns:*
            - **size** (`int`): The size of the file in bytes.
    *   **`analyze_word_count`**
        *   *Signature:* `def analyze_word_count(self)`
        *   *Description:* The `analyze_word_count` method counts the words in the file content. It splits the content by spaces and returns the number of words.
        *   *Parameters:*
            *Analysis data not available for this component.*
        *   *Returns:*
            - **word_count** (`int`): The number of words in the file content.
    *   **`__repr__`**
        *   *Signature:* `def __repr__(self)`
        *   *Description:* The `__repr__` method returns a string representation of the `RepoFile` object, including the file path.
        *   *Parameters:*
            *Analysis data not available for this component.*
        *   *Returns:*
            - **representation** (`str`): A string representation of the object.
    *   **`to_dict`**
        *   *Signature:* `def to_dict(self, include_content)`
        *   *Description:* The `to_dict` method converts the `RepoFile` object to a dictionary. It includes the file path, name, size, and type. Optionally, it can include the file content.
        *   *Parameters:*
            - **include_content** (`bool`): A boolean indicating whether to include the file content in the dictionary.
        *   *Returns:*
            - **data** (`dict`): A dictionary representation of the RepoFile object.

#### Class: `GitRepository`
*   **Summary:** The `GitRepository` class manages a Git repository, allowing cloning into a temporary directory and providing access to files within the repository. It handles cloning, file retrieval, and cleanup of the temporary directory, providing a context manager interface for resource management.
*   **Instantiation:** The class is not instantiated in the provided context.
*   **Dependencies:** The class depends on `tempfile`, `git.Repo`, `git.GitCommandError`, `logging`, and potentially `shutil` and `os` for file system operations.
*   **Constructor:**
    *   *Description:* The constructor initializes a `GitRepository` object by taking the repository URL as input. It creates a temporary directory, clones the repository into it using `Repo.clone_from`, and sets up attributes for accessing the repository and its files. It also handles potential `GitCommandError` exceptions during cloning, ensuring proper cleanup.
    *   *Parameters:*
        - **repo_url** (`string`): The URL of the Git repository to clone.
*   **Methods:**
    *   **`get_all_files`**
        *   *Signature:* `def get_all_files(self)`
        *   *Description:* The `get_all_files` method retrieves all files from the Git repository and returns them as a list of `RepoFile` objects. It uses `git.ls_files()` to get the file paths and then creates `RepoFile` instances for each valid path.
        *   *Parameters:*
            *Analysis data not available for this component.*
        *   *Returns:*
            - **files** (`list[RepoFile]`): A list of RepoFile instances representing all files in the repository.
    *   **`close`**
        *   *Signature:* `def close(self)`
        *   *Description:* The `close` method is responsible for cleaning up the temporary directory created during the initialization of the `GitRepository`. It checks if the temporary directory exists and, if so, removes it to free up resources.
        *   *Parameters:*
            *Analysis data not available for this component.*
        *   *Returns:*
            *Analysis data not available for this component.*
    *   **`__enter__`**
        *   *Signature:* `def __enter__(self)`
        *   *Description:* The `__enter__` method allows the `GitRepository` class to be used as a context manager. It simply returns the instance of the class itself, enabling the use of the `with` statement.
        *   *Parameters:*
            *Analysis data not available for this component.*
        *   *Returns:*
            *Analysis data not available for this component.*
    *   **`__exit__`**
        *   *Signature:* `def __exit__(self, exc_type, exc_val, exc_tb)`
        *   *Description:* The `__exit__` method is part of the context manager protocol. It ensures that the temporary directory is cleaned up when exiting the `with` block, by calling the `close` method.
        *   *Parameters:*
            - **exc_type** (`type`): The exception type.
            - **exc_val** (`Exception`): The exception value.
            - **exc_tb** (`TracebackType`): The traceback object.
        *   *Returns:*
            *Analysis data not available for this component.*
    *   **`get_file_tree`**
        *   *Signature:* `def get_file_tree(self, include_content)`
        *   *Description:* The `get_file_tree` method constructs a hierarchical tree structure representing the files and directories within the Git repository. It first retrieves all files using `get_all_files` if they haven't been fetched already. It then iterates through the file paths, building the directory structure based on the file paths and returns a dictionary representing the file tree.
        *   *Parameters:*
            - **include_content** (`bool`): A boolean flag indicating whether to include file content in the tree.
        *   *Returns:*
            *Analysis data not available for this component.*

---
### File: `backend/main.py`
#### Function: `main_workflow`
*   **Signature:** `def main_workflow()`
*   **Description:** The `main_workflow` function orchestrates a multi-step process for analyzing a Git repository. It begins by extracting a repository URL from user input, retrieves files from the repository, extracts basic project information, generates a file tree and performs AST analysis. It then prepares data for HelperLLM and MainLLM, generates documentation using these LLMs, and finally saves the final report.
*   **Parameters:**
    *Analysis data not available for this component.*
*   **Returns:**
    *Analysis data not available for this component.*
*   **Usage:**
    *   **Calls:** The function calls `re.search` to extract the repository URL, `GitRepository` to interact with the Git repository, `ProjektInfoExtractor` to extract project information, `ASTAnalyzer` to perform AST analysis, `LLMHelper` to generate documentation, and `MainLLM` to generate the final report.
    *   **Called By:** This function is called directly from the main execution block.

---
### File: `frontend/Frontend.py`
*Analysis data not available for this component.*
---
### File: `schemas/types.py`
#### Class: `ParameterDescription`
*   **Summary:** The `ParameterDescription` class, inheriting from `BaseModel`, is designed to represent and describe a single parameter of a function. It encapsulates the parameter's name, its data type, and a descriptive text. This class is intended to provide a structured way to document function parameters within a larger system.
*   **Instantiation:** This class is not instantiated within the provided context.
*   **Dependencies:** This class depends on `pydantic.BaseModel` for data validation and management.
*   **Constructor:**
    *   *Description:* The constructor initializes a `ParameterDescription` object with a name, type, and description.
    *   *Parameters:*
        *Analysis data not available for this component.*
*   **Methods:**
    *Analysis data not available for this component.*

#### Class: `ReturnDescription`
*   **Summary:** The `ReturnDescription` class, inheriting from `BaseModel`, is designed to encapsulate and describe the return value of a function. It provides a structured way to represent the name, type, and description of a function's return value, enhancing code documentation and readability.
*   **Instantiation:** This class is not instantiated within the provided context.
*   **Dependencies:** This class depends on `pydantic.BaseModel` for its functionality.
*   **Constructor:**
    *   *Description:* The constructor initializes a `ReturnDescription` object with a name, type, and description for the return value.
    *   *Parameters:*
        *Analysis data not available for this component.*
*   **Methods:**
    *Analysis data not available for this component.*

#### Class: `UsageContext`
*   **Summary:** The `UsageContext` class, inheriting from `BaseModel`, is designed to describe the calling context of a function. It encapsulates information about what functions a method calls and which functions call it.
*   **Instantiation:** This class is not instantiated by any other classes or functions.
*   **Dependencies:** This class does not have any external dependencies.
*   **Constructor:**
    *   *Description:* The `__init__` method initializes the `UsageContext` object. It sets up the attributes `calls` and `called_by`.
    *   *Parameters:*
        *Analysis data not available for this component.*
*   **Methods:**
    *Analysis data not available for this component.*

#### Class: `FunctionDescription`
*   **Summary:** The `FunctionDescription` class, inheriting from `BaseModel`, is designed to encapsulate a detailed analysis of a function. It stores information about the function's overall purpose, its parameters, its return values, and its usage context, providing a structured representation suitable for documentation or code analysis.
*   **Instantiation:** The class is not instantiated in the provided context.
*   **Dependencies:** The class has no external dependencies.
*   **Constructor:**
    *   *Description:* The `__init__` method initializes a `FunctionDescription` object. It sets up the attributes to store the overall description, a list of parameters, a list of return descriptions, and the usage context.
    *   *Parameters:*
        *Analysis data not available for this component.*
*   **Methods:**
    *Analysis data not available for this component.*

#### Class: `FunctionAnalysis`
*   **Summary:** The `FunctionAnalysis` class is designed to represent the JSON schema for a function, providing a structured way to store and analyze function-related information. It encapsulates the function's identifier, a detailed description, and an optional error message.
*   **Instantiation:** This class is not instantiated within the provided context.
*   **Dependencies:** This class has dependencies on `pydantic.BaseModel`, `typing.List`, `typing.Optional`, `typing.Literal`, and `pydantic.ValidationError`.
*   **Constructor:**
    *   *Description:* The constructor initializes a `FunctionAnalysis` object. It does not take any parameters.
    *   *Parameters:*
        *Analysis data not available for this component.*
*   **Methods:**
    *Analysis data not available for this component.*

#### Class: `ConstructorDescription`
*   **Summary:** The `ConstructorDescription` class, inheriting from `BaseModel`, is designed to describe the `__init__` method of a class. It encapsulates the description of the initialization process and a list of parameter descriptions.
*   **Instantiation:** This class is not instantiated within the provided context.
*   **Dependencies:** This class depends on `pydantic.BaseModel`, `typing.List` and other typing modules.
*   **Constructor:**
    *   *Description:* The class is initialized with a description (string) and a list of parameter descriptions. These attributes are used to provide information about the constructor of another class.
    *   *Parameters:*
        *Analysis data not available for this component.*
*   **Methods:**
    *Analysis data not available for this component.*

#### Class: `ClassContext`
*   **Summary:** The `ClassContext` class, inheriting from `BaseModel`, is designed to encapsulate information about a class's external dependencies and instantiation points. It serves as a data structure to hold and represent these contextual details.
*   **Instantiation:** The class is not instantiated anywhere.
*   **Dependencies:** The class has no external dependencies.
*   **Constructor:**
    *   *Description:* The constructor initializes the `ClassContext` object with dependencies and instantiation information.
    *   *Parameters:*
        - **dependencies** (`str`): A string representing the class's external dependencies.
        - **instantiated_by** (`str`): A string representing where the class is instantiated.
*   **Methods:**
    *Analysis data not available for this component.*

#### Class: `ClassDescription`
*   **Summary:** The `ClassDescription` class is designed to encapsulate and provide a structured analysis of a Python class, including its purpose, constructor, methods, and usage context. It leverages Pydantic's `BaseModel` for data validation and structure. The class is intended to be used as a data transfer object (DTO) to represent the analysis of a Python class.
*   **Instantiation:** This class is not instantiated in the provided context.
*   **Dependencies:** This class has no external dependencies.
*   **Constructor:**
    *   *Description:* The constructor initializes the `ClassDescription` object. It sets up the `overall`, `init_method`, `methods`, and `usage_context` attributes.
    *   *Parameters:*
        *Analysis data not available for this component.*
*   **Methods:**
    *Analysis data not available for this component.*

#### Class: `ClassAnalysis`
*   **Summary:** The `ClassAnalysis` class is a Pydantic model designed to encapsulate the analysis of a Python class. It serves as the main model for the entire JSON schema, providing a structured representation of class-level information, including its identifier, description, and any potential errors encountered during analysis.
*   **Instantiation:** The class is instantiated where the class analysis is performed.
*   **Dependencies:** The class depends on `BaseModel` from `pydantic` and `Optional` from `typing` for its functionality.
*   **Constructor:**
    *   *Description:* The constructor initializes a `ClassAnalysis` object. It takes an identifier, a description of type `ClassDescription`, and an optional error message as input.
    *   *Parameters:*
        - **identifier** (`str`): The name or identifier of the class being analyzed.
        - **description** (`ClassDescription`): An object containing a detailed analysis of the class, including its methods and overall purpose.
        - **error** (`Optional[str]`): An optional string containing an error message if any errors occurred during the class analysis; defaults to None.
*   **Methods:**
    *Analysis data not available for this component.*

#### Class: `FunctionContextInput`
*   **Summary:** The `FunctionContextInput` class is designed to provide structured context for analyzing a function. It uses Pydantic's `BaseModel` for data validation and defines two lists: `calls` and `called_by`, which store strings representing function calls and where the function is called from, respectively.
*   **Instantiation:** This class is not instantiated within the provided context.
*   **Dependencies:** This class depends on `pydantic.BaseModel` and `typing.List` for type hinting.
*   **Constructor:**
    *   *Description:* The class is initialized without any parameters, as it inherits from Pydantic's BaseModel, which handles initialization.
    *   *Parameters:*
        *Analysis data not available for this component.*
*   **Methods:**
    *Analysis data not available for this component.*

#### Class: `FunctionAnalysisInput`
*   **Summary:** The `FunctionAnalysisInput` class, inheriting from `BaseModel`, is designed to structure the input data required for generating a `FunctionAnalysis` object. It encapsulates the necessary information for analyzing a Python function, including the analysis mode, function identifier, source code, import statements, and context.
*   **Instantiation:** This class is not instantiated within the provided context.
*   **Dependencies:** This class depends on `pydantic.BaseModel`, `typing.List`, `typing.Literal` and `pydantic.ValidationError` for data validation and type hinting.
*   **Constructor:**
    *   *Description:* The class initializes with parameters specifying the analysis mode, identifier, source code, imports, and context for function analysis. These parameters are used to configure the analysis process.
    *   *Parameters:*
        *Analysis data not available for this component.*
*   **Methods:**
    *Analysis data not available for this component.*

#### Class: `MethodContextInput`
*   **Summary:** The `MethodContextInput` class is designed to provide structured context for a class's methods. It leverages Pydantic's `BaseModel` for data validation and type hinting, ensuring data integrity. It encapsulates information such as the method's identifier, the methods it calls, the methods that call it, the arguments it takes, and an optional docstring.
*   **Instantiation:** The class is instantiated where method context information needs to be structured and validated.
*   **Dependencies:** The class depends on `pydantic.BaseModel`, `typing.List`, and `typing.Optional` for data validation and type hinting.
*   **Constructor:**
    *   *Description:* The constructor initializes a `MethodContextInput` object. It sets the identifier, calls, called_by, args, and docstring attributes based on the provided input.
    *   *Parameters:*
        - **identifier** (`str`): The identifier of the method.
        - **calls** (`List[str]`): A list of strings representing the methods this method calls.
        - **called_by** (`List[str]`): A list of strings representing the methods that call this method.
        - **args** (`List[str]`): A list of strings representing the arguments of this method.
        - **docstring** (`Optional[str]`): An optional docstring for the method.
*   **Methods:**
    *Analysis data not available for this component.*

#### Class: `ClassContextInput`
*   **Summary:** The `ClassContextInput` class is designed to provide structured context for analyzing a class, inheriting from `BaseModel`. It encapsulates information about dependencies, instantiation locations, and method-level context, facilitating a comprehensive analysis of class behavior and usage.
*   **Instantiation:** This class is not instantiated within the provided context.
*   **Dependencies:** This class has no external dependencies.
*   **Constructor:**
    *   *Description:* The class is initialized without any parameters, as defined by the `BaseModel` class.
    *   *Parameters:*
        *Analysis data not available for this component.*
*   **Methods:**
    *Analysis data not available for this component.*

#### Class: `ClassAnalysisInput`
*   **Summary:** The `ClassAnalysisInput` class is designed to serve as the input model for a class analysis process. It leverages Pydantic's `BaseModel` to define a structured data format, ensuring data validation and type safety. The class encapsulates the necessary information for analyzing a Python class, including the analysis mode, class identifier, source code, import statements, and contextual information.
*   **Instantiation:** This class is not instantiated within the provided context.
*   **Dependencies:** This class has no external dependencies.
*   **Constructor:**
    *   *Description:* The constructor initializes the `ClassAnalysisInput` object, setting attributes based on the provided input data. It uses the `BaseModel` from Pydantic to handle data validation.
    *   *Parameters:*
        - **mode** (`Literal["class_analysis"]`): Specifies the analysis mode, which is fixed to "class_analysis".
        - **identifier** (`str`): The name or identifier of the class to be analyzed.
        - **source_code** (`str`): The raw source code of the class to be analyzed.
        - **imports** (`List[str]`): A list of import statements used in the source code.
        - **context** (`ClassContextInput`): Contextual information related to the class, such as dependencies and instantiation details.
*   **Methods:**
    *Analysis data not available for this component.*

---