# Project Documentation: repo-onboarding-agent documentation

## 1. Project Overview
- **Description:** This project is an automated code documentation agent. It takes a Git repository URL as input, clones the repository, and performs a multi-stage analysis to generate a comprehensive technical report. The agent leverages Abstract Syntax Trees (ASTs) for structural analysis, generates call graphs to map dependencies, and uses Large Language Models (LLMs) to analyze and describe the functionality of individual functions and classes.
- **Key Features:**
  - **Automated Repository Cloning:** Clones any public Git repository into a temporary environment for analysis.
  - **Static Code Analysis:** Parses Python code into Abstract Syntax Trees (ASTs) to understand its structure.
  - **Dependency Mapping:** Generates call graphs to visualize the relationships and dependencies between functions and methods.
  - **LLM-Powered Description Generation:** Utilizes Google's Gemini models to analyze code snippets and generate human-readable descriptions for functions and classes.
  - **Comprehensive Report Synthesis:** Aggregates all collected data into a single, well-structured Markdown documentation file.
- **Tech Stack:**
  - langchain
  - langchain-core
  - langchain-google-genai
  - google-generativeai
  - python-dotenv
  - pydantic
  - regex
  - networkx
  - GitPython

*   **Repository Structure:**
    ```mermaid
    graph LR;
        subgraph root
            direction LR
            A0(".env.example")
            A1(".gitignore")
            A2("analysis_output.json")
            A3("readme.md")
            A4("requirements.txt")
        end

        subgraph SystemPrompts
            direction LR
            B0("SystemPrompts/SystemPromptClassHelperLLM.txt")
            B1("SystemPrompts/SystemPromptFunctionHelperLLM.txt")
            B2("SystemPrompts/SystemPromptHelperLLM.txt")
            B3("SystemPrompts/SystemPromptMainLLM.txt")
        end

        subgraph backend
            direction LR
            C0("backend/AST_Schema.py")
            C1("backend/HelperLLM.py")
            C2("backend/MainLLM.py")
            C3("backend/basic_info.py")
            C4("backend/callgraph.py")
            C5("backend/getRepo.py")
            C6("backend/main.py")
            C7("backend/tools.py")
        end

        subgraph frontend
            direction LR
            D0("frontend/Frontend.py")
        end

        subgraph notizen
            direction LR
            E0("notizen/Report Agenda.txt")
            E1("notizen/Zwischenpraesentation Agenda.txt")
            E2("notizen/doc_bestandteile.md")
            E3("notizen/notizen.md")
            E4("notizen/paul_notizen.md")
            E5("notizen/praesentation_notizen.md")
            E6("notizen/technische_notizen.md")
        end

        subgraph notizen/grafiken
            direction LR
            F0("notizen/grafiken/AST.dot")
            F1("notizen/grafiken/Frontend.dot")
            F2("notizen/grafiken/HelperLLM.dot")
            F3("notizen/grafiken/HelperLLM.png")
            F4("notizen/grafiken/MainLLM.dot")
            F5("notizen/grafiken/agent.dot")
            F6("notizen/grafiken/basic_info.dot")
            F7("notizen/grafiken/callgraph.dot")
            F8("notizen/grafiken/getRepo.dot")
            F9("notizen/grafiken/graph_AST.png")
            F10("notizen/grafiken/graph_AST2.png")
            F11("notizen/grafiken/graph_AST3.png")
            F12("notizen/grafiken/main.dot")
            F13("notizen/grafiken/tools.dot")
            F14("notizen/grafiken/types.dot")
        end

        subgraph result
            direction LR
            G0("result/report_14_11_2025_14-52-36.md")
            G1("result/report_14_11_2025_15-21-53.md")
            G2("result/report_14_11_2025_15-26-24.md")
            G3("result/result_2025-11-11_12-30-53.md")
            G4("result/result_2025-11-11_12-43-51.md")
            G5("result/result_2025-11-11_12-45-37.md")
        end

        subgraph schemas
            direction LR
            H0("schemas/types.py")
        end

        root --> SystemPrompts;
        root --> backend;
        root --> frontend;
        root --> notizen;
        notizen --> notizen/grafiken;
        root --> result;
        root --> schemas;
    ```

## 2. Installation
### Dependencies
To install the necessary dependencies, run the following command. It is highly recommended to use a virtual environment.
```bash
pip install -r requirements.txt
```
- langchain 
- langchain-core 
- langchain-google-genai 
- google-generativeai 
- python-dotenv 
- pydantic 
- regex 
- networkx 
- GitPython

### Setup Guide
1.  **Clone the Repository:**
    ```bash
    git clone https://github.com/christiand03/repo-onboarding-agent.git
    cd repo-onboarding-agent
    ```
2.  **Create and Activate a Virtual Environment:**
    ```bash
    # For Windows
    python -m venv venv
    venv\Scripts\activate
    
    # For macOS/Linux
    python3 -m venv venv
    source venv/bin/activate
    ```
3.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
4.  **Configure Environment Variables:**
    -   Copy the example environment file: `cp .env.example .env`
    -   Open the `.env` file and add your Google Gemini API key:
        ```
        GEMINI_API_KEY="YOUR_API_KEY_HERE"
        ```

### Quick Startup
To run the main analysis workflow, execute the main script from the root directory:
```bash
python backend/main.py
```
Note: The target repository URL is currently hardcoded in `backend/main.py`. You may need to change it to analyze a different repository.

## 3. Use Cases & Commands
The primary use case of this project is to automate the technical documentation of a software repository. By providing a Git URL, a user can receive a detailed Markdown report that covers the project's overview, structure, and a deep dive into its source code.

**Primary Command:**

The core workflow is initiated by running the `main.py` script.

```bash
python backend/main.py
```

This command triggers the following automated pipeline:
1.  **URL Extraction:** A Git repository URL is extracted from the script's configuration.
2.  **Repository Cloning:** The agent clones the specified repository into a temporary local directory.
3.  **Initial Analysis:** Basic project information (name, description from README) and file structure are extracted.
4.  **AST & Call Graph Generation:** The agent performs a deep static analysis of the Python source code, building an Abstract Syntax Tree (AST) and a call graph to understand the code's structure and dependencies.
5.  **LLM-based Code Description:** The structured code data is sent to a Helper LLM (Gemini) to generate detailed, human-readable descriptions for every class and function.
6.  **Report Synthesis:** A Main LLM (Gemini) receives all the collected and generated data and synthesizes it into a final, cohesive Markdown report.
7.  **Output:** The final report is saved in the `result/` directory with a timestamped filename.

## 4. Architecture
The Mermaid Syntax to visualize Graphs is not set up yet and will be added
but if there is mermaid syntax in your input json display it here

## 5. Code Analysis
### File: `backend/AST_Schema.py`
#### Class: `ASTVisitor`
*   **Summary:** The ASTVisitor class is a specialized implementation of ast.NodeVisitor designed to traverse an Abstract Syntax Tree (AST) and extract structural metadata about the source code. Its primary function is to build a hierarchical schema containing information about imports, standalone functions, and classes, including their methods. It overrides standard visit_* methods to capture specific nodes like imports, class definitions, and function definitions, differentiating between methods nested within classes and top-level functions. It uses the raw source code to retrieve exact code segments for classes and standalone functions, facilitating subsequent analysis steps.
*   **Instantiation:** The class is not instantiated by any components listed in the context.
*   **Dependencies:** The class does not explicitly depend on other components listed in the context.
*   **Constructor:**
    *   *Description:* The constructor initializes the visitor with the raw source_code string. It sets up the internal schema dictionary, which will store extracted information about imports, functions, and classes found during the AST traversal. It also initializes _current_class to None, which is used to track whether the visitor is currently inside a class definition.
    *   *Parameters:*
        - **source_code** (`str`): The raw source code string that the visitor will analyze.
*   **Methods:**
    *   **`visit_Import`**
        *   *Signature:* `def visit_Import(self, node)`
        *   *Description:* This method handles ast.Import nodes (e.g., `import module`). It iterates through all aliases defined in the import statement and extracts the name of the imported module. It appends these module names directly to the internal schema's imports list. Finally, it calls generic_visit(node) to ensure recursive traversal continues down the AST branch.
        *   *Parameters:*
            - **node** (`ast.Import`): The AST node representing an import statement.
        *   *Returns:* None
    *   **`visit_ImportFrom`**
        *   *Signature:* `def visit_ImportFrom(self, node)`
        *   *Description:* This method handles ast.ImportFrom nodes (e.g., `from module import name`). It iterates over the imported names and constructs a fully qualified import string using the module name and the alias name. This fully qualified string is appended to the internal schema's imports list. It concludes by ensuring recursive traversal using generic_visit.
        *   *Parameters:*
            - **node** (`ast.ImportFrom`): The AST node representing a 'from ... import ...' statement.
        *   *Returns:* None
    *   **`visit_ClassDef`**
        *   *Signature:* `def visit_ClassDef(self, node)`
        *   *Description:* This method processes ast.ClassDef nodes, extracting comprehensive information about the class definition, including its name, docstring, and raw source code segment. This structured information is appended to the internal schema's classes list. Crucially, it sets the internal state variable _current_class to this new dictionary before calling generic_visit to allow nested function visitors to correctly associate methods with this class, resetting the state afterwards.
        *   *Parameters:*
            - **node** (`ast.ClassDef`): The AST node representing a class definition.
        *   *Returns:* None
    *   **`visit_FunctionDef`**
        *   *Signature:* `def visit_FunctionDef(self, node)`
        *   *Description:* This method handles both standalone functions and class methods (ast.FunctionDef). It checks the state of _current_class to determine context. If _current_class is set, the function is treated as a method, and its metadata is appended to the method_context list within the current class's schema entry. Otherwise, it is treated as a standalone function, and a full analysis structure, including source code segment retrieval, is created and appended to the schema's functions list. In both cases, generic_visit is called to continue traversal.
        *   *Parameters:*
            - **node** (`ast.FunctionDef`): The AST node representing a function definition.
        *   *Returns:* None
    *   **`visit_AsyncFunctionDef`**
        *   *Signature:* `def visit_AsyncFunctionDef(self, node)`
        *   *Description:* This method handles asynchronous function definitions (ast.AsyncFunctionDef). It delegates all processing logic directly to visit_FunctionDef, treating async functions identically to standard functions for the purpose of schema extraction.
        *   *Parameters:*
            - **node** (`ast.AsyncFunctionDef`): The AST node representing an asynchronous function definition.
        *   *Returns:* None
#### Class: `ASTAnalyzer`
*   **Summary:** The ASTAnalyzer class is responsible for performing comprehensive static analysis across a collection of source code files, typically representing a repository. It iterates through Python files, parses them into Abstract Syntax Trees (ASTs), and extracts structural information (functions, classes, imports) using an ASTVisitor. Crucially, it also generates a function call graph for each file and integrates this relational data into the structural schema, providing a holistic view of the code's components and their dependencies. The final output is a structured dictionary containing the full schema for the analyzed repository.
*   **Instantiation:** The class is not instantiated by any known entity within the provided context.
*   **Dependencies:** The class relies on the standard ast library, the networkx library (aliased as nx in the code), and the external function build_callGraph for generating the call graph.
*   **Constructor:**
    *   *Description:* The constructor for ASTAnalyzer is minimal, performing no specific initialization or attribute assignment upon instantiation.
    *   *Parameters:* None
*   **Methods:**
    *   **`_enrich_schema_with_callgraph`**
        *   *Signature:* `def _enrich_schema_with_callgraph(schema, call_graph, filename)`
        *   *Description:* This static helper method integrates call graph data into an existing structural schema dictionary. It iterates through both standalone functions and class methods defined in the schema. For each entity, it queries the NetworkX directed graph (call_graph) to identify its callers (predecessors) and callees (successors). These lists of related entities are then sorted and stored directly within the 'context' field of the corresponding function or method entry in the schema, effectively linking structural analysis with dependency analysis.
        *   *Parameters:*
            - **schema** (`dict`): The structural schema dictionary generated by the AST visitor, which needs call graph context added.
            - **call_graph** (`nx.DiGraph`): The NetworkX directed graph containing function and method call relationships.
            - **filename** (`str`): The path of the file being processed, used to construct fully qualified function names.
        *   *Returns:* None
    *   **`analyze_repository`**
        *   *Signature:* `def analyze_repository(self, files)`
        *   *Description:* This is the primary execution method, responsible for iterating over a list of file objects to generate a complete repository schema. It filters for Python files, parses their content into an AST, uses an ASTVisitor to extract structural nodes, and builds a call graph using build_callGraph. It then invokes _enrich_schema_with_callgraph to merge the dependency data. The method handles potential SyntaxErrors or ValueErrors during parsing and aggregates the results into a comprehensive dictionary keyed by file path.
        *   *Parameters:*
            - **files** (`list`): A list of file objects, each expected to contain 'path' and 'content' attributes.
        *   *Returns:*
            - **full_schema** (`dict`): A dictionary containing the complete analysis of the repository, structured by file path, including AST nodes and enriched call graph context.
---
### File: `backend/HelperLLM.py`
#### Class: `LLMHelper`
*   **Summary:** The LLMHelper class acts as a specialized wrapper for interacting with the Google Gemini API, focusing on generating structured documentation for code snippets. It centralizes API key management, loads specific system prompts for different analysis types (functions and classes), and configures dedicated LangChain clients to ensure the output strictly conforms to predefined Pydantic schemas (FunctionAnalysis and ClassAnalysis). Its primary responsibility is to handle batch requests for analysis, managing conversation construction, API calls, and robust error handling during structured parsing.
*   **Instantiation:** The input context indicates that there are no known direct instantiators, suggesting it is likely instantiated in a main execution script or configuration file.
*   **Dependencies:** The class relies on external libraries like langchain_google_genai for LLM interaction, json for serialization, logging, and custom Pydantic schemas (FunctionAnalysis, ClassAnalysis, etc.).
*   **Constructor:**
    *   *Description:* The constructor initializes the LLMHelper by setting up connections to the Gemini API using the provided API key and model name. It reads system prompts for both function and class analysis from specified file paths, raising a FileNotFoundError if they are missing. It then configures three internal LLM clients: one for raw text generation, and two structured output clients specifically configured to return Pydantic models for function and class analysis, respectively.
    *   *Parameters:*
        - **api_key** (`str`): The API key required to authenticate with the Gemini service. A ValueError is raised if it is not provided.
        - **function_prompt_path** (`str`): File path to the system prompt used for function analysis.
        - **class_prompt_path** (`str`): File path to the system prompt used for class analysis.
        - **model_name** (`str`): The name of the Gemini model to use (defaults to "gemini-flash-latest").
*   **Methods:**
    *   **`generate_for_functions`**
        *   *Signature:* `def generate_for_functions(self, function_inputs)`
        *   *Description:* This method handles the batch generation of documentation for multiple functions using the structured LLM client dedicated to function analysis (self.function_llm). It first checks if the input list is empty and returns an empty list if so. For each input object (FunctionAnalysisInput), it serializes it into a JSON payload. These payloads are then combined with the stored function system prompt to create a list of conversations, which are sent to the Gemini API using a batch call. The method returns a list of validated FunctionAnalysis objects, or a list of None values if an exception occurs during the API call or structured parsing.
        *   *Parameters:*
            - **function_inputs** (`List[FunctionAnalysisInput]`): A list of Pydantic models containing the source code and context required for function analysis.
        *   *Returns:*
            - **validated_functions** (`List[Optional[FunctionAnalysis]]`): A list of successfully parsed FunctionAnalysis objects, or None for any input that failed validation or API processing.
    *   **`generate_for_classes`**
        *   *Signature:* `def generate_for_classes(self, class_inputs)`
        *   *Description:* This method is responsible for generating structured documentation for a batch of classes using the dedicated class analysis LLM client (self.class_llm). It processes a list of ClassAnalysisInput objects, converting each into a JSON payload. These payloads are paired with the class system prompt to form conversation messages. It then executes a batch API call to Gemini, expecting the response to conform to the ClassAnalysis Pydantic schema. If successful, it returns the list of validated class analyses; otherwise, it logs the error and returns a list of None values matching the input batch size.
        *   *Parameters:*
            - **class_inputs** (`List[ClassAnalysisInput]`): A list of Pydantic models containing the source code and context required for class analysis.
        *   *Returns:*
            - **validated_classes** (`List[Optional[ClassAnalysis]]`): A list of successfully parsed ClassAnalysis objects, or None for any input that failed validation or API processing.
#### Function: `main_orchestrator`
*   **Signature:** `def main_orchestrator()`
*   **Description:** This function serves as the main orchestration and testing loop for the LLM documentation generation system. It defines several pre-computed analysis inputs and outputs using Pydantic models (FunctionAnalysisInput, FunctionAnalysis) for demonstration purposes. It initializes an LLMHelper instance, simulates the process of generating documentation for functions using the helper's methods, and then iterates through the results. Finally, it aggregates the successful analyses into a final_documentation dictionary, logs the process, and prints the resulting JSON structure.
*   **Parameters:** None
*   **Returns:** None
*   **Usage:**
    - **Calls:** This function primarily calls Pydantic validation methods (model_validate, model_dump), logging functions (logging.info, logging.warning), and methods on the initialized LLMHelper instance, specifically generate_for_functions. It also uses json.dumps to format the final output.
    - **Called By:** This function is called by the <main_block>, indicating it is likely the entry point of the script.
---
### File: `backend/MainLLM.py`
#### Class: `MainLLM`
*   **Summary:** The MainLLM class acts as the core handler for interacting with the Google Gemini large language model, utilizing the LangChain framework. It is responsible for initialization, which includes API key validation, loading a mandatory system prompt from a file, and configuring the LLM client. The class exposes methods for both synchronous (single-response) and asynchronous (streaming) interactions, ensuring all queries are framed by the loaded system prompt.
*   **Instantiation:** This class is currently not instantiated by any other component listed in the provided context.
*   **Dependencies:** The class relies on the standard Python logging module and several components from the langchain_google_genai and langchain.messages libraries, specifically ChatGoogleGenerativeAI, HumanMessage, and SystemMessage.
*   **Constructor:**
    *   *Description:* The constructor initializes the LLM client by validating the provided API key and attempting to load the system prompt from the specified file path. If the file is not found or the API key is missing, it raises an error. Upon successful setup, it instantiates the ChatGoogleGenerativeAI client with the specified model name and a temperature of 1.0, storing the client as an instance attribute.
    *   *Parameters:*
        - **api_key** (`str`): The API key required to authenticate with the Gemini service.
        - **prompt_file_path** (`str`): The path to the file containing the system prompt that guides the LLM's behavior.
        - **model_name** (`str`): The name of the Gemini model to use, defaulting to 'gemini-2.5-pro'.
*   **Methods:**
    *   **`call_llm`**
        *   *Signature:* `def call_llm(self, user_input)`
        *   *Description:* This method executes a standard, synchronous call to the configured Language Model (LLM). It constructs a list of messages, including the pre-loaded system prompt and the user's input, using LangChain's SystemMessage and HumanMessage classes. It then invokes the LLM using the internal client's invoke method. If the API call is successful, it returns the text content of the LLM's response; otherwise, it logs the error and returns None.
        *   *Parameters:*
            - **user_input** (`str`): The specific query or input provided by the user to be processed by the LLM.
        *   *Returns:*
            - **content** (`str | None`): The generated text response from the LLM, or None if an exception occurred during the API call.
    *   **`stream_llm`**
        *   *Signature:* `def stream_llm(self, user_input)`
        *   *Description:* This method initiates a streaming interaction with the LLM, allowing the response to be processed chunk by chunk as a generator. It prepares the conversation history using the system prompt and user input, then utilizes the internal LLM client's stream method to get an iterator over the response chunks. The method yields the content of each chunk as it arrives, and handles exceptions by logging the error and yielding the error message string.
        *   *Parameters:*
            - **user_input** (`str`): The specific query or input provided by the user for the streaming LLM interaction.
        *   *Returns:*
            - **chunk.content** (`Generator[str]`): A generator that yields individual text chunks of the LLM's response, or an error message string if the stream fails.
---
### File: `backend/basic_info.py`
#### Class: `ProjektInfoExtractor`
*   **Summary:** The ProjektInfoExtractor class is a specialized tool designed to systematically extract structured metadata and descriptive content from common Python project configuration files, such as pyproject.toml, requirements.txt, and README files. It operates by prioritizing these sources, ensuring that core metadata from TOML files overrides descriptive text found in READMEs. The class uses internal parsing methods and regular expressions to locate and process specific sections within file contents, compiling all extracted data into a standardized, structured dictionary format for consumption by downstream systems.
*   **Instantiation:** The instantiation context for this class was not provided in the input data.
*   **Dependencies:** The class relies on external modules such as re for regular expression processing, os for path manipulation, and tomllib for parsing TOML files.
*   **Constructor:**
    *   *Description:* The constructor initializes the class by defining the constant placeholder string 'INFO_NICHT_GEFUNDEN'. It sets up the main instance attribute `self.info`, a nested dictionary structure used to store all extracted project metadata, initializing all fields with the placeholder value.
    *   *Parameters:* None
*   **Methods:**
    *   **`_finde_datei`**
        *   *Signature:* `def _finde_datei(self, patterns, dateien)`
        *   *Description:* This private utility method searches a provided list of file objects (`dateien`) to locate a file whose path matches any of the specified `patterns`. The comparison is performed case-insensitively by converting both the file path and the pattern to lowercase. It iterates through all files and patterns, returning the first matching file object found.
        *   *Parameters:*
            - **patterns** (`List[str]`): A list of filename patterns (e.g., 'readme.md') to search for.
            - **dateien** (`List[Any]`): A list of file objects, expected to have a 'path' attribute.
        *   *Returns:*
            - **null** (`Optional[Any]`): The matching file object if found, otherwise None.
    *   **`_extrahiere_sektion_aus_markdown`**
        *   *Signature:* `def _extrahiere_sektion_aus_markdown(self, inhalt, keywords)`
        *   *Description:* This private method extracts content located beneath a level two Markdown heading (##) using regular expressions. It dynamically builds a regex pattern based on a list of provided keywords, ensuring a case-insensitive match for the section title. The method captures the text block following the matched heading up until the next H2 heading or the end of the document, returning the cleaned text.
        *   *Parameters:*
            - **inhalt** (`str`): The entire Markdown text content to be parsed.
            - **keywords** (`List[str]`): A list of alternative keywords that define the target section title.
        *   *Returns:*
            - **null** (`Optional[str]`): The extracted text section content, or None if the section is not present.
    *   **`_parse_readme`**
        *   *Signature:* `def _parse_readme(self, inhalt)`
        *   *Description:* This method parses the content of a README file, primarily serving as a source for descriptive project information when higher-priority files (like TOML) are missing data. It uses regular expressions to extract the main project title (H1) and the initial description text. It then repeatedly calls the internal method `_extrahiere_sektion_aus_markdown` to locate and extract structured sections such as Key Features, Tech Stack, Status, Installation, and Quick Start guides, updating the internal state only if the fields are currently set to the 'not found' placeholder.
        *   *Parameters:*
            - **inhalt** (`str`): The raw text content of the README file.
        *   *Returns:* None
    *   **`_parse_toml`**
        *   *Signature:* `def _parse_toml(self, inhalt)`
        *   *Description:* This method parses the content of a `pyproject.toml` file, prioritizing it as the authoritative source for core project metadata. It uses `tomllib.loads` to deserialize the content and extracts the project name, description, and dependencies from the `[project]` table. It unconditionally updates the corresponding fields in the internal `self.info` structure. The method includes a check for the availability of `tomllib` and handles potential `TOMLDecodeError` exceptions.
        *   *Parameters:*
            - **inhalt** (`str`): The raw text content of the pyproject.toml file.
        *   *Returns:* None
    *   **`_parse_requirements`**
        *   *Signature:* `def _parse_requirements(self, inhalt)`
        *   *Description:* This method parses the content of a `requirements.txt` file to extract dependencies, serving as a secondary source if dependencies were not found in the `pyproject.toml` file. It splits the content into lines, filters out comments and empty lines, and stores the resulting list of dependency strings in the `self.info` structure. It only updates the dependencies field if it still holds the 'not found' placeholder.
        *   *Parameters:*
            - **inhalt** (`str`): The raw text content of the requirements.txt file.
        *   *Returns:* None
    *   **`extrahiere_info`**
        *   *Signature:* `def extrahiere_info(self, dateien, repo_url)`
        *   *Description:* This is the core orchestration method that drives the information extraction process. It first locates relevant project files (TOML, requirements.txt, README) using `_finde_datei`. It then processes these files sequentially based on priority: TOML for core metadata, requirements.txt for dependency fallback, and README for descriptive content. After parsing, it formats the extracted dependencies into a readable string and sets the final project title based on the provided repository URL, returning the complete structured information dictionary.
        *   *Parameters:*
            - **dateien** (`List[Any]`): A list of file objects (e.g., RepoFile) from which information should be extracted.
            - **repo_url** (`str`): The URL of the repository, used to derive the final project title.
        *   *Returns:*
            - **null** (`Dict[str, Any]`): A dictionary containing all extracted project information, structured under 'projekt_uebersicht' and 'installation'.
---
### File: `backend/callgraph.py`
#### Class: `CallGraph`
*   **Summary:** The CallGraph class is an Abstract Syntax Tree (AST) visitor, inheriting from ast.NodeVisitor, designed to analyze Python source code and construct a detailed call graph. It traverses the AST, collecting function and class definitions to maintain context, and specifically processes import statements to map module names. Its core functionality resides in the `visit_Call` method, which identifies function invocations, resolves caller and callee names using fully qualified identifiers (including file and class scope), and populates the `self.edges` dictionary representing the graph structure. It also includes special handling for asynchronous functions and the `if __name__ == "__main__"` block to accurately map global entry points.
*   **Instantiation:** The instantiation points for this class are not specified in the provided context.
*   **Dependencies:** This class relies on the `ast` module for its core functionality as an AST visitor, and implicitly depends on external variables like `filename` and `graph` for context and output.
*   **Constructor:**
    *   *Description:* The constructor initializes the visitor state by setting the current function and class tracking variables to None. it also sets up core data structures required for call graph construction, including a dictionary for import mapping, a set for tracking functions (though initialized as an empty tuple), and a dictionary to store the graph edges (caller to set of callees).
    *   *Parameters:* None
*   **Methods:**
    *   **`_recursive_call`**
        *   *Signature:* `def _recursive_call(self, node)`
        *   *Description:* This private helper method recursively traverses an Abstract Syntax Tree (AST) node, typically starting from the `func` attribute of an `ast.Call` node, to extract the raw name(s) of the function being called. It handles three types of nodes: `ast.Call` (recursing further), `ast.Name` (extracting the ID), and `ast.Attribute` (extracting the attribute name). This ensures that complex call structures, such as method calls or chained calls, are broken down to their base identifiers for later resolution.
        *   *Parameters:*
            - **node** (`ast.AST`): The AST node to analyze for call names.
        *   *Returns:*
            - **all_calls** (`list[str]`): A list of strings representing the extracted call names.
    *   **`_resolve_all_callee_names`**
        *   *Signature:* `def _resolve_all_callee_names(self, callee_nodes)`
        *   *Description:* This private method takes a list of raw callee names extracted from the AST and resolves them into fully qualified names suitable for the call graph. The resolution prepends the global `filename` variable and, if the visitor is currently inside a class (`self.current_class` is set), it also includes the class name in the format `filename::[class_name]::raw_callee`. This ensures unique identification of functions or methods across the project.
        *   *Parameters:*
            - **callee_nodes** (`list[str]`): A list of raw function or method names.
        *   *Returns:*
            - **resolved_callees** (`list[str]`): A list of fully resolved, qualified function or method names.
    *   **`_make_full_name`**
        *   *Signature:* `def _make_full_name(self, basename, class_name)`
        *   *Description:* This private utility function constructs a fully qualified name for a function or method based on its base name, optionally including the class name if provided. The resulting string is standardized as `filename::basename` or `filename::class_name::basename`. This standardization is crucial for creating unique node identifiers in the call graph.
        *   *Parameters:*
            - **basename** (`str`): The simple name of the function or method.
            - **class_name** (`str | None`): The name of the enclosing class, if applicable.
        *   *Returns:*
            - **full_name** (`str`): The fully qualified name string.
    *   **`_current_caller`**
        *   *Signature:* `def _current_caller(self)`
        *   *Description:* This private method determines the identifier of the current execution scope, which acts as the caller node in the call graph. If `self.current_function` is set (meaning the visitor is inside a function definition), that name is returned. Otherwise, it defaults to a file-level scope identifier, using `<filename>` if `filename` is defined, or `<global-scope>` otherwise.
        *   *Parameters:* None
        *   *Returns:*
            - **caller** (`str`): The fully qualified name of the current calling context (function name, <filename>, or <global-scope>).
    *   **`visit_Import`**
        *   *Signature:* `def visit_Import(self, node)`
        *   *Description:* This method handles `ast.Import` nodes. It iterates through all imported modules defined in the node, mapping the module's alias (or its original name if no alias exists) to the module's actual name in `self.import_mapping`. After processing the import statement, it calls `self.generic_visit(node)` to continue traversing the rest of the AST.
        *   *Parameters:*
            - **node** (`ast.Import`): The AST node representing an import statement.
        *   *Returns:* None
    *   **`visit_ImportFrom`**
        *   *Signature:* `def visit_ImportFrom(self, node)`
        *   *Description:* This method handles `ast.ImportFrom` nodes (e.g., `from module import name`). It extracts the base module name and maps the imported names (or their aliases) to this base module name within `self.import_mapping`. It notes a TODO regarding resolving relative imports and level depth, indicating incomplete import resolution logic.
        *   *Parameters:*
            - **node** (`ast.ImportFrom`): The AST node representing a 'from ... import ...' statement.
        *   *Returns:* None
    *   **`visit_ClassDef`**
        *   *Signature:* `def visit_ClassDef(self, node)`
        *   *Description:* This method handles `ast.ClassDef` nodes. It temporarily updates `self.current_class` to the name of the class being visited, saving the previous class name for restoration. It then manually iterates over the functions defined within the class body, calling `self.visit(function)` for each, ensuring that methods are correctly processed within the class context. Finally, it restores the previous `self.current_class` state upon exiting the class definition.
        *   *Parameters:*
            - **node** (`ast.ClassDef`): The AST node representing a class definition.
        *   *Returns:* None
    *   **`visit_FunctionDef`**
        *   *Signature:* `def visit_FunctionDef(self, node)`
        *   *Description:* This method processes standard function definitions (`def`). It determines the fully qualified name of the function using `_make_full_name`, incorporating the class name if the function is a method. It then registers this function name as a node in the global `graph` object and sets `self.current_function` to the qualified name. After using `self.generic_visit(node)` to traverse the function body, it resets the current function context.
        *   *Parameters:*
            - **node** (`ast.FunctionDef`): The AST node representing a function definition.
        *   *Returns:* None
    *   **`visit_AsyncFunctionDef`**
        *   *Signature:* `def visit_AsyncFunctionDef(self, node)`
        *   *Description:* This method handles asynchronous function definitions (`async def`). For the purpose of call graph construction, it treats asynchronous functions identically to synchronous functions by delegating its entire processing logic to `visit_FunctionDef(node)`. This ensures that async functions are correctly registered as nodes and their bodies are traversed.
        *   *Parameters:*
            - **node** (`ast.AsyncFunctionDef`): The AST node representing an asynchronous function definition.
        *   *Returns:* None
    *   **`visit_Call`**
        *   *Signature:* `def visit_Call(self, node)`
        *   *Description:* This is the core method for identifying function calls and building graph edges. It determines the caller using `_current_caller`, extracts raw callee names using `_recursive_call`, and resolves them using `_resolve_all_callee_names`. It then updates the `self.edges` dictionary, adding the resolved callees to the caller's set of dependencies. The method includes robust error handling to catch unexpected issues during call processing, printing a warning instead of crashing, and ensures traversal continues via `self.generic_visit(node)`.
        *   *Parameters:*
            - **node** (`ast.Call`): The AST node representing a function or method call.
        *   *Returns:* None
    *   **`visit_If`**
        *   *Signature:* `def visit_If(self, node)`
        *   *Description:* This method checks for the `if __name__ == "__main__"` idiom. If this pattern is detected, it temporarily overrides `self.current_function` to the special identifier `<main_block>`. This ensures that any function calls found within the main execution block are correctly attributed to this special caller node in the call graph, rather than the global scope. If the `if` statement is not the main block check, it proceeds with normal traversal using `generic_visit`.
        *   *Parameters:*
            - **node** (`ast.If`): The AST node representing an if statement.
        *   *Returns:* None
#### Function: `build_callGraph`
*   **Signature:** `def build_callGraph(tree, filename, file_content)`
*   **Description:** This function constructs a directed call graph (using `networkx.DiGraph`) from a provided Python Abstract Syntax Tree (AST). It defines a nested `ast.NodeVisitor` subclass, `CallGraph`, to traverse the AST and identify function/class definitions and function calls. Nodes in the graph represent functions, methods, or the global scope, qualified by the provided filename. Edges are created based on observed function calls, including special handling for imports and the `if __name__ == "__main__"` block, which is represented as a dedicated `<main_block>` caller node.
*   **Parameters:**
    - **tree** (`ast.AST`): The Abstract Syntax Tree of the Python file to be analyzed.
    - **filename** (`str | None`): The name of the analyzed file (e.g., 'main.py'), used to fully qualify function and class names within the graph.
    - **file_content** (`str | None`): Optional content of the file. This parameter is defined in the signature but is not utilized in the provided function logic.
*   **Returns:**
    - **graph** (`nx.DiGraph`): The complete directed call graph, where nodes are qualified function/scope names and edges represent calls.
*   **Usage:**
    - **Calls:** This function initializes and manipulates a networkx DiGraph object to store the call relationships.
    - **Called By:** This function is called from the main execution block (<main_block>).
#### Function: `graph_to_adj_list`
*   **Signature:** `def graph_to_adj_list(graph)`
*   **Description:** This function converts a NetworkX directed graph (nx.DiGraph), typically representing a call graph, into a standard Python dictionary format suitable for JSON serialization. It iterates over all nodes in the graph, ensuring they are processed in sorted order for output consistency. For each node, it retrieves and sorts its successors (the functions it calls). The resulting dictionary maps each calling node (key) to a list of its called nodes (value), excluding nodes that have no outgoing edges.
*   **Parameters:**
    - **graph** (`nx.DiGraph`): The NetworkX directed graph (Call-Graph) that needs to be converted into an adjacency list representation.
*   **Returns:**
    - **adj_list** (`Dict[str, list[str]]`): An adjacency list dictionary where string keys represent caller nodes and the list of string values represents the sorted callees for that node.
*   **Usage:**
    - **Calls:** This function utilizes built-in functions like 'list' and 'sorted', and NetworkX methods such as 'nodes' and 'successors' to traverse and process the graph structure.
    - **Called By:** This function is not explicitly called by any other function listed in the provided context.
#### Function: `build_global_callgraph`
*   **Signature:** `def build_global_callgraph(all_repo_files)`
*   **Description:** This function is designed to construct a directed graph representing the global call relationships across all files in a repository. It accepts a collection of repository file objects as input and is type-hinted to return a NetworkX directed graph (nx.DiGraph). However, the current implementation is a placeholder. It immediately returns the `NotImplementedError` class, signaling that the core logic for building the call graph is missing and the feature is not yet functional.
*   **Parameters:**
    - **all_repo_files** (`list[RepoFile]`): A list containing objects representing all files within the repository, which serve as the source data for building the global call graph structure.
*   **Returns:**
    - **NotImplementedError** (`NotImplementedError`): An instance of the NotImplementedError class, indicating that the function body is currently a placeholder and the call graph generation logic has not yet been implemented.
*   **Usage:**
    - **Calls:** This function does not appear to make any internal calls to other functions or methods.
    - **Called By:** This function is not known to be called by any other functions in the provided context.
#### Function: `make_safe_dot`
*   **Signature:** `def make_safe_dot(graph, out_path)`
*   **Description:** This function prepares a NetworkX directed graph for serialization into the DOT format by ensuring all node identifiers are safe. It first creates a copy of the input graph and generates a mapping from the original, potentially complex node names to simple, indexed IDs (e.g., "n0", "n1"). It then relabels the nodes in the copied graph using these safe IDs. Crucially, the original node names are preserved by setting them as the "label" attribute for each newly labeled node, which ensures readability in the resulting visualization. Finally, the function uses the NetworkX utility to write the relabeled graph to the specified output path.
*   **Parameters:**
    - **graph** (`nx.DiGraph`): The NetworkX directed graph object whose nodes need to be relabeled for safe DOT serialization.
    - **out_path** (`str`): The file path where the resulting DOT file should be written.
*   **Returns:** None
*   **Usage:**
    - **Calls:** This function calls methods on the graph object such as `copy`, `nodes`, `list`, `enumerate`, and `items`, and utilizes NetworkX functions `relabel_nodes` and `write_dot` for graph manipulation and file output.
    - **Called By:** This function is called by the main execution block (<main_block>).
---
### File: `backend/getRepo.py`
#### Class: `RepoFile`
*   **Summary:** The RepoFile class serves as a structured representation of a single file found within a specific Git commit tree. Its primary design principle is lazy loading, meaning file data (Git Blob, content, and size) is only fetched from the repository when accessed via its properties, optimizing resource usage. It encapsulates file metadata and provides methods for analysis, such as word counting, and serialization into a dictionary format.
*   **Instantiation:** This class is currently not instantiated by any other component in the provided context.
*   **Dependencies:** The class depends on the git.Tree object for accessing file data and utilizes the os module for path manipulation.
*   **Constructor:**
    *   *Description:* The constructor initializes the RepoFile object by storing the file path and the associated Git Tree object from the commit. It also sets up internal attributes (`_blob`, `_content`, `_size`) to None, establishing the foundation for the class's lazy loading behavior.
    *   *Parameters:*
        - **file_path** (`str`): The path to the file within the repository.
        - **commit_tree** (`git.Tree`): The Git Tree object corresponding to the commit from which the file originates.
*   **Methods:**
    *   **`blob`**
        *   *Signature:* `def blob(self)`
        *   *Description:* This method is a property that handles the lazy loading of the underlying Git Blob object. If the blob has not yet been loaded, it attempts to retrieve it from the stored commit tree using the file path. If the file path is invalid or the file is missing from the tree, a FileNotFoundError is raised, ensuring data integrity.
        *   *Parameters:* None
        *   *Returns:*
            - **blob** (`git.Blob`): The Git Blob object representing the file content and metadata.
    *   **`content`**
        *   *Signature:* `def content(self)`
        *   *Description:* This property lazily loads and returns the decoded textual content of the file. If the content is not cached, it accesses the underlying Git Blob, reads the raw data stream, and decodes it using UTF-8, ignoring potential errors. This ensures efficient retrieval and caching of the file's text.
        *   *Parameters:* None
        *   *Returns:*
            - **content** (`str`): The decoded content of the file as a string.
    *   **`size`**
        *   *Signature:* `def size(self)`
        *   *Description:* This property retrieves the size of the file in bytes, utilizing the lazy loading pattern. It checks the internal cache and, if necessary, accesses the Git Blob object to retrieve its size attribute. The size is then cached for subsequent, faster access.
        *   *Parameters:* None
        *   *Returns:*
            - **size** (`int`): The size of the file in bytes.
    *   **`analyze_word_count`**
        *   *Signature:* `def analyze_word_count(self)`
        *   *Description:* This method performs a simple analysis by calculating the total number of words in the file content. It accesses the content property (triggering lazy loading if needed), splits the resulting string by whitespace, and returns the count of resulting tokens.
        *   *Parameters:* None
        *   *Returns:*
            - **word_count** (`int`): The total count of words in the file content.
    *   **`__repr__`**
        *   *Signature:* `def __repr__(self)`
        *   *Description:* This special method provides a useful string representation of the RepoFile object, primarily for debugging and logging. The output format clearly indicates the object type and includes the stored file path.
        *   *Parameters:* None
        *   *Returns:*
            - **representation** (`str`): A string representation of the object, including its file path.
    *   **`to_dict`**
        *   *Signature:* `def to_dict(self, include_content)`
        *   *Description:* This method serializes the file object into a dictionary, providing essential metadata such as path, name, size, and type. It uses the lazy-loaded size property and extracts the file name using the os module. The inclusion of the full file content is optional, controlled by the 'include_content' parameter.
        *   *Parameters:*
            - **include_content** (`bool`): If True, the decoded file content is included in the output dictionary. Defaults to False.
        *   *Returns:*
            - **data** (`dict`): A dictionary containing the file's metadata and optionally its content.
#### Class: `GitRepository`
*   **Summary:** The GitRepository class manages the lifecycle of a remote Git repository. Upon initialization, it clones the specified repository URL into a temporary directory, making the repository content accessible. It provides functionality to retrieve all files as structured RepoFile objects and to generate a hierarchical file tree representation. Furthermore, it implements the context manager protocol (__enter__ and __exit__) to ensure that the temporary directory is reliably cleaned up upon exiting the context block or if an error occurs during cloning.
*   **Instantiation:** The class is not explicitly instantiated by any other component listed in the context.
*   **Dependencies:** The class relies on external libraries and functions such as tempfile.mkdtemp, git.Repo.clone_from, git.GitCommandError, and logging.info.
*   **Constructor:**
    *   *Description:* The constructor initializes the GitRepository object by setting the repository URL and creating a temporary directory. It then attempts to clone the repository into this location. If successful, it stores the repository object, the latest commit, and the commit tree. If cloning fails due to a GitCommandError, it calls self.close() for cleanup and raises a RuntimeError.
    *   *Parameters:*
        - **repo_url** (`string`): The URL of the Git repository to be cloned.
*   **Methods:**
    *   **`get_all_files`**
        *   *Signature:* `def get_all_files(self)`
        *   *Description:* This method retrieves a list of all file paths tracked by the Git repository using the underlying git command ls-files. It processes these paths and instantiates a RepoFile object for each path, associating it with the repository's commit tree. The resulting list of RepoFile instances is stored internally in self.files and then returned for external use.
        *   *Parameters:* None
        *   *Returns:*
            - **files** (`list[RepoFile]`): A list of RepoFile instances representing all files in the repository.
    *   **`close`**
        *   *Signature:* `def close(self)`
        *   *Description:* The close method is responsible for cleaning up resources associated with the repository. It checks if the temporary directory (self.temp_dir) exists. If it does, it prints a message indicating the directory is being deleted and sets self.temp_dir to None, although the actual directory removal via shutil.rmtree is commented out in the source code.
        *   *Parameters:* None
        *   *Returns:* None
    *   **`__enter__`**
        *   *Signature:* `def __enter__(self)`
        *   *Description:* This method implements the context manager protocol's entry point. When the GitRepository object is used in a 'with' statement, this method is executed and returns the instance itself, allowing access to the object within the context block.
        *   *Parameters:* None
        *   *Returns:*
            - **self** (`GitRepository`): Returns the instance of the repository object.
    *   **`__exit__`**
        *   *Signature:* `def __exit__(self, exc_type, exc_val, exc_tb)`
        *   *Description:* This method implements the context manager protocol's exit point. It is automatically called when execution leaves the 'with' block, regardless of whether an exception occurred. Its primary function is to ensure cleanup by calling the self.close() method.
        *   *Parameters:*
            - **exc_type** (`Type`): The type of exception raised, or None if no exception occurred.
            - **exc_val** (`Exception`): The exception value, or None.
            - **exc_tb** (`Traceback`): The traceback object, or None.
        *   *Returns:* None
    *   **`get_file_tree`**
        *   *Signature:* `def get_file_tree(self, include_content)`
        *   *Description:* This method generates a hierarchical tree structure representing the files and directories within the repository. If the file list is not yet populated, it first calls get_all_files(). It then iterates through the file objects, splitting their paths to build a nested dictionary structure where directories are created dynamically. Files are added at the deepest level using file_obj.to_dict(), optionally including file content based on the input flag.
        *   *Parameters:*
            - **include_content** (`bool`): If True, the file content is included in the dictionary representation of the file object. Defaults to False.
        *   *Returns:*
            - **tree** (`dict`): A nested dictionary representing the file structure of the repository, starting with a 'root' directory.
---
### File: `backend/main.py`
#### Function: `main_workflow`
*   **Signature:** `def main_workflow()`
*   **Description:** The main_workflow function orchestrates the entire repository analysis pipeline. It starts by extracting a Git repository URL from a hardcoded input string using regular expressions, raising a ValueError if no URL is found. It then utilizes the GitRepository context manager to clone the repository and retrieve its file list. Subsequent steps involve extracting basic project information, generating a file tree, and performing detailed AST analysis using ASTAnalyzer. The function processes the resulting AST schema to construct structured inputs for Helper LLMs, which are responsible for generating documentation for individual functions and classes. Finally, it compiles all analysis data and passes it to the MainLLM for synthesis into a final report, which is saved to a timestamped Markdown file.
*   **Parameters:** None
*   **Returns:** None
*   **Usage:**
    - **Calls:** This function calls numerous constructors and methods, including re.search, logging methods, GitRepository (and its methods like get_all_files and get_file_tree), ProjektInfoExtractor.extrahiere_info, ASTAnalyzer.analyze_repository, various schema input constructors (FunctionContextInput, ClassAnalysisInput), LLMHelper methods (generate_for_functions, generate_for_classes), MainLLM.call_llm, json.dumps, os.makedirs, os.path.join, and datetime.now.
    - **Called By:** This function is primarily used by the main execution block of the script.
---
### File: `schemas/types.py`
#### Class: `ParameterDescription`
*   **Summary:** The ParameterDescription class is a Pydantic data model used to standardize the representation of a single function parameter. It inherits from BaseModel to provide strict type checking and serialization capabilities. This model ensures that every parameter is documented with its name, its data type, and a descriptive explanation of its role. It acts as a fundamental structural component for documenting function signatures across the system.
*   **Instantiation:** This class is not explicitly instantiated in the provided context but is designed to be used as a structural component within larger Pydantic models, typically in lists representing function signatures.
*   **Dependencies:** This class depends on pydantic.BaseModel to provide core data validation, serialization, and structural capabilities.
*   **Constructor:**
    *   *Description:* The constructor is implicitly generated by Pydantic's BaseModel, which handles initialization and validation for the defined fields. It accepts keyword arguments corresponding to the attributes name, type, and description, ensuring they are all valid strings before instantiation.
    *   *Parameters:*
        - **name** (`str`): The name of the parameter.
        - **type** (`str`): The inferred or specified type of the parameter.
        - **description** (`str`): A detailed explanation of the parameter's role and usage.
*   **Methods:** None
#### Class: `ReturnDescription`
*   **Summary:** The ReturnDescription class is a Pydantic data model designed to standardize the description of a function's return value. It serves as a schema definition, ensuring that any description of a returned object includes its name, its data type, and a textual explanation of its purpose. This structure is typically used within larger analysis models to document function signatures comprehensively.
*   **Instantiation:** The instantiation points for this class are not provided in the current context.
*   **Dependencies:** This class depends on Pydantic's BaseModel for its data validation and structure definition capabilities.
*   **Constructor:**
    *   *Description:* The class uses the Pydantic BaseModel constructor, which automatically initializes the instance attributes based on the provided keyword arguments for `name`, `type`, and `description`. Pydantic handles validation to ensure all three fields are provided as strings upon instantiation.
    *   *Parameters:*
        - **name** (`str`): The name of the returned value or object.
        - **type** (`str`): The data type of the returned value (e.g., 'str', 'int', 'List[str]').
        - **description** (`str`): A textual explanation of what the returned value represents.
*   **Methods:** None
#### Class: `UsageContext`
*   **Summary:** The UsageContext class is a simple Pydantic data structure used to standardize the representation of a function's interaction context within a larger system. It inherits from BaseModel to enforce type checking and validation for its two required string attributes: calls, which documents outbound dependencies, and called_by, which documents inbound usage. This model is essential for generating structured documentation regarding function relationships.
*   **Instantiation:** The instantiation points for this class are not specified in the provided context.
*   **Dependencies:** This class inherits functionality from pydantic.BaseModel but has no other explicit functional dependencies listed in the provided context.
*   **Constructor:**
    *   *Description:* As a Pydantic BaseModel, the class is initialized by accepting keyword arguments corresponding to its defined fields, calls and called_by. This sets the fundamental context strings upon object creation.
    *   *Parameters:*
        - **calls** (`str`): A summary string detailing the functions, methods, or classes that the described entity calls.
        - **called_by** (`str`): A summary string detailing the functions or methods that call the described entity.
*   **Methods:** None
#### Class: `FunctionDescription`
*   **Summary:** The FunctionDescription class is a Pydantic data model designed to store the comprehensive, structured analysis of a single Python function. It acts as a standardized container for metadata, including a high-level summary of the function's purpose, a detailed list of its input parameters, information about its return values, and context regarding its usage within the larger codebase. This model ensures that function analysis data is consistently formatted and machine-readable.
*   **Instantiation:** The instantiation points for this data model are not provided in the current context, but it is typically created by analysis components that structure function metadata.
*   **Dependencies:** This class relies on external types like List, Optional, str, ParameterDescription, ReturnDescription, and UsageContext, and inherits core functionality from pydantic.BaseModel.
*   **Constructor:**
    *   *Description:* This class inherits its constructor from pydantic.BaseModel. It is initialized by providing values for its four primary fields: overall (a string summary), parameters (a list of parameter descriptions), returns (an optional list of return descriptions), and usage_context (a context object describing its usage).
    *   *Parameters:*
        - **overall** (`str`): A high-level summary of the function's purpose and implementation.
        - **parameters** (`List[ParameterDescription]`): A list detailing all input parameters of the function.
        - **returns** (`Optional[List[ReturnDescription]]`): An optional list detailing the return values of the function.
        - **usage_context** (`UsageContext`): An object describing where the function is called and what it calls.
*   **Methods:** None
#### Class: `FunctionAnalysis`
*   **Summary:** The FunctionAnalysis class is a Pydantic data model designed to structure the comprehensive analysis of a single Python function. It serves as the main schema for representing function-level data within a larger documentation or analysis system. The model strictly defines three fields: the function's name, a nested object containing the detailed analysis, and an optional field for capturing analysis errors.
*   **Instantiation:** The instantiation context for this class is not provided in the current analysis context.
*   **Dependencies:** This class relies on the `BaseModel` from the Pydantic library for data validation and structure definition.
*   **Constructor:**
    *   *Description:* As a Pydantic BaseModel, the constructor is implicitly generated. It initializes the instance by validating and assigning the required `identifier` (string) and `description` (FunctionDescription object), and setting the optional `error` field, which defaults to None.
    *   *Parameters:*
        - **identifier** (`str`): The unique name of the function being analyzed.
        - **description** (`FunctionDescription`): A nested model containing the detailed analysis of the function's purpose, parameters, returns, and usage context.
        - **error** (`Optional[str]`): An optional field used to store any error messages encountered during the function analysis process, defaulting to None.
*   **Methods:** None
#### Class: `ConstructorDescription`
*   **Summary:** The ConstructorDescription class is a Pydantic data model used to structure the analysis of a Python class's "__init__" method. It serves as a container for the constructor's summary description and a detailed, structured list of all parameters it accepts. This model is fundamental for documenting the initialization process of other classes within the system.
*   **Instantiation:** Instantiation points were not provided in the input context.
*   **Dependencies:** This class depends on Pydantic's BaseModel for data validation and structure, and utilizes List from the typing module, along with the external type ParameterDescription.
*   **Constructor:**
    *   *Description:* The constructor is implicitly generated by Pydantic's BaseModel, requiring values for 'description' (a string summary) and 'parameters' (a list of ParameterDescription objects) upon instantiation.
    *   *Parameters:*
        - **description** (`str`): A textual summary of how the class is initialized.
        - **parameters** (`List[ParameterDescription]`): A list detailing each parameter accepted by the constructor.
*   **Methods:** None
#### Class: `ClassContext`
*   **Summary:** The ClassContext is a Pydantic data model designed to encapsulate metadata about the usage context of another class. It serves as a strict schema, ensuring that context information is consistently structured with two required string fields: 'dependencies' and 'instantiated_by'. This model is typically used within a larger analysis system to provide machine-readable summaries of a class's external relationships and instantiation points.
*   **Instantiation:** The context does not specify where this class is instantiated.
*   **Dependencies:** No explicit external dependencies were provided in the context for this class.
*   **Constructor:**
    *   *Description:* The constructor is implicitly defined by Pydantic's BaseModel. It initializes the instance by accepting and validating the 'dependencies' and 'instantiated_by' string fields, ensuring they conform to the defined schema.
    *   *Parameters:*
        - **dependencies** (`str`): A string summarizing the external dependencies required by the analyzed class.
        - **instantiated_by** (`str`): A string summarizing the locations or entities responsible for creating instances of the analyzed class.
*   **Methods:** None
#### Class: `ClassDescription`
*   **Summary:** The ClassDescription class is a Pydantic data model used to standardize the output structure for a comprehensive analysis of a Python class. It acts as a container for all synthesized information, ensuring consistency in documentation generation. This model defines four key fields: a general summary, details about the constructor, a list of analyses for all methods, and contextual usage information.
*   **Instantiation:** The instantiation points for this data model were not provided in the input context.
*   **Dependencies:** This class relies on external types like BaseModel, ConstructorDescription, List, FunctionAnalysis, and ClassContext for its structure, but no specific functional dependencies were listed in the input context.
*   **Constructor:**
    *   *Description:* As a Pydantic BaseModel, the class is initialized by accepting keyword arguments corresponding to its defined fields. It requires values for the overall summary, the constructor description, the list of method analyses, and the class's usage context to be instantiated.
    *   *Parameters:*
        - **overall** (`str`): A high-level summary string describing the class being analyzed.
        - **init_method** (`ConstructorDescription`): A structured description of the analyzed class's constructor (__init__).
        - **methods** (`List[FunctionAnalysis]`): A list containing the detailed analysis of every method within the analyzed class.
        - **usage_context** (`ClassContext`): Contextual information regarding the analyzed class's dependencies and instantiation points.
*   **Methods:** None
#### Class: `ClassAnalysis`
*   **Summary:** The ClassAnalysis class serves as the top-level container for structured analysis data of a single Python class. It is designed as a Pydantic model, ensuring that the analysis output is strictly validated against a defined schema. It encapsulates the class's name (identifier), the detailed analysis content (description), and an optional field for reporting analysis errors (error). This structure is fundamental for machine-readable documentation generation systems.
*   **Instantiation:** The instantiation context for this class is not provided in the input data.
*   **Dependencies:** This class does not list any external functional dependencies.
*   **Constructor:**
    *   *Description:* The class is initialized implicitly by the Pydantic BaseModel, requiring values for its core fields: identifier (a string), description (an instance of ClassDescription), and an optional error string, which defaults to None.
    *   *Parameters:*
        - **identifier** (`str`): The name of the class being analyzed.
        - **description** (`ClassDescription`): A detailed analysis object containing the class structure, methods, and context.
        - **error** (`Optional[str]`): An optional field used to report errors during the analysis process, defaulting to None.
*   **Methods:** None
#### Class: `FunctionContextInput`
*   **Summary:** The FunctionContextInput class serves as a Pydantic data structure designed to encapsulate the usage context of a specific function. It defines two primary fields: calls, which lists external entities invoked by the function, and called_by, which lists entities that invoke the function. This structure is crucial for providing machine-readable, validated context necessary for comprehensive function analysis.
*   **Instantiation:** The instantiation points for this class were not provided in the context.
*   **Dependencies:** This class has no explicit external functional dependencies beyond its base class inheritance from Pydantic's BaseModel.
*   **Constructor:**
    *   *Description:* As a Pydantic BaseModel, the constructor implicitly handles the initialization of the calls and called_by attributes, ensuring they conform to the specified list of strings type during instantiation.
    *   *Parameters:*
        - **calls** (`List[str]`): A list of identifiers (functions, methods, or classes) that the function under analysis calls.
        - **called_by** (`List[str]`): A list of identifiers (functions or methods) that call the function under analysis.
*   **Methods:** None
#### Class: `FunctionAnalysisInput`
*   **Summary:** The FunctionAnalysisInput class is a Pydantic model designed to strictly define the required data structure for initiating a function analysis task. It serves as a schema validator and data container, ensuring that all necessary components
The class defines mandatory fields including the analysis mode (fixed literal), the function identifier, its raw source code, a list of imports, and a nested context object (FunctionContextInput).
*   **Instantiation:** The instantiation points for this class are not specified in the provided context, but it is likely instantiated when processing inputs for function analysis.
*   **Dependencies:** The class does not appear to have direct external functional dependencies based on the provided context, relying primarily on Pydantic for structure.
*   **Constructor:**
    *   *Description:* Since this class inherits from Pydantic's BaseModel, it utilizes an implicitly generated constructor. This constructor handles the validation and assignment of the defined attributes (mode, identifier, source_code, imports, and context) based on the provided input data.
    *   *Parameters:* None
*   **Methods:** None
#### Class: `MethodContextInput`
*   **Summary:** The MethodContextInput class is a Pydantic data model designed to structure and hold contextual information about a single method within a larger codebase analysis. It captures crucial relational data such as the method's name, its dependencies (calls), its usage points (called_by), its signature (args), and its documentation (docstring). This model serves as a standardized input format for method analysis components.
*   **Instantiation:** The input context does not specify explicit instantiation points, suggesting it is likely instantiated internally within a data processing pipeline or by Pydantic's parsing mechanisms.
*   **Dependencies:** This class relies on pydantic.BaseModel for data structure and validation, and uses typing.List and typing.Optional to define the types of its internal fields.
*   **Constructor:**
    *   *Description:* As a Pydantic BaseModel, the constructor is implicitly generated. It accepts keyword arguments corresponding to the defined fields to initialize the structured context for a method, ensuring type validation upon instantiation.
    *   *Parameters:*
        - **identifier** (`str`): The unique name or identifier of the method being analyzed.
        - **calls** (`List[str]`): A list of identifiers for other functions, methods, or classes that this method calls.
        - **called_by** (`List[str]`): A list of identifiers for other functions or methods that call this method.
        - **args** (`List[str]`): A list of the parameter names defined in the method's signature.
        - **docstring** (`Optional[str]`): The raw docstring content of the method, which may be null if no docstring is present.
*   **Methods:** None
#### Class: `ClassContextInput`
*   **Summary:** The ClassContextInput is a Pydantic data model used to structure and validate the contextual information necessary for a comprehensive class analysis. It defines three primary fields: a list of external dependencies, a list of locations where the class is instantiated, and a list of contextual inputs specific to the class's methods. This model ensures that all required context is present and correctly typed before analysis proceeds.
*   **Instantiation:** The instantiation points for this class are unknown or not provided in the current context.
*   **Dependencies:** This class does not appear to rely on any external components based on the provided context.
*   **Constructor:**
    *   *Description:* The constructor is automatically generated by Pydantic's BaseModel, accepting and validating the three defined fields: dependencies, instantiated_by, and method_context. These fields are initialized as instance attributes upon creation.
    *   *Parameters:*
        - **dependencies** (`List[str]`): A list of external dependencies required by the class being analyzed.
        - **instantiated_by** (`List[str]`): A list of locations where the class being analyzed is instantiated.
        - **method_context** (`List[MethodContextInput]`): A list containing contextual information for the methods within the class being analyzed.
*   **Methods:** None
#### Class: `ClassAnalysisInput`
*   **Summary:** The ClassAnalysisInput class is a Pydantic data model designed to strictly define the required input structure for initiating a class analysis process. It acts as a standardized contract, ensuring that the downstream analysis system receives all necessary components, including the class identifier, its raw source code, associated imports, and detailed contextual information, all validated against specific types and literals.
*   **Instantiation:** The instantiation points for this class are not provided in the current context, but it is typically instantiated by the system orchestrating the code analysis pipeline.
*   **Dependencies:** This class primarily depends on pydantic.BaseModel for its structure and standard Python typing utilities (Literal, List).
*   **Constructor:**
    *   *Description:* As a Pydantic BaseModel, the class is initialized by accepting keyword arguments corresponding to its defined fields. It sets up five mandatory attributes that encapsulate the data needed for a complete class analysis.
    *   *Parameters:*
        - **mode** (`Literal["class_analysis"]`): Specifies the operational mode, which must be fixed to the string 'class_analysis'.
        - **identifier** (`str`): The name of the class being analyzed.
        - **source_code** (`str`): The raw source code string of the entire class definition.
        - **imports** (`List[str]`): A list of import statements associated with the source file containing the class.
        - **context** (`ClassContextInput`): A nested object containing contextual information necessary for the analysis, such as dependencies and call graph data.
*   **Methods:** None
---